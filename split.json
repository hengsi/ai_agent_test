{
    "version": 21,
    "org": {
        "nr": "E193-03",
        "slug": "e193-03",
        "name": "Research Unit Virtual and Augmented Reality",
        "picture": "https://informatics.tuwien.ac.at/orgs/e193-03/picture",
        "picture_caption": null,
        "picture_credits": null,
        "website": "https://www.vr.tuwien.ac.at",
        "email": null,
        "phone": "+43-1-58801-18802",
        "address": {
            "line": "1040 Wien, Favoritenstrasse 9-11/193-03, Stiege 2, 4.Stock",
            "maps": "AT 1040 Wien Favoritenstrasse 9-11",
            "zip": "1040",
            "city": "Wien",
            "street": "Favoritenstrasse 9-11/193-03, Stiege 2, 4.Stock",
            "country": "AT",
            "care_of": null,
            "latitude": 48.19521,
            "longitude": 16.369568,
            "location": "Favoritenstrasse 9-11"
        },
        "description": "The scientific areas of our unit are Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR). We conduct basic and application oriented research in a wide range of fields associated with VR/AR.\n",
        "details": "AR, VR and MR technologies and systems are more and more used in different applications:  industry (virtual prototyping), entertainment (videogames), art (cultural heritage), medicine (rehabilitation) and education (training). The objective is to provide the most immersive experience to users when they interact with a Virtual Environment. Our motivation is to improve AR/VR/MR systems by considering the whole interaction loop: we consider multiple user inputs to provide efficient interactions, but also consider multiple sensory feedback outputs to improve users’ immersion in VEs.\n\nWe focus in particular on the following research areas:\n\n- 3D interaction in virtual environments (locomotion, haptic feedback, avatars)\n- 3D large-scale collaborative systems with multiple users: AR/VR/MR systems for training, education, rehabilitation and other application areas\n- Rendering and viewing in MR systems, including tracking, motion capture, 3D reconstruction of VEs, high quality rendering for AR etc.\n- Human perception in virtual environments e.g. spatial understanding, human motion, embodiment and agency.\n\nIn the past we conducted projects on industrial high precision tracking solutions, physical rehabilitation, safety of fire fighters, high school science education in VR, prosthetics, and many more. Please see the list of current and past projects. In addition to basic research projects, we focus on innovative applications of VR/AR technology which are beneficial to society.\n",
        "keywords": [
            "Augmented Reality",
            "Media processing",
            "Virtual Reality",
            "Motion and Tracking",
            "Multimedia",
            "Image forensics",
            "Human-Robot Interaction",
            "Haptics",
            "Education in Mixed Reality",
            "Visual Computing"
        ],
        "head": {
            "tid": "46406",
            "first_name": "Hannes",
            "last_name": "Kaufmann"
        },
        "research": []
    },
    "people": [
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "126596",
            "old_tids": [],
            "last_name": "Bischof",
            "first_name": "Horst",
            "slug": "horst-bischof",
            "name_prefix": "Ao.Univ.Prof. Dipl.-Ing. Dr.techn.",
            "name_suffix": null,
            "name_prefix_short": "Ao.Univ.Prof. DI Dr.",
            "name_suffix_short": null,
            "picture": null,
            "email": "horst.bischof@tuwien.ac.at",
            "phone": null,
            "room": null,
            "orcid": null,
            "websites": [],
            "address": null,
            "about": null,
            "keywords": [],
            "roles": [
                "Affiliated"
            ],
            "foci": [],
            "research": [],
            "courses": [],
            "projects": [],
            "publications": [
                "13509",
                "159926",
                "178858",
                "181466",
                "187961",
                "189997",
                "190844",
                "192169",
                "192220",
                "195358",
                "25564",
                "36341",
                "36452",
                "36454",
                "54614",
                "54615",
                "54640",
                "54641",
                "54939",
                "54944",
                "54946",
                "54951",
                "54953",
                "54954",
                "54954",
                "54955",
                "54955",
                "54956",
                "54973",
                "54976",
                "54987",
                "54987",
                "54988",
                "54990",
                "55004",
                "55215",
                "55217",
                "55218",
                "55243",
                "55244",
                "55245",
                "55696",
                "55697",
                "55698",
                "56303",
                "56670",
                "60190",
                "9606"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "368723",
            "old_tids": [
                "374904"
            ],
            "last_name": "Bosco",
            "first_name": "Matteo",
            "slug": "matteo-bosco",
            "name_prefix": "Projektass. Dott.mag.",
            "name_suffix": null,
            "name_prefix_short": "Mag.",
            "name_suffix_short": null,
            "picture": "https://informatics.tuwien.ac.at/people/matteo-bosco/picture",
            "email": "matteo.bosco@tuwien.ac.at",
            "phone": "+43-1-58801-193321",
            "room": "HE0406",
            "orcid": null,
            "websites": [
                {
                    "url": "https://www.vr.tuwien.ac.at/people/matteo-bosco/",
                    "title": "Virtual and Augmented Reality Website"
                }
            ],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": null,
            "keywords": [],
            "roles": [
                "PreDoc Researcher"
            ],
            "foci": [],
            "research": [],
            "courses": [],
            "projects": [],
            "publications": [
                "199080"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "159676",
            "old_tids": [],
            "last_name": "Breiteneder",
            "first_name": "Christian",
            "slug": "christian-breiteneder",
            "name_prefix": "Univ.-Prof. i.R. Dipl.-Ing. Dr.techn.",
            "name_suffix": null,
            "name_prefix_short": "Univ.Prof.i.R. DI Dr.",
            "name_suffix_short": null,
            "picture": "https://informatics.tuwien.ac.at/people/christian-breiteneder/picture",
            "email": "christian.breiteneder@tuwien.ac.at",
            "phone": "+43-1-58801-18850",
            "room": null,
            "orcid": null,
            "websites": [],
            "address": null,
            "about": "Multimedia, Multimedia Information Retrieval, Pattern Recognition, Mixed Reality (Virtual and Augmented Reality)",
            "keywords": [
                "Visual Computing",
                "Augmented Reality",
                "Media Processing",
                "Multimedia",
                "Content-based Retrieval"
            ],
            "roles": [
                "Retired Professor"
            ],
            "foci": [
                {
                    "name": "Visual Computing and Human-Centered Technology",
                    "percent": 100
                }
            ],
            "research": [],
            "courses": [],
            "projects": [
                "1559899",
                "220239",
                "4037",
                "4151",
                "5427",
                "6289",
                "9257"
            ],
            "publications": [
                "10046",
                "10987",
                "126493",
                "126964",
                "126978",
                "126982",
                "127798",
                "128036",
                "13594",
                "13844",
                "14098",
                "14389",
                "144744",
                "14735",
                "154967",
                "157790",
                "159926",
                "166745",
                "166815",
                "169538",
                "169692",
                "170929",
                "170938",
                "172966",
                "175252",
                "188428",
                "194813",
                "195261",
                "2017",
                "26917",
                "2895",
                "29665",
                "2971",
                "30100",
                "32215",
                "3305",
                "42083",
                "54722",
                "55841",
                "56570",
                "56679",
                "56682",
                "56931",
                "56944",
                "57301",
                "57367",
                "57876",
                "57940",
                "57943",
                "58251",
                "58583",
                "58584",
                "58917",
                "60190",
                "60229",
                "60308",
                "60312",
                "60804",
                "6584",
                "89209",
                "89645",
                "91096",
                "91097",
                "930",
                "9643"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "359272",
            "old_tids": [],
            "last_name": "Brument",
            "first_name": "Hugo",
            "slug": "hugo-brument",
            "name_prefix": "Univ.Ass.",
            "name_suffix": "PhD",
            "name_prefix_short": null,
            "name_suffix_short": "PhD",
            "picture": "https://informatics.tuwien.ac.at/people/hugo-brument/picture",
            "email": "hugo.brument@tuwien.ac.at",
            "phone": "+43-1-58801-18893",
            "room": "HE0402",
            "orcid": "0000-0001-6595-5922",
            "websites": [
                {
                    "url": "https://www.vr.tuwien.ac.at/people/hugo-brument/",
                    "title": "https://www.vr.tuwien.ac.at/people/hugo-brument/"
                }
            ],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": null,
            "keywords": [],
            "roles": [
                "PostDoc Researcher"
            ],
            "foci": [],
            "research": [],
            "courses": [
                "188456-2024S",
                "188460-2023W",
                "188520-2024S",
                "188913-2023W",
                "193021-2023W",
                "193021-2024S",
                "193022-2023W",
                "193022-2024S",
                "193023-2023W",
                "193023-2024S",
                "193024-2023W",
                "193024-2024S",
                "193116-2023W",
                "193116-2024S",
                "193118-2023W",
                "193118-2024S",
                "193148-2024S"
            ],
            "projects": [],
            "publications": [
                "196991",
                "199077"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "357410",
            "old_tids": [],
            "last_name": "De Pace",
            "first_name": "Francesco",
            "slug": "francesco-pace",
            "name_prefix": "Projektass.(FWF) Dr.",
            "name_suffix": null,
            "name_prefix_short": "Dr.",
            "name_suffix_short": null,
            "picture": "https://informatics.tuwien.ac.at/people/francesco-pace/picture",
            "email": "francesco.pace@tuwien.ac.at",
            "phone": "+43-1-58801-18857",
            "room": "HE0402",
            "orcid": "0000-0001-8772-4105",
            "websites": [],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": null,
            "keywords": [],
            "roles": [
                "PostDoc Researcher"
            ],
            "foci": [],
            "research": [],
            "courses": [
                "188456-2024S",
                "193023-2023W"
            ],
            "projects": [],
            "publications": [
                "147405",
                "147412",
                "147415",
                "199002",
                "199077",
                "199080",
                "206023",
                "206024"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "97117",
            "old_tids": [],
            "last_name": "Eidenberger",
            "first_name": "Horst",
            "slug": "horst-eidenberger",
            "name_prefix": "Ao.Univ.Prof. Ing. Mag.rer.soc.oec. Dr.rer.soc.oec.",
            "name_suffix": null,
            "name_prefix_short": "Ao.Univ.Prof. Mag. Dr.",
            "name_suffix_short": null,
            "picture": "https://informatics.tuwien.ac.at/people/horst-eidenberger/picture",
            "email": "horst.eidenberger@tuwien.ac.at",
            "phone": "+43-1-58801-18853",
            "room": "HG0401",
            "orcid": null,
            "websites": [
                {
                    "url": "https://www.vr.tuwien.ac.at/topics/",
                    "title": "Themen für Studierendenarbeiten / Topics for Student Projects"
                },
                {
                    "url": "http://www.vreeclimber.at/student_projects/handbook-of-multimedia-information-retrieval-eidenberger-2012.pdf",
                    "title": "Handbook of Multimedia Information Retrieval (2013)"
                },
                {
                    "url": "https://www.vreeclimber.at/",
                    "title": "Virtual Reality Research Project Vreeclimber"
                },
                {
                    "url": "https://www.gut8en.info/",
                    "title": "Allgemein beeideter und gerichtlich zertifizierter Sachverständiger für Informatik, Signalverarbeitung, angewandte Statistik"
                },
                {
                    "url": "https://www.vr.tuwien.ac.at/people/horst-eidenberger/",
                    "title": "Research Website at the Virtual Reality group (193-03)"
                }
            ],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": "<p>Computational perception of audiovisual media (e.g. face recognition, speech recognition, speaker recognition), forensic application of biometric media understanding technology, development of methods and prcesses in the area of big data analysis (summarization, statistical filtering, machine learning &amp; classification including deep learning), development of multimedia and mixed reality applications (in particular, of hybrid systems), and analysis of software quality and usability of large software projects and web-based systems.</p>",
            "keywords": [
                "Information Retrieval",
                "Media processing",
                "Video Analysis",
                "Image forensics",
                "Virtual Reality",
                "Audio Retrieval",
                "Software Quality Assurance",
                "Machine Learning",
                "Multimedia"
            ],
            "roles": [
                "Associate Professor"
            ],
            "foci": [
                {
                    "name": "Visual Computing and Human-Centered Technology",
                    "percent": 50
                },
                {
                    "name": "Information Systems Engineering",
                    "percent": 30
                },
                {
                    "name": "Logic and Computation",
                    "percent": 20
                }
            ],
            "research": [],
            "courses": [
                "183293-2023W",
                "188081-2024S",
                "188082-2024S",
                "188456-2024S",
                "188498-2023W",
                "188499-2024S",
                "188501-2023W",
                "188502-2024S",
                "188519-2024S",
                "188520-2024S",
                "188981-2023W",
                "188981-2024S",
                "193021-2023W",
                "193021-2024S",
                "193022-2023W",
                "193022-2024S",
                "193023-2023W",
                "193023-2024S",
                "193024-2023W",
                "193024-2024S",
                "193083-2023W",
                "193116-2023W",
                "193116-2024S",
                "193118-2023W",
                "193118-2024S",
                "193119-2023W",
                "193119-2024S",
                "193120-2023W",
                "193144-2024S",
                "193145-2024S",
                "194077-2023W",
                "194104-2024S",
                "194122-2023W"
            ],
            "projects": [
                "1515667",
                "2543",
                "606384",
                "8002"
            ],
            "publications": [
                "10452",
                "1204",
                "12809",
                "12810",
                "12936",
                "13035",
                "1332",
                "13693",
                "13747",
                "14389",
                "151584",
                "151939",
                "16143",
                "162803",
                "166781",
                "168483",
                "168580",
                "168858",
                "169364",
                "17445",
                "1775",
                "177687",
                "177706",
                "178929",
                "178930",
                "178931",
                "178932",
                "181391",
                "181392",
                "181393",
                "181439",
                "181468",
                "181469",
                "186167",
                "187779",
                "188683",
                "189040",
                "19006",
                "19007",
                "194813",
                "19635",
                "198350",
                "20143",
                "2056",
                "208308",
                "253",
                "25545",
                "26743",
                "26805",
                "26806",
                "28679",
                "4067",
                "4192",
                "4323",
                "4516",
                "4788",
                "5115",
                "5204",
                "5350",
                "54623",
                "54733",
                "5498",
                "55026",
                "55027",
                "55558",
                "55559",
                "56432",
                "58275",
                "59218",
                "59522",
                "59892",
                "60421",
                "60424",
                "61050",
                "61051",
                "61052",
                "61126",
                "61466",
                "6561",
                "6573",
                "7101",
                "7149",
                "7269",
                "7421",
                "7523",
                "8059",
                "8082",
                "8126",
                "8471",
                "8475",
                "85662",
                "8614",
                "89057",
                "89058",
                "89059",
                "89060",
                "89061",
                "89062",
                "89209",
                "9167",
                "9622",
                "9985"
            ]
        },
        {
            "org_nrs": [
                "E193-03",
                "E193-04"
            ],
            "tid": "90409",
            "old_tids": [],
            "last_name": "Fromm",
            "first_name": "Johannes",
            "slug": "johannes-fromm",
            "name_prefix": "ARat",
            "name_suffix": null,
            "name_prefix_short": null,
            "name_suffix_short": null,
            "picture": null,
            "email": "johannes.fromm@tuwien.ac.at",
            "phone": "+43-1-58801-18846",
            "room": "HG0405",
            "orcid": null,
            "websites": [],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": null,
            "keywords": [],
            "roles": [
                "IT Services"
            ],
            "foci": [],
            "research": [],
            "courses": [],
            "projects": [],
            "publications": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "370584",
            "old_tids": [
                "370910"
            ],
            "last_name": "Ghazanfari",
            "first_name": "Mohammad",
            "slug": "mohammad-ghazanfari",
            "name_prefix": "Projektass.(FWF)",
            "name_suffix": "MSc",
            "name_prefix_short": null,
            "name_suffix_short": "MSc",
            "picture": "https://informatics.tuwien.ac.at/people/mohammad-ghazanfari/picture",
            "email": "mohammad.ghazanfari@tuwien.ac.at",
            "phone": "+43-1-58801-193322",
            "room": "HE0452",
            "orcid": null,
            "websites": [],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": null,
            "keywords": [],
            "roles": [
                "PreDoc Researcher"
            ],
            "foci": [],
            "research": [],
            "courses": [],
            "projects": [],
            "publications": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E194-06"
            ],
            "tid": "339163",
            "old_tids": [],
            "last_name": "Kacmaz",
            "first_name": "Ülkühan",
            "slug": "uelkuehan-kacmaz",
            "name_prefix": null,
            "name_suffix": null,
            "name_prefix_short": null,
            "name_suffix_short": null,
            "picture": null,
            "email": "uelkuehan.kacmaz@tuwien.ac.at",
            "phone": "+43-1-58801-18802",
            "room": "HC0413",
            "orcid": null,
            "websites": [],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 9",
                "maps": "AT 1040 Wien Favoritenstrasse 9",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 9",
                "country": "AT",
                "care_of": null,
                "latitude": 48.19521,
                "longitude": 16.369568,
                "location": "Favoritenstrasse 9"
            },
            "about": null,
            "keywords": [],
            "roles": [
                "Office Services"
            ],
            "foci": [],
            "research": [],
            "courses": [],
            "projects": [
                "2323188"
            ],
            "publications": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "231177",
            "old_tids": [],
            "last_name": "Kan",
            "first_name": "Peter",
            "slug": "peter-kan",
            "name_prefix": "Senior Scientist Mag. Dr.techn.",
            "name_suffix": null,
            "name_prefix_short": "Mag. Dr.",
            "name_suffix_short": null,
            "picture": "https://informatics.tuwien.ac.at/people/peter-kan/picture",
            "email": "peter.kan@tuwien.ac.at",
            "phone": "+43-1-58801-188645",
            "room": "HG0406",
            "orcid": "0000-0001-7437-9955",
            "websites": [
                {
                    "url": "https://www.vr.tuwien.ac.at/people/peter-kan/",
                    "title": "https://www.vr.tuwien.ac.at/people/peter-kan/"
                }
            ],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": "<p>Peter Kán is currently a Senior Scientist at TU Wien. He finished his doctoral studies at the same university. During his PhD studies, he conducted a research on the topic of High-Quality Real-Time Global Illumination in Augmented Reality. Peter is also responsible for management of <a href=\"https://www.vr.tuwien.ac.at/mrlab/\" target=\"_blank\" rel=\"nofollow\">Mixed Reality laboratory at TU Wien</a>.</p>\n<h3>Research Interests</h3>\n<ul>\n <li>Photorealistic rendering</li>\n <li>Augmented reality, Virtual reality</li>\n <li>Automatic 3D content generation</li>\n <li>Real-time rendering</li>\n <li>Deep learning</li>\n <li>Motion learning in VR/AR</li>\n <li>Embodied conversational agents</li>\n</ul>\n<p><br></p>",
            "keywords": [],
            "roles": [
                "Senior Scientist"
            ],
            "foci": [],
            "research": [],
            "courses": [
                "188913-2023W",
                "193021-2023W",
                "193021-2024S",
                "193022-2023W",
                "193022-2024S",
                "193023-2023W",
                "193023-2024S",
                "193024-2023W",
                "193024-2024S",
                "193116-2023W",
                "193118-2023W",
                "193118-2024S"
            ],
            "projects": [
                "2353000",
                "624243",
                "716533"
            ],
            "publications": [
                "1329",
                "142912",
                "142955",
                "142958",
                "1431",
                "143921",
                "145639",
                "145642",
                "145738",
                "150894",
                "186166",
                "198802",
                "200288",
                "201183",
                "2021",
                "23366",
                "3307",
                "43787",
                "43927",
                "57930",
                "57932",
                "58356",
                "58393",
                "58795",
                "59823",
                "59934",
                "59954",
                "60814",
                "61097",
                "61157",
                "61709",
                "62339",
                "62340",
                "62341",
                "62426",
                "66716",
                "66789",
                "722",
                "7745"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "46406",
            "old_tids": [],
            "last_name": "Kaufmann",
            "first_name": "Hannes",
            "slug": "hannes-kaufmann",
            "name_prefix": "Univ.Prof. Mag.rer.nat. Dr.techn.",
            "name_suffix": null,
            "name_prefix_short": "Univ.Prof. Mag. Dr.",
            "name_suffix_short": null,
            "picture": "https://informatics.tuwien.ac.at/people/hannes-kaufmann/picture",
            "email": "hannes.kaufmann@tuwien.ac.at",
            "phone": "+43-1-58801-18860",
            "room": "HC0415",
            "orcid": "0000-0002-0322-9869",
            "websites": [
                {
                    "url": "https://www.vr.tuwien.ac.at/people/hannes-kaufmann/",
                    "title": "VR Group Personal Webpage"
                }
            ],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 9",
                "maps": "AT 1040 Wien Favoritenstrasse 9",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 9",
                "country": "AT",
                "care_of": null,
                "latitude": 48.19521,
                "longitude": 16.369568,
                "location": "Favoritenstrasse 9"
            },
            "about": null,
            "keywords": [
                "Mobile and Ubiquitous Computing",
                "3D User Interface Design",
                "Education in Mixed Reality",
                "Virtual Reality",
                "Motion and Tracking",
                "Augmented Reality",
                "User Interface Design for Collaborative AR and Educational Applications"
            ],
            "roles": [
                "Head of Research Unit",
                "Full Professor"
            ],
            "foci": [
                {
                    "name": "Visual Computing and Human-Centered Technology",
                    "percent": 100
                }
            ],
            "research": [
                {
                    "type": "Award",
                    "begins_on": "2015-01-01",
                    "ends_on": null,
                    "grouping": "National",
                    "weight": 30,
                    "title": "Houska Price Finalist",
                    "description": "Houskapreis",
                    "role": null,
                    "country": "Austria",
                    "website": null,
                    "project_tid": null
                },
                {
                    "type": "Award",
                    "begins_on": "2000-01-01",
                    "ends_on": null,
                    "grouping": "TUW-internal",
                    "weight": 20,
                    "title": "Research Grant by Vienna University of Technology (2000-2001)",
                    "description": null,
                    "role": null,
                    "country": "Austria",
                    "website": null,
                    "project_tid": null
                },
                {
                    "type": "Award",
                    "begins_on": "2013-01-01",
                    "ends_on": null,
                    "grouping": "National",
                    "weight": 30,
                    "title": "Winner of the Kardinal-Innitzer Förderpreis",
                    "description": "Kardinal-Innitzer-Preis",
                    "role": null,
                    "country": "Austria",
                    "website": null,
                    "project_tid": null
                },
                {
                    "type": "Award",
                    "begins_on": "2013-01-01",
                    "ends_on": null,
                    "grouping": "National",
                    "weight": 30,
                    "title": "Winner of the INiTS Award (1st place)\nfor \"3D Building Reconstruction and Thermal Mapping in Fire Brigade Operations\"",
                    "description": "INiTS-Award",
                    "role": null,
                    "country": "Austria",
                    "website": null,
                    "project_tid": null
                },
                {
                    "type": "Award",
                    "begins_on": "1999-01-01",
                    "ends_on": null,
                    "grouping": "National",
                    "weight": 30,
                    "title": "Windhag Stipend",
                    "description": "Niederösterreichische Landesregierung",
                    "role": null,
                    "country": "Austria",
                    "website": null,
                    "project_tid": null
                }
            ],
            "courses": [
                "180774-2023W",
                "180774-2024S",
                "180775-2023W",
                "180775-2024S",
                "183293-2023W",
                "188369-2023W",
                "188456-2024S",
                "188460-2023W",
                "188461-2023W",
                "188470-2023W",
                "188520-2024S",
                "188913-2023W",
                "193021-2023W",
                "193021-2024S",
                "193022-2023W",
                "193022-2024S",
                "193023-2023W",
                "193023-2024S",
                "193024-2023W",
                "193024-2024S",
                "193052-2023W",
                "193083-2023W",
                "193116-2023W",
                "193116-2024S",
                "193117-2024S",
                "193118-2023W",
                "193118-2024S",
                "193119-2023W",
                "193119-2024S",
                "193140-2024S",
                "193148-2024S"
            ],
            "projects": [
                "1366370",
                "1370201",
                "1372951",
                "1427227",
                "1534385",
                "1559899",
                "1577824",
                "165772",
                "1734269",
                "1739399",
                "1745908",
                "1753013",
                "1754584",
                "1875977",
                "1915420",
                "1923969",
                "2076037",
                "2154621",
                "2232316",
                "2258104",
                "2353000",
                "2367677",
                "2457349",
                "276135",
                "288695",
                "358043",
                "424614",
                "448705",
                "5064",
                "508747",
                "624243",
                "624284",
                "7026",
                "716533",
                "7711",
                "9895"
            ],
            "publications": [
                "10109",
                "10185",
                "10217",
                "1023",
                "10408",
                "10628",
                "10631",
                "106642",
                "1074",
                "11999",
                "12185",
                "12329",
                "12656",
                "12900",
                "1329",
                "141146",
                "141721",
                "142639",
                "142912",
                "142955",
                "142958",
                "1431",
                "14345",
                "145639",
                "145654",
                "145738",
                "145804",
                "145868",
                "146840",
                "14879",
                "1496",
                "150894",
                "151647",
                "154791",
                "155582",
                "157421",
                "158236",
                "160200",
                "161595",
                "162660",
                "163192",
                "163374",
                "16351",
                "16352",
                "16356",
                "164418",
                "164463",
                "166744",
                "166753",
                "166758",
                "168405",
                "168427",
                "168587",
                "168596",
                "171979",
                "173644",
                "173645",
                "175052",
                "176295",
                "177712",
                "177714",
                "17832",
                "181432",
                "182906",
                "18421",
                "184386",
                "184388",
                "184389",
                "184390",
                "184391",
                "184763",
                "186166",
                "188675",
                "1930",
                "195200",
                "196677",
                "196858",
                "196969",
                "196991",
                "199002",
                "199165",
                "200991",
                "201183",
                "2014",
                "2021",
                "202317",
                "202567",
                "20259",
                "203368",
                "205357",
                "205717",
                "205759",
                "20668",
                "23366",
                "23816",
                "27084",
                "27108",
                "28649",
                "28656",
                "28660",
                "2962",
                "29665",
                "2994",
                "3045",
                "30713",
                "31760",
                "3304",
                "3307",
                "3316",
                "3915",
                "4063",
                "40991",
                "40994",
                "43787",
                "44060",
                "4493",
                "5097",
                "5114",
                "5336",
                "5423",
                "55002",
                "55018",
                "55580",
                "55582",
                "55595",
                "55596",
                "55779",
                "56143",
                "56150",
                "56151",
                "56152",
                "56153",
                "56292",
                "56834",
                "56998",
                "57296",
                "57299",
                "57670",
                "57671",
                "57672",
                "57927",
                "57928",
                "57930",
                "57931",
                "57932",
                "57933",
                "57934",
                "58054",
                "58273",
                "58274",
                "58277",
                "58282",
                "58283",
                "58284",
                "58333",
                "58356",
                "58373",
                "58383",
                "58393",
                "58399",
                "58419",
                "58775",
                "58789",
                "58795",
                "58872",
                "58920",
                "58931",
                "59585",
                "5963",
                "59834",
                "59835",
                "59934",
                "59954",
                "59961",
                "60066",
                "60275",
                "60276",
                "60278",
                "60704",
                "60715",
                "60814",
                "61030",
                "61031",
                "61033",
                "61097",
                "61157",
                "61413",
                "61414",
                "61579",
                "61590",
                "61709",
                "61725",
                "62076",
                "62082",
                "62109",
                "62340",
                "62341",
                "6252",
                "6373",
                "6378",
                "66716",
                "6720",
                "7108",
                "722",
                "7338",
                "7730",
                "7745",
                "8055",
                "8120",
                "8220",
                "82210",
                "82508",
                "82839",
                "8313",
                "8895",
                "89051",
                "89261",
                "89262",
                "89263",
                "89264",
                "89270",
                "89271",
                "89284",
                "89383",
                "89384",
                "89627",
                "89645",
                "90320",
                "90323",
                "90367",
                "90391",
                "90449",
                "90450",
                "90920",
                "90921",
                "91144",
                "91256",
                "91257",
                "91258",
                "91259",
                "91260",
                "9506",
                "9752",
                "9763"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "38472",
            "old_tids": [],
            "last_name": "Kropatsch",
            "first_name": "Walter",
            "slug": "walter-kropatsch",
            "name_prefix": "Em.O.Univ.Prof. Dipl.-Ing. Dr.techn.",
            "name_suffix": null,
            "name_prefix_short": "Em.O.Univ.Prof. DI Dr.",
            "name_suffix_short": null,
            "picture": "https://informatics.tuwien.ac.at/people/walter-kropatsch/picture",
            "email": "walter.kropatsch@tuwien.ac.at",
            "phone": "+43-1-58801-18660",
            "room": "HC0107",
            "orcid": null,
            "websites": [
                {
                    "url": "http://www.prip.tuwien.ac.at/people/walter_g_kropatsch.php",
                    "title": "home@prip"
                },
                {
                    "url": "http://www.prip.tuwien.ac.at/people/walter-g-kropatsch",
                    "title": "http://www.prip.tuwien.ac.at/people/walter-g-kropatsch"
                }
            ],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 9",
                "maps": "AT 1040 Wien Favoritenstrasse 9",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 9",
                "country": "AT",
                "care_of": null,
                "latitude": 48.19521,
                "longitude": 16.369568,
                "location": "Favoritenstrasse 9"
            },
            "about": "1) Theoretical and practical research in digital image processing\n2) Pattern recognition\n3) Remote sensing\n4) Art history, archeology, industry\n5) Analysis of medical imagery\n6) Digital elevation models\n7) Expert vision systems and image pyramids.",
            "keywords": [
                "Graph-based representations",
                "Cognitive Vision",
                "digital topology in images",
                "Motion detection",
                "Motion Tracking",
                "hierarchical methods",
                "Image representations"
            ],
            "roles": [
                "Emerit. Professor"
            ],
            "foci": [
                {
                    "name": "Visual Computing and Human-Centered Technology",
                    "percent": 100
                }
            ],
            "research": [
                {
                    "type": "Award",
                    "begins_on": "2002-01-01",
                    "ends_on": null,
                    "grouping": "International",
                    "weight": 40,
                    "title": "1st Vice-President of the IAPR.",
                    "description": null,
                    "role": null,
                    "country": null,
                    "website": null,
                    "project_tid": null
                },
                {
                    "type": "Award",
                    "begins_on": "1994-01-01",
                    "ends_on": null,
                    "grouping": "International",
                    "weight": 40,
                    "title": "FELLOW of the IAPR.",
                    "description": null,
                    "role": null,
                    "country": null,
                    "website": null,
                    "project_tid": null
                },
                {
                    "type": "Award",
                    "begins_on": "2004-01-01",
                    "ends_on": null,
                    "grouping": "International",
                    "weight": 40,
                    "title": "President of the International Association for Pattern Recognition",
                    "description": null,
                    "role": null,
                    "country": null,
                    "website": null,
                    "project_tid": null
                },
                {
                    "type": "Award",
                    "begins_on": "1998-01-01",
                    "ends_on": null,
                    "grouping": "International",
                    "weight": 40,
                    "title": "Treasurer of the IAPR.",
                    "description": null,
                    "role": null,
                    "country": null,
                    "website": null,
                    "project_tid": null
                }
            ],
            "courses": [
                "180774-2024S",
                "183293-2023W",
                "193023-2023W",
                "193023-2024S",
                "193083-2023W",
                "193116-2023W",
                "193116-2024S",
                "193119-2023W",
                "193119-2024S"
            ],
            "projects": [
                "1754584",
                "4293",
                "5344",
                "538645",
                "629204",
                "6614",
                "8659",
                "9289"
            ],
            "publications": [
                "13805",
                "148050",
                "148070",
                "148438",
                "149886",
                "1557",
                "157156",
                "158233",
                "159151",
                "159153",
                "159155",
                "159158",
                "161230",
                "161231",
                "161233",
                "161252",
                "162062",
                "162064",
                "163481",
                "163485",
                "163486",
                "163487",
                "163599",
                "163601",
                "165214",
                "165251",
                "165645",
                "168570",
                "170517",
                "170720",
                "170723",
                "170745",
                "170785",
                "170830",
                "172195",
                "173300",
                "175453",
                "175458",
                "175566",
                "17820",
                "178858",
                "181379",
                "181462",
                "181487",
                "182870",
                "193566",
                "194385",
                "196050",
                "205807",
                "208523",
                "2174",
                "23364",
                "25563",
                "25613",
                "26059",
                "26101",
                "26271",
                "26650",
                "26651",
                "26976",
                "26987",
                "27313",
                "27620",
                "27621",
                "28664",
                "28681",
                "30982",
                "30983",
                "3278",
                "3461",
                "36340",
                "36341",
                "36449",
                "36453",
                "36575",
                "40645",
                "4230",
                "43337",
                "4487",
                "45148",
                "4576",
                "5431",
                "54614",
                "54615",
                "54640",
                "54641",
                "54939",
                "54940",
                "54941",
                "54942",
                "54943",
                "54944",
                "54945",
                "54977",
                "54978",
                "54986",
                "55204",
                "55205",
                "55206",
                "55207",
                "55208",
                "55209",
                "55210",
                "55211",
                "55624",
                "55625",
                "55626",
                "55627",
                "55637",
                "55652",
                "55653",
                "56020",
                "56021",
                "56110",
                "56111",
                "56112",
                "56113",
                "56276",
                "56277",
                "56278",
                "56567",
                "56582",
                "56583",
                "56584",
                "56585",
                "56586",
                "56587",
                "56588",
                "56589",
                "56596",
                "56597",
                "56600",
                "56718",
                "57064",
                "57065",
                "57066",
                "57067",
                "57515",
                "57526",
                "57559",
                "57577",
                "57578",
                "57580",
                "57583",
                "57585",
                "57587",
                "57963",
                "57964",
                "57967",
                "58132",
                "58181",
                "58181",
                "58426",
                "58426",
                "58427",
                "58427",
                "58428",
                "58428",
                "58446",
                "58447",
                "58448",
                "58449",
                "58919",
                "58998",
                "58999",
                "59000",
                "59002",
                "59003",
                "59004",
                "59005",
                "59184",
                "59214",
                "59276",
                "59336",
                "59618",
                "59941",
                "59958",
                "59964",
                "59965",
                "59966",
                "59975",
                "60086",
                "60384",
                "60385",
                "60387",
                "60390",
                "60392",
                "60393",
                "60397",
                "60869",
                "60875",
                "60877",
                "60878",
                "60879",
                "61328",
                "61329",
                "61535",
                "61649",
                "61652",
                "61653",
                "61654",
                "61658",
                "61670",
                "62063",
                "62365",
                "62365",
                "7090",
                "7129",
                "76902",
                "82096",
                "89611",
                "89612",
                "89613",
                "89793",
                "89805",
                "89987",
                "89988",
                "90178",
                "90179",
                "90515",
                "90674",
                "90684",
                "90929",
                "90932",
                "91118",
                "91515",
                "91533"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "340876",
            "old_tids": [],
            "last_name": "Lautenbach",
            "first_name": "Tom",
            "slug": "tom-lautenbach",
            "name_prefix": "Projektass. Dipl.-Ing.",
            "name_suffix": null,
            "name_prefix_short": "DI",
            "name_suffix_short": null,
            "picture": null,
            "email": "tom.lautenbach@tuwien.ac.at",
            "phone": null,
            "room": "HE0406",
            "orcid": "0009-0001-5789-2228",
            "websites": [],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": null,
            "keywords": [],
            "roles": [
                "PreDoc Researcher"
            ],
            "foci": [],
            "research": [],
            "courses": [],
            "projects": [],
            "publications": [
                "205759"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "47484",
            "old_tids": [],
            "last_name": "Moik",
            "first_name": "Johannes",
            "slug": "johannes-moik",
            "name_prefix": "Univ.Doz. Dipl.-Ing. Dr.techn.",
            "name_suffix": null,
            "name_prefix_short": "DI Dr.",
            "name_suffix_short": null,
            "picture": null,
            "email": "johannes.moik@tuwien.ac.at",
            "phone": null,
            "room": null,
            "orcid": null,
            "websites": [],
            "address": null,
            "about": null,
            "keywords": [],
            "roles": [
                "Affiliated"
            ],
            "foci": [],
            "research": [],
            "courses": [],
            "projects": [],
            "publications": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "252414",
            "old_tids": [],
            "last_name": "Mortezapoor",
            "first_name": "Soroosh",
            "slug": "soroosh-mortezapoor",
            "name_prefix": "Univ.Ass. Dipl.-Ing.",
            "name_suffix": null,
            "name_prefix_short": "DI",
            "name_suffix_short": null,
            "picture": "https://informatics.tuwien.ac.at/people/soroosh-mortezapoor/picture",
            "email": "soroosh.mortezapoor@tuwien.ac.at",
            "phone": "+43-1-58801-18828",
            "room": null,
            "orcid": "0000-0003-4941-3220",
            "websites": [],
            "address": null,
            "about": null,
            "keywords": [],
            "roles": [
                "PreDoc Researcher"
            ],
            "foci": [],
            "research": [],
            "courses": [
                "188456-2024S",
                "188461-2023W",
                "188913-2023W",
                "193021-2023W",
                "193021-2024S",
                "193022-2023W",
                "193022-2024S",
                "193023-2023W",
                "193023-2024S",
                "193024-2023W",
                "193024-2024S",
                "193118-2023W",
                "193148-2024S"
            ],
            "projects": [],
            "publications": [
                "196969",
                "196991",
                "23816",
                "62436"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "247245",
            "old_tids": [],
            "last_name": "Podkosova",
            "first_name": "Iana",
            "slug": "yana-podkosova",
            "name_prefix": "Projektass.in Dr.in techn.",
            "name_suffix": "BSc MSc",
            "name_prefix_short": "Dr.",
            "name_suffix_short": "BSc MSc",
            "picture": "https://informatics.tuwien.ac.at/people/yana-podkosova/picture",
            "email": "yana.podkosova@tuwien.ac.at",
            "phone": "+43-1-58801-188583",
            "room": "HE0402",
            "orcid": null,
            "websites": [
                {
                    "url": "https://www.vr.tuwien.ac.at/people/iana-podkosova/",
                    "title": "Profile on the website of Virtual and Augmented Reality Research Unit"
                }
            ],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": null,
            "keywords": [],
            "roles": [
                "PostDoc Researcher"
            ],
            "foci": [],
            "research": [],
            "courses": [
                "193021-2023W",
                "193024-2023W"
            ],
            "projects": [
                "1875977"
            ],
            "publications": [
                "141146",
                "141721",
                "142639",
                "145804",
                "145868",
                "146840",
                "196858",
                "199077",
                "2014",
                "202317",
                "205759",
                "31760",
                "3316",
                "5336",
                "60066",
                "60275",
                "60276",
                "61031",
                "61033",
                "61413",
                "61414",
                "61590",
                "62371",
                "66716",
                "7108"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "54835",
            "old_tids": [],
            "last_name": "Schönauer",
            "first_name": "Christian",
            "slug": "christian-schoenauer",
            "name_prefix": "Projektass. Dipl.-Ing. Dr.techn.",
            "name_suffix": null,
            "name_prefix_short": "DI Dr.",
            "name_suffix_short": null,
            "picture": "https://informatics.tuwien.ac.at/people/christian-schoenauer/picture",
            "email": "christian.schoenauer@tuwien.ac.at",
            "phone": "+43-1-58801-18894",
            "room": "HE0406",
            "orcid": null,
            "websites": [],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": null,
            "keywords": [],
            "roles": [
                "PostDoc Researcher"
            ],
            "foci": [],
            "research": [],
            "courses": [],
            "projects": [
                "1875977",
                "2457349",
                "448705",
                "7711"
            ],
            "publications": [
                "14345",
                "146840",
                "14879",
                "1496",
                "151647",
                "161595",
                "163192",
                "163374",
                "16356",
                "166744",
                "171979",
                "1930",
                "199165",
                "200991",
                "20668",
                "23816",
                "40994",
                "44060",
                "56178",
                "56292",
                "57296",
                "57670",
                "57671",
                "57672",
                "57927",
                "58054",
                "58277",
                "58283",
                "58284",
                "58795",
                "59585",
                "59822",
                "60259",
                "60276",
                "60704",
                "61725",
                "62109",
                "6252",
                "6720",
                "82508",
                "89626",
                "90161",
                "91705",
                "9752",
                "9763"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "232367",
            "old_tids": [],
            "last_name": "Vasylevska",
            "first_name": "Khrystyna",
            "slug": "khrystyna-vasylevska",
            "name_prefix": "Projektass.in(FWF)",
            "name_suffix": "BSc MSc",
            "name_prefix_short": null,
            "name_suffix_short": "BSc MSc",
            "picture": "https://informatics.tuwien.ac.at/people/khrystyna-vasylevska/picture",
            "email": "khrystyna.vasylevska@tuwien.ac.at",
            "phone": "+43-1-58801-188646",
            "room": "HG0406",
            "orcid": "0000-0003-4485-9928",
            "websites": [
                {
                    "url": "https://www.vr.tuwien.ac.at/people/khrystyna-vasylevska/",
                    "title": "Personal page at VR Group website"
                }
            ],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": null,
            "keywords": [],
            "roles": [
                "PreDoc Researcher"
            ],
            "foci": [],
            "research": [],
            "courses": [],
            "projects": [],
            "publications": [
                "1074",
                "142959",
                "157421",
                "196969",
                "196991",
                "205717",
                "31760",
                "58282",
                "58872",
                "59961",
                "60276",
                "61030",
                "61031",
                "61579",
                "62082",
                "62436",
                "90323"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "tid": "40682",
            "old_tids": [],
            "last_name": "Vonach",
            "first_name": "Emanuel",
            "slug": "emanuel-vonach",
            "name_prefix": "Projektass.(FWF) Dipl.-Ing. Mag.rer.soc.oec.",
            "name_suffix": "Bakk.techn.",
            "name_prefix_short": "DI Mag.",
            "name_suffix_short": "BSc",
            "picture": "https://informatics.tuwien.ac.at/people/emanuel-vonach/picture",
            "email": "emanuel.vonach@tuwien.ac.at",
            "phone": "+43-1-58801-188053",
            "room": "HE0406",
            "orcid": null,
            "websites": [
                {
                    "url": "https://www.vr.tuwien.ac.at/people/emanuel-vonach/",
                    "title": "Homepage"
                }
            ],
            "address": {
                "line": "1040 Wien, Favoritenstrasse 11",
                "maps": "AT 1040 Wien Favoritenstrasse 11",
                "zip": "1040",
                "city": "Wien",
                "street": "Favoritenstrasse 11",
                "country": "AT",
                "care_of": null,
                "latitude": 48.194951,
                "longitude": 16.36984,
                "location": "Favoritenstrasse 11"
            },
            "about": null,
            "keywords": [],
            "roles": [
                "PreDoc Researcher"
            ],
            "foci": [],
            "research": [],
            "courses": [
                "193023-2023W",
                "193024-2023W"
            ],
            "projects": [
                "165772",
                "288695",
                "358043",
                "6289",
                "7711"
            ],
            "publications": [
                "10109",
                "10408",
                "145654",
                "16352",
                "164463",
                "18421",
                "196969",
                "196991",
                "2962",
                "3304",
                "40994",
                "5114",
                "58277",
                "58283",
                "58284",
                "58920",
                "60276",
                "60278",
                "60715",
                "61347",
                "82839",
                "90320",
                "91144",
                "9763"
            ]
        }
    ],
    "courses": [
        {
            "org_nrs": [
                "E180"
            ],
            "cid": "180774-2023W",
            "semester": "2023W",
            "nr": "180774",
            "nr_pretty": "180.774",
            "type": "SE",
            "title": "Seminar for Master Students in Visual Computing",
            "hours": "1.0",
            "language": "German",
            "objective": "<p>The goal of this course is to additionally support students during the creation of their Master thesis and preparing for the final defense: students get feedback from professors of visual computing and human-centered technology and other students, and can learn from the presentations of their colleagues.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=180774&semester=2023W",
            "people": [
                {
                    "tid": "269459",
                    "first_name": "Manuela",
                    "last_name": "Waldner",
                    "role": "Lecturer"
                },
                {
                    "tid": "40666",
                    "first_name": "Michael",
                    "last_name": "Wimmer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "306405",
                    "first_name": "Renata Georgia",
                    "last_name": "Raidou",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E180"
            ],
            "cid": "180775-2023W",
            "semester": "2023W",
            "nr": "180775",
            "nr_pretty": "180.775",
            "type": "SE",
            "title": "Seminar for Master Students in Media and Human-Centered Computing",
            "hours": "1.0",
            "language": "English",
            "objective": "<p>-</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=180775&semester=2023W",
            "people": [
                {
                    "tid": "139584",
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "183293-2023W",
            "semester": "2023W",
            "nr": "183293",
            "nr_pretty": "183.293",
            "type": "SE",
            "title": "Diploma Seminar",
            "hours": "2.0",
            "language": "if required in English",
            "objective": "<p>Presentation and defense of own scientific work.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=183293&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188369-2023W",
            "semester": "2023W",
            "nr": "188369",
            "nr_pretty": "188.369",
            "type": "VO",
            "title": "Virtual and Augmented Reality",
            "hours": "2.0",
            "language": "English",
            "objective": "<p><strong>Introductory Meeting (VO + UE) on 1.10.2018 at 10:00 until approx. 10:45 in Zemanek HS!</strong></p> \n<p><strong>Right after the introductory meeting, the course will start. The course will be BLOCKED, meaning that the lecture times are condensed within the first 2 weeks of October. <br></strong></p> \n<p><strong>These are the lecture times for the Vorlesung:&nbsp; <br>====================================<br></strong></p> \n<p><strong><strong>Introduction on</strong> 1.10. 10:00-10:45</strong></p> \n<p><strong> The first lecture starts immediately after the introduction. <br></strong></p> \n<p><strong>Vorlesung: ALL lectures will be in Zemanek HS.<br></strong></p> \n<p><strong>MO 1.10. 11:00 - 13:00 &nbsp; </strong></p> \n<p><strong>DI 2.10. 13:00 - 16:00<br></strong></p> \n<p><strong>MI 3.10. 10:00 - 13:00 </strong></p> \n<p><strong>DO 4.10. 13:00 - 16:00</strong></p> \n<p><strong>FR 5.10. 10:00 - 13:00<br></strong></p> \n<p><strong>MO 8.10. 13:00-16:00</strong></p> \n<p>This course introduces to Virtual and Augmented Reality. Students learn the basics about VR/AR hardware and software, input and output technologies and devices, user specific aspects, usability and psychological aspects. An overview of current areas of research is given as well.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188369&semester=2023W",
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188460-2023W",
            "semester": "2023W",
            "nr": "188460",
            "nr_pretty": "188.460",
            "type": "VO",
            "title": "Multimedia Interfaces",
            "hours": "2.0",
            "language": "English",
            "objective": "<p>Special lecture about multimedia based and non conventional user interfaces.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188460&semester=2023W",
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188461-2023W",
            "semester": "2023W",
            "nr": "188461",
            "nr_pretty": "188.461",
            "type": "LU",
            "title": "Multimedia Interfaces",
            "hours": "1.0",
            "language": "English",
            "objective": "<p>Development of multi media User Interfaces</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188461&semester=2023W",
            "people": [
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188470-2023W",
            "semester": "2023W",
            "nr": "188470",
            "nr_pretty": "188.470",
            "type": "VU",
            "title": "Creative Media Production",
            "hours": "4.0",
            "language": "English",
            "objective": "<p>In this course, students will learn how to script, direct and produce short video productions in non-interactive media.</p> \n<p>Within the first weeks of course, groups will be formed and the creative process begins. Students will learn how to describe their ideas through a plot and treatment. Once their ideas are cohesive and clear, they can move on to the next step to build their dramatic script. Finally, it is up to the group to produce their idea using basic video production tools (camera, lighting sets, audio equipment).</p> \n<p>This course is not just about producing a video project, but strongly depends on time management and individual responsibility - skills that are required in every line of work - therefore, these skills are emphasized during the semester.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188470&semester=2023W",
            "people": [
                {
                    "tid": "44691",
                    "first_name": "Karyn",
                    "last_name": "Laudisi",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "46148",
                    "first_name": "Julian Roman",
                    "last_name": "Pölsler",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188498-2023W",
            "semester": "2023W",
            "nr": "188498",
            "nr_pretty": "188.498",
            "type": "VU",
            "title": "Similarity Modeling 2 - Computational Seeing and Hearing",
            "hours": "2.0",
            "language": "English",
            "objective": "<p>Computational media understanding with deep learning, classic machine intelligence and clustering.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188498&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188501-2023W",
            "semester": "2023W",
            "nr": "188501",
            "nr_pretty": "188.501",
            "type": "VU",
            "title": "Similarity Modeling 1 - Computational Seeing and Hearing",
            "hours": "2.0",
            "language": "English",
            "objective": "<p>Computational media understanding with deep learning, classic machine intelligence and clustering.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188501&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188913-2023W",
            "semester": "2023W",
            "nr": "188913",
            "nr_pretty": "188.913",
            "type": "UE",
            "title": "Virtual and Augmented Reality",
            "hours": "3.0",
            "language": "English",
            "objective": "<p>Based on three smaller assignments the understanding of object tracking, virtual reality environments, 3D interaction and networking in distributed virtual environments are trained. After this initial training phase a more complex project is implemented.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188913&semester=2023W",
            "people": [
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188981-2023W",
            "semester": "2023W",
            "nr": "188981",
            "nr_pretty": "188.981",
            "type": "VU",
            "title": "Strategy Game Programming",
            "hours": "2.0",
            "language": "English",
            "objective": "<ol> \n <li>Students understand the fundamental setting of strategy games.</li> \n <li>Students understand the fundamental algorithms of AI game engines.</li> \n <li>Students have experience in the implementation of the Minimax algorithms and related heuristics.</li> \n <li>Students have experience in the implementation of Monte Carlo Tree Sampling and related heuristics (UCB1, RAVE, etc.).</li> \n</ol>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188981&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193021-2023W",
            "semester": "2023W",
            "nr": "193021",
            "nr_pretty": "193.021",
            "type": "PR",
            "title": "Project Media and Human-Centered Computing 1",
            "hours": "4.0",
            "language": "English",
            "objective": "<p>Undertake a project in media and human-centered computing.&nbsp;</p>\n<p><strong>==============================================</strong><br> <strong>Für Gruppe 193-07 (CVAST Team: Silvia Miksch und Team)</strong></p>\n<p><strong>Themen</strong></p>\n<ul>\n<li><a href=\"http://www.cvast.tuwien.ac.at/topics\" rel=\"nofollow\">http://www.cvast.tuwien.ac.at/topics&nbsp;</a></li>\n</ul>\n<p><strong>TUWEL</strong></p>\n<ul>\n<li><a href=\"https://tuwel.tuwien.ac.at/course/view.php?id=58865\" rel=\"nofollow\">https://tuwel.tuwien.ac.at/course/view.php?id=58865</a></li>\n<li><strong><em>WICHTIG: Alle wichtigen Informationen und Termine stehen im TUWEL-Kurs</em></strong></li>\n</ul>\n<p><strong>==============================================</strong></p>\n<p><strong>For 193-04 (ACUR) </strong>- Please write an email to the respective colleague you want to have as a supervisor and would like to make an appointment.</p>\n<ul>\n<li><a href=\"mailto:hilda.tellioglu@tuwien.ac.at?subject=Interested%20for%20PMHCC1\" rel=\"nofollow\">Hilda Tellioglu</a></li>\n<li><a href=\"mailto:florian.michahelles@tuwien.ac.at\" rel=\"nofollow\">Florian Michahelles</a></li>\n<li><a href=\"mailto:florian.wolling@tuwien.ac.at\" rel=\"nofollow\">Florian Wolling</a></li>\n<li><a href=\"mailto:ambika.shahu@tuwien.ac.at\" rel=\"nofollow\">Ambika Shahu</a></li>\n<li><a href=\"mailto:khaled.kassem@tuwien.ac.at\" rel=\"nofollow\">Khaled Kassem</a></li>\n<li><a href=\"mailto:gerfried.mikusch@tuwien.ac.at\" rel=\"nofollow\">Gerfried Mikusch</a></li>\n</ul>\n<p><strong>========================================</strong></p>\n<p><strong><br></strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193021&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "139584",
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "247245",
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "role": "Lecturer"
                },
                {
                    "tid": "354774",
                    "first_name": "Ignacio Baltazar",
                    "last_name": "Perez Messina",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                },
                {
                    "tid": "331501",
                    "first_name": "Nikolaus",
                    "last_name": "Piccolotto",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "372512",
                    "first_name": "Pedro",
                    "last_name": "Hermosilla Casajus",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "57469",
                    "first_name": "Markus",
                    "last_name": "Bögl",
                    "role": "Lecturer"
                },
                {
                    "tid": "289877",
                    "first_name": "Davide",
                    "last_name": "Ceneda",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                },
                {
                    "tid": "58906",
                    "first_name": "Velitchko",
                    "last_name": "Filipov",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193022-2023W",
            "semester": "2023W",
            "nr": "193022",
            "nr_pretty": "193.022",
            "type": "PR",
            "title": "Project Media and Human-Centered Computing 2",
            "hours": "4.0",
            "language": "English",
            "objective": "<p>Undertake a project in media and human-centered computing.&nbsp;</p>\n<p><strong>==============================================</strong><br> <strong>Group 193-07 (CVAST Team: Silvia Miksch and Team)</strong></p>\n<p><strong>Topics</strong></p>\n<ul>\n<li><a href=\"http://www.cvast.tuwien.ac.at/topics\" rel=\"nofollow\">http://www.cvast.tuwien.ac.at/topics&nbsp;</a></li>\n</ul>\n<p><strong>TUWEL</strong></p>\n<ul>\n<li>https://tuwel.tuwien.ac.at/course/view.php?id=58865</li>\n<li><strong><em>IMPORTANT: All important information and dates are in the TUWEL course</em></strong></li>\n</ul>\n<p><strong>==============================================</strong></p>\n<p><strong>For 193-04 (ACUR)&nbsp;</strong>- Please write an email to the respective colleague you want to have as a supervisor and would like to make an appointment.</p>\n<ul>\n<li><a href=\"mailto:hilda.tellioglu@tuwien.ac.at?subject=Interested%20for%20PMHCC1\" rel=\"nofollow\">Hilda Tellioglu</a></li>\n<li><a href=\"mailto:florian.michahelles@tuwien.ac.at\" rel=\"nofollow\">Florian Michahelles</a></li>\n<li><a href=\"mailto:florian.wolling@tuwien.ac.at\" rel=\"nofollow\">Florian Wolling</a></li>\n<li><a href=\"mailto:ambika.shahu@tuwien.ac.at\" rel=\"nofollow\">Ambika Shahu</a></li>\n<li><a href=\"mailto:khaled.kassem@tuwien.ac.at\" rel=\"nofollow\">Khaled Kassem</a></li>\n<li><a href=\"mailto:gerfried.mikusch@tuwien.ac.at\" rel=\"nofollow\">Gerfried Mikusch</a></li>\n</ul>\n<p><strong>========================================</strong></p>\n<strong><br></strong>\n<p><strong><br></strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193022&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "139584",
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "354774",
                    "first_name": "Ignacio Baltazar",
                    "last_name": "Perez Messina",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "346648",
                    "first_name": "Khaled",
                    "last_name": "Kassem",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                },
                {
                    "tid": "331501",
                    "first_name": "Nikolaus",
                    "last_name": "Piccolotto",
                    "role": "Lecturer"
                },
                {
                    "tid": "284695",
                    "first_name": "Katta",
                    "last_name": "Spiel",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "372512",
                    "first_name": "Pedro",
                    "last_name": "Hermosilla Casajus",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "57469",
                    "first_name": "Markus",
                    "last_name": "Bögl",
                    "role": "Lecturer"
                },
                {
                    "tid": "289877",
                    "first_name": "Davide",
                    "last_name": "Ceneda",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                },
                {
                    "tid": "58906",
                    "first_name": "Velitchko",
                    "last_name": "Filipov",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193023-2023W",
            "semester": "2023W",
            "nr": "193023",
            "nr_pretty": "193.023",
            "type": "PR",
            "title": "Project in Visual Computing 1",
            "hours": "4.0",
            "language": "English",
            "objective": "<p>Implementation of the topics treated in the basic lectures within a larger, technically oriented software project.</p>\n<p><strong>==============================================</strong><br> <strong>Group 193-02 (Computer Graphics: Eduard Gröller, Renata Raidou, Manuela Waldner, Michael Wimmer)</strong></p>\n<p><strong>Topics and Supervision Information: </strong></p>\n<ul>\n<li><a href=\"https://www.cg.tuwien.ac.at/courses/Topics\" rel=\"nofollow\">https://www.cg.tuwien.ac.at/courses/Topics</a><strong><br> </strong></li>\n</ul>\n<p><strong>==============================================</strong><br> <strong></strong></p>\n<p><strong>Group 193-03 (Research Area on Virtual Reality: Kaufmann, Eidenberger, Brumet)</strong></p>\n<p><strong>Topics</strong></p>\n<ul>\n<li><a href=\"https://www.vr.tuwien.ac.at/topics/\" rel=\"nofollow\">https://www.vr.tuwien.ac.at/topics/</a></li>\n</ul>\n<p><strong>==============================================</strong></p>\n<p><strong>Group 193-07 (CVAST Team: Silvia Miksch and Team)</strong></p>\n<p><strong>Topics</strong></p>\n<ul>\n<li><a href=\"http://www.cvast.tuwien.ac.at/topics\" rel=\"nofollow\">http://www.cvast.tuwien.ac.at/topics&nbsp;</a></li>\n</ul>\n<p><strong>TUWEL</strong></p>\n<ul>\n<li>https://tuwel.tuwien.ac.at/course/view.php?id=58865</li>\n<li><strong><em>IMPORTANT: All important information and dates are in the TUWEL course</em></strong></li>\n</ul>\n<p><strong>==============================================</strong></p>\n<p><strong>For 193-04 (ACUR)&nbsp;</strong>- Please write an email to the respective colleague you want to have as a supervisor and would like to make an appointment.</p>\n<ul>\n<li><a href=\"mailto:hilda.tellioglu@tuwien.ac.at?subject=Interested%20for%20PMHCC1\" rel=\"nofollow\">Hilda Tellioglu</a></li>\n<li><a href=\"mailto:florian.michahelles@tuwien.ac.at\" rel=\"nofollow\">Florian Michahelles</a></li>\n<li><a href=\"mailto:florian.wolling@tuwien.ac.at\" rel=\"nofollow\">Florian Wolling</a></li>\n<li><a href=\"mailto:ambika.shahu@tuwien.ac.at\" rel=\"nofollow\">Ambika Shahu</a></li>\n<li><a href=\"mailto:khaled.kassem@tuwien.ac.at\" rel=\"nofollow\">Khaled Kassem</a></li>\n<li><a href=\"mailto:gerfried.mikusch@tuwien.ac.at\" rel=\"nofollow\">Gerfried Mikusch</a></li>\n</ul>\n<p><strong>========================================</strong></p>\n<p><strong><br></strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193023&semester=2023W",
            "people": [
                {
                    "tid": "269459",
                    "first_name": "Manuela",
                    "last_name": "Waldner",
                    "role": "Lecturer"
                },
                {
                    "tid": "40666",
                    "first_name": "Michael",
                    "last_name": "Wimmer",
                    "role": "Lecturer"
                },
                {
                    "tid": "40682",
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "role": "Lecturer"
                },
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "139584",
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "306405",
                    "first_name": "Renata Georgia",
                    "last_name": "Raidou",
                    "role": "Lecturer"
                },
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "354774",
                    "first_name": "Ignacio Baltazar",
                    "last_name": "Perez Messina",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                },
                {
                    "tid": "357410",
                    "first_name": "Francesco",
                    "last_name": "De Pace",
                    "role": "Lecturer"
                },
                {
                    "tid": "331501",
                    "first_name": "Nikolaus",
                    "last_name": "Piccolotto",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "372512",
                    "first_name": "Pedro",
                    "last_name": "Hermosilla Casajus",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "57469",
                    "first_name": "Markus",
                    "last_name": "Bögl",
                    "role": "Lecturer"
                },
                {
                    "tid": "289877",
                    "first_name": "Davide",
                    "last_name": "Ceneda",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Lecturer"
                },
                {
                    "tid": "58906",
                    "first_name": "Velitchko",
                    "last_name": "Filipov",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193024-2023W",
            "semester": "2023W",
            "nr": "193024",
            "nr_pretty": "193.024",
            "type": "PR",
            "title": "Project in Visual Computing 2",
            "hours": "4.0",
            "language": "English",
            "objective": "<p>Implementation of the topics treated in the basic lectures within a larger, technically oriented software project.</p>\n<p><strong>==============================================</strong><br> <strong>Group 193-02 (Computer Graphics: Eduard Gröller, Renata Raidou, Manuela Waldner, Michael Wimmer)</strong></p>\n<p><strong>Topics and Supervision Information: <br> </strong></p>\n<ul>\n<li><a href=\"https://www.cg.tuwien.ac.at/courses/Topics\" rel=\"nofollow\">https://www.cg.tuwien.ac.at/courses/Topics</a><strong><br></strong></li>\n</ul>\n<p><strong>==============================================</strong><br> <strong></strong></p>\n<p><strong>Group 193-03 (Research Area on Virtual Reality: Kaufmann, Eidenberger, Brumet)</strong></p>\n<p><strong>Topics</strong></p>\n<ul>\n<li><a href=\"https://www.vr.tuwien.ac.at/topics/\" rel=\"nofollow\">https://www.vr.tuwien.ac.at/topics/</a></li>\n</ul>\n<p><strong>==============================================</strong><br> <strong>Group 193-07 (CVAST Team: Silvia Miksch and Team)</strong></p>\n<p><strong>Topics</strong></p>\n<ul>\n<li><a href=\"http://www.cvast.tuwien.ac.at/topics\" rel=\"nofollow\">http://www.cvast.tuwien.ac.at/topics&nbsp;</a></li>\n</ul>\n<p><strong>TUWEL</strong></p>\n<ul>\n<li>https://tuwel.tuwien.ac.at/course/view.php?id=58865</li>\n<li><strong><em>IMPORTANT: All important information and dates are in the TUWEL course</em></strong></li>\n</ul>\n<p><strong>==============================================</strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193024&semester=2023W",
            "people": [
                {
                    "tid": "269459",
                    "first_name": "Manuela",
                    "last_name": "Waldner",
                    "role": "Lecturer"
                },
                {
                    "tid": "40666",
                    "first_name": "Michael",
                    "last_name": "Wimmer",
                    "role": "Lecturer"
                },
                {
                    "tid": "40682",
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "role": "Lecturer"
                },
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "306405",
                    "first_name": "Renata Georgia",
                    "last_name": "Raidou",
                    "role": "Lecturer"
                },
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "247245",
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "role": "Lecturer"
                },
                {
                    "tid": "354774",
                    "first_name": "Ignacio Baltazar",
                    "last_name": "Perez Messina",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "331501",
                    "first_name": "Nikolaus",
                    "last_name": "Piccolotto",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "372512",
                    "first_name": "Pedro",
                    "last_name": "Hermosilla Casajus",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "57469",
                    "first_name": "Markus",
                    "last_name": "Bögl",
                    "role": "Lecturer"
                },
                {
                    "tid": "289877",
                    "first_name": "Davide",
                    "last_name": "Ceneda",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                },
                {
                    "tid": "58906",
                    "first_name": "Velitchko",
                    "last_name": "Filipov",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193052-2023W",
            "semester": "2023W",
            "nr": "193052",
            "nr_pretty": "193.052",
            "type": "SE",
            "title": "Scientific Research and Writing",
            "hours": "2.0",
            "language": "German",
            "objective": "<p>Introduction to science and the operation of the scientific community; based on a self-chosen or a given topic students practice developing a research question, performing a literature search, basic skills of scientific writing, and giving a scientific presentation</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193052&semester=2023W",
            "people": [
                {
                    "tid": "269459",
                    "first_name": "Manuela",
                    "last_name": "Waldner",
                    "role": "Lecturer"
                },
                {
                    "tid": "40666",
                    "first_name": "Michael",
                    "last_name": "Wimmer",
                    "role": "Lecturer"
                },
                {
                    "tid": "215880",
                    "first_name": "Uwe",
                    "last_name": "Egly",
                    "role": "Lecturer"
                },
                {
                    "tid": "41293",
                    "first_name": "Thomas",
                    "last_name": "Lukasiewicz",
                    "role": "Lecturer"
                },
                {
                    "tid": "42248",
                    "first_name": "Schahram",
                    "last_name": "Dustdar",
                    "role": "Lecturer"
                },
                {
                    "tid": "139584",
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "306405",
                    "first_name": "Renata Georgia",
                    "last_name": "Raidou",
                    "role": "Lecturer"
                },
                {
                    "tid": "42095",
                    "first_name": "Agata",
                    "last_name": "Ciabattoni",
                    "role": "Lecturer"
                },
                {
                    "tid": "147972",
                    "first_name": "Gerti",
                    "last_name": "Kappel",
                    "role": "Lecturer"
                },
                {
                    "tid": "251491",
                    "first_name": "Georg",
                    "last_name": "Weissenbacher",
                    "role": "Lecturer"
                },
                {
                    "tid": "263178",
                    "first_name": "Sascha",
                    "last_name": "Hunold",
                    "role": "Lecturer"
                },
                {
                    "tid": "271598",
                    "first_name": "Astrid",
                    "last_name": "Weiss",
                    "role": "Lecturer"
                },
                {
                    "tid": "45040",
                    "first_name": "Günther",
                    "last_name": "Raidl",
                    "role": "Lecturer"
                },
                {
                    "tid": "46163",
                    "first_name": "Jens",
                    "last_name": "Knoop",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "134415",
                    "first_name": "Wolfdieter",
                    "last_name": "Merkl",
                    "role": "Lecturer"
                },
                {
                    "tid": "125009",
                    "first_name": "M. Anton",
                    "last_name": "Ertl",
                    "role": "Lecturer"
                },
                {
                    "tid": "140014",
                    "first_name": "Christian",
                    "last_name": "Fermüller",
                    "role": "Lecturer"
                },
                {
                    "tid": "330800",
                    "first_name": "Thomas",
                    "last_name": "Gärtner",
                    "role": "Lecturer"
                },
                {
                    "tid": "331545",
                    "first_name": "Jiehua",
                    "last_name": "Chen",
                    "role": "Lecturer"
                },
                {
                    "tid": "333566",
                    "first_name": "Florian",
                    "last_name": "Michahelles",
                    "role": "Lecturer"
                },
                {
                    "tid": "252311",
                    "first_name": "Julia Maria",
                    "last_name": "Jaklin",
                    "role": "Tutor"
                },
                {
                    "tid": "129556",
                    "first_name": "Thomas",
                    "last_name": "Eiter",
                    "role": "Lecturer"
                },
                {
                    "tid": "373729",
                    "first_name": "Katja",
                    "last_name": "Hose",
                    "role": "Lecturer"
                },
                {
                    "tid": "50708",
                    "first_name": "Stefan",
                    "last_name": "Nastic",
                    "role": "Lecturer"
                },
                {
                    "tid": "51423",
                    "first_name": "Ivona",
                    "last_name": "Brandic",
                    "role": "Lecturer"
                },
                {
                    "tid": "52478",
                    "first_name": "Stefan",
                    "last_name": "Woltran",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "118510",
                    "first_name": "Christian",
                    "last_name": "Huemer",
                    "role": "Lecturer"
                },
                {
                    "tid": "55361",
                    "first_name": "Magdalena",
                    "last_name": "Ortiz",
                    "role": "Lecturer"
                },
                {
                    "tid": "140530",
                    "first_name": "Gernot",
                    "last_name": "Salzer",
                    "role": "Lecturer"
                },
                {
                    "tid": "293301",
                    "first_name": "Martin",
                    "last_name": "Nöllenburg",
                    "role": "Lecturer"
                },
                {
                    "tid": "137136",
                    "first_name": "Andreas",
                    "last_name": "Steininger",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "148297",
                    "first_name": "Ulrich",
                    "last_name": "Schmid",
                    "role": "Lecturer"
                },
                {
                    "tid": "127285",
                    "first_name": "Stefan",
                    "last_name": "Szeider",
                    "role": "Lecturer"
                },
                {
                    "tid": "68412",
                    "first_name": "Jürgen",
                    "last_name": "Cito",
                    "role": "Lecturer"
                },
                {
                    "tid": "124531",
                    "first_name": "Stefan",
                    "last_name": "Biffl",
                    "role": "Lecturer"
                },
                {
                    "tid": "272401",
                    "first_name": "Robert",
                    "last_name": "Ganian",
                    "role": "Lecturer"
                },
                {
                    "tid": "191914",
                    "first_name": "Florian",
                    "last_name": "Zuleger",
                    "role": "Lecturer"
                },
                {
                    "tid": "135034",
                    "first_name": "Reinhard",
                    "last_name": "Pichler",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193083-2023W",
            "semester": "2023W",
            "nr": "193083",
            "nr_pretty": "193.083",
            "type": "SE",
            "title": "PhD Seminar",
            "hours": "2.0",
            "language": "English",
            "objective": "<p>This seminar for doctoral students focuses on the presentation and discussion of research work. Contents are defined in individual consultation with the PhD supervisors. In the context of the seminar, the students are invited to train their presentation skills for conference talks and are offered feedback from their peers. All presentations are held in English.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193083&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193116-2023W",
            "semester": "2023W",
            "nr": "193116",
            "nr_pretty": "193.116",
            "type": "PR",
            "title": "Bachelor Thesis",
            "hours": "5.0",
            "language": "if required in English",
            "objective": "<p>Topics will be discussed with the supervisor.</p>\n<p><strong>Topics</strong></p>\n<ul>\n <li>For topics of Horst Eidenberger (Machine Learning, Computational Perception, Data Science, Virtual Reality, Strategy Game Programming) please see <a href=\"http://www.vreeclimber.at/student_projects/\" rel=\"nofollow\">http://www.vreeclimber.at/student_projects/</a>. Start anytime, contact by e-mail.</li>\n <li>Topics of Hannes Kaufmann: <strong>to appear</strong>.</li>\n</ul>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193116&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193118-2023W",
            "semester": "2023W",
            "nr": "193118",
            "nr_pretty": "193.118",
            "type": "PR",
            "title": "Project in Computer Science 2",
            "hours": "4.0",
            "language": "if required in English",
            "objective": "<p>During the course of this project students will realize a practical assignment beginning with a coarse-grained requirements definition culminating in a final prototypical realization. The exact definition of your project assignment will be done together with your advisor and depends on the chosen topic, the topic's complexity, and the size of your team (if you are working in one).</p>\n<p><strong>==============================================</strong><br> <strong>Group 193-07 (CVAST Team: Silvia Miksch and Team)</strong></p>\n<p><strong>Topics</strong></p>\n<ul>\n<li><a href=\"http://www.cvast.tuwien.ac.at/topics\" rel=\"nofollow\">http://www.cvast.tuwien.ac.at/topics&nbsp;</a></li>\n</ul>\n<p><strong>TUWEL</strong></p>\n<ul>\n<li>https://tuwel.tuwien.ac.at/course/view.php?id=58865</li>\n<li><strong><em>IMPORTANT: All important information and dates are in the TUWEL course</em></strong></li>\n</ul>\n<p><strong>==============================================</strong></p>\n<p><strong>For 193-04 (ACUR)&nbsp;</strong>- Please write an email to the respective colleague you want to have as a supervisor and would like to make an appointment.</p>\n<ul>\n<li><a href=\"mailto:hilda.tellioglu@tuwien.ac.at?subject=Interested%20for%20PMHCC1\" rel=\"nofollow\">Hilda Tellioglu</a></li>\n<li><a href=\"mailto:florian.michahelles@tuwien.ac.at\" rel=\"nofollow\">Florian Michahelles</a></li>\n<li><a href=\"mailto:florian.wolling@tuwien.ac.at\" rel=\"nofollow\">Florian Wolling</a></li>\n<li><a href=\"mailto:ambika.shahu@tuwien.ac.at\" rel=\"nofollow\">Ambika Shahu</a></li>\n<li><a href=\"mailto:khaled.kassem@tuwien.ac.at\" rel=\"nofollow\">Khaled Kassem</a></li>\n<li><a href=\"mailto:gerfried.mikusch@tuwien.ac.at\" rel=\"nofollow\">Gerfried Mikusch</a></li>\n</ul>\n<p><strong>========================================</strong></p>\n<p><strong><br></strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193118&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "139584",
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "354774",
                    "first_name": "Ignacio Baltazar",
                    "last_name": "Perez Messina",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                },
                {
                    "tid": "331501",
                    "first_name": "Nikolaus",
                    "last_name": "Piccolotto",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "372512",
                    "first_name": "Pedro",
                    "last_name": "Hermosilla Casajus",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "57469",
                    "first_name": "Markus",
                    "last_name": "Bögl",
                    "role": "Lecturer"
                },
                {
                    "tid": "289877",
                    "first_name": "Davide",
                    "last_name": "Ceneda",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                },
                {
                    "tid": "58906",
                    "first_name": "Velitchko",
                    "last_name": "Filipov",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193119-2023W",
            "semester": "2023W",
            "nr": "193119",
            "nr_pretty": "193.119",
            "type": "SE",
            "title": "Research seminar for PhD students",
            "hours": "2.0",
            "language": "English",
            "objective": "<p>Presentation of challenging topics in the fields of Virtual and Augmented Reality.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193119&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193120-2023W",
            "semester": "2023W",
            "nr": "193120",
            "nr_pretty": "193.120",
            "type": "SE",
            "title": "Seminar Media and Human-Centered Computing - Gaps of Digitalization",
            "hours": "2.0",
            "language": "if required in English",
            "objective": "<p>Seminar on selected topics in media informatics; in the winter term 2023/24 this is: <strong>Gaps of Digitalization</strong>.<br><br>The students' task is to find gaps in the digitization landscape in business and society, describe them and propose solutions on how they could be reduced or automated with the help of IT. The results will be summarized in a seminar paper and presented and discussed in the group using appropriate presentation materials.<br><br>See the e-learning course and below for details.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193120&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "194077-2023W",
            "semester": "2023W",
            "nr": "194077",
            "nr_pretty": "194.077",
            "type": "VU",
            "title": "Applied Deep Learning",
            "hours": "2.0",
            "language": "English",
            "objective": "<p>The focus of this course is a project, which has to be solved applying Deep Learning methods. The topic of that project can be chosen by the student. To be able to complete the project within this course, weekly lectures cover the most essential methods. Apart from an overview over Deep Learning and Neural Networks, the following advanced topics will be presented in this course and the project must use at least one of them:</p>\n<ul>\n<li>\n<p><span>Convolutional Neural Networks for Image Analysis</span></p>\n</li>\n<li>\n<p><span>Recurrent Neural Networks for Sequence modeling</span></p>\n</li>\n<li>\n<p><span>Deep Reinforcement Learning</span></p>\n</li>\n<li>\n<p><span>Autoencoders and Deep Generative models</span></p>\n</li>\n<li>\n<p><span>Transformers</span></p>\n</li>\n<li>\n<p><span>Graph Neural Networks</span></p>\n</li>\n<li>\n<p><span>Explainable AI</span></p>\n</li>\n</ul>\n<p>Additionally, practical aspects are presented throughout the course, like software libraries and frameworks, which aid the implementation process and allow to communicate the results from the project by presenting them in a meaningful way.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=194077&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "68727",
                    "first_name": "Alexander",
                    "last_name": "Pacha",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "194122-2023W",
            "semester": "2023W",
            "nr": "194122",
            "nr_pretty": "194.122",
            "type": "UE",
            "title": "Augmented Software Engineering Skills for Data Scientists",
            "hours": "2.0",
            "language": "if required in English",
            "objective": "<p>Exercise supplementary software engineering skills in the context of large open source projects: static source code quality management, documentation for roundtrip engineering and effort/cost estimation. All three skills are important for software development in large projects/teams.</p>\n<p><em>ECTS breakdown: 2h pre-lecture meeting + kick-off presentation, 48h report/project paper (milestones: project selection, source code analysis, UML documentation, COCOMO-2 effort estimation), 24h presentation (milestones: intermediate hand-in, presentation), 1h organization = 75h total.</em></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=194122&semester=2023W",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E180"
            ],
            "cid": "180774-2024S",
            "semester": "2024S",
            "nr": "180774",
            "nr_pretty": "180.774",
            "type": "SE",
            "title": "Seminar for Master Students in Visual Computing",
            "hours": "1.0",
            "language": "German",
            "objective": "<p>The goal of this course is to additionally support students during the creation of their Master thesis and preparing for the final defense: students get feedback from professors of visual computing and human-centered technology and other students, and can learn from the presentations of their colleagues.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=180774&semester=2024S",
            "people": [
                {
                    "tid": "269459",
                    "first_name": "Manuela",
                    "last_name": "Waldner",
                    "role": "Lecturer"
                },
                {
                    "tid": "40666",
                    "first_name": "Michael",
                    "last_name": "Wimmer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "306405",
                    "first_name": "Renata Georgia",
                    "last_name": "Raidou",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "57469",
                    "first_name": "Markus",
                    "last_name": "Bögl",
                    "role": "Lecturer"
                },
                {
                    "tid": "289877",
                    "first_name": "Davide",
                    "last_name": "Ceneda",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E180"
            ],
            "cid": "180775-2024S",
            "semester": "2024S",
            "nr": "180775",
            "nr_pretty": "180.775",
            "type": "SE",
            "title": "Seminar for Master Students in Media and Human-Centered Computing",
            "hours": "1.0",
            "language": "English",
            "objective": "<p>-</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=180775&semester=2024S",
            "people": [
                {
                    "tid": "271598",
                    "first_name": "Astrid",
                    "last_name": "Weiss",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E194"
            ],
            "cid": "188081-2024S",
            "semester": "2024S",
            "nr": "188081",
            "nr_pretty": "188.081",
            "type": "SE",
            "title": "Research seminar for PhD students",
            "hours": "2.0",
            "language": "German",
            "objective": "<p>New research results are investigated</p> \n<p>&nbsp;</p> \n<p><strong>Topics</strong></p> \n<p>&nbsp;</p> \n<ul> \n <li>Silvia Miksch (Information Visualization, Visual Analytics, Plan Management):&nbsp;<a href=\"http://www.cvast.tuwien.ac.at/topics\" rel=\"nofollow\">http://www.cvast.tuwien.ac.at/topics <br></a></li> \n <li>Stefan Biffl: http://qse.ifs.tuwien.ac.at</li> \n</ul>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188081&semester=2024S",
            "people": [
                {
                    "tid": "39902",
                    "first_name": "A Min",
                    "last_name": "Tjoa",
                    "role": "Lecturer"
                },
                {
                    "tid": "39608",
                    "first_name": "Andreas",
                    "last_name": "Rauber",
                    "role": "Lecturer"
                },
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "279186",
                    "first_name": "Dimitrios",
                    "last_name": "Simos",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "47426",
                    "first_name": "Peter",
                    "last_name": "Knees",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "54162",
                    "first_name": "Edgar",
                    "last_name": "Weippl",
                    "role": "Lecturer"
                },
                {
                    "tid": "154644",
                    "first_name": "Gerald",
                    "last_name": "Futschek",
                    "role": "Lecturer"
                },
                {
                    "tid": "124531",
                    "first_name": "Stefan",
                    "last_name": "Biffl",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E194"
            ],
            "cid": "188082-2024S",
            "semester": "2024S",
            "nr": "188082",
            "nr_pretty": "188.082",
            "type": "SE",
            "title": "Seminar: Literature-work for PhD students",
            "hours": "2.0",
            "language": "German",
            "objective": "<p>Literature inquiries for research work</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188082&semester=2024S",
            "people": [
                {
                    "tid": "39902",
                    "first_name": "A Min",
                    "last_name": "Tjoa",
                    "role": "Lecturer"
                },
                {
                    "tid": "39608",
                    "first_name": "Andreas",
                    "last_name": "Rauber",
                    "role": "Lecturer"
                },
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "54162",
                    "first_name": "Edgar",
                    "last_name": "Weippl",
                    "role": "Lecturer"
                },
                {
                    "tid": "154644",
                    "first_name": "Gerald",
                    "last_name": "Futschek",
                    "role": "Lecturer"
                },
                {
                    "tid": "68412",
                    "first_name": "Jürgen",
                    "last_name": "Cito",
                    "role": "Lecturer"
                },
                {
                    "tid": "124531",
                    "first_name": "Stefan",
                    "last_name": "Biffl",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188456-2024S",
            "semester": "2024S",
            "nr": "188456",
            "nr_pretty": "188.456",
            "type": "PR",
            "title": "Virtual and Augmented Reality: Advanced Topics",
            "hours": "2.0",
            "language": "English",
            "objective": "<p>Gaining deeper insight into advanced topics of Virtual and Augmented Reality based on the basic course \"Virtual and Augmented Reality\" from winter term.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188456&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "357410",
                    "first_name": "Francesco",
                    "last_name": "De Pace",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188499-2024S",
            "semester": "2024S",
            "nr": "188499",
            "nr_pretty": "188.499",
            "type": "VU",
            "title": "Media and Brain 2 Artificial Consciousness",
            "hours": "2.0",
            "language": "English",
            "objective": "<p>New model lecture on media perception and media reception emphasizing the cybernetic neural processes involved. We aim at establishing a link between media theory and media analysis by employing artificial intelligence and machine learning.</p> \n<p><strong>Topic 2019 is</strong> <strong>artificial consciousness, for example, in deep neural networks. </strong>Some aspects will be: analogical reasoning, creativity, lying, killer robots &amp; ethics in AI, etc.<strong><br></strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188499&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188502-2024S",
            "semester": "2024S",
            "nr": "188502",
            "nr_pretty": "188.502",
            "type": "VU",
            "title": "Media and Brain 1: Artificial Consciousness",
            "hours": "2.0",
            "language": "English",
            "objective": "<p>New model lecture on media perception and media reception emphasizing the cybernetic neural processes involved. We aim at establishing a link between media theory and media analysis by employing artificial intelligence and machine learning.</p> \n<p><strong>Topic 2019 is</strong> <strong>artificial consciousness, for example, in deep neural networks. </strong>Some aspects will be: analogical reasoning, creativity, lying, killer robots &amp; ethics in AI, etc.<strong><br></strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188502&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188519-2024S",
            "semester": "2024S",
            "nr": "188519",
            "nr_pretty": "188.519",
            "type": "UE",
            "title": "Ruby on Rails Business Programming",
            "hours": "2.0",
            "language": "if required in English",
            "objective": "<p>The students learn to:</p> \n<ul> \n <li>Program in the programming language Ruby</li> \n <li>Generate and adapt Ruby on Rails applications</li> \n <li>Implement business processes in Ruby</li> \n <li>Use REST interfaces for data communication</li> \n <li>Install and maintain a Ruby server</li> \n</ul> \n<p>All exercises are performed in groups based on an agile development process and implement projects in the context of the TU information systems.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188519&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188520-2024S",
            "semester": "2024S",
            "nr": "188520",
            "nr_pretty": "188.520",
            "type": "PR",
            "title": "Virtual Reality Maker Lab",
            "hours": "6.0",
            "language": "if required in English",
            "objective": "<p>Construction of novel appliances for movement in virtual reality applications such as jumping, flying, hovering, etc. Construction comprises the process of design and implementation of the hardware as well as the provision of control APIs and the proper documentation of the project. The actual programming of virtual reality applications (e.g. in Unity) is not in the scope of this course. Students interested in virtual reality programming may refer to the courses of the virtual reality module in the media informatics master program.</p> \n<p>Please find topics on my <a href=\"http://www.ifs.tuwien.ac.at/~heidenbe/#tasks\" rel=\"nofollow\">teaching website</a>.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188520&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "188981-2024S",
            "semester": "2024S",
            "nr": "188981",
            "nr_pretty": "188.981",
            "type": "VU",
            "title": "Strategy Game Programming",
            "hours": "2.0",
            "language": "English",
            "objective": "<ol> \n <li>Students understand the fundamental setting of strategy games.</li> \n <li>Students understand the fundamental algorithms of AI game engines.</li> \n <li>Students have experience in the implementation of the Minimax algorithms and related heuristics.</li> \n <li>Students have experience in the implementation of Monte Carlo Tree Sampling and related heuristics (UCB1, RAVE, etc.).</li> \n</ol>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=188981&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193021-2024S",
            "semester": "2024S",
            "nr": "193021",
            "nr_pretty": "193.021",
            "type": "PR",
            "title": "Project Media and Human-Centered Computing 1",
            "hours": "4.0",
            "language": "English",
            "objective": "<p>Undertake a project in media and human-centered computing.&nbsp;</p>\n<p><strong>==============================================</strong></p>\n<p><strong>Group 193-07 (CVAST Team: Silvia Miksch and Team)</strong></p>\n<p><strong>Topics</strong></p>\n<ul>\n<li><a href=\"http://www.cvast.tuwien.ac.at/topics\" rel=\"nofollow\">http://www.cvast.tuwien.ac.at/topics&nbsp;</a></li>\n</ul>\n<p><strong>TUWEL</strong></p>\n<ul>\n<li><a href=\"https://tuwel.tuwien.ac.at/course/view.php?id=62762\" rel=\"nofollow\">https://tuwel.tuwien.ac.at/course/view.php?id=62762</a></li>\n<li><strong><em>IMPORTANT: All important information and dates are in the TUWEL course</em></strong></li>\n</ul>\n<p><strong>==============================================</strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193021&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "139584",
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "271598",
                    "first_name": "Astrid",
                    "last_name": "Weiss",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "346648",
                    "first_name": "Khaled",
                    "last_name": "Kassem",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                },
                {
                    "tid": "347351",
                    "first_name": "Ambika",
                    "last_name": "Shahu",
                    "role": "Lecturer"
                },
                {
                    "tid": "333566",
                    "first_name": "Florian",
                    "last_name": "Michahelles",
                    "role": "Lecturer"
                },
                {
                    "tid": "284695",
                    "first_name": "Katta",
                    "last_name": "Spiel",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "372512",
                    "first_name": "Pedro",
                    "last_name": "Hermosilla Casajus",
                    "role": "Lecturer"
                },
                {
                    "tid": "372533",
                    "first_name": "Florian",
                    "last_name": "Wolling",
                    "role": "Lecturer"
                },
                {
                    "tid": "384234",
                    "first_name": "Kevin Marc",
                    "last_name": "Blasiak",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "57469",
                    "first_name": "Markus",
                    "last_name": "Bögl",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193022-2024S",
            "semester": "2024S",
            "nr": "193022",
            "nr_pretty": "193.022",
            "type": "PR",
            "title": "Project Media and Human-Centered Computing 2",
            "hours": "4.0",
            "language": "English",
            "objective": "<p>Undertake a project in media and human-centered computing.&nbsp;</p>\n<p><strong>==============================================</strong></p>\n<p><strong>Group 193-07 (CVAST Team: Silvia Miksch and Team)</strong></p>\n<p><strong>Topics</strong></p>\n<ul>\n<li><a href=\"http://www.cvast.tuwien.ac.at/topics\" rel=\"nofollow\">http://www.cvast.tuwien.ac.at/topics&nbsp;</a></li>\n</ul>\n<p><strong>TUWEL</strong></p>\n<ul>\n<li><a href=\"https://tuwel.tuwien.ac.at/course/view.php?id=62762\" rel=\"nofollow\">https://tuwel.tuwien.ac.at/course/view.php?id=62762</a></li>\n<li><strong><em>IMPORTANT: All important information and dates are in the TUWEL course</em></strong></li>\n</ul>\n<p><strong>==============================================</strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193022&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "139584",
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "271598",
                    "first_name": "Astrid",
                    "last_name": "Weiss",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "346648",
                    "first_name": "Khaled",
                    "last_name": "Kassem",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                },
                {
                    "tid": "347351",
                    "first_name": "Ambika",
                    "last_name": "Shahu",
                    "role": "Lecturer"
                },
                {
                    "tid": "333566",
                    "first_name": "Florian",
                    "last_name": "Michahelles",
                    "role": "Lecturer"
                },
                {
                    "tid": "284695",
                    "first_name": "Katta",
                    "last_name": "Spiel",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "372512",
                    "first_name": "Pedro",
                    "last_name": "Hermosilla Casajus",
                    "role": "Lecturer"
                },
                {
                    "tid": "372533",
                    "first_name": "Florian",
                    "last_name": "Wolling",
                    "role": "Lecturer"
                },
                {
                    "tid": "384234",
                    "first_name": "Kevin Marc",
                    "last_name": "Blasiak",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "57469",
                    "first_name": "Markus",
                    "last_name": "Bögl",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193023-2024S",
            "semester": "2024S",
            "nr": "193023",
            "nr_pretty": "193.023",
            "type": "PR",
            "title": "Project in Visual Computing 1",
            "hours": "4.0",
            "language": "English",
            "objective": "<p>Implementation of the topics treated in the basic lectures within a larger, technically oriented software project.</p>\n<p><strong>==============================================</strong></p>\n<p><strong>Group 193-07 (CVAST Team: Silvia Miksch and Team)</strong></p>\n<p><strong>Topics</strong></p>\n<ul>\n<li><a href=\"http://www.cvast.tuwien.ac.at/topics\" rel=\"nofollow\">http://www.cvast.tuwien.ac.at/topics&nbsp;</a></li>\n</ul>\n<p><strong>TUWEL</strong></p>\n<ul>\n<li><a href=\"https://tuwel.tuwien.ac.at/course/view.php?id=62762\" rel=\"nofollow\">https://tuwel.tuwien.ac.at/course/view.php?id=62762</a></li>\n<li><strong><em>IMPORTANT: All important information and dates are in the TUWEL course</em></strong></li>\n</ul>\n<p><strong>==============================================</strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193023&semester=2024S",
            "people": [
                {
                    "tid": "269459",
                    "first_name": "Manuela",
                    "last_name": "Waldner",
                    "role": "Lecturer"
                },
                {
                    "tid": "40666",
                    "first_name": "Michael",
                    "last_name": "Wimmer",
                    "role": "Lecturer"
                },
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "139584",
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "306405",
                    "first_name": "Renata Georgia",
                    "last_name": "Raidou",
                    "role": "Lecturer"
                },
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "372512",
                    "first_name": "Pedro",
                    "last_name": "Hermosilla Casajus",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "57469",
                    "first_name": "Markus",
                    "last_name": "Bögl",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193024-2024S",
            "semester": "2024S",
            "nr": "193024",
            "nr_pretty": "193.024",
            "type": "PR",
            "title": "Project in Visual Computing 2",
            "hours": "4.0",
            "language": "English",
            "objective": "<p>Implementation of the topics treated in the basic lectures within a larger, technically oriented software project.</p>\n<p><strong>==============================================</strong></p>\n<p><strong>Group 193-07 (CVAST Team: Silvia Miksch and Team)</strong></p>\n<p><strong>Topics</strong></p>\n<ul>\n<li><a href=\"http://www.cvast.tuwien.ac.at/topics\" rel=\"nofollow\">http://www.cvast.tuwien.ac.at/topics&nbsp;</a></li>\n</ul>\n<p><strong>TUWEL</strong></p>\n<ul>\n<li><a href=\"https://tuwel.tuwien.ac.at/course/view.php?id=62762\" rel=\"nofollow\">https://tuwel.tuwien.ac.at/course/view.php?id=62762</a></li>\n<li><strong><em>IMPORTANT: All important information and dates are in the TUWEL course</em></strong></li>\n</ul>\n<p><strong>==============================================</strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193024&semester=2024S",
            "people": [
                {
                    "tid": "269459",
                    "first_name": "Manuela",
                    "last_name": "Waldner",
                    "role": "Lecturer"
                },
                {
                    "tid": "40666",
                    "first_name": "Michael",
                    "last_name": "Wimmer",
                    "role": "Lecturer"
                },
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "306405",
                    "first_name": "Renata Georgia",
                    "last_name": "Raidou",
                    "role": "Lecturer"
                },
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "372512",
                    "first_name": "Pedro",
                    "last_name": "Hermosilla Casajus",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "57469",
                    "first_name": "Markus",
                    "last_name": "Bögl",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193116-2024S",
            "semester": "2024S",
            "nr": "193116",
            "nr_pretty": "193.116",
            "type": "PR",
            "title": "Bachelor Thesis",
            "hours": "5.0",
            "language": "if required in English",
            "objective": "<p>Topics will be discussed with the supervisor.</p>\n<p>List of topics: <a href=\"https://www.vr.tuwien.ac.at/topics/\" rel=\"nofollow\">https://www.vr.tuwien.ac.at/topics/</a>.</p>\n<p>&nbsp;</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193116&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193117-2024S",
            "semester": "2024S",
            "nr": "193117",
            "nr_pretty": "193.117",
            "type": "PR",
            "title": "Project in Computer Science 1",
            "hours": "4.0",
            "language": "if required in English",
            "objective": "<p>During the course of this project students will realize a practical assignment beginning with a coarse-grained requirements definition culminating in a final prototypical realization. The exact definition of your project assignment will be done together with your advisor and depends on the chosen topic, the topic's complexity, and the size of your team (if you are working in one).</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193117&semester=2024S",
            "people": [
                {
                    "tid": "139584",
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                },
                {
                    "tid": "372512",
                    "first_name": "Pedro",
                    "last_name": "Hermosilla Casajus",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193118-2024S",
            "semester": "2024S",
            "nr": "193118",
            "nr_pretty": "193.118",
            "type": "PR",
            "title": "Project in Computer Science 2",
            "hours": "4.0",
            "language": "if required in English",
            "objective": "<p>During the course of this project students will realize a practical assignment beginning with a coarse-grained requirements definition culminating in a final prototypical realization. The exact definition of your project assignment will be done together with your advisor and depends on the chosen topic, the topic's complexity, and the size of your team (if you are working in one).</p>\n<p><strong>==============================================</strong><br> <strong>Group 193-07 (CVAST Team: Silvia Miksch and Team)</strong></p>\n<p><strong>Topics</strong></p>\n<ul>\n<li><a href=\"http://www.cvast.tuwien.ac.at/topics\" rel=\"nofollow\">http://www.cvast.tuwien.ac.at/topics&nbsp;</a></li>\n</ul>\n<p><strong>TUWEL</strong></p>\n<ul>\n<li><a href=\"https://tuwel.tuwien.ac.at/course/view.php?id=50092\" rel=\"nofollow\">https://tuwel.tuwien.ac.at/course/view.php?id=50092</a>&nbsp;</li>\n<li><strong><em>IMPORTANT: All important information and dates are in the TUWEL course</em></strong></li>\n</ul>\n<p><strong>==============================================</strong></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193118&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Lecturer"
                },
                {
                    "tid": "139584",
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "role": "Lecturer"
                },
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "134749",
                    "first_name": "Silvia",
                    "last_name": "Miksch",
                    "role": "Lecturer"
                },
                {
                    "tid": "215561",
                    "first_name": "Hilda",
                    "last_name": "Tellioglu",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                },
                {
                    "tid": "372512",
                    "first_name": "Pedro",
                    "last_name": "Hermosilla Casajus",
                    "role": "Lecturer"
                },
                {
                    "tid": "49535",
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "role": "Lecturer"
                },
                {
                    "tid": "63348",
                    "first_name": "Wolfgang",
                    "last_name": "Aigner",
                    "role": "Lecturer"
                },
                {
                    "tid": "57469",
                    "first_name": "Markus",
                    "last_name": "Bögl",
                    "role": "Lecturer"
                },
                {
                    "tid": "143572",
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "role": "Lecturer"
                },
                {
                    "tid": "38392",
                    "first_name": "Margrit",
                    "last_name": "Gelautz",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193119-2024S",
            "semester": "2024S",
            "nr": "193119",
            "nr_pretty": "193.119",
            "type": "SE",
            "title": "Research seminar for PhD students",
            "hours": "2.0",
            "language": "English",
            "objective": "<p>Presentation of challenging topics in the fields of Virtual and Augmented Reality.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193119&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193140-2024S",
            "semester": "2024S",
            "nr": "193140",
            "nr_pretty": "193.140",
            "type": "VU",
            "title": "Audio and Video Production",
            "hours": "4.0",
            "language": "English",
            "objective": "<p>The following topics will covered:<br>* Framing and Composition<br>* Storytelling/Scriptwriting<br>* Recording techniques<br>* Lighting techniques<br>* Editing<br>* 360º Video<br>* Sound design<br>* Recording voice<br>* Editing<br>* Mixing (equalization and dynamic processing)<br>* Overview of DAWs<br>* Psychoacoustics fundamentals</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193140&semester=2024S",
            "people": [
                {
                    "tid": "44691",
                    "first_name": "Karyn",
                    "last_name": "Laudisi",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193144-2024S",
            "semester": "2024S",
            "nr": "193144",
            "nr_pretty": "193.144",
            "type": "PR",
            "title": "Computation Center Internship",
            "hours": "12.0",
            "language": "if required in English",
            "objective": "<p>The main contents are:</p>\n<ol>\n<li>Getting to know the work in a computer center at TU Wien.</li>\n<li>Working in the network team to get to know the various tasks, problems and solutions.</li>\n<li>Working in the server and storage area to get to know basic methods in the Platform-as-a-Service area.</li>\n<li>Reflection of experiences and evaluation of personal interests.</li>\n</ol>\n<p>In this internship, students come into practical contact with the following technologies, among others: Platform as a service, VMware, Kubernetes, OpenShift, Firewall as a service, FC networks, Wifi networks, network operations center, SAN technology, etc.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193144&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193145-2024S",
            "semester": "2024S",
            "nr": "193145",
            "nr_pretty": "193.145",
            "type": "UE",
            "title": "Bug Bounty Program of TU Wien",
            "hours": "2.0",
            "language": "English",
            "objective": "<p>Similarly to a <a href=\"https://en.wikipedia.org/wiki/Bug_bounty_program\" rel=\"nofollow\">bug bounty program</a>, students will be tasked with identifying and reporting security vulnerabilities on TU Wien's IT systems. Participants will gain hands-on experience in security testing, including web application security, network security, and software security. The course will provide insights on standard tools and techniques to identify and exploit security vulnerabilities, as well as documenting and reporting their findings. Additional topics will include legal and ethical aspects of security testing, responsible disclosure, mitigation of security vulnerabilities, standard metrics for rating the severity of security issues (CVEs), and the adoption of bug bounty programs in the industry.</p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193145&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                },
                {
                    "tid": "323023",
                    "first_name": "Marco",
                    "last_name": "Squarcina",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "193148-2024S",
            "semester": "2024S",
            "nr": "193148",
            "nr_pretty": "193.148",
            "type": "VU",
            "title": "Multimedia",
            "hours": "4.0",
            "language": "English",
            "objective": "<p>Regarding the theoretical part, the lecture will be in German with English slides. Content will cover topics such as: &nbsp;</p>\n<ul>\n<li>Time-dependent and independent media types (text, audio, images, video) and details about media types</li>\n<li>Psychoacoustics</li>\n<li>Digitization of signals</li>\n<li>Data compression (JPEG, MPEG-1 &amp; MP3, MPEG-2, H.264, H.265)</li>\n<li>Current multimedia installations</li>\n</ul>\n<p>Regarding the practical part, the tutorials will be in English with English slides. Content will cover topics such as:</p>\n<ul>\n<li>media programming&nbsp;</li>\n<li>handling of multimedia (video, audio, graphics, animations,...)&nbsp;</li>\n<li>basics of Unity3D</li>\n<li>development of a multimedia application</li>\n</ul>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193148&semester=2024S",
            "people": [
                {
                    "tid": "252414",
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "role": "Lecturer"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Lecturer"
                },
                {
                    "tid": "359272",
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "role": "Lecturer"
                }
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "cid": "194104-2024S",
            "semester": "2024S",
            "nr": "194104",
            "nr_pretty": "194.104",
            "type": "PR",
            "title": "Mobile (App) Prototyping and Evaluation",
            "hours": "4.0",
            "language": "if required in English",
            "objective": "<p>Topics and contents are defined individually with the supervisor: <a href=\"https://www.vr.tuwien.ac.at/topics/\" rel=\"nofollow\">Web page with current topics</a></p>",
            "website": "https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=194104&semester=2024S",
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Lecturer"
                }
            ]
        }
    ],
    "projects": [
        {
            "coordinator_org_nrs": [
                "E193-02"
            ],
            "member_org_nrs": [],
            "tid": "1366370",
            "name": "Gebäudesoftskills",
            "title": "Qualification Network for Human Sciences and Structural Engineering",
            "website": null,
            "begins_on": "2017-01-01",
            "ends_on": "2018-12-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "Forschungskompetenzen für die Wirtschaft",
                        "Structural programme"
                    ],
                    "call": "FoKo Qualifizierungsnetze 2015 3. Ausschreibung",
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-06"
            ],
            "member_org_nrs": [],
            "tid": "1370201",
            "name": "VR Sehhilfe",
            "title": "Feasibility Study: iCplus - Vr based Reading Device for Maculadegeneration",
            "website": null,
            "begins_on": "2016-12-01",
            "ends_on": "2017-12-31",
            "context": "DI Ewald Weizenbauer",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "DI Ewald Weizenbauer",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-06"
            ],
            "member_org_nrs": [],
            "tid": "1372951",
            "name": "IRYStec Studie",
            "title": "Influence of Head Mounted Display Brightness on Cybersickness",
            "website": null,
            "begins_on": "2016-12-15",
            "ends_on": "2017-09-30",
            "context": "IRYStec Software inc.",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "IRYStec Software inc.",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-02"
            ],
            "member_org_nrs": [],
            "tid": "1427227",
            "name": "EvaluARte",
            "title": "Systematic Evaluation for AR Controllers",
            "website": null,
            "begins_on": "2017-08-01",
            "ends_on": "2019-07-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "Bridge",
                        "Structural programme"
                    ],
                    "call": "25. Aussschreibung",
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E194-01"
            ],
            "member_org_nrs": [],
            "tid": "1515667",
            "name": "DeepOMR",
            "title": "Optical Music Recognition with Deep Learning",
            "website": null,
            "begins_on": "2017-03-01",
            "ends_on": "2020-02-28",
            "context": "Universidad de Alicante",
            "abstract": "<p><i>Optical Music Recognition (OMR) aims at automatically processing written music scores, comparable to Optical Character Recognition for text, but significantly more complex. The goal is to teach the computer to \"understand\" music scores. The potential applications are manifold, reaching from digitization for preservation and enabling to edit music scores easily to simply playing music back or accompanying musicians that practice their performance.</i></p> \n<p><i>OMR has been subject to research for many years and although there are even commercial products, many questions of OMR remain open. Especially when dealing with complex scenarios, like orchestral scores, handwritten drafts or deteriorated manuscripts, the state-of-the-art is far from perfect.</i></p> \n<p><i>The goal of this project is to improve the state-of-the-art by applying machine learning, i.e. deep learning to reconstruct the visual information and the semantic interpretation. This should enable to build an extensible, robust OMR system, which is capable of handling complex scenarios with high accuracy, comparable to human performance.</i></p>",
            "keywords": [],
            "people": [
                {
                    "tid": "68727",
                    "first_name": "Alexander",
                    "last_name": "Pacha",
                    "role": "Subleader"
                },
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "Charles University",
                    "city": "Prague",
                    "country": "Czech Republic"
                },
                {
                    "name": "McGill University",
                    "city": "Quebec",
                    "country": "Canada"
                },
                {
                    "name": "Universidad de Alicante",
                    "city": "ORT",
                    "country": "Spain"
                },
                {
                    "name": "Universität Rennes",
                    "city": "Rennes",
                    "country": "France"
                }
            ],
            "fundings": []
        },
        {
            "coordinator_org_nrs": [
                "E193-02"
            ],
            "member_org_nrs": [],
            "tid": "1534385",
            "name": "Haas VR",
            "title": "Presentation of virtual machines",
            "website": null,
            "begins_on": "2018-04-01",
            "ends_on": "2018-10-31",
            "context": "Haas Food Equipment GmbH",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "Haas Food Equipment GmbH",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [
                "E193-01",
                "E194-04"
            ],
            "tid": "1559899",
            "name": "VHH",
            "title": "Visual History of the Holocaust: Rethinking Curation in the Digital Age",
            "website": "https://cvl.tuwien.ac.at/project/visual-history-of-the-holocaust/",
            "begins_on": "2019-01-01",
            "ends_on": "2023-03-31",
            "context": "European Commission",
            "abstract": "<p>This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 822670.</p>",
            "keywords": [],
            "people": [
                {
                    "tid": "133566",
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "role": "Leader"
                },
                {
                    "tid": "159676",
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "role": "Subleader"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                },
                {
                    "tid": "48222",
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "Bundesanstalt KZ-Gedenkstätte Mauth Mauthausen Memorial",
                    "city": "Mauthausen",
                    "country": "Austria"
                },
                {
                    "name": "Centre National de la Recherche Scientifique",
                    "city": "Paris",
                    "country": "France"
                },
                {
                    "name": "Deutsches Filminstitut",
                    "city": "Frankfurt am Main",
                    "country": "Germany"
                },
                {
                    "name": "Fritz Bauer Institut",
                    "city": "Frankfurt am Main",
                    "country": "Germany"
                },
                {
                    "name": "Hebräische Universität Jerusalem",
                    "city": "Jerusalem",
                    "country": "Israel"
                },
                {
                    "name": "Justus-Liebig-Universität",
                    "city": "Giessen",
                    "country": "Germany"
                },
                {
                    "name": "Ludwig Boltzmann Gesellschaft (LBG)",
                    "city": "Wien",
                    "country": "Austria"
                },
                {
                    "name": "Max.Recall Information System GmbH",
                    "city": "Wien",
                    "country": "Austria"
                },
                {
                    "name": "Stiftung Bayrische Gedenkstätten",
                    "city": "München",
                    "country": "Germany"
                },
                {
                    "name": "Stiftung Niedersächsische Gedenkstätten",
                    "city": "Celle",
                    "country": "Germany"
                },
                {
                    "name": "Universität Bremen",
                    "city": "Bremen",
                    "country": "Germany"
                },
                {
                    "name": "Österreichisches Filmmuseum",
                    "city": "Wien",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "European Commission",
                    "region": "EU",
                    "group": null,
                    "body": "European Commission",
                    "programs": [
                        "H2020 III.6 Europe in a changing world - inclusive, innovative a. reflective Societies",
                        "Societal Challenges",
                        "Horizon 2020"
                    ],
                    "call": "H2020-SC6-Transormations-2018-2019-2020",
                    "proposal": "SEP-210499877"
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E234-01"
            ],
            "member_org_nrs": [
                "E193-02"
            ],
            "tid": "1577824",
            "name": "AR-AQ-Bau",
            "title": "Use of Augmented Reality for Building Inspection and Quality Assurance on Construction Sites",
            "website": null,
            "begins_on": "2018-10-01",
            "ends_on": "2020-12-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": "<p><span style=\"color: black;\">The aim of this research project is the development of a construction site-suitable augmented reality (AR) system included a Remote-Expert-System and a BIM-Closed-Loop data transfer system for improving the quality of construction, building security and energy efficiency as well as increasing the efficiency of construction investigation.</span></p>",
            "keywords": [
                "Augmented Reality"
            ],
            "people": [
                {
                    "tid": "111250",
                    "first_name": "Gerald",
                    "last_name": "Goger",
                    "role": "Member"
                },
                {
                    "tid": "253092",
                    "first_name": "Harald",
                    "last_name": "Urban",
                    "role": "Subleader"
                },
                {
                    "tid": "254350",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "36976",
                    "first_name": "Christian",
                    "last_name": "Schranz",
                    "role": "Leader"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "Stadt der Zukunft",
                        "Thematic programme"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "165772",
            "name": "ACTO",
            "title": "Evaluation of Modular Actuated Tangible User Interfaces",
            "website": null,
            "begins_on": "2012-09-01",
            "ends_on": "2014-09-30",
            "context": "Federal Ministry of Science and Research (bm:wf)",
            "abstract": "<p>\"Tangible Computing\" is an active area of research in the field of human-computer-interaction. Its intention is the design of intuitive and meaningful user interfaces through a tight coupling of physical and virtual objects. Virtual objects are represented by a physical representation, which enhances interaction by feeling and direct physical manipulation, making the interaction \"tangible\". A key aspect are so-called \"Tangible User Interface Objects\" (short \"tangibles\"). These physical object are typically used on an interactive work surface to allow the manipulation of virtual objects and models. Tangible user interfaces provide unique properties like intuitive use, immediate feedback and minimal necessary visual attention.</p> \n<p>Existing systems utilize physical objects in interaction tasks for input only, enabling the user to communicate changes to the system. Changes of the virtual objects on the other hand are not reported to the user in a similar way, since the employed tangibles do not have any output channels. Self-positioning tangibles, so-called \"actuated\" tangibles, have been researched only sparsely so far; all existing solutions have significant technical limitations and require considerable construction effort.</p> \n<p>In the course of this project we developed new \"Actuated Tangible User Interface Objects\" (short ACTOs) in order to address this situation. A flexible and modular design allows the integration of different input and output devices, ranging from simple buttons over complex sensors to graphical displays or active mechanical components. Typically integral parts like actuation mechanism or tracking technology can be individually customized and interchanged. This makes the ACTO system ideal for research and education.</p> \n<p>&nbsp;---</p> \n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Calibri;mso-bidi-theme-font:\nminor-latin\">„Tangible Computing“ ist ein aktives Forschungsfeld im Bereich Mensch-Computer-Interaktion. Das Ziel ist die Entwicklung intuitiver und ausdrucksstarker Benutzerschnittstellen durch stärkere Kopplung von physischen und virtuellen Objekten. Dabei wird virtuellen Gegenständen eine physische Repräsentation verliehen, wodurch sie leichter erfahrbar und manipulierbar, also „Tangible“ (engl. greifbar, fühlbar, erlebbar), werden. Einen Schwerpunkt der Forschung stellen sogenannte „Tangible User Interface Objects“ (kurz Tangibles) dar. Dabei handelt es sich um physische Objekte welche auf einer interaktiven Arbeitsfläche benutzt werden, um virtuelle Objekte und Modelle zu manipulieren. Angreifbare Benutzerschnittstellen bieten einzigartige Eigenschaften, wie beispielsweise intuitive Verwendbarkeit, unmittelbares Feedback und minimale notwendige visuelle Aufmerksamkeit.</span></p> \n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Calibri;mso-bidi-theme-font:\nminor-latin\">Existierende Systeme verwenden meist physische Interaktionsobjekte ausschließlich zur Eingabe, um Änderungen durch einen Benutzer an das System weiterzuleiten. Änderungen der virtuellen Objekte können aber nicht auf gleiche Weise an den Benutzer zurückgegeben werden, da die eingesetzten Tangibles über keinerlei Ausgabekanäle verfügen. Selbst positionierbare, sogenannte „Actuated“ (engl. in Gang gebracht, ausgelöst, betätigt) Tangibles, wurden bisher nur wenig erforscht. Alle existierenden Lösungen haben große technische Einschränkungen und stellen außerdem einen großen Konstruktionsaufwand dar.</span></p> \n<p class=\"MsoNormal\"><span style=\"mso-bidi-font-family:Calibri;mso-bidi-theme-font:\nminor-latin\">Im Rahmen dieses Projekts wurden daher neue „Actuated Tangible User Interface Objects“ (kurz ACTOs) entwickelt, welche diese Einschränkungen aufheben. Ein flexibles, modulares Design ermöglicht die Integration unterschiedlichster Ein- und Ausgabemodalitäten von einfachen Buttons über komplexe Sensoren bis zu Displays und aktiven mechanischen Komponenten. Üblicherweise unveränderbare Bestandteile wie der Fortbewegungsmechanismus oder die Technik zur Positionsbestimmung können individuell angepasst und ausgetauscht werden. Damit bietet sich unsere ACTO-Plattform besonders für den Einsatz zu Forschungszwecken und in pädagogischem Kontext an.</span></p> \n<p>&nbsp;</p>",
            "keywords": [
                "Tangible User Interfaces",
                "tabletop interaction",
                "actuation",
                "prototyping platform",
                "widgets",
                "haptics"
            ],
            "people": [
                {
                    "tid": "40682",
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "role": "Member"
                },
                {
                    "tid": "40923",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "Höhere technische Bundes- Lehr und Versuchsanstalt",
                    "city": "Spengergasse 20, A-1050",
                    "country": "Austria"
                },
                {
                    "name": "Österreichische Gesellschaft für innovative Computerwissenschaften (INNOC)",
                    "city": "Haussteinstraße 4/2, A-1020 Wien",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "BM für Wissenschaft, Forschung und Wirtschaft (bm:wfw)",
                    "region": "National",
                    "group": null,
                    "body": "Federal Ministry of Science and Research (bm:wf)",
                    "programs": [],
                    "call": "Sparkling Science",
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E210-01"
            ],
            "member_org_nrs": [
                "E193-03"
            ],
            "tid": "1734269",
            "name": "BIM_Flexi",
            "title": "BIM-based digital Plattform for design and optimisation of flexible facilities for Industry 4.0",
            "website": null,
            "begins_on": "2020-03-01",
            "ends_on": "2022-08-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "42543",
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "role": "Leader"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "BRIDGE",
                        "BRIDGE 1",
                        "BRIDGE",
                        "Basic programs"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "1739399",
            "name": "VR Tennis Trainer",
            "title": "Virtual Reality Tennis Trainer",
            "website": null,
            "begins_on": "2020-02-01",
            "ends_on": "2022-10-31",
            "context": "VR Motion Learning GmbH & Co KG",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "VR Motion Learning GmbH & Co KG",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-02"
            ],
            "member_org_nrs": [
                "E104-03",
                "E104-04",
                "E193-03",
                "E202-02",
                "E210-01",
                "E259-01"
            ],
            "tid": "1745908",
            "name": "ACD",
            "title": "Advanced Computational Design",
            "website": "http://acd.tuwien.ac.at",
            "begins_on": "2020-03-01",
            "ends_on": "2028-02-29",
            "context": "Austrian Science Fund (FWF)",
            "abstract": "<p><em style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\">Research question:</em><span style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\"> The main research question addressed by the SFB </span><strong style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\">“</strong><em style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\">Advanced Computational Design</em><strong style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\">”</strong><span style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\"> is how to advance design tools and processes through multi- and interdisciplinary basic research in the areas of digital architecture, integrated building design, computer graphics and virtual reality, discrete and applied geometry, and computational mechanics.&nbsp;</span></p>\n<p><em style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\">Wider research context:</em><span style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\"> Architecture, Engineering and Construction (AEC) shapes our built environment, exerting substantial environmental, cultural and economic influence on society. However, it is among the least digitized industries, still caught in silo-thinking and sequential planning processes. The Information and Communication Technology field, on the other hand, is highly innovative, creating digital design tools that are well-founded in basic research, but often lack relevant domain knowledge, thus hardly meeting designers’ needs.</span></p>\n<p><em style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\">Innovation:</em><span style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\"> We will connect architecture, computer science, mathematics and engineering in order to develop advanced computational design tools able to incorporate implicit and explicit design knowledge. This unique combination of scientific disciplines and collaborative research with strong theoretical foundations aims to bring radical innovation in computational design by allowing immediate feedback already in early design phases and by expanding solution spaces by computing design variants that cannot be found by current methods.</span></p>\n<p><em style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\">Approach:</em><span style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\"> The proposed research is structured in three areas: Design Methodology (A1), Visual and Haptic Design Interaction (A2) and Form Finding (A3). A1 uses ontologies to describe AEC design semantics, coupled to a novel digital mixed-reality sketchbook, and an innovative implicit modeling approach based on retrieving and enhancing 2D images and 3D point clouds through transfer learning. A1 also acts as a platform for integrating and evaluating the computational tools and methods developed in A2 and A3. These areas will investigate research questions in computational design involving algorithmic solutions. A2 investigates real-time global-illumination and optimization algorithms for lighting design, as well as a new method for large-scale haptic interactions in virtual reality based on a mobile robotic platform. In A3, form finding will be explored regarding geometric, mechanical and material constraints, in particular: paneling of complex shapes by patches of certain surface classes while optimizing the number of molds; algorithms for finding new transformable quad-surfaces; mechanical models for an efficient simulation of bio-composite material systems. Furthermore, new ways of form-finding will be explored experimentally, which will allow for validating the developed algorithmic approaches and reconsidering model assumptions and constraints.</span></p>\n<p><em style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\">Added value:</em><span style=\"background-color:transparent;color:rgb( 0 , 0 , 0 )\"> The fundamental computational tools and methods to be developed will be applicable in AEC and other fields of the creative industries. </span></p>",
            "keywords": [
                "computer graphics",
                "architectural geometry",
                "virtual reality",
                "interactive computational design",
                "form finding",
                "material-aware design optimization"
            ],
            "people": [
                {
                    "tid": "135460",
                    "first_name": "Peter",
                    "last_name": "Ferschin",
                    "role": "Leader"
                },
                {
                    "tid": "318163",
                    "first_name": "Ivan",
                    "last_name": "Izmestiev",
                    "role": "Leader"
                },
                {
                    "tid": "39140",
                    "first_name": "Martin",
                    "last_name": "Kilian",
                    "role": "Leader"
                },
                {
                    "tid": "40666",
                    "first_name": "Michael",
                    "last_name": "Wimmer",
                    "role": "Leader"
                },
                {
                    "tid": "42543",
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "role": "Leader"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                },
                {
                    "tid": "46732",
                    "first_name": "Josef",
                    "last_name": "Füssl",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "TU Graz",
                    "city": "Graz",
                    "country": "Austria"
                },
                {
                    "name": "Universität Innsbruck",
                    "city": "Innsbruck",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FWF - Österr. Wissenschaftsfonds",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Science Fund (FWF)",
                    "programs": [
                        "Special Research Program (SFB)"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "1753013",
            "name": "BRIDGES",
            "title": "A hyBRID (physical-diGital) multi-user Extended reality platform as a stimulus for industry uptake of interactive technologieS",
            "website": null,
            "begins_on": "2020-10-01",
            "ends_on": "2023-03-31",
            "context": "European Commission",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "European Commission",
                    "region": "EU",
                    "group": null,
                    "body": "European Commission",
                    "programs": [
                        "H2020 II.1.1 ICT  Information and Communication Technologies",
                        "Industrial Leadership",
                        "Horizon 2020"
                    ],
                    "call": "H2020-ICT-2019-3",
                    "proposal": "952043"
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "1754584",
            "name": "Watergate",
            "title": "Water's gateway to heaven: 3D imaging and modeling of transient stomatal responses in plant leaves under dynamic environments",
            "website": "https://www.prip.tuwien.ac.at/research/current_projects/wgh/",
            "begins_on": "2020-02-01",
            "ends_on": "2024-01-31",
            "context": "Vienna Science and Technology Fund (WWTF)",
            "abstract": "<p>Stomata are tiny pores on the surface of plant leaves and play a central role in global water and carbon cycles. The pores cover less than 5% of the leaf area but facilitate the majority of the exchange of gases between the atmosphere and terrestrial vegetation. The opening of stomata is adjusted to provide CO<sub>2</sub> for photosynthesis and to limit water loss. This process exhibits transient responses under fluctuating environmental conditions. The speed at which stomata respond influences productivity and water use efficiency of both crops and natural ecosystems. Although stomatal responses are a target for crop improvement, we lack a clear description of the process, impeding its complete mechanistic understanding. Novel temporal 3D imaging can address the need for a better description of stomatal movements. By complementing fast high-resolution X-ray microcomputed tomography (μCT) with fluorescence microscopy, we will provide in vivo 3D imaging of variations in epidermal cell size and shape and its effects on stomatal movements, scaling from subcellular to whole leaf traits. To fully harness the high volume of data generated by μCT, we will develop novel computational methods to automatically segment images and track single 3D cells over time. This project will answer long-standing questions about stomatal movements and will generate basic knowledge on how to improve stomatal responses under dynamic environments in order to increase net productivity and water-use efficiency.</p>",
            "keywords": [
                "Plants",
                "Stomata",
                "Turgor pressure",
                "Arabidopsis",
                "Deep learning",
                "Pattern recognition",
                "Fluorescence microscopy",
                "Microcomputed tomography",
                "Abstract cellular complexes",
                "Digital image representation",
                "Topology"
            ],
            "people": [
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Member"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                },
                {
                    "tid": "53161",
                    "first_name": "Jiri",
                    "last_name": "Hladuvka",
                    "role": "Subleader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "WWTF Wiener Wissenschafts-, Forschu und Technologiefonds",
                    "region": "National",
                    "group": null,
                    "body": "Vienna Science and Technology Fund (WWTF)",
                    "programs": [
                        "Life Sciences"
                    ],
                    "call": "LS19-013",
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "1832114",
            "name": "Embroideries",
            "title": "Patter Recognition for Embroideries",
            "website": null,
            "begins_on": "2020-08-14",
            "ends_on": "2022-01-15",
            "context": "Wiehler Gobelin e.U.",
            "abstract": "<p>The goal of the project is to create an application that is capable of semi-automatically digitizing scanned embroidery templates.</p>",
            "keywords": [],
            "people": [
                {
                    "tid": "53161",
                    "first_name": "Jiri",
                    "last_name": "Hladuvka",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "Wiehler Gobelin e.U.",
                    "city": "Maria-Enzersdorf",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "Wiehler Gobelin e.U.",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "1875977",
            "name": "BIMCheck",
            "title": "Smart automated check of BIM models with real buildings",
            "website": null,
            "begins_on": "2021-09-01",
            "ends_on": "2024-08-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "247245",
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "role": "Member"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                },
                {
                    "tid": "54835",
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "role": "Member"
                }
            ],
            "partners": [
                {
                    "name": "FH St.Pölten",
                    "city": "St.Pölten",
                    "country": "Austria"
                },
                {
                    "name": "rtech engineering GmbH",
                    "city": "Wien",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "BRIDGE 1",
                        "BRIDGE",
                        "Basic programs"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "1915420",
            "name": "DENOISING",
            "title": "Denoising for Real-Time Ray Tracing",
            "website": null,
            "begins_on": "2021-01-01",
            "ends_on": "2022-07-31",
            "context": "HUAWEI Technologies CO, Ltd.",
            "abstract": "<p>The project aims at leveraging specific image generation processes to improve quality and speed of denoise algorithms on mobile platform.</p>",
            "keywords": [
                "denoise"
            ],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "HUAWEI Technologies CO, Ltd.",
                    "city": "Shenzhen",
                    "country": "China"
                }
            ],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "HUAWEI Technologies CO, Ltd.",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "1923969",
            "name": "MRespond",
            "title": "Multi-User Mixed Reality System for flexible First Responder Training",
            "website": null,
            "begins_on": "2021-11-01",
            "ends_on": "2023-10-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": "<p>Effective action and the correct assessment of dangerous situations - especially in large-scale disaster operations - are essential for first-aiders from the fire brigade, rescue service and army. In order to acquire these skills, large-scale training with different first aiders in an environment that is as realistic as possible is necessary. However, such scenarios are often too dangerous, expensive or complex to simulate realistically enough - which reduces the training benefit. Mixed reality (MR) technologies can realistically depict virtual hazards within a physical training environment. There are currently isolated solutions with MR concepts, but these offer insufficient freedom of movement. Interaction between first responders and real/virtual objects is neglected.</p> \n<p>&nbsp;Adaptation options for the exercise leader - to practice different variants of a scenario - are limited. The aim of MRespond is to research technologies that bring about a significant improvement in MR training in the areas of freedom of movement, interaction and dynamic adaptability. Their suitability for use is checked with the support of the users. To this end, two large-scale deployment scenarios – a building fire and a scenario involving chemical hazards – are being developed and implemented. Management and operational forces have to deal with dangerous situations in an interdisciplinary manner. The main focus here is on the free mobility of all trainees in outdoor and multi-storey indoor areas, the interaction between real and virtual objects (e.g for the use of usual equipment) as well as the adaptability of the scenarios. This requires the conception of a robust localization and tracking of the trainees indoors and outdoors as well as suitable object recognition algorithms. Both visible (fire, smoke, ...) and invisible hazards (e.g. chemical substances detectable with measuring devices) can be placed virtually on the site.</p> \n<p>&nbsp;Emergency puppets represent casualties and are overlaid with virtual injuries to show realistic injury patterns.</p> \n<p>&nbsp;The actions of the trainees have an influence on virtual elements (e.g. smoke extraction when windows are opened, triage decisions). A trainer interface makes it possible to adapt the training process, place hazards and evaluate actions. The acceptance of training with virtual elements, training effectiveness and legal aspects of training standards are examined. Furthermore, research is carried out into the influence of professional experience and the gender of the trainees on success and whether diversity-specific differences (gender, age, ethnicity) occur in the treatment of virtual patients.</p>",
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "AIT Austrian Institute of Technolog GmbH",
                    "city": "Wien",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "KIRAS",
                        "Thematic programme"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "2076037",
            "name": "Zoll4D",
            "title": "Detection of Vehicle Hiding Places with Augmented Reality",
            "website": null,
            "begins_on": "2021-12-01",
            "ends_on": "2022-06-30",
            "context": "Bundesrechenzentrum GmbH",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "Bundesrechenzentrum GmbH",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "2154621",
            "name": "GeoSemanticCrow",
            "title": "GeoSemantic and Crowdsourced enhanced Virtual Reality for Situational Awareness",
            "website": null,
            "begins_on": "2023-02-01",
            "ends_on": "2025-01-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "Technische Universität Graz",
                    "city": "Graz",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "FORTE- Nationales Verteidigungsprogramm",
                        "Thematic programme"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "220239",
            "name": "FAMOUS",
            "title": "Unusual sequences detection in very large video collections",
            "website": null,
            "begins_on": "2012-10-01",
            "ends_on": "2016-09-30",
            "context": "Vienna Science and Technology Fund (WWTF)",
            "abstract": "<p>Large media repositories are often facing large additions of new material with no prior knowledge about the content. The screening, assessment, and filtering of potentially interesting sequences is currently a tedious process feasible manually only. Despite recent advances in computer vision approaches, in most cases, existing media retrieval systems allow only for a straightforward retrieval based on available, but limited metadata such as title and performer, or user recommendations. Interesting and fresh material easily<br> gets lost in the flood of data. This project counteracts these trends by providing efficient access and indexing methods for large video repositories that additionally account for similarity in the context of film grammatical and semantic concepts. As a result, the project introduces a radically different approach to the context-based retrieval of very large video collections by addressing unusual and, thus, potentially interesting material only.<br> &nbsp;</p>",
            "keywords": [
                "video data mining",
                "Outlier detection",
                "unusual sequence detection",
                "Media analysis",
                "video data mining",
                "Outlier detection",
                "unusual sequence detection",
                "Media analysis",
                "video data mining",
                "Outlier detection",
                "unusual sequence detection",
                "Media analysis"
            ],
            "people": [
                {
                    "tid": "159676",
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "role": "Subleader"
                },
                {
                    "tid": "281480",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "39017",
                    "first_name": null,
                    "last_name": null,
                    "role": "Leader"
                },
                {
                    "tid": "48362",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "52647",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "53598",
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "role": "Member"
                },
                {
                    "tid": "64498",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                }
            ],
            "partners": [
                {
                    "name": "Universität Wien Fakultät für Informatik Forschungsgruppe Multimedia Information Systems",
                    "city": "ORT",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "WWTF Wiener Wissenschafts-, Forschu und Technologiefonds",
                    "region": "National",
                    "group": null,
                    "body": "Vienna Science and Technology Fund (WWTF)",
                    "programs": [],
                    "call": "ICT Call 2012",
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "2232316",
            "name": "VERTIgO - VR CBRN Training",
            "title": "Virtual Enhanced Reality for Interoperable Training of CBRN",
            "website": null,
            "begins_on": "2022-10-01",
            "ends_on": "2024-04-01",
            "context": "Altheria Solutions SRL",
            "abstract": "<p>VERTIgO will develop a simulation platform for military CBRN training, which integrates a Virtual Reality headset and a CBRN mask for enhanced realism.</p> \n<p>The project “Virtual Enhanced Reality for inTeroperable traIning of CBRN military and civilian Operators” (VERTIgO) supports an integrated approach to conflicts and disaster relief by virtualization and simulation of CBRN (chemical, biological, radiological and nuclear) defence training. The project’s overall objective is the validation of a European Exercise Simulation Platform (EESP) for virtual reality (VR) applications to CBRN training, complemented by the prototyping of an ad-hoc hardware solution, which integrates a VR headset and CBRN mask for enhanced realism and user experience.</p> \n<p>The VERTIgO project has three main objectives: (1) design of a European simulation platform: Create and design an innovative and inter-operable CBRN VR training system for EU military actors, capable of integrating with civilian first responders’ requirements, called EESP (European Exercises Simulation Platform). (2) Training scenarios: Create and design harmonized and interoperable CBRN VR simulation scenarios both for military and civilian first responders. (3) Design of a CBRN mask: Design, prototype and validate the first integrated CBRN VR Mask to be used with the EESP system.</p> \n<p>The project is funded by the European Union’s European Defence Industrial Development Programme (EDIDP) – G.A. EDIDP-SVTE-2020-047-VERTIgO. The <a href=\"https://cbrn-vertigo.eu/\" target=\"_blank\" rel=\"nofollow noopener\">project website</a> provides further detail.</p> \n<p>TU Wien is subcontractor of the project partner Altheria in this project.</p>",
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "Altheria Solutions SRL",
                    "city": "Auderghem",
                    "country": "Belgium"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "Altheria Solutions SRL",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E210-01"
            ],
            "member_org_nrs": [
                "E193-03"
            ],
            "tid": "2258104",
            "name": "Circular Twin",
            "title": "Circular Twin - A Digital Ecosystem for the Generation and Evaluation of Circular Digital Twins",
            "website": null,
            "begins_on": "2022-12-01",
            "ends_on": "2025-05-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": "<p><span style=\"color: black;\">By 2030, more than the equivalent of two Earths will be needed to meet human demand for natural resources, with the construction industry responsible for 60% of the world's raw material extraction and 25% of the world's total waste generation. As waste management and waste reduction are not considered in the early design stages, the phase with the greatest impact throughout the life cycle, 50% construction and demolition waste is generated at the end of the building life cycle. In order to reduce raw material extraction and the amount of waste generated, it is of utmost importance to increase recycling rates by reusing materials and building elements, thus enabling a circular economy. A transition to a circular system is essential, especially in the construction industry, as it is responsible for about 38% of all C02 emissions and has a major impact on resource depletion.&nbsp;The European Green Deal developed the classification system of the \"EU Taxonomy\" based on six environmental objectives, according to which sustainability of investments are evaluated, and EU Taxonomy compliance is assessed.</span></p> \n<p><span style=\"color: black;\">By creating a digital ecosystem based on the components of circular digital twins, generative design algorithms and virtual reality (VR), this research project aims to support the decisions that enable the circularity of building structures and integrate end-of-life concepts into the design at early design stages. Building on the research project \"Wohnen 4.0 - Digitale Plattform für leistbares Wohnen\" (Housing 4.0 - Digital Platform for Affordable Housing), the project \"CircularTwin\" aims to use the knowledge base generated in \"Wohnen 4.0\", the BIM object libraries and associated material building passports and enrich them with the criteria of the circular economy. The framework for BIM-supported material building passports was already elaborated in the predecessor research project \"BIMMaterial\". With regard to building information modeling and life cycle changes (adaptability and flexibility), \"CircularTwin\" envisages the application of BIM-based digital twins, which are generated and evaluated by means of Generative Design in the context of a large number of variants in the early design phase and the re-use phase. In addition to the Generative Design methods, which enable an automated and variant generation of Digital Twins coupled to a BIM object database, algorithms are also designed that enable an evaluation of the circularity, Material Building Passports (MGP) and EU taxonomy compliance. By visualizing the end-of-life (EoL) scenarios of the generated digital twins in a VR platform, including user interaction and variant generation, the decision-making process of the stakeholders involved in the design and construction will be supported. The digital \"Circular Twin\" ecosystem thus enables the early implementation of the goals of the circular economy and sustainability as well as end-to-end digitization in the construction industry.</span></p> \n<p><span style=\"color: black;\">&nbsp;</span></p>",
            "keywords": [
                "Digital Twin, Circular Economy, Generative Design, Virtual Reality, Ecosystem"
            ],
            "people": [
                {
                    "tid": "270440",
                    "first_name": "Anastasia Stephanie",
                    "last_name": "Wieser",
                    "role": "Member"
                },
                {
                    "tid": "42543",
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "role": "Leader"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "ATP sustain GmbH",
                    "city": "Wien",
                    "country": "Austria"
                },
                {
                    "name": "Eisler ZT-GmbH",
                    "city": null,
                    "country": "Austria"
                },
                {
                    "name": "IBO - Österr. Institut für Bauen un Ökologie GmbH",
                    "city": "Wien",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "Stadt der Zukunft",
                        "Thematic programme"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E194-06"
            ],
            "member_org_nrs": [],
            "tid": "2323188",
            "name": "VRG-TOSN",
            "title": "Towards Trustworthy Recommendation Systems for Online Social Networks",
            "website": null,
            "begins_on": "2023-09-01",
            "ends_on": "2030-08-31",
            "context": "Vienna Science and Technology Fund (WWTF)",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "308031",
                    "first_name": "Stefan",
                    "last_name": "Neumann",
                    "role": "Subleader"
                },
                {
                    "tid": "330800",
                    "first_name": "Thomas",
                    "last_name": "Gärtner",
                    "role": "Leader"
                },
                {
                    "tid": "339163",
                    "first_name": "Ülkühan",
                    "last_name": "Kacmaz",
                    "role": "Subleader"
                },
                {
                    "tid": "387931",
                    "first_name": "Sebastian Johannes",
                    "last_name": "Lüderssen",
                    "role": "Member"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "WWTF Wiener Wissenschafts-, Forschu und Technologiefonds",
                    "region": "National",
                    "group": null,
                    "body": "Vienna Science and Technology Fund (WWTF)",
                    "programs": [
                        "Vienna Research Groups for Young Inverstigators"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "2353000",
            "name": "Conversational Agents",
            "title": "PropXX Innovationsscheck Conversational Agents",
            "website": null,
            "begins_on": "2023-05-01",
            "ends_on": "2024-01-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": "<p>The proposed project focuses on the investigation of conversational embodied&nbsp;agents in virtual reality visualizations of urban scenarios. The humanoid agents in VR&nbsp;should have the capability of maintaining conversation with a user. Several systems,&nbsp;such as ChatGPT have been proposed which are capable of maintaining fluent&nbsp;conversation with the user. However, the impact of integration of these&nbsp;conversational systems on user perception in VR urban exploration scenario have&nbsp;not yet been widely studied.&nbsp;&nbsp;</p>\n<p>We plan to address this gap by studying conversational agents in VR urban&nbsp;exploration with respect to the perception of users, particularly perception of&nbsp;presence, realism, visual quality, the error rate of agents and naturalism of&nbsp;conversation. In order to achieve this goal we plan to conduct a wide literature study&nbsp;of conversational agents in VR and explore available technologies for text-to-speech&nbsp;translation, speech synthesis and natural language processing. Additionally, we will&nbsp;conduct a user study to investigate the influence of conversational agents on human&nbsp;perception. The integration and deployment of ChatGPT and other conversational&nbsp;programs as well as AI-generated AVATARS into urban visualization in VR by our&nbsp;partner company PropXX will be used in this user study.&nbsp;</p>\n<p>The expected outcome of the proposed project will be new knowledge about the&nbsp;user perception of conversational agents in urban VR visualizations. The findings of&nbsp;this project can be beneficial for future research and development of solutions&nbsp;utilizing conversational agents in VR by providing guidelines for fast selection and&nbsp;deployment of appropriate technology given the desired requirements. Such fast&nbsp;deployment can boost productivity in the research and development of real estate&nbsp;solutions, navigation or urban planning.&nbsp;</p>",
            "keywords": [],
            "people": [
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Subleader"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "Innovationsscheck mit Selbstbehalt",
                        "Basic programs"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E210-01"
            ],
            "member_org_nrs": [
                "E193-03"
            ],
            "tid": "2367677",
            "name": "RE:STOCK INDUSTRY",
            "title": "RE:STOCK INDUSTRY - Digital framework for the circular economy orientated reuse of existing building structures for vertical production",
            "website": null,
            "begins_on": "2024-02-01",
            "ends_on": "2027-01-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": "<p>The strong growth of the Austrian industry leads to increased construction activity and land sealing. The construction sector accounts for 60% of raw material extraction, 40% of energy-related CO<sub>2</sub> emissions and 70% of waste generated in Austria comes from construction sector - facts that call for sustainable use and reuse of existing infrastructures. Even though there are more than 82200 existing industrial and warehouse buildings, 20000 ha of unused industrial land and 6000 vacant industrial buildings, new production settlements/expansions are rarely integrated into existing structures. Concepts for the implementation of vertical production processes and, consequently, vertical upgrading of existing industrial buildings could significantly reduce waste generation and land sealing. The load-bearing structure of the buildings is of crucial importance. There is a lack of information on the actual load-bearing capacity and usability for reuse and of methods for precise capturing and life-cycle-oriented assessment of the structural building stock.</p>\n<p>The <strong>main objective</strong> of RE:STOCK INDUSTRY is the development of a consistent methodology for capturing, modeling and analyzing existing industrial load-bearing structures in order to determine their potential for reuse, modernization and retrofitting in case of vertical expansions under consideration of circular economy aspects. Real use-cases from steel and reinforced concrete frame structures are captured by LIDAR scans and photogrammetry. New AI-algorithms automatically generate an <em>as-built FEM</em> (finite element method) model, which is used for cycle-oriented documentation and evaluation of the re-use capability of the structure besides traditional structural analysis. Novel concepts for vertical production processes in existing industrial buildings as well as approaches for vertical retrofits are integrated into the <em>FEM</em> <em>models</em> as 3D layout models. Modernization and retrofitting measures can thus be mapped with cost and sustainability feedback in order to evaluate the re-use capability of existing structures for new use. An interactive augmented reality (AR) application enables visualization of the re-use concepts directly at the building site and promotes decisions for retrofit instead of demolition and new construction. The <strong>innovation</strong> is in the coupling of different methods in one framework: application of AI algorithms for automated generation of <em>as-built FEM</em> models from scan data; integration of vertical 3D layout concepts into structural analysis; methods for evaluation and documentation of structural information regarding the building's reuse potential; and integration of AR technology as visual decision support for building stock assessment. The <strong>main benefit</strong> of the project is the targeted reuse of the structural stock of industrial buildings for expansions or conversions, instead of relying on recycling or disposal processes.</p>\n<p>In future, the solutions can be extended to other building types where analysis of existing structures for reuse is required, like school-, office- and residential buildings.</p>\n<p>The <strong>broad market and customer spectrum</strong> includes industrial companies and municipalities as building owners, architects, production and structural planners, as well as companies involved in digital infrastructure inspection and demand planning. The interdisciplinary consortium of researchers, experienced planners, and users ensures a high level of innovation and practical implementation.</p>\n<p>&nbsp;</p>\n<p>For better readability, generic masculine terms have been used. Female and other gender identities are expressly included.</p>",
            "keywords": [],
            "people": [
                {
                    "tid": "230671",
                    "first_name": "Julia",
                    "last_name": "Reisinger",
                    "role": "Leader"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "ATP Wien Planungs GmbH",
                    "city": "Wien",
                    "country": "Austria"
                },
                {
                    "name": "Delta Projektconsult GmbH",
                    "city": "Wien",
                    "country": "Austria"
                },
                {
                    "name": "Fraunhofer Austria Research GmbH",
                    "city": "Wien",
                    "country": "Austria"
                },
                {
                    "name": "Palfinger Structural Inspection GmbH",
                    "city": "Wien",
                    "country": "Austria"
                },
                {
                    "name": "RM Umweltkonsulenten ZT GmbH",
                    "city": "Frohnleiten",
                    "country": "Austria"
                },
                {
                    "name": "diebauplaner salzer&partner zt GmbH",
                    "city": "Wien",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "Circular Economy",
                        "Thematic programme"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "2457349",
            "name": "VROnSite-CatSim",
            "title": "Katastrophensimulation in weiträumigen Umgebungen",
            "website": null,
            "begins_on": "2023-01-01",
            "ends_on": "2023-12-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Subleader"
                },
                {
                    "tid": "54835",
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "M²D MasterMind Development GmbH",
                    "city": null,
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "Innovationsscheck mit Selbstbehalt",
                        "Basic programs"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "2543",
            "name": "VizIR",
            "title": "VizIR - A Framework for Visual Information Retrieval",
            "website": "http://vizir.ims.tuwien.ac.at",
            "begins_on": "2003-03-01",
            "ends_on": "2006-02-28",
            "context": "Austrian Science Fund (FWF)",
            "abstract": "The project ¿VizIR - A Framework for Visual Information Retrieval¿, submitted by the Institute of Software Technology and Interactive Systems at the Vienna University of Technology, aims at three major goals: (1) Integration of past visual information retrieval research results with our current research work on similarity modeling, (semantic) feature extraction and query acceleration. We have developed a process-oriented similarity model that is based on psychological insights about human similarity perception as well as information retrieval methods. The major idea is that visual similarity is more than distance measurement of numerical feature vectors. In feature design we have developed a concept for semantic feature modeling. Additionally, we are integrating and evaluating the visual MPEG-7 descriptors and developing suitable descriptor schemes for visual querying. (2) Implementation of an asset framework for content-based retrieval of visual media (image, video). Assets include class frameworks for feature extraction, querying methods and user interface components as well as benchmarking algorithms, test sets and documentation. This asset framework has to be open, portable, extendible and well-documented. Open means that the VizIR outcome (including source-code and API documentation) will be regularly released to the public and interested researchers are invited (in publications, etc.) to use this toolbox. VizIR is portable, because it is fully based on Java and the JavaSDK. Where platform-dependent packages are used (database, media handling), they are encapsulated in wrapper classes to guarantee that these components can be replaced without having to change the framework API. VizIR is designed to be extendible: users can add feature extraction methods, query engines, indexing methods, user interface components, etc. Finally, well-documented APIs and components are guaranteed through using Javadoc and state-of-the-art software development processes and tools. (3) Cooperation with other visual information retrieval research groups. In VizIR, we are using innovations from other groups like the Multimedia Retrieval Markup Language, test sets, etc. and contributing to other project (e.g. Benchathlon, an initiative to design benchmarks for visual information retrieval). The VizIR project has already been started in Autumn 2001. With funding from the FWF we hope to accelerate project progress and maximize the scientific output.",
            "keywords": [
                "Visual Information Retrieval"
            ],
            "people": [
                {
                    "tid": "53451",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "53598",
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "role": "Member"
                },
                {
                    "tid": "93452",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FWF - Österr. Wissenschaftsfonds",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Science Fund (FWF)",
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "276135",
            "name": "GEODIKON",
            "title": "Development of a Didactic Concept for Modern 3D Geometry Teaching Materials",
            "website": null,
            "begins_on": "2012-11-01",
            "ends_on": "2014-12-31",
            "context": "Bundesministerium für Unterricht, Kunst und Kultur",
            "abstract": "<p>In GEODIKON we work on a didactic and methodic design of modern 3D&nbsp;geometry teaching material, especially for mathematics courses and \"Linear Drawing\"/\"Geometrisches Zeichnen\" courses.&nbsp; The aim is to design teaching material in a way to optimally and individually support the development of spatial, mathematic-logic and geometric skills and to enhance students' spatial abilities. In addition we want to reduce gender effects in geometry education.</p> \n<p><span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\">„GEODIKON“ (Geometrie – Didaktik – Konzept) is sponsered by the </span>Austrian Federal Ministry for Education, the Arts and Culture.</p> \n<p><strong><span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\">Project coordinator:</span></strong><span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\"> </span><span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\">Mag. Dr. Günter Maresch, PH Salzburg<br> </span></p> \n<p><span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\"><strong>Project participants/institutions:</strong> </span>PH Niederösterreich: Mag. Doris Miestinger;&nbsp;<span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\">PH Wien: Mag. Katharina Luksch;&nbsp;</span><span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\">PH Steiermark: Mag. Klaus Scheiber</span>;&nbsp;<span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\">KPH Wien Krems: Mag. Dr. Thomas Müller</span>;&nbsp;<span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\">TU Wien: Priv.Doz. Mag. Dr. Hannes Kaufmann; </span>U<span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\">niversität Salzburg: Ao. Univ.-Prof. Mag. Dr. Karl Fuchs</span>;&nbsp;<span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\">Universität Innsbruck: Univ.Prof. Mag. Dr. Manfred Husty;&nbsp;</span><span style=\"FONT-FAMILY: &quot;Arial&quot;, &quot;sans-serif&quot;\">Arbeitsgruppe Didaktische Innovation (14 Geometry teachers from different educational sectors)</span></p>",
            "keywords": [
                "Geometry Education",
                "teaching materials",
                "Spatial Abilities"
            ],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "Bundesministerium für Unterricht, Kunst und Kultur",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "288695",
            "name": "HyMoTrack",
            "title": "Mobile Hybrid Tracking in Large and Changing Interior Spaces",
            "website": null,
            "begins_on": "2013-01-01",
            "ends_on": "2014-12-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": "<p> </p> \n<p class=\"MsoNormal\"><span>The goal of this project is to develop a hybrid indoor tracking system for mobile devices like smartphones or tablets. The system will be specifically designed for complex interior spaces, like airports, hospitals or shopping malls, that may change their visual appearance to a certain amount for example due to seasonal decoration.</span></p> \n<p class=\"MsoNormal\"><span>For this purpose we combine multiple optical tracking technologies. The algorithm takes advantage of the available sensors already built into standard smartphones, like the inertial sensor and the high resolution RGB camera. In addition novel imaging hardware like a light field capturing device is included in the system. 2D natural features of the environment as well as SLAM features of more complex three dimensional structures will be processed to calculate the position within the building.<span class=\"msoDel\"> </span></span></p> \n<p class=\"MsoNormal\"><span>This device position allows the development of a robust indoor navigation system which is able to interactively guide the user within an unknown terrain, thereby solving complex tasks more efficiently.</span></p> \n<p></p>",
            "keywords": [
                "mobile tracking",
                "SLAM",
                "light field camera"
            ],
            "people": [
                {
                    "tid": "40682",
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "role": "Member"
                },
                {
                    "tid": "40923",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "Card eMotion Consulting & System GmbH",
                    "city": "Wien",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "BRIDGE Brückenschlagprogramm",
                        "Structural programme"
                    ],
                    "call": "3305844",
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "358043",
            "name": "MusketIR",
            "title": "Conceptional Development of image recognition engine for outdoor 3D objects on smartphones",
            "website": null,
            "begins_on": "2013-02-01",
            "ends_on": "2014-10-31",
            "context": "Wikitude GmbH",
            "abstract": "<p></p> \n<p></p> \n<p></p> \n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\"><br> </span><span style=\"mso-ansi-language:EN-US\">The goal of this project is not only to track, but also to recognize 3 dimensional structures in open space with the use of devices like smartphones or tablets. Visual cues as well as sensor data are going to be combined to fulfill this task. </span></p> \n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\">The system will be designed specially to work with facades of public buildings throughout the whole year. Thus issues like varying lightning condition, occlusions and different viewing angles have to be considered in order to extract time-independent robust features. <span style=\"mso-spacerun:yes\">&nbsp;</span>The expected accuracy of this large scale tracking should allow 3D real-time video augmentation with all kind of visual content.</span></p> \n<p>&nbsp;</p>",
            "keywords": [
                "Mobile Tracking",
                "Large Scale Tracking",
                "3D Outdoor Object Recognition",
                "SLAM"
            ],
            "people": [
                {
                    "tid": "40682",
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "role": "Member"
                },
                {
                    "tid": "40923",
                    "first_name": null,
                    "last_name": null,
                    "role": "Subleader"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "Wikitude GmbH",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "4037",
            "name": "Construct3D-Lernmodule",
            "title": "Geometrie content with Construct3D",
            "website": null,
            "begins_on": "2005-12-01",
            "ends_on": "2006-08-31",
            "context": "Bundesministerium für Bildung Wissenschaft und Kultur",
            "abstract": "Erstellung von Geometrieinhalten für den Unterricht in Darstellender Geometrie in Virtual Reality mit Construct 3D",
            "keywords": [
                "Virtual Reality",
                "Augmented Reality",
                "Construct 3D",
                "Geometry"
            ],
            "people": [
                {
                    "tid": "159676",
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "Bundesministerium für Bildung Wissenschaft und Kultur",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "4151",
            "name": "Tracking-System",
            "title": "Development of Opitcal Tracking-System",
            "website": null,
            "begins_on": "2006-01-01",
            "ends_on": "2006-12-31",
            "context": "University of Otago",
            "abstract": "In this project we develop and improve an infrared-optical tracking system and the according tracking software in order to track multiple targets in 3D space with millimeter accuracy.",
            "keywords": [
                "Computer vision software",
                "Infrared Optical Tracking",
                "High speed cameras"
            ],
            "people": [
                {
                    "tid": "159676",
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "role": "Leader"
                },
                {
                    "tid": "211104",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "University of Otago",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "424614",
            "name": "MobileBRDF",
            "title": "Material reconstruction and high quality real-time visualisation on mobile devices",
            "website": null,
            "begins_on": "2014-01-01",
            "ends_on": "2015-12-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "Bridge Brückenschlagprogramm",
                        "Structural programme"
                    ],
                    "call": "18. Ausschreibung",
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E183"
            ],
            "member_org_nrs": [],
            "tid": "4293",
            "name": "TWIST-CV",
            "title": "Tracking with structure in Computer Vision",
            "website": "http://www.prip.tuwien.ac.at/Research/twist/index.html",
            "begins_on": "2006-03-01",
            "ends_on": "2009-12-31",
            "context": "Austrian Science Fund (FWF)",
            "abstract": "The task of tracking objects in image sequences is very important in computer vision. Tracking is, for example, indispensable for automatically following people in scenes filmed by surveillance cameras, or for following the position of a head and hands in a human-computer interaction application. An interesting extension is to use 3D information obtained from two or more cameras to assist in the tracking. There exist many approaches to solve the problem of object tracking. Although these approaches are successful, it is often the case that they are not robust enough, or that different approaches need to be used for different applications. Recent work by one of the project partners (PRIP) has shown that the use of matching of graph pyramids and of combinatorial map pyramids is a powerful means to solve problems in computer vision. Promising initial results have been obtained for applying this methodology to tracking. The main goal of the proposed project is to develop a general framework that enables solutions to practical problems of computer vision, in particular Tracking, using approaches that strongly use image structure. The project will make use of structural techniques such as graph and combinatorial map image representations, graph and combinatorial map pyramids and matching to attempt to provide a solution to the following tasks within a single framework: 1. Finding object correspondences in image sequences (Tracking). 2. Finding object correspondences in images taken from different viewpoints (Stereo matching). 3. Finding object correspondences in image sequences taken from different viewpoints (a combination of the above two techniques). The use of this single framework would simplify the solutions of many practical problems. In order to properly evaluate the developed algorithms and framework, we intend to rigorously compare them to existing algorithms. To do this, we will make use of existing benchmarking databases and of data arising from real applications in surveillance and man-machine interfaces that are investigated by the second project partner (ACV) in industrial research projects.",
            "keywords": [
                "Image sequences",
                "graph based tracking",
                "image pyramid",
                "tracking structured objects"
            ],
            "people": [
                {
                    "tid": "178947",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "229671",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Leader"
                },
                {
                    "tid": "43295",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "64562",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "68001",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "70372",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "70910",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "71600",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                }
            ],
            "partners": [
                {
                    "name": "Seibersdorf Labor GmbH",
                    "city": "Seibersdorf",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FWF - Österr. Wissenschaftsfonds",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Science Fund (FWF)",
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "448705",
            "name": "Life-Stage",
            "title": "Multimodal Feedback for Supporting Gestural Interaction in Smart Environments",
            "website": null,
            "begins_on": "2014-01-01",
            "ends_on": "2015-12-31",
            "context": "OEAD",
            "abstract": "<p><span>The smart environments of our future ubiquitous computing era need to deliver their users with appropriate interfaces that go beyond today’s standard interaction modalities. In this context, gestural interfaces represent a viable solution to deliver new levels of experience to the users of these scenarios. Although gestural interfaces are now common for mobile devices and video games in the form of touch, accelerated motion, and whole body movements, interacting with gestures outdoors in new, smart environments is still problematic. Moreover, each day we see more applications being deployed in outdoor environments that expose gestural interfaces to their users, such as touch screens and interactive floors and ceilings installed in public places. This project addresses the current hot topic of designing gestural interfaces for smart environments (i.e., public ambient displays) and for new miniaturized wearables (e.g., smart watches) that constitute the interactive targets of the ubiquitous computing era. To this end, we investigate feedback modalities for the users of such new environments in order to support gesture-based interaction. For example,<b style=\"mso-bidi-font-weight:normal\"> visual feedback</b> supplied on the mobile phone through augmented reality browsing can help discover gesture commands (and help advance on the currently unsolved “invisibility” problem of gestures); <b style=\"mso-bidi-font-weight:normal\">audio</b> <b style=\"mso-bidi-font-weight:normal\">feedback</b> can inform on the successfulness of gesture articulation (contributing thus to an effective user experience in new environments); and <b style=\"mso-bidi-font-weight:normal\">vibrotactile feedback</b> can guide the articulation process of the gesture (and help training procedures for new environments, a problem never explored before). With this research, we attempt to add new knowledge on the topic of designing gestural interfaces by implementing and evaluating feedback modalities to support gestural interaction in new, smart environments.</span></p>",
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                },
                {
                    "tid": "54835",
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "role": "Subleader"
                },
                {
                    "tid": "58429",
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "role": "Subleader"
                }
            ],
            "partners": [
                {
                    "name": "University Stefan cel Mare of Suceava",
                    "city": "Suceava",
                    "country": "Romania"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "Vereine, Stiftungen, Preise",
                    "region": null,
                    "group": "Vereine, Stiftungen, Preise",
                    "body": null,
                    "programs": [
                        "OEAD"
                    ],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "5064",
            "name": "ARST",
            "title": "Development of an Augmented-Reality Dynamic Spatial Test",
            "website": null,
            "begins_on": "2006-11-01",
            "ends_on": "2009-10-31",
            "context": "Austrian Science Fund (FWF)",
            "abstract": "In the proposed project we intend to develop a new type of test for the assessment of spatial abilities that differs from conventional spatial ability tests in several aspects. First, traditional spatial ability tests (paper-pencil as well as on-screen computer versions) assess 3dimensional spatial abilities with 2dimensional means. The new test will measure the ability to visualize and mentally manipulate 3dimensional objects in actual 3dimensional space, and should thus have a higher ecological validity than previous spatial ability tests. This will be possible through use of the Augmented Reality tool Construct3D, which allows for projecting virtual 3dimensional objects into real space where they can be seen and manipulated by means of special glasses and input devices. Furthermore, the planned test will be a dynamic learning test; thus, other than conventional tests, it does not only measure a person¿s current status, but also his or her learning potential. Performance in conventional tests is generally, and particularly when spatial abilities are concerned, significantly dependent on factors such as test experience or experience with similar tasks and materials. Gender differences, which are still frequently found in spatial tests, can partly be attributed to these experience-based factors. Often, such differences can be reduced or eliminated through a relatively short training, sometimes even through a simple re-take of the test. With a dynamic learning test (usually consisting of a pretest, a training phase, and a posttest) the influence of short-term learning experiences on test performance can be assessed, which may yield higher internal consistency and predictive power of the test scores. The new item material will assess the mental manipulation (rotation, combination, intersection, etc.) of 3dimensional objects. Stimuli and instructions are presented sequentially. Hence, in contrast to most other tests, participants need to actually encode and manipulate mental representations of the spatial objects. Thereby, the range of possible strategies (e.g., comparison of single features and elimination of possible answers) is reduced. In order to guarantee homogeneity of the testing material and to avoid problems such as ceiling effects, item-response-models will be employed for the development of test items as well as for the measurement of change in performance.",
            "keywords": [
                "training",
                "Spatial Abilities",
                "Dynamic Testin",
                "Augmented Reality",
                "Item-Response-Models"
            ],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                },
                {
                    "tid": "47153",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "53333",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "58429",
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "role": "Member"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FWF - Österr. Wissenschaftsfonds",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Science Fund (FWF)",
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "508747",
            "name": "ABCAbw. VR-Training",
            "title": "Virtual Reality Training for CBRN-Defense and first responders",
            "website": null,
            "begins_on": "2014-10-01",
            "ends_on": "2015-09-30",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                },
                {
                    "tid": "58429",
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "role": "Subleader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "KIRAS",
                        "Thematic programme"
                    ],
                    "call": "KIRAS Call 2014",
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E183"
            ],
            "member_org_nrs": [],
            "tid": "5344",
            "name": "CogVis",
            "title": "Representations for Cognitive Processes",
            "website": "http://www.prip.tuwien.ac.at/Research/FSPCogVis",
            "begins_on": "2007-01-01",
            "ends_on": "2009-12-15",
            "context": "Austrian Science Fund (FWF)",
            "abstract": "Project Title: Cognitive Vision - Key Technology for Personal Assistance (http://fsp.acin.tuwien.ac.at/) Subproject 3: Representations for Cognitive Vision (SP3) The subproject \"Representations for Cognitive Vision\" provides new and improves existing representations that are useful for cognitive vision. It became obvious in the first part of the project (2004-2006) that not a single representation is suitable for all tasks. For generic vision tasks there is the necessity for complementary representations. In particular, both generative (reconstructive) and discriminative representations are needed. This is required for more autonomous learning and for obtaining robustness in general. In a similar spirit a cognitive vision system needs both global (holistic) and local (part-based) representations. In the second half of the project, SP3 plans to build on the results achieved within the first period of the research project. We continue along our three major lines of research in appearance-based, spatio-temporal, and graph-based representations. As a major application in SP3, we will test our newly developed representations on specific and generic object recognition tasks. PRIP (http://www.prip.tuwien.ac.at/Research/FSPCogVis/), Task SP3.3 - Graph-based representations: We will address representation and abstraction in cognitive vision from the local, object level, to the global, scene level, all with a special care for the dynamic aspects, concentrating on (i) dynamic skeletons to represent moving shapes, (ii) invariants using homology groups, (iii) topology preserving description of dynamic scenes, and (iv) landmark-based addressing schemes to link topology with metric properties.",
            "keywords": [
                "cognitive computer vision",
                "local + global representations",
                "visual abstraction",
                "graph based representations "
            ],
            "people": [
                {
                    "tid": "108231",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "110312",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "179282",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "207860",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "229413",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Leader"
                },
                {
                    "tid": "43295",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "44748",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FWF - Österr. Wissenschaftsfonds",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Science Fund (FWF)",
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E186"
            ],
            "member_org_nrs": [],
            "tid": "538645",
            "name": "Digibook",
            "title": "Development of a software-prototype for semi-automatical digitization of books with follow-up corrections.",
            "website": null,
            "begins_on": "2014-11-01",
            "ends_on": "2015-03-31",
            "context": "Markus Reithofer",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "Markus Reithofer",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "5427",
            "name": "Digital Formalism",
            "title": "Digital Formalism - The Vienna Vertov Collection",
            "website": "http://www.digitalformalism.org",
            "begins_on": "2007-02-01",
            "ends_on": "2010-07-31",
            "context": "Vienna Science and Technology Fund (WWTF)",
            "abstract": "Digital Formalism: The Vienna Vertov Collection focuses on the computer-aided digital analysis of the ¿senses of cinema¿. On the foundation of theoretical basic research on digital formalism we will develop the technical tools to digitally analyse the principal cinematic elements in the films by Russian Avant-Garde film maker Dziga Vertov (1896 ¿ 1954) with the aim of a) applying the tools developed for this important oeuvre to certain cultural industries branches like graphics design, creative advertising etc. and of b) making the worldwide unique Vienna Vertov Collection, which contains film material, scripts, photographs etc., accessible for both the general public and the international Vertov research community. Core piece of the project is the development of tools for the computational understanding of media aesthetics, inspired by Vertov¿s work, which is in its availability at the Austrian Film Museum exclusive in quality. Computational understanding comprises the automated extraction of high-level film elements such as rhythm, contrast, dialogue sequences, etc. The involved film analysis scientists and media processing scientists will work hand in hand and thus ascertain the interdisciplinary merging of film theory, advanced digital technology and materiality of film.",
            "keywords": [
                "video analysis",
                "Dziga Vertov",
                "formalism"
            ],
            "people": [
                {
                    "tid": "159676",
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "role": "Leader"
                },
                {
                    "tid": "39017",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "53451",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "53598",
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "role": "Member"
                },
                {
                    "tid": "93425",
                    "first_name": "Robert",
                    "last_name": "Fuchs",
                    "role": "Member"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "WWTF Wiener Wissenschafts-, Forschu und Technologiefonds",
                    "region": "National",
                    "group": null,
                    "body": "Vienna Science and Technology Fund (WWTF)",
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-03"
            ],
            "member_org_nrs": [],
            "tid": "606384",
            "name": "TU Hybrid Lab: Virtual Jumpcube & Vreeclimber",
            "title": "TU Fly into the Future & Vreeclimber climbing wall",
            "website": "http://www.ifs.tuwien.ac.at/~heidenbe/hybrid-systems/",
            "begins_on": "2015-04-14",
            "ends_on": "2025-12-31",
            "context": "TU Hybrid Lab: Virtual Jumpcube & Vreeclimber",
            "abstract": "<p>Project goal is the development of large sized hybrid systems in the area of virtual reality. Please see the websites of the <a href=\"http://www.jumpcube.at\" target=\"_blank\" rel=\"nofollow noopener\">Virtual Jumpcube</a> and the <a href=\"http://www.vreeclimber.at\" target=\"_blank\" rel=\"nofollow noopener\">Vreeclimber</a> installations for details.</p>",
            "keywords": [
                "Virtual Reality",
                "Zero-Gravity Systems",
                "Hybrid Systems"
            ],
            "people": [
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": []
        },
        {
            "coordinator_org_nrs": [
                "E193-06"
            ],
            "member_org_nrs": [],
            "tid": "624243",
            "name": "Virtual Architect",
            "title": "Automated Architectural 3D Model Generation for Virtual Reality Walk-Through",
            "website": null,
            "begins_on": "2015-08-01",
            "ends_on": "2017-07-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": "<p>The proposed project addresses the interactive virtual exploration of buildings. Virtual walkthroughs of property are becoming increasingly popular in the real estate sector. However, currently virtual walkthroughs are costly due to the labor intensive manual generation of virtual 3D content and walkthrough design, and thus, not applicable for brokerage of residential properties. A cost reduction to create 3D virtual walkthroughs and interior would change the entire business in residential property. Especially for designed but not yet built properties, it is a key factor in financing to sell objects already in the development state. However, clients usually fail to imagine a virtually designed building using only state-of-the-art static 2D images or non-interactive 3D renderings. This often results in not purchasing the real estate object until the end of the construction process. For that reason, there is an enormous interest, especially of property developers, to be able to present a real estate object in a convincing and realistic manner at the earliest stage possible.</p> \n<p><br> In this project we aim to develop novel methods for automated geometry creation from the floorplan of a building and automatic interior design by furniture and material placement. Moreover, we plan to investigate methods for immersive VR exploration and intuitive interaction. The methods investigated in this project will enable the automated generation of VR walkthroughs from the floorplan of a building with minimal user intervention within only a few minutes. Additionally, the developed methods will enable a reconfiguration of interior design in real-time. The generated virtual walkthrough will feature high-quality rendering, dynamic lighting (for variable daytime simulation), real walking, and advanced interaction to provide immersive and outstanding user experience. This project also addresses open topics in state-of-the-art virtual exploration and interaction with generated 3D environments. Furthermore, the combination of using immersive VR to explore 3D generated content by using natural walking and precise hand and finger interaction has, to our best knowledge, not been demonstrated before.</p> \n<p><br> Based on the expected results of the proposed research project, it will be possible to develop a market ready solution by our partner company after the project finishes to provide realistic, user-centered and interactive visualizations of residential real estate objects. With the intended methodological approach, it is possible to tremendously reduce the costs for 3D content creation by using a novel automated 3D extrusion process based on commonly used 2D blue prints. Other application scenarios of our proposed solution are detached and semi-detached houses. Here, our novel technology can replace the very cost-intensive sample houses by providing a virtual house walk through. Furthermore, we expect our technology to be beneficial for architects and development planners.</p> \n<p><br> In addition to the main focus of the proposed research on indoor spaces and interior design, the results of this project will be beneficial for many areas which require automated generation of 3D digital content, including training, education, rehabilitation, architectural visualization, psychological studies, simulation, in the entertainment industry, and other fields.</p>",
            "keywords": [],
            "people": [
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Member"
                },
                {
                    "tid": "40923",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "Bridge",
                        "Structural programme"
                    ],
                    "call": "21. Ausschr. Bridge (2015)",
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "624284",
            "name": "VR on-site",
            "title": "Virtual Simulation and On-Site Training for First Responders",
            "website": null,
            "begins_on": "2015-08-01",
            "ends_on": "2017-07-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [
                        "Bridge",
                        "Structural programme"
                    ],
                    "call": "21. Ausschr. Bridge (2015)",
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "6289",
            "name": "Vision",
            "title": "Immersive interface technologies for life-cycle human-oriented activities in interactive aircraft-related virtual products",
            "website": null,
            "begins_on": "2008-11-01",
            "ends_on": "2011-06-30",
            "context": "European Commission",
            "abstract": "Although Virtual Reality (VR) has demonstrated a significant potential for interactive applications on product and process development, the proven quality of the underlying technologies is still far from satisfying the real-life needs of aerospace industrial practice. VISION aims to specify and develop key interface features in fundamental cornerstones of VR technology, namely in immersive visualization and interaction, so as to improve the flexibility, the performance and cost efficiency of human-oriented life cycle procedures, related to critical aircraft-related virtual products (e.g. virtual cockpit, virtual cabin etc.). The upstream research roadmap of VISION will involve a) specific human-oriented developments on simulation features, such as the visual perception, the real-time rendering, the markerless body tracking, the smart objects interaction, the interaction metaphors, b) an integration of the features in a common IT platform, and c) a validation based on test cases focused on specific aircraft-related virtual products. The achievements of VISION will enhance the credibility of the human-in-the loop aircraft-related VR simulations. They will further enhance the engineering context of the aircraft-related virtual products by enabling their increased use for activities, such as design verification, ergonomics validation, specifications of equipment displays, operational and situational training. The VISION project aims to develop new technology in support of the design and ¿virtual prototyping¿ of critical aircraft-related products. It will improve the human-oriented functionality and usage of these virtual products along their life-cycle. Thus, it addresses the \"Design Systems &amp; Tools\" and \"Human Factors\" Call topics. The Consortium of VISION includes representatives from all major stakeholders, including end users, research partners and VR IT vendors. VISION is supported by the European Aeronautics Science Network (EASN)",
            "keywords": [
                "interaction",
                "Visualization",
                "interaction",
                "Visualization",
                "virtual cockpit",
                "virtual cabin",
                "immersion"
            ],
            "people": [
                {
                    "tid": "159676",
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "role": "Leader"
                },
                {
                    "tid": "40682",
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "role": "Member"
                },
                {
                    "tid": "40923",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "47153",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "50321",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "53333",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                }
            ],
            "partners": [
                {
                    "name": "EADS",
                    "city": "ORT",
                    "country": "Germany"
                },
                {
                    "name": "EADS",
                    "city": "ORT",
                    "country": "Germany"
                },
                {
                    "name": "University of Patras",
                    "city": "ORT",
                    "country": "Greece"
                },
                {
                    "name": "Universität des Saarlandes",
                    "city": "Saarbrücken",
                    "country": "Germany"
                },
                {
                    "name": "VTT",
                    "city": "Espoo",
                    "country": "Finland"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "European Commission",
                    "region": "EU",
                    "group": null,
                    "body": "European Commission",
                    "programs": [
                        "FP7 I.7 COOPERATION Transport (including Aeronautics)",
                        "7.Rahmenprogramm für Forschung",
                        "European Commission - Framework Programme"
                    ],
                    "call": "FP7-AAT-2007-RTD-1",
                    "proposal": "211567"
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E186"
            ],
            "member_org_nrs": [],
            "tid": "629204",
            "name": "INDIECAM",
            "title": "Embedded Image Warping for VR on the Basis of the Image-Pyramid Model",
            "website": null,
            "begins_on": "2015-10-01",
            "ends_on": "2016-05-31",
            "context": "INDIECAM GmbH",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "INDIECAM GmbH",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E186"
            ],
            "member_org_nrs": [],
            "tid": "6614",
            "name": "CHIC",
            "title": "Computing Homology within Image Context",
            "website": "http://www.prip.tuwien.ac.at/research/research-areas/structure-and-topology/homology-computation",
            "begins_on": "2008-08-01",
            "ends_on": "2011-07-31",
            "context": "Austrian Science Fund (FWF)",
            "abstract": "Object class invariants play a key role in computer imagery, and more specifically in image analysis and geometric modeling. Computing and representing topological information (neighborhood, connectedness, orientation, etc.) form an important part in applications such as image classification, indexing, shape description, shape recognition. Geometric modeling applications also take topological criteria into account to ensure the reliability of construction or to control the result of construction operations. Homology is an algorithmically computable topological invariant that characterizes an object by its \"holes\". The notion of \"hole\" is defined in any dimension. Informally ¿holes¿ of a 3D-object are its connected components in dimension 0, its tunnels in dimension 1, its cavities in dimension 2. This project deals with the computation of homological information (homology groups and their generators) of objects contained in images, and its use for image applications. We plan to develop a theoretical and practical framework for efficiently extracting ¿meaningful¿ homology information in the context of computer imagery. To achieve this goal, we intend to combine known techniques in algebraic topology, discrete geometry and computational geometry in order to develop new homology based algorithms for computer imagery. One challenge, and originality, of the project will be to acquire a better understanding of the behavior of homology information on structures and under operators used in computer imagery. The results of this study will be used both to reduce the complexity of computing homology groups of image objects and to determine the relevance of homology elements depending on the application context. Our research will be led along the following topics: stability of generators under image operations, homological classification of images, specificity of different combinatorial structures, and efficient computation of homology information, and to succeed we will address the following questions: ¿ How stable are homology generators under different kinds of perturbations (noise, data distortion¿), or transformations (fusion of objects, cutting ¿holes¿¿)? ¿ Is it possible to deduce homological information of an object from some of its projections or cuts? ¿ Which combinatorial structures are well suited for efficient homology computation? ¿ Is there a notion of ¿adjacency¿ for classes of objects, defined by their generators? ¿ Given an application (video tracking, object categorization¿), is it possible to determine a well-suited set of homology generators? This project is based on the complementary scientific expertise of the partners. PRIP (Vienna, Austria), SIC (Poitiers, France), and LAIC (Clermont, France) have already shown their interest and competences through publications dealing with the computation of topological invariants in digital imagery. Moreover the advanced theoretical background that is needed in this project belongs to the area of expertise of LMA (Poitiers, France).",
            "keywords": [
                "image topology",
                "digital image abstraction",
                "homology groups",
                "invariant structures"
            ],
            "people": [
                {
                    "tid": "179282",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "217144",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "233419",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Leader"
                },
                {
                    "tid": "45707",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "55000",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                }
            ],
            "partners": [
                {
                    "name": "University of Sevilla",
                    "city": "Sevilla",
                    "country": "Spain"
                },
                {
                    "name": "Université de Poitiers \nDépartement XLIM - SIC",
                    "city": "Futuroscope Chasseneuil Cedex",
                    "country": "France"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FWF - Österr. Wissenschaftsfonds",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Science Fund (FWF)",
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "7026",
            "name": "Joint Seminar",
            "title": "Seminar: Virtual Reality Systems for Psychology",
            "website": null,
            "begins_on": "2008-09-01",
            "ends_on": "2008-11-22",
            "context": "Austrian Science Fund (FWF)",
            "abstract": "The aim of the bilateral seminar is to report ongoing work and projects, to exchange views and specify directions of future joint work. The focus of our joint research interest is the development of psychology-related VR systems for research, education, therapy, and rehabilitation. Both partners are experts in the related fields and have published internationally in recent years. Applications of VR in psychology started to slowly emerge within the last decade with increasing success. As part of this seminar we will discuss well established application areas as well as new fields with high potential. The Austrian applicant has already established a very fruitful national collaboration with very good results in national projects e.g. the cooperation between the University Klagenfurt and TU Vienna. The proposed seminar can open the door for a long term international cooperation between Austrian and Russian experts. This would greatly enhance the chances in future international joint research projects for all participants.",
            "keywords": [
                "Virtual Reality",
                "Spatial Abilities",
                "Virtual Reality in Education",
                "Virtual Reality in Therapy",
                "Psychology"
            ],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "Moscow Lomonosov State University",
                    "city": "Moscow",
                    "country": "Russian Federation"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FWF - Österr. Wissenschaftsfonds",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Science Fund (FWF)",
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-02"
            ],
            "member_org_nrs": [],
            "tid": "716533",
            "name": "ARPathVis",
            "title": "Realistic Indoor Path Visualization with Real-Time Obstacle Avoidance in Augmented Reality",
            "website": null,
            "begins_on": "2016-03-01",
            "ends_on": "2018-08-31",
            "context": "Vienna Science and Technology Fund (WWTF)",
            "abstract": "<p>Accurate navigation systems for large indoor environments utilizing visual features, which are able to<br> provide real time 3D positions and rotations, are currently under development. However, research on indoor navigation and especially augmented reality (AR) indoor path generation and visualization is sparse.<br> Recent methods that focus mainly on outdoor environments are not suitable for indoor scenarios due to the complexity of floor plans, the limited field-of-view of AR devices, and do not consider fast changes in the viewing direction while being guided. Therefore new methods for path planning, obstacles avoidance and navigation visualization have to be developed.<br> In this project, we propose novel navigation methods to assist mobile indoor navigation by utilizing AR. We present a new dynamic path planning algorithm with real-time obstacle avoidance reacting to changing environments. We also propose three new navigation visualization methods utilizing AR: particles, object-following, and realistic human avatars as guides. In addition, we plan to research real light estimation from shadows using a monocular moving RGB-D camera for realistic lighting, to embed an avatar as a guide. For navigation visualization, different stages of realism will be developed and evaluated on mobile devices. Finally, we will research and integrate haptic feedback to aid navigation.<br> We plan to evaluate the developed algorithms in comprehensive user studies with respect to the efficiency in navigation, sense of presence and user comfort. We expect our approaches to advance theory and practice of personal indoor navigation.</p>",
            "keywords": [
                "Indoor Navigation",
                "Augmented Reality",
                "Mobile Devices "
            ],
            "people": [
                {
                    "tid": "231177",
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "role": "Member"
                },
                {
                    "tid": "40923",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "WWTF Wiener Wissenschafts-, Forschu und Technologiefonds",
                    "region": "National",
                    "group": null,
                    "body": "Vienna Science and Technology Fund (WWTF)",
                    "programs": [],
                    "call": "ICT Call 2015",
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "7711",
            "name": "ProFiTex",
            "title": "ProFiTex - Providing Fire Fighters with Technology for Excellent Work Safety",
            "website": null,
            "begins_on": "2009-10-01",
            "ends_on": "2012-09-30",
            "context": "European Commission",
            "abstract": "The aim of project ProFiTex is to support fire fighters in their perilous work with a system that supplies mission-relevant information without overwhelming the fire fighter. The design approach will be user-centered with tests starting at an early point in the project to gain maximum user acceptance. Professional fire fighters will be involved from the beginning of the project to ensure, that the system will be tailored to their needs. Project ProFiTex will continue the work of the successful EU-funded project wearIT@work. The ProFiTex system comprises electronic devices like an infrared camera, localisation sensors and a human-computer interface device integrated into the fire fighters jacket. Since wireless communication is difficult over long distances and through several walls of a building, an innovative method to transmit information will be applied. A security rope carried by fire fighters during a mission shall be equipped with data transmission capabilities. This allows information to be sent outside to the command post and back to the fire fighter. By monitoring several parameters of the fire fighters condition like his movement pattern and stance, problems can be detected immediately. The fire fighter himself is supplied with the possibility to navigate even in smoky environments thanks to the infrared camera and the positioning system implemented into his equipment. Localized information (e.g. ¿door¿, ¿victim¿) can be fed into the system using the garment-integrated human-computer interface device. Information will be displayed to the fire fighters, their group leaders and the commander outside the building. The amount and type of information supplid will be carefully chosen, considering the physical danger and psychic stress fire fighters are opposed to. Work safety of fire fighters shall be increased, thus lowering the number of work-related accidents and casualties. Fire fighting missions will be more efficient using the system.",
            "keywords": [
                "work safety",
                "protective clothing ",
                "Fire brigade",
                "textile-based system",
                "emergency responders",
                "personal protective equipment"
            ],
            "people": [
                {
                    "tid": "191422",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "40682",
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "role": "Member"
                },
                {
                    "tid": "40923",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                },
                {
                    "tid": "46416",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "47153",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "50321",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "53333",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "53598",
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "role": "Member"
                },
                {
                    "tid": "54835",
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "role": "Member"
                }
            ],
            "partners": [
                {
                    "name": "ACCELVISION GBR",
                    "city": "Remagen",
                    "country": "Germany"
                },
                {
                    "name": "ACONDICIONAMIENTO TARRASENSE ASSOCIACION",
                    "city": "Terrassa",
                    "country": "Spain"
                },
                {
                    "name": "Controlling",
                    "city": "ORT",
                    "country": "Austria"
                },
                {
                    "name": "D'Appolonia Spa",
                    "city": "Genova",
                    "country": "Italy"
                },
                {
                    "name": "ETH Zürich",
                    "city": "ORT",
                    "country": "Switzerland"
                },
                {
                    "name": "Fraunhofer-Gesellschaft zur Förderu der angewandten Forschung e.V.",
                    "city": "München",
                    "country": "Germany"
                },
                {
                    "name": "HEAT GmbH",
                    "city": "Düsseldorf",
                    "country": "Germany"
                },
                {
                    "name": "LABOR S.R.L.",
                    "city": "Roma",
                    "country": "Italy"
                },
                {
                    "name": "RWTH Aachen",
                    "city": "Aachen",
                    "country": "Germany"
                },
                {
                    "name": "TEXCLUBTEC",
                    "city": "Milano",
                    "country": "Italy"
                },
                {
                    "name": "Texport GmbH",
                    "city": "Salzburg",
                    "country": "Austria"
                },
                {
                    "name": "active Photonics AG Visualisierungs- und Kommunikationssysteme",
                    "city": "Villach",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "European Commission",
                    "region": "EU",
                    "group": null,
                    "body": "European Commission",
                    "programs": [
                        "FP7 I.4 COOPERATION Nano-sciences, Nano-technlogies, Materials and new Production Technologies",
                        "7.Rahmenprogramm für Forschung",
                        "European Commission - Framework Programme"
                    ],
                    "call": "FP7-NMP-2008-SME-2",
                    "proposal": "228855"
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "8002",
            "name": "IV-ART",
            "title": "Intelligent Video Annotation and Retrieval Techniques",
            "website": null,
            "begins_on": "2009-09-01",
            "ends_on": "2012-08-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": "<p> </p> \n<p>Although content based image and video analysis have been used successfully to solve a set of real world tasks in recent years, approaches to employ such techniques for video retrieval have been significantly outperformed by pure text-based approaches operating on manually generated annotations. In this dissertation project, we intend to improve the state of the art for video retrieval within specific video domains. In order to achieve this, we will work on a novel video annotation approach where automatically detected objects-of-interest can be annotated by a user in an easy and comfortable way. A prototype will be developed to show the resulting video retrieval facility.</p> \n<p>From a technical point of view, this object-of-interest detection will be realized by the use of local features as starting point. Such features have already been studied extensively and it is expected that they are suitable to find correspondences between video frames automatically, and that such correspondences can be used to detect objects that appear repeatedly in a video. To increase the results of this baseline system, our research activities will focus on the following points:</p> \n<p>1) Combine local features to semantically higher features representing single objects</p> \n<p>2) Use domain-specific knowledge with emphasis on the relations between objects&nbsp;</p> \n<p>In this context, we will investigate different techniques to model, collect, and use the visual appearance of objects within videos and the relations of objects in specific video domains for object-of-interest detection. This basic research fits perfectly to the application of video annotation and retrieval because it is further planned to integrate the generation of such domain specific knowledge (which is also an open research issue) into the semi-automatic video annotation process.</p> \n<p></p>",
            "keywords": [
                "Video Annotation",
                "Video Retrieval",
                "Image Descriptor"
            ],
            "people": [
                {
                    "tid": "69044",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "97117",
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "role": "Leader"
                }
            ],
            "partners": [
                {
                    "name": "Joanneum Research Forschungsgesellschaft m. b. H.",
                    "city": "Graz",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E186"
            ],
            "member_org_nrs": [],
            "tid": "8659",
            "name": "Dokumentanalyse von Lebensläufen",
            "title": "Analysis of documents of curricula vitae",
            "website": null,
            "begins_on": "2010-06-01",
            "ends_on": "2010-12-31",
            "context": "JoinVision E-Services GmbH",
            "abstract": "Die Firma JoinVision e-Services GmbH ist Betreiber eines Jobportals für IT und Technik und beschäftigt sich unter anderem mit der semantischen Analyse von Lebensläufen in PDF-Format. In ihrem derzeitigen System, CVlizer, wird als ersten Schritt dieser Analyse eine einfache Strukturerkennung durchgeführt, die bei vielen Dokumenten unzureichende Ergebnisse liefert. Ziel dieses Projekts ist es, mit der Erfahrung und dem Know-How des Instituts für Computergraphik und Algorithmen der Technischen Universität Wien in den Bereichen document analysis und document understanding die ersten Schritte zu einer verbesserten Strukturerkennung vorzunehmen.",
            "keywords": [],
            "people": [
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Leader"
                },
                {
                    "tid": "44748",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Contract/collaboration",
                    "source": "JoinVision E-Services GmbH",
                    "region": null,
                    "group": null,
                    "body": null,
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E193-06"
            ],
            "member_org_nrs": [],
            "tid": "9257",
            "name": "Elevoc",
            "title": "Automatic Analysis of Elephant Vocalizations",
            "website": null,
            "begins_on": "2011-02-01",
            "ends_on": "2015-01-31",
            "context": "Austrian Science Fund (FWF)",
            "abstract": "The decline of habitat for elephants as a result of expanding human activity combined with increasing elephant numbers in spatially separated clusters of conservation areas is a serious conservation problem in Africa. Nearly 80% of the distributional range of elephants in Southern Africa stretches beyond the borders of officially protected areas. This fact leads to deadly conflicts between humans and elephants. We are a team of biologists and computer scientists who want to alleviate these conflicts by the development of early warning and information systems for humans living near the corridors who regularly get into serious conflict with traveling elephants. Such systems require the robust recognition and categorization of elephant vocalizations. Today, no system exists that fulfills the requirements for automatic recognition of elephant vocalizations under natural conditions. The goals of the proposed project are (i) to investigate the complex vocal communication system of elephants and (ii) to develop automatic techniques for the analysis of elephant calls. The acquired knowledge and the automatic techniques will form the basis for a future autonomous warning and information system. Elephants make extensive use of powerful infrasonic calls which travel distances of up to several kilometers. This qualifies the elephant as a perfect model species for acoustic observation since it is possible to detect elephants even if they are out of sight. We will incorporate visual information into the automatic analyses where improvements can be expected. We will investigate and evaluate among others automatic statistical methods to detect elephants, estimate group-size from vocalization rates, and classify call types automatically. Another focus will be the manual and automatic analysis of age and gender of the caller due to characteristics in call structure. This will allow us to determine the demography of families and populations. We additionally aim at defining family and bond group specific acoustic signatures and vocal dialects within the study population. The project is proposed for an initial duration of three years and will be conducted by the Department for Cognitive Biology of the University of Vienna in collaboration with the Interactive Media Systems Group, Institute of Software Technology and Interactive Systems, Vienna University of Technology. One 50% post-doc and two 60% doctoral students shall be employed by the project partners. There is a clear definition of responsibilities and exchange of results between the project partners. At the end of the project, a large set of fully annotated audio and video material will be available together with statistical evaluations of the material. Additionally, novel analysis techniques will be available that are able to robustly recognize and classify elephant calls from data obtained under natural conditions.",
            "keywords": [
                "Multimodal content-based analysis ",
                "automatic monitoring ",
                "bioacoustics ",
                "vocal communication ",
                "elephants"
            ],
            "people": [
                {
                    "tid": "159676",
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "role": "Leader"
                },
                {
                    "tid": "53451",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "53598",
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "role": "Member"
                }
            ],
            "partners": [
                {
                    "name": "Universität Wien, Department für Kognitionsbiologie",
                    "city": "Wien",
                    "country": "Austria"
                }
            ],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FWF - Österr. Wissenschaftsfonds",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Science Fund (FWF)",
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E186"
            ],
            "member_org_nrs": [],
            "tid": "9289",
            "name": "DocWrap",
            "title": "Next-Generation Wrapping from Print-Oriented Documents",
            "website": null,
            "begins_on": "2011-01-01",
            "ends_on": "2013-03-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": null,
            "keywords": [],
            "people": [
                {
                    "tid": "38472",
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "role": "Leader"
                },
                {
                    "tid": "44748",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                },
                {
                    "tid": "50520",
                    "first_name": null,
                    "last_name": null,
                    "role": "Member"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        },
        {
            "coordinator_org_nrs": [
                "E188"
            ],
            "member_org_nrs": [],
            "tid": "9895",
            "name": "Hofburg Tracking",
            "title": "Visual Tracking in historical buildings",
            "website": null,
            "begins_on": "2011-10-01",
            "ends_on": "2012-08-31",
            "context": "Austrian Research Promotion Agency (FFG)",
            "abstract": "Gegenstand des geförderten Vorhabens ist die Entwicklung und Implementierung eines neuartigen Algorithmus für mobile Endgeräte zur genauen Ortsbestimmung (Lokalisierung) in Echtzeit von Besuchern der Hofburg Vienna.",
            "keywords": [
                "Tracking",
                "Historical Buildings",
                "Optical Tracking"
            ],
            "people": [
                {
                    "tid": "46406",
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "role": "Leader"
                }
            ],
            "partners": [],
            "fundings": [
                {
                    "type": "Grant funds",
                    "source": "FFG - Österr. Forschungsförderungs- gesellschaft mbH",
                    "region": "National",
                    "group": null,
                    "body": "Austrian Research Promotion Agency (FFG)",
                    "programs": [],
                    "call": null,
                    "proposal": null
                }
            ]
        }
    ],
    "publications1": [
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "10046",
            "handle": "20.500.12708/10059",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Features in visual media analysis",
            "keywords": [
                "features",
                "film analysis",
                "film understanding"
            ],
            "abstract": "Today, film analysis is still a tedious process performed mostly manually by film experts. Existing computer vision approaches aim at improved retrieval and summarization methods rather than at film understanding. While current research is predominantly focused on the question what can we learn and extract from a film as the final product, this thesis aims at the study of the filmmaking process as a source for high-level content information.<br />The central question of this thesis is what can computer vision methods provide to support film analysis as performed by film expert? We discuss a possible mapping between factors that influence the production, presentation, and perception of movies, their application by means of well-established film techniques, and existing feature extraction methods in computer vision. This novel view on film analysis allows for the exploration and identification of three areas in the domain of automated film analysis and understanding. The first area comprises research tasks that have been subject to active research in the recent past. The second area covers research topics that are not immediately solvable for a fully automated computer vision approach without any prior knowledge. The last area identifies research tasks that are still open in the context of automated film analysis and understanding.<br />Finally, we introduce three novel research questions and possible solutions: camera take reconstruction, film comparison, and recur- ring element detection. Performed experiments reveal two significant potentials. First, they can assist film experts by providing support for tasks that are currently performed manually. Second, proposed algorithms blaze the trail for advanced application scenarios such as the analysis of different montage patterns, the identification of missing shots,  the reconstruction of the original film cut, or the detection of recurring elements.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaharieva, M. (2011). <i>Features in visual media analysis</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-55393</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "20111",
                    "name": "Zaharieva Maia - 2011 - Features in visual media analysis.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4962996,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10059/2/Zaharieva%20Maia%20-%202011%20-%20Features%20in%20visual%20media%20analysis.pdf"
                },
                {
                    "bsid": "87596",
                    "name": "Zaharieva Maia - 2011 - Features in visual media analysis.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 293086,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10059/5/Zaharieva%20Maia%20-%202011%20-%20Features%20in%20visual%20media%20analysis.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Stephane",
                    "last_name": "Marchand-Maillet",
                    "position": 1,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "10109",
            "handle": "20.500.12708/10122",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Development of an active motion capture suit for teaching motion skills",
            "keywords": [
                "Motion Skill Training",
                "Active Optical Motion Suit",
                "Full Body Motion Capturing",
                "Active Infrared Markers",
                "Virtual Reality"
            ],
            "abstract": "Current multimedia support for teaching and practicing motion skills is usually limited to video and two-dimensional graphics.<br />Especially in areas like rehabilitation and sports the employment of virtual reality applications for educational purposes could offer considerable benefits, like a detailed real-time feedback about the training progress or the support of motivation and autonomy of the users. However for any virtual reality motion skill training system a means for motion input is essential.<br />In particular application areas related to sports impose a number of special requirements that have to be considered. Most full body motion capture devices are either specifically designed for a certain application or not suitable for sports due to different reasons. For example in a lot of sporting activities it might be required to lie on the floor, perform rolls or make direct contact with other tracked persons. For that reason motion capture techniques where the user has to be equipped with relatively large devices could cause pain if making physical contact to hard surfaces like the floor. Furthermore matters like possible fast movements, sufficient freedom of motion and hygienic issues have to be considered as well.<br />The authors show in the course of this work, that a motion capture suit with active infrared optical markers can be constructed to meet the special challenges of motion skill training. For that purpose they compile a wide range of related requirements and devise concepts to fulfill these needs. Subsequently these concepts are applied to construct a fully functional prototype, suitable for a broad range of sporting activities. In order to assess the performance of the active motion suit in an educational context and to demonstrate the potential of the employment of a virtual environment,  the constructed input device is used in an actual virtual reality application for teaching motion skills.<br />The active motion capture suit developed in this project is suitable for sports and rehabilitation, but not limited to these specific application areas. Most importantly the authors successfully accomplished to incorporate all required wiring and electronic components unobtrusive.<br />Due to individually controllable markers the layout can be optimized for different setups and tracking algorithms.<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vonach, E., &#38; Gerstweiler, G. (2011). <i>Development of an active motion capture suit for teaching motion skills</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-55804</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "20237",
                    "name": "Vonach Emanuel - 2011 - Development of an active motion capture suit for...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 5932179,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10122/2/Vonach%20Emanuel%20-%202011%20-%20Development%20of%20an%20active%20motion%20capture%20suit%20for...pdf"
                },
                {
                    "bsid": "87225",
                    "name": "Vonach Emanuel - 2011 - Development of an active motion capture suit for...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 268500,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10122/5/Vonach%20Emanuel%20-%202011%20-%20Development%20of%20an%20active%20motion%20capture%20suit%20for...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 2,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "10185",
            "handle": "20.500.12708/10198",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Pivy : embedding a dynamic scripting language into a scene graph library",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fahmy, T. (2006). <i>Pivy : embedding a dynamic scripting language into a scene graph library</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-25649</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "20389",
                    "name": "Fahmy Tamer - 2006 - Pivy embedding a dynamic scripting language into a scene...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2205448,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10198/2/Fahmy%20Tamer%20-%202006%20-%20Pivy%20embedding%20a%20dynamic%20scripting%20language%20into%20a%20scene...pdf"
                },
                {
                    "bsid": "87821",
                    "name": "Fahmy Tamer - 2006 - Pivy embedding a dynamic scripting language into a scene...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 288252,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10198/5/Fahmy%20Tamer%20-%202006%20-%20Pivy%20embedding%20a%20dynamic%20scripting%20language%20into%20a%20scene...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Tamer",
                    "last_name": "Fahmy",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "46406"
                },
                {
                    "first_name": "Dieter",
                    "last_name": "Schmalstieg",
                    "position": 1,
                    "role": "Supervisor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "10217",
            "handle": "20.500.12708/10230",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "A vision-based system for fingertip detection on tracked interactive surfaces",
            "keywords": [
                "computer vision",
                "multi-touch",
                "user interface",
                "interactive",
                "surface tracking",
                "fingertip detection"
            ],
            "abstract": "Multi-touch sensing on interactive tabletops and other flat surfaces has become a major trend in the field of human-computer interaction over the past years. The main objective is to provide a touch interface for the direct manipulation of digital content by multiple users at the same time. Within these terms the appropriate design of the interactive surface as well as the automatic detection and tracking of fingertips are crucial.<br />Popular techniques for fingertip and touch detection use specific contact-sensitive computer hardware that is either relying on optical sensing in a controlled environment or capacitive surface technology.<br />Since such hardware is usually custom-made, those interaction systems are mostly expensive, inconvenient to move, install and operate and not scalable. To overcome these drawbacks, a number of multi-touch researchers strive for alternative techniques to provide more adjustable interfaces. Here, everyday surfaces shall be augmented with the functionality of touch-based user interfaces, while using none but off-the-shelf and affordable vision hardware and relying on state-of-the-art computer vision methods and algorithms.<br />This work starts off with the description, discussion and evaluation of common surface hardware technologies as well as existing techniques based on simple video hardware. After that, a set of straightforward computer vision algorithms is selected in order to develop a stand-alone software application. The application is capable of continuously tracking a rectangular surface as well as detecting multiple fingertips that hover above its top. This work is concluded by providing relevant empirical results on computer vision-based rectangle and fingertip detection in natural indoor environments.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Autengruber, M. (2010). <i>A vision-based system for fingertip detection on tracked interactive surfaces</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-39079</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "20453",
                    "name": "Autengruber Markus - 2010 - A vision-based system for fingertip detection on...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 8442770,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10230/2/Autengruber%20Markus%20-%202010%20-%20A%20vision-based%20system%20for%20fingertip%20detection%20on...pdf"
                },
                {
                    "bsid": "87639",
                    "name": "Autengruber Markus - 2010 - A vision-based system for fingertip detection on...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 174673,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10230/5/Autengruber%20Markus%20-%202010%20-%20A%20vision-based%20system%20for%20fingertip%20detection%20on...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Autengruber",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "46406"
                },
                {
                    "first_name": "Johannes",
                    "last_name": "Mehling",
                    "position": 1,
                    "role": "Supervisor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "1023",
            "handle": "20.500.12708/1039",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Backward compatible HDR stereo matching: a hybrid tone-mapping-based framework",
            "keywords": [
                "Tone mapping",
                "Stereo matching",
                "Disparity map",
                "Low dynamic range",
                "High dynamic range",
                "Markov random field",
                "Graph cut"
            ],
            "abstract": "Stereo matching under complex circumstances, such as low-textured areas and high dynamic range (HDR) scenes, is an ill-posed problem. In this paper, we introduce a stereo matching approach for real-world HDR scenes which is backward compatible to conventional stereo matchers. For this purpose, (1) we compare and evaluate the tone-mapped disparity maps to find the most suitable tone-mapping approach for the stereo matching purpose. Thereof, (2) we introduce a combining graph-cut based framework for effectively fusing the tone-mapped disparity maps obtained from different tone-mapped input image pairs. And finally, (3) we generate reference ground truth disparity maps for our evaluation using the original HDR images and a customized stereo matching method for HDR inputs. Our experiments show that, combining the most effective features of tone-mapped disparity maps, an improved version of the disparity is achieved. Not only our results reduce the low dynamic range (LDR), conventional disparity errors by the factor of 3, but also outperform the other well-known tone-mapped disparities by providing the closest results to the original HDR disparity maps.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Akhavan, T., &#38; Kaufmann, H. (2015). Backward compatible HDR stereo matching: a hybrid tone-mapping-based framework. <i>EURASIP JOURNAL ON IMAGE AND VIDEO PROCESSING</i>. https://doi.org/10.1186/s13640-015-0092-3</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "2065",
                    "name": "Akhavan Tara - 2015 - Backward compatible HDR stereo matching a hybrid...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1744873,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1039/2/Akhavan%20Tara%20-%202015%20-%20Backward%20compatible%20HDR%20stereo%20matching%20a%20hybrid...pdf"
                },
                {
                    "bsid": "44066",
                    "name": "Backward compatible HDR stereo matching a hybrid tone-mapping-based framework.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 52675,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1039/8/Backward%20compatible%20HDR%20stereo%20matching%20a%20hybrid%20tone-mapping-based%20framework.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Tara",
                    "last_name": "Akhavan",
                    "position": 1,
                    "role": "Author",
                    "tid": "247963"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "10408",
            "handle": "20.500.12708/10421",
            "doi": "10.34726/hss.2018.47642",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "TrACTOr & StARboard : tracking and haptic interaction for learning in AR",
            "keywords": [
                "Augmented Reality",
                "AR",
                "Haptic Interaction",
                "Depth Tracking",
                "HCI",
                "Human Computer Interaction",
                "Learning Application",
                "Educational Application"
            ],
            "abstract": "In our modern and computer-driven environment the topic of Human Computer Interaction becomes more important, in order to design efficient and flexible interfaces. A field of increasing importance is Augmented Reality, where current interaction techniques and principles are not yet satisfying or sufficient for the great potential of these technologies. Especially haptic feedback during user interaction is an important and complex topic. With the ACTO system, Vonach et al. developed a Tangible User Interface (TUI) for prototyping haptic interfaces. In this thesis we use this system to provide haptic interaction for Augmented Reality (AR). Therefore we first developed an alternative tracking system for the ACTO robots, called TrACTOr. With this new tracking solution we want to overcome some shortcomings of the existing tracking solution, as well as fit the needs of an AR application better. Based on the new tracking system we implemented an AR based learning application with haptic user interaction, called StARboard. With StARboard we wanted to investigate the benefits of haptic interaction for learning systems for the users. We present the concepts of the tracking solution and the learning application. The development of additional hardware for the ACTO robots, as well as the system setup is explained. Also the implementation of TrACTOr and StARboard is discussed in detail. The technical evaluation of the system showed that the TrACTOr tracking system has a high enough performance and accuracy to provide real time tracking data for an AR based application. The user evaluation we performed with the implemented system showed that for learning systems, haptic interaction in combination with AR can match the performance of conventional desktop applications or gesture based AR applications. The users rated this new interaction technique as very intuitive, fast and easy to use.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schindler, C. (2018). <i>TrACTOr &#38; StARboard : tracking and haptic interaction for learning in AR</i> [Diploma Thesis]. reposiTUm. https://doi.org/10.34726/hss.2018.47642</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "20835",
                    "name": "Schindler Christoph - 2018 - TrACTOr StARboard tracking and haptic interaction...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 17913022,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10421/2/Schindler%20Christoph%20-%202018%20-%20TrACTOr%20StARboard%20tracking%20and%20haptic%20interaction...pdf"
                },
                {
                    "bsid": "88890",
                    "name": "Schindler Christoph - 2018 - TrACTOr StARboard tracking and haptic interaction...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 275842,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10421/5/Schindler%20Christoph%20-%202018%20-%20TrACTOr%20StARboard%20tracking%20and%20haptic%20interaction...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Christoph",
                    "last_name": "Schindler",
                    "position": 1,
                    "role": "Author",
                    "tid": "68690"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "40682"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "10452",
            "handle": "20.500.12708/10465",
            "doi": "10.34726/hss.2019.33082",
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automation of the virtual jump simulator",
            "keywords": [
                "Virtual Reality",
                "Automation",
                "Internet of Things"
            ],
            "abstract": "This thesis presents an approach for integrating multi-sensory feedback with a computer graphics environment designed for use with Virtual Reality, namely the Virtual Jump Simulator, or Jumpcube for short. Since none of the existing solutions would meet our requirements, we decided to build an open, modular, and scalable platform for creating multi-sensory feedback devices to be used in Virtual Reality. To keep development costs and time low we used mainly off-the-shelf hardware components. For the same reason we also opted for a communication backbone based on Ethernet, modern Web technologies, namely JSON and the SocketIO protocol, and Python as the programming language for the logic running on the multi-sensory feedback devices. We demonstrated, that such a communication backbone is flexible and both easy to implement and to maintain. Further, we showed that the additional latency introduced by the use of high-level programming languages and non-real time capable communication is negligible in the scope of a computer graphics environment. To determine the impact the system has on the user we conducted both a quantitative and a qualitative experiment. Both showed that multi-sensory feedback in a Virtual Reality environment is noticed by the vast majority of the users and has a positive influence on the overall user experience, in particular on the degree of presence reached by the test subject.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Berlanda, J. (2019). <i>Automation of the virtual jump simulator</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2019.33082</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "20923",
                    "name": "Berlanda Juri - 2019 - Automation of the virtual jump simulator.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 5152385,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10465/2/Berlanda%20Juri%20-%202019%20-%20Automation%20of%20the%20virtual%20jump%20simulator.pdf"
                },
                {
                    "bsid": "88489",
                    "name": "Berlanda Juri - 2019 - Automation of the virtual jump simulator.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 203605,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10465/5/Berlanda%20Juri%20-%202019%20-%20Automation%20of%20the%20virtual%20jump%20simulator.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Juri",
                    "last_name": "Berlanda",
                    "position": 1,
                    "role": "Author",
                    "tid": "180019"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "10628",
            "handle": "20.500.12708/10641",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "MINT - a framework for the design and development of multimodal interaction on multi-touch surfaces",
            "keywords": [
                "multi-touch",
                "interaction",
                "framework",
                "gestures",
                "human-computer interaction"
            ],
            "abstract": "In recent years, multi-touch technology has had its breakthrough in research as well as in consumer electronics. This development was mainly driven by multi-touch enabled devices entering the mass market.<br />Compared to conventional single-pointer interaction using a mouse, multi- touch surfaces allow for much richer and more diverse ways of Human-Computer Interaction (HCI). Users can interact with an application in a direct and natural way using their fingers or whole hands to touch the surface, hence the term natural user interfaces (NUI).<br />To support the development of applications for multi-touch devices a lot of different software frameworks have emerged to assist developers in the process. Although all of these frameworks provide a set of well-known and ready-to-use gestures, they lack flexibility concerning the definition and extensibility of gestures as well as reusability in different application scenarios.<br />The goal of this work is to overcome those shortcomings and develop methods for the generalization of multi-touch interactions to allow a modular definition of gestures at a high level of abstraction. This enables to decouple the logic of the interaction relevant parts of an application completely from the application's run-time logic and use interaction techniques in a platform and application independent manner.<br />Therefore, different gestures can easily be interchanged and modified without having to change the application itself.<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pilz, F. (2011). <i>MINT - a framework for the design and development of multimodal interaction on multi-touch surfaces</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-51342</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "21275",
                    "name": "Pilz Ferdinand - 2011 - MINT - a framework for the design and development of...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 13499895,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10641/2/Pilz%20Ferdinand%20-%202011%20-%20MINT%20-%20a%20framework%20for%20the%20design%20and%20development%20of...pdf"
                },
                {
                    "bsid": "88918",
                    "name": "Pilz Ferdinand - 2011 - MINT - a framework for the design and development of...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 281545,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10641/5/Pilz%20Ferdinand%20-%202011%20-%20MINT%20-%20a%20framework%20for%20the%20design%20and%20development%20of...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Ferdinand",
                    "last_name": "Pilz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Pintaric",
                    "position": 1,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "10631",
            "handle": "20.500.12708/10644",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Design eines interaktiven taktilen Shape Displays mittels Schrittmotoren und dem Bowdenzug-Konzept",
            "keywords": [
                "Reactive shape display tactile haptics"
            ],
            "abstract": "This thesis presents the so called reactive Shape display, a tactile display for displaying moving and static two and a half-dimensional shapes, which allows the user to interact. The user interaction takes place by pressing the display pins. For this purpose, a concept is introduced and implemented which decouples the drive mechanically from the tactile element to allow a flexible design in terms of display resolution. The design and implementation generally aime at a technical simple and economical cost-effective solution. It is given an introduction to the concepts of touch, providing the theoretical foundations of touch and the determination of biophysical parameters in this context. Furthermore, improvement and expansion opportunities that resulted from the implementation process are cited.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Weissenböck, J. (2011). <i>Design eines interaktiven taktilen Shape Displays mittels Schrittmotoren und dem Bowdenzug-Konzept</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-48979</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "21281",
                    "name": "Weissenboeck Johannes - 2011 - Design eines interaktiven taktilen Shape Displays...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 7437277,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10644/2/Weissenboeck%20Johannes%20-%202011%20-%20Design%20eines%20interaktiven%20taktilen%20Shape%20Displays...pdf"
                },
                {
                    "bsid": "88731",
                    "name": "Weissenboeck Johannes - 2011 - Design eines interaktiven taktilen Shape Displays...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 154261,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/10644/5/Weissenboeck%20Johannes%20-%202011%20-%20Design%20eines%20interaktiven%20taktilen%20Shape%20Displays...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Johannes",
                    "last_name": "Weissenböck",
                    "position": 1,
                    "role": "Author",
                    "tid": "44686"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E104-03",
                "E104-04",
                "E193-02",
                "E193-03",
                "E202-02",
                "E210-01",
                "E259-01"
            ],
            "pid": "106642",
            "handle": "20.500.12708/101881",
            "doi": null,
            "year": 2022,
            "issued": "2022-08-26",
            "issued_on": "2022-08-26",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Advanced Computational Design – digitale Methoden für die frühe Entwurfsphase",
            "keywords": [
                "Building materials",
                "CAD – IT/Automatical/CAD",
                "design interaction",
                "design methodology",
                "Digital design/Optimization",
                "digitalization",
                "form finding",
                "simulation"
            ],
            "abstract": "Advanced Computational Design. The SFB Advanced Computational Design addresses the research question of how to advance design tools and processes through multi- and interdisciplinary basic research. We will develop advanced computational design tools in order to improve design quality and efficiency of processes in architecture and construction. The proposed research is structured in three areas: design methodology (A1), visual and haptic design interaction (A2) and form finding (A3). A1 focuses on the conceptual basis for new digital methods of design based on machine learning. A1 also acts as a platform for integrating and evaluating the computational tools and methods developed in A2 and A3. A2 investigates real-time global-illumination and optimization algorithms for lighting design, as well as a new method for large-scale haptic interactions in virtual reality. In A3, form finding will be explored regarding geometric, mechanical and material constraints, in particular: paneling of complex shapes by patches of certain surface classes while optimizing the number of molds; algorithms for finding new transformable quad-surfaces; mechanical models for an efficient simulation of bio-composite material systems. Furthermore, new ways of form finding will be explored through physical experiments, which will allow for reconsidering model assumptions and constraints, validating the developed algorithmic approaches, and finding new ones.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Wimmer, M., Kovacic, I., Ferschin, P., Rist, F., Hensel, M., Schinegger, K., Rutzinger, S., Kaufmann, H., Kilian, M., Müller, C., Izmestiev, I., Nawratil, G., Füssl, J., Stavric, M., Hahn, D., &#38; Suter, G. (2022). Advanced Computational Design – digitale Methoden für die frühe Entwurfsphase. <i>Bautechnik</i>, <i>99</i>(10), 720–730. https://doi.org/10.1002/bate.202200057</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Wimmer",
                    "position": 1,
                    "role": "Author",
                    "tid": "40666"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 2,
                    "role": "Author",
                    "tid": "42543"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Ferschin",
                    "position": 3,
                    "role": "Author",
                    "tid": "135460"
                },
                {
                    "first_name": "Florian",
                    "last_name": "Rist",
                    "position": 4,
                    "role": "Author",
                    "tid": "49692"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Hensel",
                    "position": 5,
                    "role": "Author",
                    "tid": "315688"
                },
                {
                    "first_name": "Kristina",
                    "last_name": "Schinegger",
                    "position": 6,
                    "role": "Author",
                    "tid": "39103"
                },
                {
                    "first_name": "Stefan",
                    "last_name": "Rutzinger",
                    "position": 7,
                    "role": "Author",
                    "tid": "43261"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 8,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kilian",
                    "position": 9,
                    "role": "Author",
                    "tid": "39140"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Müller",
                    "position": 10,
                    "role": "Author",
                    "tid": "200745"
                },
                {
                    "first_name": "Ivan",
                    "last_name": "Izmestiev",
                    "position": 11,
                    "role": "Author",
                    "tid": "318163"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Nawratil",
                    "position": 12,
                    "role": "Author",
                    "tid": "53597"
                },
                {
                    "first_name": "Josef",
                    "last_name": "Füssl",
                    "position": 13,
                    "role": "Author",
                    "tid": "46732"
                },
                {
                    "first_name": "Milena",
                    "last_name": "Stavric",
                    "position": 14,
                    "role": "Author"
                },
                {
                    "first_name": "David",
                    "last_name": "Hahn",
                    "position": 15,
                    "role": "Author",
                    "tid": "340839"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Suter",
                    "position": 16,
                    "role": "Author",
                    "tid": "46013"
                }
            ],
            "foci": [],
            "projects": [
                "1745908"
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "1074",
            "handle": "20.500.12708/1090",
            "doi": "10.34726/hss.2020.73320",
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "VR Bridges: Simulation von unebenen Flächen in VR",
            "keywords": [
                "Virtuelle Realität",
                "Simulation von unebenen Oberflächen",
                "multi-sensorische Sinnesreize"
            ],
            "abstract": "Virtual reality (VR) promises boundless potential for experiences. Yet, due to technical restrictions, current VR experiences are often limited in many ways and incomparable to their real-world counterparts. Walkable smooth uneven surfaces are inherent to reality but lacking in VR. At the same time, VR enables the alteration and manipulation of perception, offering tools for reshaping the experience. In this thesis, we explore the possibility of simulating walkable smooth uneven surfaces in VR via a multi-sensory stimulation approach. We examine human height and slant perception and incorporate our findings into a multi-modal approach by combining visual manipulations, haptic and vibrotactile stimuli. Our approach is realized by constructing physical bridge props and creating a complex software application to introduce multi-sensory stimuli to the user. The simulation is evaluated in two user studies, each focusing on one of two differently shaped physical bridge props. In the studies, we evaluate the feasibility of a flat and an upward curved prop for the simulation of different virtual surface heights. The data collected during the studies is subjected to a qualitative and quantitative analysis. Our results suggest that the use of a curved prop enables the convincing simulation of significantly higher uneven surfaces than the actual height of the prop. The haptic feedback of the curved surface and the proprioceptive cues of actual vertical traversal facilitate user provided height and slant estimations to be closer to the values suggested by the visual cues. The use of a flat prop is less realistic and leads to height and slant underestimations, despite the simulated visual height and slant cues.However, a flat surface might be still used to simulate indentations and protrusions with smaller height differences.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kovács, B. I. (2020). <i>VR Bridges: Simulation von unebenen Flächen in VR</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2020.73320</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "2167",
                    "name": "Kovacs Balint Istvan - 2020 - VR Bridges Simulation von unebenen Flaechen in VR.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 5736880,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1090/2/Kovacs%20Balint%20Istvan%20-%202020%20-%20VR%20Bridges%20Simulation%20von%20unebenen%20Flaechen%20in%20VR.pdf"
                },
                {
                    "bsid": "73487",
                    "name": "Kovacs Balint Istvan - 2020 - VR Bridges Simulation von unebenen Flaechen in VR.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 242710,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1090/5/Kovacs%20Balint%20Istvan%20-%202020%20-%20VR%20Bridges%20Simulation%20von%20unebenen%20Flaechen%20in%20VR.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Bálint István",
                    "last_name": "Kovács",
                    "position": 1,
                    "role": "Author",
                    "tid": "262858"
                },
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "232367"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "10987",
            "handle": "20.500.12708/11000",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Mobiles Lifelogging auf der Android-Plattform",
            "keywords": [
                "lifelog",
                "electronic diary",
                "Android",
                "mobile programming"
            ],
            "abstract": "This thesis deals with the design and development of a lifelogging architecture based on the Android platform. The aim is to realize a prototype called Android Lifelogging (ALL), which enables the autonomous recording of personal multimedia data. By a dynamic configuration, ALL is adaptable to many application scenarios. The collected data such as photos, SMS messages or phone calls can be directly exchanged into common formats. A visualization of the collected data is also supported. The basic ideas and procedures of lifelogging systems are explained. Technologies and practices that are relevant for indexing of Lifelogging data are explained. Multimodal approaches, a comparison of existing approaches and commercial Lifelogging systems are shown. The requirements of ALL, whose architecture and basic concepts are explained in detail. The extensive configuration, called Lifelogging Description Language (LLDL) is presented. Finally, the findings are interpreted and visualized. Experiments show the accuracy of data collection.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hochstöger, R. (2013). <i>Mobiles Lifelogging auf der Android-Plattform</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-47439</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "21993",
                    "name": "Hochstoeger Roman - 2013 - Mobiles Lifelogging auf der Android-Plattform.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 6289080,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/11000/2/Hochstoeger%20Roman%20-%202013%20-%20Mobiles%20Lifelogging%20auf%20der%20Android-Plattform.pdf"
                },
                {
                    "bsid": "88535",
                    "name": "Hochstoeger Roman - 2013 - Mobiles Lifelogging auf der Android-Plattform.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 238186,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/11000/5/Hochstoeger%20Roman%20-%202013%20-%20Mobiles%20Lifelogging%20auf%20der%20Android-Plattform.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Roman",
                    "last_name": "Hochstöger",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "11999",
            "handle": "20.500.12708/12011",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Long distance distribution of virtual and augmented reality applications",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Csisinko, M. (2006). <i>Long distance distribution of virtual and augmented reality applications</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-36132</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "24016",
                    "name": "Csisinko Mathis - 2006 - Long distance distribution of virtual and augmented...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 12013650,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12011/2/Csisinko%20Mathis%20-%202006%20-%20Long%20distance%20distribution%20of%20virtual%20and%20augmented...pdf"
                },
                {
                    "bsid": "88681",
                    "name": "Csisinko Mathis - 2006 - Long distance distribution of virtual and augmented...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 176437,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12011/5/Csisinko%20Mathis%20-%202006%20-%20Long%20distance%20distribution%20of%20virtual%20and%20augmented...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 1,
                    "role": "Author",
                    "tid": "47153"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "1204",
            "handle": "20.500.12708/1220",
            "doi": "10.34726/hss.2020.58980",
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Evaluation of microservice implementation approaches for image processing",
            "keywords": [
                "Microservices",
                "FaaS",
                "Software-Architectures"
            ],
            "abstract": "In recent years we can observe an increasing interest in the relatively young topic of Microservice Architectures. As a consequence thereof, the number of experiences with Microservices is increasing and both advantages, as well as disadvantages become a commonly discussed topic. While first best practices and architectural patterns regarding the construction of such systems emerged, there is a lack of discussion regarding the construction and applicability in certain contexts of Microservice Systems. The current state of knowledge revolves primarily around general properties of microservices and therefore use cases in contexts with special characteristics are still uncharted terrain. This thesis provides a closer look at Microservices in the context of image processing systems. The evaluation was performed based on a system offering five commonly used image transformations. With a given set of requirements this system was implemented without the use of a framework, with a framework and with an Function as a Service approach to achieve a Microservice Architecture. All three of the resulting applications were evaluated via quantitative metrics such as the time spent on development and performance characteristics. A qualitative evaluation was performed as well, which compared advantages and disadvantages of the different approaches and yielded findings regarding Microservice Architectures in the context of image processing. The results showed that no single, generally preferable approach exists for such a system. All three implementations offered different advantages and disadvantages in the form of a trade-off between flexibility and development speed. However, there were indications that generally a Function as a Service approach can yield the most benefits, because of a strong reduction in both development time and the complexity of the resulting source code. The combination of image processing and Microservices lead to properties which had a strong influence on the resulting architecture. This was primarily caused by the long processing time of the requests and the high load on the network caused by the transfers of image files between services. As a result thereof, commonly used and simple approaches would often have resulted in serious disadvantages, and more complex mechanisms such as asynchronous communication were necessary throughout the whole system. However, it became apparent that a correct application of microservice principles leads to better maintainability and scalability of such a system.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kruisz, M. (2020). <i>Evaluation of microservice implementation approaches for image processing</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2020.58980</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "2427",
                    "name": "Kruisz Manuel - 2020 - Evaluation of microservice implementation approaches for...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 976012,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1220/2/Kruisz%20Manuel%20-%202020%20-%20Evaluation%20of%20microservice%20implementation%20approaches%20for...pdf"
                },
                {
                    "bsid": "73836",
                    "name": "Kruisz Manuel - 2020 - Evaluation of microservice implementation approaches for...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 221073,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1220/5/Kruisz%20Manuel%20-%202020%20-%20Evaluation%20of%20microservice%20implementation%20approaches%20for...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Manuel",
                    "last_name": "Kruisz",
                    "position": 1,
                    "role": "Author",
                    "tid": "218261"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "12185",
            "handle": "20.500.12708/12197",
            "doi": null,
            "year": 2004,
            "issued": "2004",
            "issued_on": "2004-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Geometry education with augmented reality",
            "keywords": [],
            "abstract": "To fill the gap of next-generation user interfaces for mathematics and geometry education Construct3D is introduced a three-dimensional dynamic geometry construction tool that can be used in high school and university education. This system uses Augmented Reality (AR) to provide a natural setting for face-to-face collaboration of teachers and students. The main advantage of using AR is that students actually see three dimensional objects which they until now had to calculate and construct with traditional (mostly pen and paper) methods.<br />By working directly in 3D space, complex spatial problems and spatial relationships may be comprehended better and faster than with traditional methods.<br />After a description of Construct3D's design various hardware setups are presented that are suitable for educational purposes. Immersive, semi-immersive, mobile and hybrid setups are studied for their applicability to geometry education. An immersive setup that uses head mounted displays is most favored by teachers and students. It allows users to actually \"walk around\" geometric objects which are fixed in space. In order to adapt software and hardware to users' needs user interfaces were redesigned and in depth research was done on usability design.<br />Development steps and results present how Construct3D's look and feel was considerably improved.<br />A wide range of selected geometric content from basic and advanced high school geometry to university education is presented. Diverse examples demonstrate the potential and robust constructive capabilities of the dynamic geometry application.<br />Finally results from two evaluations show that Construct3D is easy to use, requires little time to learn, encourages learners to explore geometry and can be used in a consistent way.<br />At the end an outlook is given on  ongoing and future work and basic guidelines to developers of educational VR/AR applications are summarized.<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2004). <i>Geometry education with augmented reality</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-12667</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "24388",
                    "name": "Kaufmann Hannes - 2004 - Geometry education with augmented reality.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4432031,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12197/2/Kaufmann%20Hannes%20-%202004%20-%20Geometry%20education%20with%20augmented%20reality.pdf"
                },
                {
                    "bsid": "90992",
                    "name": "Kaufmann Hannes - 2004 - Geometry education with augmented reality.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 388140,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12197/5/Kaufmann%20Hannes%20-%202004%20-%20Geometry%20education%20with%20augmented%20reality.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Dieter",
                    "last_name": "Schmalstieg",
                    "position": 1,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Wagner",
                    "position": 2,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Billinghurst",
                    "position": 1,
                    "role": "Supervisor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "12329",
            "handle": "20.500.12708/12340",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Dynamic differential geometry in an educational augmented reality application",
            "keywords": [
                "differential geometry",
                "augmented reality",
                "Construct3D",
                "geometry",
                "education"
            ],
            "abstract": "In this thesis a number of geometry software packages leading both to static and dynamic constructions and their particular features will be presented. Afterwards Construct3D a 3D dynamic geometry construction tool will be introduced. It is based on the Augmented Reality System Studierstube. Construct3D's greatest advantage compared to other dynamic geometry software is the possibility for users to see the real environment augmented with virtual content with the aid of a head mounted display. That gives the users, mainly high school and university students, the opportunity to actually construct, explore and interact with three dimensional content in \"real\" 3D space. The practical part of this thesis was the implementation of a number of new functions for Construct3D. Several tools have been developed to enhance the understanding of the term curvature of curves and surfaces. To complement the already available sweep function of Construct3D helical and general sweeps have been implemented.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Tschurlovits, M.-T. (2008). <i>Dynamic differential geometry in an educational augmented reality application</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-22146</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "24674",
                    "name": "Tschurlovits Marie-Theres - 2008 - Dynamic differential geometry in an...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3289990,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12340/2/Tschurlovits%20Marie-Theres%20-%202008%20-%20Dynamic%20differential%20geometry%20in%20an...pdf"
                },
                {
                    "bsid": "90797",
                    "name": "Tschurlovits Marie-Theres - 2008 - Dynamic differential geometry in an...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 132510,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12340/5/Tschurlovits%20Marie-Theres%20-%202008%20-%20Dynamic%20differential%20geometry%20in%20an...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Marie-Theres",
                    "last_name": "Tschurlovits",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-06"
            ],
            "pid": "126493",
            "handle": "20.500.12708/121546",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Guided projections for analysising the structure of high dimensional data",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ortner, T., Filzmoser, P., Zaharieva, M., Breiteneder, C., &#38; Brodinova, S. (2016). <i>Guided projections for analysising the structure of high dimensional data</i>. International Conference of the ERCIM WG on Computational and Methodological Statistics, Seville, Spain, EU. http://hdl.handle.net/20.500.12708/121546</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 1,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 2,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 3,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 5,
                    "role": "Author",
                    "tid": "281480"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "12656",
            "handle": "20.500.12708/12667",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Event-driven 3D vision for human activity analysis in context of dance and fitness training of elderly people",
            "keywords": [
                "Event-Driven 3D Vision",
                "Human Activity Analysis",
                "Human Motion",
                "Hidden Markov Models",
                "Dance and Fitness Training of Elderly People",
                "Machine Learning",
                "relative Pixel Count",
                "relative Disparity"
            ],
            "abstract": "Over the last years many implementations concerning the recognition of human motion have been developed. In doing so different systems for human motion detection reaching from recognition of simple gestures to more dynamic complex motions have been invented. The application area of these systems is thereby wide spread from input for Human Computer Interaction to human motion analysis in the field of rehabilitation exercises or sports.<br />Systems that are designed for elderly people are becoming more important, especially in the physical training application area. This is because the population is tending to live to an older age and there will be more and more elderly people in the near future.<br />In this thesis a system for recognition of human motion in the area of dance and fitness training for elderly people is introduced. This module within the EU project Silvergame is thereby intended to help elderly people to keep their level of health as well as to gain a higher fitness level so that they can stay healthy to an older age. With the system the users can then be encouraged to move more by performing the dance which they see on their home TV screen. In doing so such a dance consists of different human activities which the system recognizes. Furthermore, it also provides some sort of feedback via the given output device. As the input device, a novel event-driven 3D vision sensor, developed at the AIT Austrian Institute of Technology is used in this approach. What is special in this case is that only data is transferred if an intensity change in the field of view is detected. Therefore, less data then with ordinary video systems is generated. Another difference worth mentioning is that this information is communicated not frame-based but pixel wise.<br />Keeping this constraint in mind and based on the  information transferred from this sensor, elementary features that are used as input for classification are obtained.<br />Through a detailed research of the literature about the up-to-date classification methods, the most promising technique and features for the motion detection system were chosen. This thesis thereby shows the performance of the designed application and points out the opportunity for further employments. Though it was significant how the chosen classification method can be used for the obtained features from the received data. Additionally first performance measurements were done.<br />For this first implementation MATLAB was chosen as the main platform and further applications shall be based on this gained knowledge.<br />For experimentation with the implemented algorithm a database including 580 samples with 8 different activities from 15 individuals, using the 3D sensor, was recorded. To obtain representative experimentation results a cross validation was applied and different settings were used to compare the results. Additionally, test sessions were done on different data sets and for the best results the training and evaluation time was recorded to point out the possibility of real-time usage. The best results thereby reached an average correct recognition rate of around 96%.<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hahn, T. (2011). <i>Event-driven 3D vision for human activity analysis in context of dance and fitness training of elderly people</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-43662</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "25328",
                    "name": "Hahn Thomas - 2011 - Event-driven 3D vision for human activity analysis in...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1387992,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12667/2/Hahn%20Thomas%20-%202011%20-%20Event-driven%203D%20vision%20for%20human%20activity%20analysis%20in...pdf"
                },
                {
                    "bsid": "90269",
                    "name": "Hahn Thomas - 2011 - Event-driven 3D vision for human activity analysis in...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 131952,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12667/5/Hahn%20Thomas%20-%202011%20-%20Event-driven%203D%20vision%20for%20human%20activity%20analysis%20in...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Hahn",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-06"
            ],
            "pid": "126964",
            "handle": "20.500.12708/122019",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Local projection for outlier detection",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ortner, T., Filzmoser, P., Brodinova, S., Zaharieva, M., &#38; Breiteneder, C. (2017). <i>Local projection for outlier detection</i>. Olomouc Days of Applied Mathematics (ODAM 2017), Olomouc, EU. http://hdl.handle.net/20.500.12708/122019</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 1,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 2,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 3,
                    "role": "Author",
                    "tid": "281480"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 4,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 5,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": [
                "220239"
            ]
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-06"
            ],
            "pid": "126978",
            "handle": "20.500.12708/122033",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Finding groups in large and high-dimensional data using a k-means-based algorithm",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brodinova, S., Filzmoser, P., Ortner, T., Breiteneder, C., &#38; Zaharieva, M. (2017). <i>Finding groups in large and high-dimensional data using a k-means-based algorithm</i>. MOVISS - Metabolomic Bio &#38; Data 2017, Vorau, Austria. http://hdl.handle.net/20.500.12708/122033</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 1,
                    "role": "Author",
                    "tid": "281480"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 2,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 3,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 5,
                    "role": "Author",
                    "tid": "39017"
                }
            ],
            "foci": [],
            "projects": [
                "220239"
            ]
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-06"
            ],
            "pid": "126982",
            "handle": "20.500.12708/122037",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Grouping and outlier detection using robust sparse clustering",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brodinova, S., Filzmoser, P., Ortner, T., Zaharieva, M., &#38; Breiteneder, C. (2017). <i>Grouping and outlier detection using robust sparse clustering</i>. Olomouc Days of Applied Mathematics (ODAM 2017), Olomouc, EU. http://hdl.handle.net/20.500.12708/122037</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 1,
                    "role": "Author",
                    "tid": "281480"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 2,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 3,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 4,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 5,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": [
                "220239"
            ]
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-02",
                "E193-06"
            ],
            "pid": "127798",
            "handle": "20.500.12708/122853",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Robust k-means-based clustering for high-dimensional data",
            "keywords": [],
            "abstract": "We introduce a robust k-means-based clustering method for high-dimensional data where not only outliers but also a large number of noise variables are very likely to be present. Although Kondo et al. [2] already addressed such an application scenario, our approach\r\ngoes even further. Firstly, the introduced method is designed to identify clusters, informative variables, and outliers simultaneously. Secondly, the proposed clustering technique\r\nadditionally aims at optimizing required parameters, e.g. the number of clusters. This is a great advantage over most existing methods. Moreover, the robustness aspect is achieved through a robust initialization [3] and a proposed weighting function using the\r\nLocal Outlier Factor [1]. The weighting function provides a valuable source of information about the outlyingness of each observation for a subsequent outlier detection. In order to reveal both clusters and informative variables properly, the approach uses a lasso-type\r\npenalty [4]. The method has thoroughly been tested on simulated as well as on real highdimensional datasets. The conducted experiments demonstrated a great ability of the clustering method to identify clusters, outliers, and informative variables.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Filzmoser, P., Brodinova, S., Ortner, T., Breiteneder, C., &#38; Rohm, M. (2019). <i>Robust k-means-based clustering for high-dimensional data</i>. International Conference on Robust Statistics (ICORS 2019), Guayaquil, Non-EU. http://hdl.handle.net/20.500.12708/122853</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 1,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 2,
                    "role": "Author",
                    "tid": "281480"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 3,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Rohm",
                    "position": 5,
                    "role": "Author",
                    "tid": "39017"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-02",
                "E193-06"
            ],
            "pid": "128036",
            "handle": "20.500.12708/123091",
            "doi": null,
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Robust and sparse k-means clustering in high dimension",
            "keywords": [],
            "abstract": "We introduce a robust k-means-based clustering method for high-dimensional data where not only outliers but also a large number of noise variables are very likely to be present [4]. Although Kondo et al. [2] already addressed such an application scenario, our approach goes even further. Firstly, the introduced method is designed to identify clusters, informative variables, and outliers simultaneously. Secondly, the proposed clustering technique additionally aims at\r\noptimizing required parameters, e.g. the number of clusters. This is a great advantage over most existing methods. Moreover, the robustness aspect is achieved through a robust initialization [3] and a proposed weighting function using the Local Outlier Factor [1]. The weighting function provides a valuable source of information about the outlyingness of each observation for a subsequent outlier detection. In order to reveal both clusters and informative variables properly,\r\nthe approach uses a lasso-type penalty [5]. The method has thoroughly been tested on simulated as well as on real high-dimensional datasets. The conducted experiments demonstrated a great ability of the clustering method to identify clusters, outliers, and informative variables.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Filzmoser, P., Brodinova, S., Ortner, T., Breiteneder, C., &#38; Rohm, M. (2020). <i>Robust and sparse k-means clustering in high dimension</i>. Seminarvortrag an der JKU Linz, Linz, Austria. http://hdl.handle.net/20.500.12708/123091</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 1,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Rohm",
                    "position": 5,
                    "role": "Author",
                    "tid": "39017"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "12809",
            "handle": "20.500.12708/12820",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Discrimination and retrieval of animal sounds",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zeppelzauer, M. (2005). <i>Discrimination and retrieval of animal sounds</i> [Master Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-16942</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "25634",
                    "name": "Zeppelzauer Matthias - 2005 - Discrimination and retrieval of animal sounds.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 714635,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12820/2/Zeppelzauer%20Matthias%20-%202005%20-%20Discrimination%20and%20retrieval%20of%20animal%20sounds.pdf"
                },
                {
                    "bsid": "90676",
                    "name": "Zeppelzauer Matthias - 2005 - Discrimination and retrieval of animal sounds.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 127496,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12820/5/Zeppelzauer%20Matthias%20-%202005%20-%20Discrimination%20and%20retrieval%20of%20animal%20sounds.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "12810",
            "handle": "20.500.12708/12821",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Discrimination and retrieval of environmental sounds",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mitrovic, D. (2005). <i>Discrimination and retrieval of environmental sounds</i> [Master Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-14575</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "25636",
                    "name": "Mitrovic Dalibor - 2005 - Discrimination and retrieval of environmental sounds.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 549213,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12821/2/Mitrovic%20Dalibor%20-%202005%20-%20Discrimination%20and%20retrieval%20of%20environmental%20sounds.pdf"
                },
                {
                    "bsid": "90677",
                    "name": "Mitrovic Dalibor - 2005 - Discrimination and retrieval of environmental sounds.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 136580,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12821/5/Mitrovic%20Dalibor%20-%202005%20-%20Discrimination%20and%20retrieval%20of%20environmental%20sounds.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 1,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "12900",
            "handle": "20.500.12708/12911",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Augmented Reality video : situated video compositions in panorama-based Augmented Reality applications",
            "keywords": [
                "Augmented Reality",
                "Video",
                "mobile",
                "outdoor",
                "smart phone",
                "artificial",
                "multimedia information system"
            ],
            "abstract": "The rapid development of mobile devices such as smart phones has led to new possibilities in the context of Mobile Augmented Reality (AR). While there exists a broad range of AR applications providing static content, such as textual annotations, there is still a lack of supporting dynamic content, such as video, in the field of Mobile AR. In this work a novel approach to record and replay video content composited in-situ with a live view of the real environment, with respect to the user's view onto the scene, is presented. The proposed technique works in real-time on currently available mobile phones, and uses a panorama-based tracker to create visually seamless and spatially registered overlays of video content, hence giving end users the chance to re-experience past events at a different point of time. To achieve this, a temporal foreground-background segmentation of video footage is applied and it is shown how the segmented information can be precisely registered in real-time in the camera view of a mobile phone.<br />Furthermore, the user interface and video post effects implemented in a first prototype within a skateboard training application are presented.<br />To evaluate the proposed system, a user study was conducted. The results are given at the end of this work along with an outlook on possible future work.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zingerle, M. (2012). <i>Augmented Reality video : situated video compositions in panorama-based Augmented Reality applications</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-56559</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "25816",
                    "name": "Zingerle Mathaeus - 2012 - Augmented Reality video situated video compositions...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 13199781,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12911/2/Zingerle%20Mathaeus%20-%202012%20-%20Augmented%20Reality%20video%20situated%20video%20compositions...pdf"
                },
                {
                    "bsid": "90490",
                    "name": "Zingerle Mathaeus - 2012 - Augmented Reality video situated video compositions...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 192971,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12911/5/Zingerle%20Mathaeus%20-%202012%20-%20Augmented%20Reality%20video%20situated%20video%20compositions...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Mathäus",
                    "last_name": "Zingerle",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "12936",
            "handle": "20.500.12708/12947",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Intelligent video annotation and retrieval techniques",
            "keywords": [
                "content-based video analysis",
                "object recognition",
                "visual features",
                "automatic annotation",
                "video search engines"
            ],
            "abstract": "Videos are an integral part of current information technologies and the web. The demand for efficient retrieval rises with the increasing number of videos, and thus better annotation tools are needed as today's retrieval systems mainly rely on manually generated metadata.<br />The situation is even more critical when it comes to user-generated videos where rough and inaccurate annotations are the common practice.<br />Attempts to employ content-based analysis for video annotation and retrieval already exist, but they are still in an infant stage compared to the retrieval of web documents.<br />In this work, we address the use of object recognition techniques to annotate what is shown where in videos. These annotations are suitable to retrieve specific video scenes for object related text queries, thought the manual generation of such metadata would be impractical and expensive. A sophisticated presentation of the retrieval results is further exploited that indicates the relevance of the retrieved scenes at a first glance. The presented semi-automatic annotation approach can be used in an easy and comfortable way, and it builds on a novel framework with following outstanding features. First, it can be easily integrated into existing video environments. Second, it is not based on a fixed analysis chain but on an extensive recognition infrastructure that can be used with all kinds of visual features, matching and machine learning techniques. New recognition approaches can be integrated into this infrastructure with low development costs and a configuration of the used recognition approaches can be performed even on a running system. Thus, this framework might also benefit from future advances in computer vision. Third, we present an automatic selection approach to support the use of different recognition strategies  for the annotation of different objects. Moreover, visual analysis can be performed efficiently on distributed, multi-processor environments and the resulting video annotations and low-level features can be stored in a compact form.<br />We demonstrate the proposed annotation approach in an extensive case study with promising results. A video object annotation prototype as well as the generated scene classification ground-truth are freely available to foster reproducible research. Additional contributions of this work consider the generation of motion-based and segmentation-based features and their use for specific annotation tasks, such as the detection of action scenes in professional and user-generated video.<br />Furthermore, we participated at the two tasks instance search and semantic indexing of the TRECVID challenge in the three consecutive years 2010, 2011, and 2012.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sorschag, R. (2012). <i>Intelligent video annotation and retrieval techniques</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-50967</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "25888",
                    "name": "Sorschag Robert - 2012 - Intelligent video annotation and retrieval techniques.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4597099,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12947/2/Sorschag%20Robert%20-%202012%20-%20Intelligent%20video%20annotation%20and%20retrieval%20techniques.pdf"
                },
                {
                    "bsid": "91079",
                    "name": "Sorschag Robert - 2012 - Intelligent video annotation and retrieval techniques.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 429588,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/12947/5/Sorschag%20Robert%20-%202012%20-%20Intelligent%20video%20annotation%20and%20retrieval%20techniques.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Robert",
                    "last_name": "Sorschag",
                    "position": 1,
                    "role": "Author",
                    "tid": "69044"
                },
                {
                    "first_name": "Ansgar",
                    "last_name": "Scherp",
                    "position": 1,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "13035",
            "handle": "20.500.12708/13046",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Programmierung von Multimediasensoren für mobile Informationsszenarien",
            "keywords": [
                "Android",
                "Software Engineering",
                "GPS Tracking",
                "Accelerometer",
                "Stepcounter",
                "Panorama-Stitching",
                "Smartphone",
                "GData",
                "Google Maps"
            ],
            "abstract": "This thesis deals with software development and media processing on mobile handsets running the Android operating system. It is centered round a practical project, which aims to develop a program that continually tracks and saves the device's location. Furthermore the program should implement a stepcounter and make it possible to create panoramaphotos. Additionally it should display all the collected data in an appropriate manner.<br />At the outset the Android system with its basic concepts is introduced and the additional technologies drawn upon in the project are elaborated. These technologies include GPS-tracking, accelerometer-sensors, panorama-stitching and visualizationtechniques as they come into operation on the mobile handset as well as on the web.<br />Finally the practical project is modeled using graphical UML-diagrams and implemented, with the results achieved being discussed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Spreitzer, J. (2010). <i>Programmierung von Multimediasensoren für mobile Informationsszenarien</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-35587</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "26086",
                    "name": "Spreitzer Johannes - 2010 - Programmierung von Multimediasensoren fuer mobile...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 8204401,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13046/2/Spreitzer%20Johannes%20-%202010%20-%20Programmierung%20von%20Multimediasensoren%20fuer%20mobile...pdf"
                },
                {
                    "bsid": "90918",
                    "name": "Spreitzer Johannes - 2010 - Programmierung von Multimediasensoren fuer mobile...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 140990,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13046/5/Spreitzer%20Johannes%20-%202010%20-%20Programmierung%20von%20Multimediasensoren%20fuer%20mobile...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Johannes",
                    "last_name": "Spreitzer",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "1329",
            "handle": "20.500.12708/1345",
            "doi": "10.34726/hss.2020.42482",
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Data-driven generation of virtual city layout",
            "keywords": [
                "city layout generation",
                "procedural modeling",
                "data-driven modeling"
            ],
            "abstract": "Procedural Content Generation (PCG) is a growing field in computer science that is progressively gaining attention. Through the automated or semi-automated creation of digital content, creativity can be increased, new experiences can be offered, and development costs can be reduced. Furthermore, it makes the fabrication of scenes and landscapes with theoretically infinite dimensions possible. The generation of virtual environments, particularly cities and urban areas, can provide numerous advantages to designers and developers. It has seen adoption in various domains, including, but not limited to, video games, movie production, and social simulations. Most existing city generation approaches investigate the artificial generation of new layouts, making only limited use of real-life data. In this thesis, we explore the direct integration of existing city layout information into the generation process. A filter function is provided to obtain the required data in the correct format. The proposed method for city layout generation simulates the attraction and adaptation of two or more urban road networks to each other. As a result, a combined layout containing interpolated features of the selected input files is generated. The implementation was done in Unity, as we rely on the internal physics engine for certain operations. User interaction is possible during the generation process, allowing structures to be modified as desired. The performance is comparable to existing simulation- and agent-based approaches for medium-sized areas. A user study confirms that the generated layouts include recognizable characteristics of the input data.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hochgruber, F. (2020). <i>Data-driven generation of virtual city layout</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2020.42482</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "2677",
                    "name": "Hochgruber Felix - 2020 - Data-driven generation of virtual city layout.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 16616431,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1345/2/Hochgruber%20Felix%20-%202020%20-%20Data-driven%20generation%20of%20virtual%20city%20layout.pdf"
                },
                {
                    "bsid": "73636",
                    "name": "Hochgruber Felix - 2020 - Data-driven generation of virtual city layout.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 250313,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1345/5/Hochgruber%20Felix%20-%202020%20-%20Data-driven%20generation%20of%20virtual%20city%20layout.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Felix",
                    "last_name": "Hochgruber",
                    "position": 1,
                    "role": "Author",
                    "tid": "254443"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "1332",
            "handle": "20.500.12708/1348",
            "doi": "10.34726/hss.2020.56860",
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Deep learning of humor from Gary Larson’s cartoons",
            "keywords": [
                "Deep Learning",
                "Recurrent Networks",
                "Natural Language Processing"
            ],
            "abstract": "The aim of this thesis is to model humor using deep learning based on Gary Larsonscartoons. The recent success of deep learning in computer vision and natural languageprocessing shows that similar techniques can be applied in the field of computationalhumor. The training of deep learning models requires a dataset with many trainingsamples, which is why I created a novel dataset containing several thousands of GaryLarsons cartoons, punchlines and corresponding funniness annotations. The dataset wasannotated using a custom labelling tool, by the single person. Therefore, the datasetentails the humor of a single person. With this dataset it is possible to quantitativelycompare humor with the results of the deep learning models or with other people.After an extensive dataset analysis, I designed and trained several deep neural architectures.First, focusing on the visual domain (cartoons) using convolutional neuralnetworks, transfer learning and object detection techniques. Afterwards, I focused onthe text domain (punchlines) using Long Short-Term Memory networks, several wordembeddings (deep learning based and classical) and an automated machine learningapproach. Finally, I tried to combine all the findings into a unified two stage architecture.Unfortunately, the evaluation revealed that this task is not yet tractable by the deeplearning techniques applied. I chose two performance metrics (Mean absolute error andaccuracy) and several baseline models (most frequent class, mean class, etc.) and nomodel improved on the baselines significantly. On the test set a transfer learning basedapproach scored the best accuracy of 26.10%, while the most frequent class scored 24.50%.Both a deep learning approach and the mean class reached a mean absolute error of 1.57.These results show, that the semantic gap between computers and humans is too largefor current deep learning based approaches to successfully model the humor of a singleperson. It seems another breakthrough besides deep learning is required for this task.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fischer, R. (2020). <i>Deep learning of humor from Gary Larson’s cartoons</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2020.56860</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "2683",
                    "name": "Fischer Robert - 2020 - Deep learning of humor from Gary Larsons cartoons.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3038448,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1348/2/Fischer%20Robert%20-%202020%20-%20Deep%20learning%20of%20humor%20from%20Gary%20Larsons%20cartoons.pdf"
                },
                {
                    "bsid": "73637",
                    "name": "Fischer Robert - 2020 - Deep learning of humor from Gary Larsons cartoons.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 231194,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1348/5/Fischer%20Robert%20-%202020%20-%20Deep%20learning%20of%20humor%20from%20Gary%20Larsons%20cartoons.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Robert",
                    "last_name": "Fischer",
                    "position": 1,
                    "role": "Author",
                    "tid": "282443"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E183"
            ],
            "pid": "13509",
            "handle": "20.500.12708/13520",
            "doi": null,
            "year": 2002,
            "issued": "2002",
            "issued_on": "2002-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Visual traffic surveillance using real-time tracking",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pflugfelder, R. P. (2002). <i>Visual traffic surveillance using real-time tracking</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-77819</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "27034",
                    "name": "Pflugfelder Roman Peter - 2002 - Visual traffic surveillance using real-time...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4028550,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13520/2/Pflugfelder%20Roman%20Peter%20-%202002%20-%20Visual%20traffic%20surveillance%20using%20real-time...pdf"
                },
                {
                    "bsid": "92940",
                    "name": "Pflugfelder Roman Peter - 2002 - Visual traffic surveillance using real-time...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 266570,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13520/5/Pflugfelder%20Roman%20Peter%20-%202002%20-%20Visual%20traffic%20surveillance%20using%20real-time...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Roman Peter",
                    "last_name": "Pflugfelder",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "13594",
            "handle": "20.500.12708/13605",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Face detection in historic documentaries with a cascaded classifier",
            "keywords": [
                "face detection",
                "Vertov",
                "documentary",
                "historic",
                "Viola",
                "Jones",
                "classification",
                "cascade",
                "forward feature selection"
            ],
            "abstract": "Face detection aims at detecting and localizing an unknown number of faces in a still image or video frame. The challenges are to detect all faces while keeping the false positive rate small and to minimize the detection time per frame.<br />We study face detection in the context of historic documentaries. The source material for this work are films of the Soviet film maker Dziga Vertov that date back to the 1920's. The digitally available material bears major image deficiencies including flicker, scratches, dirt, bad lighting and contrast and visible frame lines. Naturally, the material is monochromatic and silent.<br />Based on a literature survey on different approaches for face detection, we select a method introduced by Viola and Jones for this investigation.<br />Their approach employs a cascaded classifier, i.e. a sequence of nodes, that distinguishes faces from non-faces. These nodes are organized as a hierarchy of classifiers that are built from simple, Haar-like features.<br />The main advantage of using a cascade is that only a moderate false-positive rate is needed for individual nodes as the individual rates multiply up to the overall false-positive rate.<br />We describe how the detection framework is set up for and adapted to the historic material and how it is implemented. Additionally, we suggest several post-processing steps to ameliorate the false-positive rate.<br />Finally, we provide detailed results for several sample scenes from the documentaries, and analyze the performance of the training and detection stages.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schleser, T. (2009). <i>Face detection in historic documentaries with a cascaded classifier</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-24738</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "27204",
                    "name": "Schleser Tobias - 2009 - Face detection in historic documentaries with a...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 12536467,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13605/2/Schleser%20Tobias%20-%202009%20-%20Face%20detection%20in%20historic%20documentaries%20with%20a...pdf"
                },
                {
                    "bsid": "91728",
                    "name": "Schleser Tobias - 2009 - Face detection in historic documentaries with a...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 170669,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13605/5/Schleser%20Tobias%20-%202009%20-%20Face%20detection%20in%20historic%20documentaries%20with%20a...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Tobias",
                    "last_name": "Schleser",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "13693",
            "handle": "20.500.12708/13704",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Interaktive Analyse audiovisueller Medien",
            "keywords": [
                "media understanding",
                "image understanding",
                "audio analysis",
                "speech recognition",
                "body part detection",
                "face similarity"
            ],
            "abstract": "Media understanding is the domain of creating human-like perception of media objects in computers. This thesis addresses three main topics within this area: similarity detection of human faces, object recognition (body part detection in images, in particular) and fast speech recognition. In order to illustrate the practical purpose and the benefit of media understanding for the target group (undergraduate students in computer science) exemplary applications are implemented and discussed. The first part of this thesis elaborates on the theory behind the methods applied. Based on existing systems the most suitable features and classifiers for a given problem statement are investigated. The second section of the thesis deals with the practical implementation. In the first application the measurement of the similarity of prominent faces is investigated. Template matching is used as a method for calculating similarity. In the second application different body parts, recorded with a webcam, are detected and classified. The software uses a local feature extraction method for this task. For the accurate classification, a probabilistic method is applied (among others). The third application facilitates verbal interaction with the computer. The user is required to simulate the sound of a given species. Then, the system confirms whether the user's interpretation matches the one of the animal. In the same way the sound can be presented to users, asking them to reply with the correct animal name.<br />Here, the software uses spectral audio features, which are recognized by dynamic time warping. All three applications together prove that media understanding can be implemented successfully today.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fried, A. (2012). <i>Interaktive Analyse audiovisueller Medien</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-48421</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "27402",
                    "name": "Fried Alexander - 2012 - Interaktive Analyse audiovisueller Medien.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1487314,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13704/2/Fried%20Alexander%20-%202012%20-%20Interaktive%20Analyse%20audiovisueller%20Medien.pdf"
                },
                {
                    "bsid": "92155",
                    "name": "Fried Alexander - 2012 - Interaktive Analyse audiovisueller Medien.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 194298,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13704/5/Fried%20Alexander%20-%202012%20-%20Interaktive%20Analyse%20audiovisueller%20Medien.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Alexander",
                    "last_name": "Fried",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "13747",
            "handle": "20.500.12708/13758",
            "doi": "10.34726/hss.2019.51520",
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Hand simulation for virtual climbing",
            "keywords": [
                "Virtual Reality",
                "Tracking"
            ],
            "abstract": "VR applications enable users to immerse into a virtual world. In current VR systems, users wear a Head-Mounted Display (HMD) and can use handheld controllers to interact with the virtual world. The controllers are suitable for a variety of applications, but they cannot be used if users are to interact with real objects. An example for such an application is the VreeClimber project. It combines a moveable climbing wall with VR climbing. Since the hands of the users should also be visible in the virtual world during climbing, an optical tracking system is used to capture the hand positions in real time. In the course of this thesis, two software components were created for the VreeClimber project. At first, the software of an already developed tracking system called VreeTracker was rewritten with the computer vision library OpenCV. During the development of this software, the affordable Vive Tracker was released, which can also be used to track extremities. The evaluation compares the accuracy of the two tracking systems by different tests. Since the users are not able to see their own hands during climbing, an algorithm was created in the second part of this thesis, which uses the detected hand positions to simulate the hand movements of the climbers. The developed hand simulation shows promising results for typical grasp movements during climbing, however especially fast movements or small climbing holds can result in deviations from the real hand pose. An important requirement is an accurate calibration before climbing, otherwise the positions of the real and virtual climbing holds may differ, which reduces the climbing experience significantly. The evaluation shows that the commercial Vive Tracker achieves better results than the previously developed VreeTracker system. Due to better precision and an easy integration into the already used VR system, it makes sense to use the Vive Tracker for the VreeClimber project in the future. In general, the virtual hand simulation performed well in the evaluation, however minor flaws in the grasp movements of individual fingers have been revealed. After a short analysis, appropriate suggestions for improvement have been presented in this thesis.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Voglhuber, R. (2019). <i>Hand simulation for virtual climbing</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2019.51520</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "27510",
                    "name": "Voglhuber Roman - 2019 - Hand simulation for virtual climbing.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 16526833,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13758/2/Voglhuber%20Roman%20-%202019%20-%20Hand%20simulation%20for%20virtual%20climbing.pdf"
                },
                {
                    "bsid": "93278",
                    "name": "Voglhuber Roman - 2019 - Hand simulation for virtual climbing.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 151614,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13758/5/Voglhuber%20Roman%20-%202019%20-%20Hand%20simulation%20for%20virtual%20climbing.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Roman",
                    "last_name": "Voglhuber",
                    "position": 1,
                    "role": "Author",
                    "tid": "244284"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "13805",
            "handle": "20.500.12708/13816",
            "doi": "10.34726/hss.2019.65422",
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Unraveled skeletons",
            "keywords": [
                "shape representation",
                "invariant to articulation",
                "shape coordinates"
            ],
            "abstract": "This thesis presents a new shape representation called Unraveled Skeleton. It is designed to be robust to a large number of transformations, especially articulated movement. Based on this new representation, its general properties are studied and its application domain established. The representation is based on the skeleton or medial axis of a shape. It uses the very robust Voronoi skeletonization and \"walks\" around the resulting skeleton tree. At every point the minimum distance to the boundary is saved to a vector, resulting in a list of distance measures, a shape signature. This Unraveled Skeleton vector can then be used for further processing like normalization. This thesis explores two possible directions to use the Unraveled Skeletons: Shape analysis and pose independent coordinate systems. In shape analysis the Unraveled Skeleton vector can be used for articulation independent recognition. For this purpose a number of different normalization and optimization techniques are introduced. It is also possible to use parts of the vector to match parts of objects. The pose independent coordinate system assigns every point inside the shape and on its boundary a unique coordinate independent of pose or articulated movement. Using the closest points on the skeleton and its distance as provided by the Unraveled Skeleton one can address convex patches. Given a third component to chose a point from this patch a coordinate with three components is introduced. This paper states three applications to use the Unraveled Skeleton coordinates: Shape Blending, Super-resolution and Segmentation refinement. It also explores possibilities of non-centered skeletons in regard to Unraveled Skeleton coordinates. All results are evaluated on multiple datasets.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langer, M. (2019). <i>Unraveled skeletons</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2019.65422</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "27626",
                    "name": "Langer Maximilian - 2019 - Unraveled skeletons.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4471802,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13816/2/Langer%20Maximilian%20-%202019%20-%20Unraveled%20skeletons.pdf"
                },
                {
                    "bsid": "92485",
                    "name": "Langer Maximilian - 2019 - Unraveled skeletons.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 109797,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13816/5/Langer%20Maximilian%20-%202019%20-%20Unraveled%20skeletons.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Maximilian",
                    "last_name": "Langer",
                    "position": 1,
                    "role": "Author",
                    "tid": "254918"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "13844",
            "handle": "20.500.12708/13855",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "A query algebra for ontology-enhanced management of multimedia meta objects",
            "keywords": [],
            "abstract": "Today's multimedia content formats primarily encode the presentation of content but not the information the content conveys.<br />However, this presentation-oriented modeling only permits the inflexible, hard-wired presentation of multimedia content. For the realization of advanced operations the multimedia content has to be enriched by additional semantic information. To provide a basis for the semantic modeling of multimedia content in content sharing and collaborative applications, we have developed Enhanced Multimedia Meta Objects (EMMOs). An EMMO constitutes a self-contained piece of multimedia content that indivisibly unites three of the content's aspects. The media aspect reflects that an EMMO aggregates the basic media objects of which the multimedia content consists, the semantic aspect allows the specification of semantic associations between an EMMO's media objects, and, finally, the functional aspect provides means for the definition of arbitrary, domain-specific operations on the content that can be invoked by applications. Furthermore, EMMOs are versionable and tradeable.<br />To enable the efficient retrieval of EMMOs, we have developed the query algebra EMMA, which is adequate and complete with regard to the EMMO model. By providing simple and orthogonal operators, which can be combined to formulate more complex queries, EMMA enables efficient query optimization. Both, the EMMO model and the EMMA algebra, provide a sound basis for the integration of ontological knowledge. We demonstrate how ontological knowledge can be used within the authoring process of EMMOs, can be integrated within the EMMO knowledge structures, and can be exploited for refining EMMA queries.<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zillner, S. (2005). <i>A query algebra for ontology-enhanced management of multimedia meta objects</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-9171</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "27704",
                    "name": "Zillner Sonja - 2005 - A query algebra for ontology-enhanced management of...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 6396142,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13855/2/Zillner%20Sonja%20-%202005%20-%20A%20query%20algebra%20for%20ontology-enhanced%20management%20of...pdf"
                },
                {
                    "bsid": "92619",
                    "name": "Zillner Sonja - 2005 - A query algebra for ontology-enhanced management of...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 295410,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/13855/5/Zillner%20Sonja%20-%202005%20-%20A%20query%20algebra%20for%20ontology-enhanced%20management%20of...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Sonja",
                    "last_name": "Zillner",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Werner",
                    "last_name": "Winiwarter",
                    "position": 1,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "14098",
            "handle": "20.500.12708/14109",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Visual Information Retrieval : automatisierte Klassifikation von Snowboardclips",
            "keywords": [
                "Visual Information Retrieval",
                "VIR",
                "optical flow",
                "features",
                "CBIR",
                "Snowboard"
            ],
            "abstract": "Practical snowboard instructor training is an iterative process divided into two steps. In the first step, the future snowboard instructors are recorded on video performing on the slope. In the second step, these video recordings are analyzed and discussed with the focus on possible improvements of the future instructor's personal snowboarding style. The future instructors then try to apply the improvements in the next iteration of the first step.<br />This thesis presents a way of adequately supporting the second step by content-based classification and retrieval of snowboard videoclips.<br />The theory of snowboarding defines several turn types with different difficul- ties that are practiced step-by-step. Because rhythm and speed are the two main characteristics of different turn types, this thesis explores the feasibility to measure them via motion-detection and investigates how to deal with di- sturbing factors like camera shaking.<br />The proposed method uses the output of optical flow analysis to compute the duration between two turns and the speed of the turn to classify the types of turns.<br />The audience in theoretical snowboard lessons is ususally bigger than one person, but everyone needs individual feedback during analysis. As a result it is very important for trainers to be able to quickly present appropriate video samples - either from the same or from another person.<br />This personalized feedback motivates the second presented method in this thesis. This method employs an established color analysis technique to distinguish which person is shown in the videoclips. The method enables trainers to select individual videoclips for presentation.<br />In order to evaluate the acquired techniques and developed methods, they are applied on a manually generated test-set of videoclips which were recor-  ded during several days of training by this thesis' author. Turn type classification yields good results in computing the average number of frames between two shifts in direction (wide driven carving turns versus fast moving short turns) so 85% percent of videoclips are classified correctly (65% even clearly).<br />The distinction of videoclips based on depicted persons is highly dependent on scenery and illumination, which disturbs classification results because color-matching fails (classification error-rate rises linearly with the number of analyzed videoclips).",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Lepizh, D. (2010). <i>Visual Information Retrieval : automatisierte Klassifikation von Snowboardclips</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-43256</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "28212",
                    "name": "Lepizh Dominik - 2010 - Visual Information Retrieval automatisierte...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1330252,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/14109/2/Lepizh%20Dominik%20-%202010%20-%20Visual%20Information%20Retrieval%20automatisierte...pdf"
                },
                {
                    "bsid": "92804",
                    "name": "Lepizh Dominik - 2010 - Visual Information Retrieval automatisierte...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 112161,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/14109/5/Lepizh%20Dominik%20-%202010%20-%20Visual%20Information%20Retrieval%20automatisierte...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Dominik",
                    "last_name": "Lepizh",
                    "position": 1,
                    "role": "Author",
                    "tid": "46831"
                },
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "53451"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E210-01"
            ],
            "pid": "141146",
            "handle": "20.500.12708/136961",
            "doi": null,
            "year": 2022,
            "issued": "2022-02-25",
            "issued_on": "2022-02-25",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "BIMFlexi-VR: A Virtual Reality Framework for Early-Stage Collaboration in Flexible Industrial Building Design",
            "keywords": [
                "collaborative BIM",
                "collaborative VR",
                "parametric modeling in VR",
                "virtual reality",
                "VR in AEC",
                "VR in Industry 4.0"
            ],
            "abstract": "Integrated industrial building design is an interdisciplinary task, in which planning of flexible building structures requires effective communication and collaboration between all stakeholders already in early design stage. This paper presents BIMFlexi-VR, a collaborative framework which implements a real-time bidirectional link between a parametric modelling component created in Grasshopper for Rhinoceros that performs optimized structural calculations of an industrial building, and an immersive Virtual Reality environment in which the automatically calculated building is visualized. Users of BIMFlexi-VR are able to change parameters defining the outcome of the structural calculation directly inside the virtual environment and see the modified building design together with the associated fitness metrics in a matter of seconds. Providing an efficient and intuitive platform for early exploration of industrial building designs, BIMFlexi-VR enables collaborative decision making and facilitates the creation of more efficient and sustainable industrial constructions.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Podkosova, I., Reisinger, J., Kaufmann, H., &#38; Kovacic, I. (2022). BIMFlexi-VR: A Virtual Reality Framework for Early-Stage Collaboration in Flexible Industrial Building Design. <i>Frontiers in Virtual Reality</i>, <i>3</i>, Article 782169. https://doi.org/10.3389/frvir.2022.782169</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Julia",
                    "last_name": "Reisinger",
                    "position": 2,
                    "role": "Author",
                    "tid": "230671"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 4,
                    "role": "Author",
                    "tid": "42543"
                }
            ],
            "foci": [],
            "projects": [
                "1734269"
            ]
        },
        {
            "org_nrs": [
                "E193-03",
                "E210-01"
            ],
            "pid": "141721",
            "handle": "20.500.12708/142189",
            "doi": null,
            "year": 2022,
            "issued": "2022-11-06",
            "issued_on": "2022-11-06",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluation of a Real-time Optimization and Decision Making Framework in Virtual Reality for Life Cycle Analysis of Flexible Industrial Building Structures Incoporating Production Processes",
            "keywords": [
                "Virtual Reality",
                "Industrial Building Design",
                "Integrated Design",
                "Generative Design"
            ],
            "abstract": "Integrated Industrial Building Design is based on collaborative decision making and coupling of discipline-specific tools. This process is subject to following problems: 1. Conflicting stakeholder goals and absence of effective communication workflows during early design stage, 2. Industrial building structures often lack flexibility for future production changes; once a building has been designed for a certain production type it cannot be used for other production types 3. No performance feedback in terms of life cycle cost (LCC), life cycle assessment (LCA), recycling potential and flexibility in early structural design. Interdisciplinary optimization and decision making tools in virtual reality providing collaborative visualization and enable variant studies within design meetings are lacking. We propose a Virtual Reality-based  optimization and decision-making framework. Multidisciplinary stakeholders can collaboratively change parameters related to structural and production planning and observe changes in the performance of the building design in real-time. This way, the limitations posed by different planning components and potential compromises can be easily explored by all stakeholders. The VR framework is coupled to an automated background structural optimization process that considers production layout planning and gives feedback to LCC, LCA, recycling potential and flexibility assessment. This paper presents the VR framework for integrated industrial building meetings for early design stage. The framework builds on our previous studies conducted in the funded research project BIMFlexi. The parametric design approach, integrating production planning and structural design with automated calculation of the load-bearing structure and simultaneous assessment of LCC, LCA, recycling potential and flexibility, has already been presented. The VR framework, based on Grasshopper for Rhino3D (parametric) and Unity3D (VR), is tested within a pilot evaluation with multidisciplinary teams to extend the workflow towards improved collaboration and visualization. Forty industry professionals (architects, civil engineers, and production planners) tested the framework and were then interviewed about the usability, VR navigation, collaboration, and decision-making experience using the VR-tool. The results of the expert interviews are finally supplemented with findings from literature to develop the holistic VR workflow for early stage industrial building design meetings.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reisinger, J., Podkosova, I., Kovacic, I., &#38; Kaufmann, H. (2022). Evaluation of a Real-time Optimization and Decision Making Framework in Virtual Reality for Life Cycle Analysis of Flexible Industrial Building Structures Incoporating Production Processes. In <i>17th sdewes Conference Paphos 2022, Book of Abstracts</i> (pp. 279–279). http://hdl.handle.net/20.500.12708/142189</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julia",
                    "last_name": "Reisinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "230671"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 2,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 3,
                    "role": "Author",
                    "tid": "42543"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": [
                "1734269"
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "142639",
            "handle": "20.500.12708/136314",
            "doi": "10.34726/hss.2022.99064",
            "year": 2022,
            "issued": "2022",
            "issued_on": "2022-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Point cloud registration in BIM models for robot localization",
            "keywords": [
                "Building Information Modeling",
                "Virtual Reality",
                "Point Cloud",
                "Robot Localization",
                "Lidar"
            ],
            "abstract": "As digital tools find increasing use in the construction industry, Building Information Modeling (BIM) is used for an increasing amount of tasks. BIM models contain acombination of 3D and semantic information that can be applied for construction planning, progress evaluation, quality control and documentation purposes. In manycases, the BIM model needs to be compared to LiDAR scans of the building, but this process requires manual registration of the gathered data to the correct location in theBIM model.This thesis introduces a novel registration algorithm for localizing point clouds in a BIM model automatically. The registration algorithm uses voxelization, normal alignment, anovel normal filtered template matching approach and the iterative closest point (ICP) algorithm to reliably register the point cloud to the BIM model.For applying the algorithm, a modular software system was developed to gather pointclouds using a mobile LiDAR and Simultaneous Localization and Mapping (SLAM).A 3D visualization shows the live position of the LiDAR, the current point cloud of the enironment and the BIM model at the same time. The viewer includes features for evaluating matching performance and comparing the BIM model to the real environment.We also developed a hand held setup and a robot carried setup to test the system and its possible deployment.The performance of the registration algorithm was evaluated in different environments.We gathered ground truth data to determine the registration accuracy of the algorithm.Our method achieves state-of-the-art accuracy and reliability. In realistic environments,our registration algorithm reached an average accuracy of 15.24 cm and a reliability of more than 96%, while being accurate to up to 3.7 cm in a simpler individual room.We also discuss the influence of different environments and setups on the registration performance.The algorithm overcomes different limitations of other approaches and is suitable tobe used by autonomous systems. Our registration method could also be applied to different use cases in construction and other fields such as archaeology or geology, where environments are scanned and compared to a 3D model.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schaub, L. (2022). <i>Point cloud registration in BIM models for robot localization</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2022.99064</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "219023",
                    "name": "Schaub Linus - 2022 - Point cloud registration in BIM models for robot...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3734436,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/136314/1/Schaub%20Linus%20-%202022%20-%20Point%20cloud%20registration%20in%20BIM%20models%20for%20robot...pdf"
                },
                {
                    "bsid": "219140",
                    "name": "Schaub Linus - 2022 - Point Cloud Registration in BIM Models for Robot...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 195703,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/136314/4/Schaub%20Linus%20-%202022%20-%20Point%20Cloud%20Registration%20in%20BIM%20Models%20for%20Robot...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Linus",
                    "last_name": "Schaub",
                    "position": 1,
                    "role": "Author",
                    "tid": "330744"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "142912",
            "handle": "20.500.12708/142177",
            "doi": null,
            "year": 2022,
            "issued": "2022-06",
            "issued_on": "2022-06-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Precomputed fast rejection ray-triangle intersection",
            "keywords": [
                "Ray-Triangle Intersection",
                "Ray shooting",
                "Collision",
                "Point location",
                "Intersections"
            ],
            "abstract": "We propose a ray-triangle intersection algorithm with fast-rejection strategies. We intersect the ray with the triangle plane, then transform the intersection problem into 2D by applying a transformation matrix to the ray-plane intersection point. For 2D transformation, we study two different approaches. The first approach uses a transformation matrix which transforms the triangle into a unit triangle. Then, simple 2D tests are performed. The second approach transforms the triangle into a 2D triangle while preserving similarity. This allows us to prune (i.e., to clip away) areas surrounding the triangle, determining whether the transformed intersection point lies within the triangle. We discuss several optimizations for this pruning approach. We implemented both approaches into the CPU-based ray-tracing framework PBRT, version 3, and we performed a time-based comparison against PBRT’s default intersection algorithm and Baldwin and Weber’s algorithm. The results show that our algorithms are faster than the default algorithm. They are comparable to or slightly slower than Baldwin and Weber’s algorithm, however, the pruning approach produces watertight results and may be further optimized. Moreover, additional CPU/GPU experiments outside of PBRT document promising speedup over the standard Möller–Trumbore algorithm in areas like ray-casting or collision detection.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pichler, T. A., Ferko, A., Ferko, M., Kán, P., &#38; Kaufmann, H. (2022). Precomputed fast rejection ray-triangle intersection. <i>Graphics and Visual Computing</i>, <i>6</i>, Article 200047. https://doi.org/10.1016/j.gvc.2022.200047</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas Alois",
                    "last_name": "Pichler",
                    "position": 1,
                    "role": "Author",
                    "tid": "243211"
                },
                {
                    "first_name": "Andrej",
                    "last_name": "Ferko",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Michal",
                    "last_name": "Ferko",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 4,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 5,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E210-01"
            ],
            "pid": "142955",
            "handle": "20.500.12708/139337",
            "doi": null,
            "year": 2022,
            "issued": "2022-07-24",
            "issued_on": "2022-07-24",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Evaluation of parametric multi-objective optimization and decision support tool for flexible industrial building design",
            "keywords": [
                "parametric design",
                "industrial building",
                "multi-objective optimization"
            ],
            "abstract": "Parametric multi-objective optimization tools bear the potential to integrate, optimize, and explore design spaces to support interdisciplinary decision-making. A parametric optimization and decision support tool was developed (POD tool), and an evolutionary multi-objective optimization algorithm implemented (POD MOO tool) to automate design search for flexible integrated industrial building design. Both tools were tested and compared within a user study, simulating an interdisciplinary industrial building design process to evaluate if the MOO creates advanced building options in design search. Evaluations of questionnaires show the preference to search for a design by manipulating parameters instead of automatically generated designs from the algorithm.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zahlbruckner, M. A., Reisinger, J., Wang-Sukalia, X., Kan, P., Knoll, M., Kovacic, I., &#38; Kaufmann, H. (2022). Evaluation of parametric multi-objective optimization and decision support tool for flexible industrial building design. In L. C. Tagliabue, A. Chassiakos, D. M. Hall, D. Nikolic, &#38; R. Soman (Eds.), <i>Proceedings of the 2022 European Conference on Computing in Construction</i> (pp. 115–122). https://doi.org/10.35490/EC3.2022.202</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maria Antonia",
                    "last_name": "Zahlbruckner",
                    "position": 1,
                    "role": "Author",
                    "tid": "324294"
                },
                {
                    "first_name": "Julia",
                    "last_name": "Reisinger",
                    "position": 2,
                    "role": "Author",
                    "tid": "230671"
                },
                {
                    "first_name": "Xi",
                    "last_name": "Wang-Sukalia",
                    "position": 3,
                    "role": "Author",
                    "tid": "253667"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 4,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Maximilian",
                    "last_name": "Knoll",
                    "position": 5,
                    "role": "Author",
                    "tid": "273554"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 6,
                    "role": "Author",
                    "tid": "42543"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 7,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Lavinia Chiara",
                    "last_name": "Tagliabue",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Athanasios",
                    "last_name": "Chassiakos",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Daniel M.",
                    "last_name": "Hall",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Dragana",
                    "last_name": "Nikolic",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Ranjith",
                    "last_name": "Soman",
                    "position": 5,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": [
                "1734269"
            ]
        },
        {
            "org_nrs": [
                "E193-03",
                "E210-01"
            ],
            "pid": "142958",
            "handle": "20.500.12708/139477",
            "doi": null,
            "year": 2022,
            "issued": "2022-07-24",
            "issued_on": "2022-07-24",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Framework for integrated multi-objective optimization of production and industrial building design",
            "keywords": [
                "industrial building design",
                "Integrated Design",
                "Multi-Objective Optimization",
                "Production Layout Planning"
            ],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reisinger, J., Wang-Sukalia, X., Kan, P., Kovacic, I., Kaufmann, H., Knoll, M., &#38; Zahlbruckner, M. A. (2022). Framework for integrated multi-objective optimization of production and industrial building design. In L. C. Tagliabue, Hall Daniel M., A. Chassiakos, D. Nikolic, &#38; R. Soman (Eds.), <i>Proceedings of the 2022 European Conference on Computing in Construction</i>. https://doi.org/10.35490/EC3.2022.223</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julia",
                    "last_name": "Reisinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "230671"
                },
                {
                    "first_name": "Xi",
                    "last_name": "Wang-Sukalia",
                    "position": 2,
                    "role": "Author",
                    "tid": "253667"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 3,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 4,
                    "role": "Author",
                    "tid": "42543"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 5,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Maximilian",
                    "last_name": "Knoll",
                    "position": 6,
                    "role": "Author",
                    "tid": "273554"
                },
                {
                    "first_name": "Maria Antonia",
                    "last_name": "Zahlbruckner",
                    "position": 7,
                    "role": "Author",
                    "tid": "324294"
                },
                {
                    "first_name": "Lavinia Chiara",
                    "last_name": "Tagliabue",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Hall Daniel M.",
                    "last_name": "Hall Daniel M.",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Athanasios",
                    "last_name": "Chassiakos",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Dragana",
                    "last_name": "Nikolic",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Ranjith",
                    "last_name": "Soman",
                    "position": 5,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": [
                "1734269"
            ]
        },
        {
            "org_nrs": [
                "E193",
                "E193-02",
                "E193-03",
                "E210",
                "E210-01",
                "E259",
                "E259-01"
            ],
            "pid": "142959",
            "handle": "20.500.12708/188467",
            "doi": null,
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Integrating AEC Domain-Specific Multidisciplinary Knowledge for Informed and Interactive Feedback in Early Design Stages",
            "keywords": [
                "Integrated Design",
                "Early Design Stage",
                "Mixed Reality Sketching",
                "Shape Inference",
                "Computational Design",
                "Integration Platform",
                "Digital Fabrication"
            ],
            "abstract": "In the context of digitalization in the industry, a variety of technologies has been developed for system integration and enhanced team collaboration in the Architecture, Engineering and Construction (AEC) industry. Multidisciplinary design requirements are characterized by a high degree of complexity. Early design methods often rely on implicit or experiential design knowledge, whereas contemporary digital design tools mostly reflect domain-specific silo thinking with time-consuming iterative design processes. Yet, the early design stages hold the greatest potential for design optimization. This paper presents a framework of a multidisciplinary computational integration platform for early design stages that enables integration of AEC domain-specific methods from architecture, engineering, mathematics and computer science. The platform couples a semantic integrative mixed reality sketching application to a shape inference machine-learning based algorithm to link methods for different computation, simulation and digital fabrication tasks. A proof of concept of the proposed framework is presented for the use case of a freeform geometry wall. Future research will explore the potential of the framework to be extended to larger building projects with the aim to connect the method into BIM-processes.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reisinger, J., Rasoulzadeh, S., Kovacs, B. I., Ferschin, P., Vasylevska, K., Hensel, M. U., Kovacic, I., &#38; Wimmer, M. (2023). Integrating AEC Domain-Specific Multidisciplinary Knowledge for Informed and Interactive Feedback in Early Design Stages. In S. Skatulla &#38; H. Beushausen (Eds.), <i>Advances in Information Technology in Civil and Building Engineering: Proceedings of ICCCBE 2022 - Volume 2</i> (pp. 153–170). Springer. https://doi.org/10.1007/978-3-031-32515-1_12</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julia",
                    "last_name": "Reisinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "230671"
                },
                {
                    "first_name": "Shervin",
                    "last_name": "Rasoulzadeh",
                    "position": 2,
                    "role": "Author",
                    "tid": "348077"
                },
                {
                    "first_name": "Balint Istvan",
                    "last_name": "Kovacs",
                    "position": 3,
                    "role": "Author",
                    "tid": "262858"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Ferschin",
                    "position": 4,
                    "role": "Author",
                    "tid": "135460"
                },
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 5,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Michael Ulrich",
                    "last_name": "Hensel",
                    "position": 6,
                    "role": "Author",
                    "tid": "315688"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 7,
                    "role": "Author",
                    "tid": "42543"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Wimmer",
                    "position": 8,
                    "role": "Author",
                    "tid": "40666"
                },
                {
                    "first_name": "Sebastian",
                    "last_name": "Skatulla",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Hans",
                    "last_name": "Beushausen",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": [
                "1745908"
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "1431",
            "handle": "20.500.12708/1447",
            "doi": "10.34726/hss.2020.65186",
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Spatio-temporal filtering for real-time path tracing in virtual reality",
            "keywords": [
                "Spatio-temporal filtering",
                "real-time path tracing",
                "virtual reality",
                "real-time rendering",
                "global illumination",
                "blue noise"
            ],
            "abstract": "Ray tracing has raised its importance for real-time rendering since the presentation of NVIDIAs Turing architecture and Microsofts extension DirectX Raytracing (DXR). It allows for computing physically correct renderings dynamically for various global lighting effects such as shadows or reflection for example. Integration into major game engines such as Unreal Engine3 and Unity4 made these features quickly accessible to a wide range of game developers and graphics programmers. At the same time, consumer-level virtual reality (VR) systems gained more attention not just in the entertainment industry but also for medical, educational and training purposes. Perception of virtual scenes depends not only on geometric complexity but also heavily on physically plausible lighting. Hence, the combination of aforementioned real-time ray tracing with VR to produce visually more plausible lighting scenarios and increase immersion seems natural. Unfortunately, rendering for VR systems requires more computational power than ordinary graphics applications for desktop systems and real-time ray tracing is still limited in its extent.This thesis investigates if the light transport method - path tracing - is suitable to be used in a virtual reality setup in order to produce higher quality dynamic lighting. Therefore, a hybrid rendering pipeline is proposed combining path tracing with a low number of samples per pixel and spatio-temporal filtering of the noisy path tracer output. The pipeline comprises rasterization of first hits, tracing of a low amount of indirect rays per pixel, temporal accumulation of path tracing samples and, finally, de-noising of the highly noisy path traced image. In addition, this thesis introduces a novel masking system which provides a trade-off between the number of traced rays and the visual quality of dynamic objects. Furthermore, an improvement to the variance estimation of the chosen de-noising approach is proposed. A prototype shows that path tracing together with de-noising can achieve interactive frame rates suitable for VR applications. Resulting quality is comparable to reference renderings. The proposed masking system allows for further tuning of performance without introducing noticeable artifacts, making this approach viable for a large range of hardware setups.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gersthofer, L. (2020). <i>Spatio-temporal filtering for real-time path tracing in virtual reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2020.65186</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "2881",
                    "name": "Gersthofer Lukas - 2020 - Spatio-temporal filtering for real-time path tracing...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4353746,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1447/2/Gersthofer%20Lukas%20-%202020%20-%20Spatio-temporal%20filtering%20for%20real-time%20path%20tracing...pdf"
                },
                {
                    "bsid": "73564",
                    "name": "Gersthofer Lukas - 2020 - Spatio-temporal filtering for real-time path tracing...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 245986,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1447/5/Gersthofer%20Lukas%20-%202020%20-%20Spatio-temporal%20filtering%20for%20real-time%20path%20tracing...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Lukas",
                    "last_name": "Gersthofer",
                    "position": 1,
                    "role": "Author",
                    "tid": "273649"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "14345",
            "handle": "20.500.12708/14356",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Skeletal structure generation for optical motion capture",
            "keywords": [
                "Motion Capture",
                "Skeleton",
                "Motion Capturing",
                "Skeleton Generation",
                "Skeleton Parameterization"
            ],
            "abstract": "Motion capture systems today have to deliver high quality motion data, while being flexible and easily adaptable to different actors. Therefore, accurately determining parameters of a subject's skeletal structure is\\ crucial. Inferring these values automatically from optical motion capture data without additional measurements, however, is a challenging task.<br />This thesis describes the steps necessary to calculate the joint positions and limb lengths using data from a passive optical tracking system.<br />The algorithm is a multi-stage process that includes the tasks of automatic marker labeling, limb-wise clustering of markers and calculation of joint positions. Finally an estimate of the topology and the parameters of the articulated structure are computed. Since the topology is inferred from the data, no model has to exist in advance. This in turn makes the implemented system flexible enough to capture not only human motions, but motions of an arbitrary articulated structure, without any adaptations or additional effort. The core functionality of the system, which is the skeleton fitting task, is done using a distance function, that is applied to marker positions. This function then is minimized by a non-linear minimization algorithm.<br />Tests of the system have been performed with artificially generated data, and a construction of rods linked with articulations. The results show high accuracy for the artificial data. For the tracked data sets also satisfactory outcome is produced..<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C. (2007). <i>Skeletal structure generation for optical motion capture</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-19215</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "28706",
                    "name": "Schoenauer Christian - 2007 - Skeletal structure generation for optical motion...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2902126,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/14356/2/Schoenauer%20Christian%20-%202007%20-%20Skeletal%20structure%20generation%20for%20optical%20motion...pdf"
                },
                {
                    "bsid": "91848",
                    "name": "Schoenauer Christian - 2007 - Skeletal structure generation for optical motion...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 296690,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/14356/5/Schoenauer%20Christian%20-%202007%20-%20Skeletal%20structure%20generation%20for%20optical%20motion...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "14389",
            "handle": "20.500.12708/14400",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Implementierung eines historischen 3D-Weltatlasses auf der Basis von X3D",
            "keywords": [
                "Web 2.0",
                "atlas",
                "wiki",
                "X3D",
                "XSLT",
                "XML",
                "geoinformatics"
            ],
            "abstract": "This diploma thesis deals with the realization of a historical 3D-Worldatlas. An important principle of Web 2.0 is the active attendance of users as producers of contents. Wikis are a part of Web 2.0 and offer exactly this functionality for users. Graphical visualization is done by X3D. The database will be administrate by a wiki and can be extended and arranged whereas the geographical data is directly embedded in the wiki text.<br />Geographical and the historical structuring will be resolved by links, the basic elements of wikis. The transformation from XML to X3D take place by XSL. Because of the openness of the system and the usage from well known environments, a large expansion of the historical database is given.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Preißler, A. (2008). <i>Implementierung eines historischen 3D-Weltatlasses auf der Basis von X3D</i> [Master Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-22598</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "28794",
                    "name": "Preissler Angelika - 2008 - Implementierung eines historischen 3D-Weltatlasses...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 8683572,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/14400/2/Preissler%20Angelika%20-%202008%20-%20Implementierung%20eines%20historischen%203D-Weltatlasses...pdf"
                },
                {
                    "bsid": "91669",
                    "name": "Preissler Angelika - 2008 - Implementierung eines historischen 3D-Weltatlasses...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 139591,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/14400/5/Preissler%20Angelika%20-%202008%20-%20Implementierung%20eines%20historischen%203D-Weltatlasses...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Angelika",
                    "last_name": "Preißler",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "97117"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "143921",
            "handle": "20.500.12708/150226",
            "doi": null,
            "year": 2022,
            "issued": "2022",
            "issued_on": "2022-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "AUIT – the Adaptive User Interfaces Toolkit for Designing XR Applications",
            "keywords": [
                "adaptive user interfaces",
                "context-awareness",
                "extended reality",
                "multi-objective optimization",
                "toolkit"
            ],
            "abstract": "Adaptive user interfaces can improve experiences in Extended Reality (XR) applications by adapting interface elements according to the user's context. Although extensive work explores different adaptation policies, XR creators often struggle with their implementation, which involves laborious manual scripting. The few available tools are underdeveloped for realistic XR settings where it is often necessary to consider conflicting aspects that affect an adaptation. We fill this gap by presenting AUIT, a toolkit that facilitates the design of optimization-based adaptation policies. AUIT allows creators to flexibly combine policies that address common objectives in XR applications, such as element reachability, visibility, and consistency. Instead of using rules or scripts, specifying adaptation policies via adaptation objectives simplifies the design process and enables creative exploration of adaptations. After creators decide which adaptation objectives to use, a multi-objective solver finds appropriate adaptations in real-time. A study showed that AUIT allowed creators of XR applications to quickly and easily create high-quality adaptations.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Evangelista Belo, J. M., Lystbæk, M., Feit, A. M., Pfeuffer, K., Kan, P., Oulasvirta, A., &#38; Grønbæk, K. (2022). AUIT – the Adaptive User Interfaces Toolkit for Designing XR Applications. In M. Agrawala, J. Wobbrock, E. Adar, &#38; V. R. Setlur (Eds.), <i>UIST ’22: Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</i> (pp. 1–16). ACM. https://doi.org/10.1145/3526113.3545651</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "João Marcelo",
                    "last_name": "Evangelista Belo",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Mathias",
                    "last_name": "Lystbæk",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Anna Maria",
                    "last_name": "Feit",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Ken",
                    "last_name": "Pfeuffer",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 5,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Antti",
                    "last_name": "Oulasvirta",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Kaj",
                    "last_name": "Grønbæk",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Maneesh",
                    "last_name": "Agrawala",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Jacob",
                    "last_name": "Wobbrock",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Eytan",
                    "last_name": "Adar",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Vidya Raghavan",
                    "last_name": "Setlur",
                    "position": 4,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-02",
                "E193-06"
            ],
            "pid": "144744",
            "handle": "20.500.12708/137222",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Local projections for high-dimensional outlier detection",
            "keywords": [],
            "abstract": "A novel approach for outlier detection is proposed, called local projections, which is based on concepts of the Local Outlier Factor (LOF) (Breunig et al. in Lof: identifying densitybased local outliers. In: ACM sigmod record, ACM, volume 29, pp. 93-104, 2000) and ROBPCA (Hubert et al. in Technometrics 47(1):64-79, 2005). By using aspects of both methods, this algorithm is robust towards noise variables and is capable of performing outlier detection in multi-group situations. The idea is to focus on local descriptions of the observations and their neighbors using linear projections. The outlyingness of an observation is determined by a weighted distance of the observation to all identified projection spaces, with weights depending on the appropriateness of the local description. Experiments with simulated and real data demonstrate the usefulness of this method when compared to existing outlier detection algorithms.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ortner, T., Filzmoser, P., Rohm, M., Brodinova, S., &#38; Breiteneder, C. (2021). Local projections for high-dimensional outlier detection. <i>Metron</i>, <i>79</i>(2), 189–206. https://doi.org/10.1007/s40300-020-00183-5</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 2,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Rohm",
                    "position": 3,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 5,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "145639",
            "handle": "20.500.12708/138107",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Effects of Using Vibrotactile Feedback on Sound Localization by Deaf and Hard-of-Hearing People in Virtual Environments",
            "keywords": [],
            "abstract": "Sound source localization is important for spatial awareness and immersive Virtual Reality (VR) experiences. Deaf and Hard-of-Hearing (DHH) persons have limitations in completing sound-related VR tasks efficiently because they perceive audio information differently. This paper presents and evaluates a special haptic VR suit that helps DHH persons efficiently complete sound-related VR tasks. Our proposed VR suit receives sound information from the VR environment wirelessly and indicates the direction of the sound source to the DHH user by using vibrotactile feedback. Our study suggests that using different setups of the VR suit can significantly improve VR task completion times compared to not using a VR suit. Additionally, the results of mounting haptic devices on different positions of users' bodies indicate that DHH users can complete a VR task significantly faster when two vibro-motors are mounted on their arms and ears compared to their thighs. Our quantitative and qualitative analysis demonstrates that DHH persons prefer using the system without the VR suit and prefer mounting vibro-motors in their ears. In an additional study, we did not find a significant difference in task completion time when using four vibro-motors with the VR suit compared to using only two vibro-motors in users' ears without the VR suit.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mirzaei, M., Kán, P., &#38; Kaufmann, H. (2021). Effects of Using Vibrotactile Feedback on Sound Localization by Deaf and Hard-of-Hearing People in Virtual Environments. <i>Electronics</i>, <i>10</i>(22), 2794. https://doi.org/10.3390/electronics10222794</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mohammadreza",
                    "last_name": "Mirzaei",
                    "position": 1,
                    "role": "Author",
                    "tid": "306356"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 2,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "145642",
            "handle": "20.500.12708/138110",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Automatic Interior Design in Augmented Reality Based on Hierarchical Tree of Procedural Rules",
            "keywords": [],
            "abstract": "Augmented reality has a high potential in interior design due to its capability of visualizing numerous prospective designs directly in a target room. In this paper, we present our research on utilization of augmented reality for interactive and personalized furnishing. We propose a new algorithm for automated interior design which generates sensible and personalized furniture configurations. This algorithm is combined with mobile augmented reality system to provide a user with an interactive interior design try-out tool. Personalized design is achieved via a recommender system which uses user preferences and room data as input. We conducted three user studies to explore different aspects of our research. The first study investigated the user preference between augmented reality and on-screen visualization for interactive interior design. In the second user study, we studied the user preference between our algorithm for automated interior design and optimization-based algorithm. Finally, the third study evaluated the probability of sensible design generation by the compared algorithms. The main outcome of our research suggests that augmented reality is viable technology for interactive home furnishing.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., Kurtic, A., Radwan, M., &#38; Rodríguez, J. M. L. (2021). Automatic Interior Design in Augmented Reality Based on Hierarchical Tree of Procedural Rules. <i>Electronics</i>, <i>10</i>(3), 245. https://doi.org/10.3390/electronics10030245</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Andrija",
                    "last_name": "Kurtic",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Mohamed",
                    "last_name": "Radwan",
                    "position": 3,
                    "role": "Author",
                    "tid": "290302"
                },
                {
                    "first_name": "Jorge M. Loáiciga",
                    "last_name": "Rodríguez",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "145654",
            "handle": "20.500.12708/138122",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": true,
            "title": "StARboard & TrACTOr: Actuated Tangibles in an Educational TAR Application",
            "keywords": [],
            "abstract": "We explore the potential of direct haptic interaction in a novel approach to Tangible Augmented Reality in an educational context. Employing our prototyping platform ACTO, we developed a tabletop Augmented Reality application StARboard for sailing students. In this personal viewpoint environment virtual objects, e.g., sailing ships, are physically represented by actuated micro robots. These align with virtual objects, allowing direct physical interaction with the scene. When a user tries to pick up a virtual ship, its physical robot counterpart is grabbed instead. We also developed a tracking solution TrACTOr, employing a depth sensor to allow tracking independent of the table surface. In this paper we present concept and development of StARboard and TrACTOr. We report results of our user study with 18 participants using our prototype. They show that direct haptic interaction in tabletop AR scores en-par with traditional mouse interaction on a desktop setup in usability (mean SUS = 86.7 vs. 82.9) and performance (mean RTLX = 15.0 vs. 14.8), while outperforming the mouse in factors related to learning like presence (mean 6.0 vs 3.1) and absorption (mean 5.4 vs. 4.2). It was also rated the most fun (13× vs. 0×) and most suitable for learning (9× vs. 4×).",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vonach, E., Schindler, C., &#38; Kaufmann, H. (2021). StARboard &#38; TrACTOr: Actuated Tangibles in an Educational TAR Application. <i>Multimodal Technologies and Interaction</i>, <i>5</i>(2), 6. https://doi.org/10.3390/mti5020006</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Christoph",
                    "last_name": "Schindler",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "165772"
            ]
        },
        {
            "org_nrs": [
                "E193-02",
                "E194-01",
                "E234-02"
            ],
            "pid": "145738",
            "handle": "20.500.12708/138206",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Integrated multi-objective evolutionary optimization of production layout scenarios for parametric structural design of flexible industrial buildings",
            "keywords": [],
            "abstract": "Due to product individualization, customization and rapid technological advances in manufacturing, production systems are faced with frequent reconfiguration and expansion. Industrial buildings that allow changing production scenarios require flexible load-bearing structures and a coherent planning of the production layout and building systems. Yet, current production planning and structural building design are mostly sequential and the data and models lack interoperability. In this paper, a novel parametric evolutionary design method for automated production layout generation and optimization (PLGO) is presented, producing layout scenarios to be respected in structural building design. Results of a state-of-the-art analysis and a case study are combined to develop a novel concept of integrated production cubes and the design space for PLGO as basis for a parametric production layout design method. The integrated production cubes concept is then translated into a parametric PLGO framework, which is tested on a pilot-project of a hygiene production facility to evaluate the framework and validate the defined constraints and objectives. Results suggest that our framework can produce feasible production layout scenarios which respect flexibility and building requirements. In future research the design process will be extended by the development of a multi-objective evolutionary optimization process for industrial buildings to provide flexible building solutions that can accommodate a selection of several prioritized production layouts.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reisinger, J., Zahlbruckner, M. A., Kovacic, I., Kán, P., Wang-Sukalia, X., &#38; Kaufmann, H. (2021). Integrated multi-objective evolutionary optimization of production layout scenarios for parametric structural design of flexible industrial buildings. <i>Journal of Building Engineering</i>, <i>46</i>(103766), 103766. https://doi.org/10.1016/j.jobe.2021.103766</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julia",
                    "last_name": "Reisinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "230671"
                },
                {
                    "first_name": "Maria Antonia",
                    "last_name": "Zahlbruckner",
                    "position": 2,
                    "role": "Author",
                    "tid": "324294"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 3,
                    "role": "Author",
                    "tid": "42543"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 4,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Xi",
                    "last_name": "Wang-Sukalia",
                    "position": 5,
                    "role": "Author",
                    "tid": "253667"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 6,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": [
                "1734269"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "145769",
            "handle": "20.500.12708/138237",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Deep learning for early detection of pathological changes in X-ray bone microstructures: case of osteoarthritis",
            "keywords": [],
            "abstract": "Texture features are designed to quantitatively evaluate patterns of spatial distribution of image pixels for purposes of image analysis and interpretation. Unexplained variations in the texture patterns often lead to misinterpretation and undesirable consequences in medical image analysis. In this paper we explore the ability of machine learning (ML) methods to design a radiology test of Osteoarthritis (OA) at early stage when the number of patients' cases is small. In our experiments we use high-resolution X-ray images of knees in patients which were identified with Kellgren-Lawrence scores progressing from 1. The existing ML methods have provided a limited diagnostic accuracy, whilst the proposed Group Method of Data Handling strategy of Deep Learning has significantly extended the diagnostic test. The comparative experiments demonstrate that the proposed framework using the Zernike-based texture features has significantly improved the diagnostic accuracy on average by 11%. This allows us to conclude that the designed model for early diagnostic of OA will provide more accurate radiology tests, although new study is required when a large number of patients' cases will be available.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Livija, J., Vitaly, S., Hladuvka, J., Sergey, M., Aziz, A., &#38; Krzanowski, W. (2021). Deep learning for early detection of pathological changes in X-ray bone microstructures: case of osteoarthritis. <i>Nature</i>, <i>11</i>(2294). https://doi.org/10.1038/s41598-021-81786-4</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Jakaite",
                    "last_name": "Livija",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Schetinin",
                    "last_name": "Vitaly",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Jiri",
                    "last_name": "Hladuvka",
                    "position": 3,
                    "role": "Author",
                    "tid": "53161"
                },
                {
                    "first_name": "Minaev",
                    "last_name": "Sergey",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Ambia",
                    "last_name": "Aziz",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Wojtek",
                    "last_name": "Krzanowski",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "145804",
            "handle": "20.500.12708/138272",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Colocation for SLAM-Tracked VR Headsets with Hand Tracking",
            "keywords": [],
            "abstract": "In colocated multi-user Virtual Reality applications, relative user positions in the virtual environment need to match their relative positions in the physical tracking space. A mismatch between virtual and real relative user positions might lead to harmful events such as physical user collisions. This paper examines three calibration methods that enable colocated Virtual Reality scenarios for SLAM-tracked head-mounted displays without the need for an external tracking system. Two of these methods-fixed-point calibration and marked-based calibration-have been described in previous research; the third method that uses hand tracking capabilities of head-mounted displays is novel. We evaluated the accuracy of these three methods in an experimental procedure with two colocated Oculus Quest devices. The results of the evaluation show that our novel hand tracking-based calibration method provides better accuracy and consistency while at the same time being easy to execute. The paper further discusses the potential of all evaluated calibration methods.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reimer, D., Podkosova, I., Scherzer, D., &#38; Kaufmann, H. (2021). Colocation for SLAM-Tracked VR Headsets with Hand Tracking. <i>Computers</i>, <i>10</i>(5), 58. https://doi.org/10.3390/computers10050058</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Dennis",
                    "last_name": "Reimer",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 2,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Daniel",
                    "last_name": "Scherzer",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "145868",
            "handle": "20.500.12708/138336",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "The VRNetzer platform enables interactive network analysis in Virtual Reality",
            "keywords": [],
            "abstract": "Networks provide a powerful representation of interacting components within complex systems, making them ideal for visually and analytically exploring big data. However, the size and complexity of many networks render static visualizations on typically-sized paper or screens impractical, resulting in proverbial 'hairballs'. Here, we introduce a Virtual Reality (VR) platform that overcomes these limitations by facilitating the thorough visual, and interactive, exploration of large networks. Our platform allows maximal customization and extendibility, through the import of custom code for data analysis, integration of external databases, and design of arbitrary user interface elements, among other features. As a proof of concept, we show how our platform can be used to interactively explore genome-scale molecular networks to identify genes associated with rare diseases and understand how they might contribute to disease development. Our platform represents a general purpose, VR-based data exploration platform for large and diverse data types by providing an interface that facilitates the interaction between human intuition and state-of-the-art analysis methods.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pirch, S., Müller, F., Iofinova, E., Pazmandi, J., Hütter, C., Chiettini, M., Sin, C., Kaan, B., Podkosova, I., Kaufmann, H., &#38; Menche, J. (2021). The VRNetzer platform enables interactive network analysis in Virtual Reality. <i>Nature Communications</i>, <i>12</i>(2432). https://doi.org/10.1038/s41467-021-22570-w</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Sebastian",
                    "last_name": "Pirch",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Felix",
                    "last_name": "Müller",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Eugenia",
                    "last_name": "Iofinova",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Julia",
                    "last_name": "Pazmandi",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Christiane",
                    "last_name": "Hütter",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Chiettini",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Celine",
                    "last_name": "Sin",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Boztug",
                    "last_name": "Kaan",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 9,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 10,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Jörg",
                    "last_name": "Menche",
                    "position": 11,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E192-01",
                "E193-03"
            ],
            "pid": "146071",
            "handle": "20.500.12708/138537",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "The complexity landscape of decompositional parameters for ILP: Programs with few global variables and constraints",
            "keywords": [],
            "abstract": "Integer Linear Programming (ILP) has a broad range of applications in various areas of artificial intelligence. Yet in spite of recent advances, we still lack a thorough understanding of which structural restrictions make ILP tractable. Here we study ILP instances consisting of a small number of \"global\" variables and/or constraints such that the remaining part of the instance consists of small and otherwise independent components; this is captured in terms of a structural measure we call fracture backdoorswhich generalizes, for instance, the well-studied class of N-fold ILP instances.\r\nOur main contributions can be divided into three parts. First, we formally develop fracture backdoors and obtain exact and approximation algorithms for computing these. Second, we exploit these backdoors to develop several new parameterized algorithms for ILP; the performance of these algorithms will naturally scale based on the number of global variables or constraints in the instance. Finally, we complement the developed algorithms with matching lower bounds. Altogether, our results paint a near-complete complexity landscape of ILP with respect to fracture backdoors.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dvořák, P., Eiben, E., Ganian, R., Knop, D., &#38; Ordyniak, S. (2021). The complexity landscape of decompositional parameters for ILP: Programs with few global variables and constraints. <i>Artificial Intelligence</i>, <i>300</i>(103561), 103561. https://doi.org/10.1016/j.artint.2021.103561</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Pavel",
                    "last_name": "Dvořák",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Eduard",
                    "last_name": "Eiben",
                    "position": 2,
                    "role": "Author",
                    "tid": "284763"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Ganian",
                    "position": 3,
                    "role": "Author",
                    "tid": "272401"
                },
                {
                    "first_name": "Dušan",
                    "last_name": "Knop",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Sebastian",
                    "last_name": "Ordyniak",
                    "position": 5,
                    "role": "Author",
                    "tid": "192021"
                }
            ],
            "foci": [
                "Logic and Computation"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "146840",
            "handle": "20.500.12708/139858",
            "doi": null,
            "year": 2022,
            "issued": "2022-12-15",
            "issued_on": "2022-12-15",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Point cloud to BIM registration for robot localization and Augmented Reality",
            "keywords": [
                "BIM",
                "indoor localization",
                "Augmented Reality",
                "mobile robots",
                "point cloud to BIM registration",
                "template matching",
                "ICP",
                "localization in BIM",
                "LIDAR"
            ],
            "abstract": "Building Information Modeling (BIM) technology is often used in construction, not only at the building planning stage but also for life-cycle related tasks such as building progress control, digitally-assisted maintenance and remote inspection. These tasks require methods for localization of a mobile sensor in the BIM model. This paper presents an approach for LiDAR scan localization in BIM that is based on a combination of SLAM tracking and point cloud to BIM registration, embedded in a flexible system that can be used for Augmented Reality inspection of buildings or remote robot control. We used axis alignment, a novel normal-filtered template matching approach and an Interactive Closest Point algorithm to register the scan point cloud to the BIM point cloud. First localization accuracy evaluations demonstrate the effectiveness of our method. Localization in a 28 m long hallway is accurate up to 0.03 m when the BIM model used for the localization consists only of the hallway. In a more complex environment, containing multiple similarly shaped rooms and hallways, the localization is accurate up to 0.2 m in the first 60 m of sensor movement, and up to 0.3 m in the following travel up to 130 m.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schaub, L., Podkosova, I., Schönauer, C., &#38; Kaufmann, H. (2022). Point cloud to BIM registration for robot localization and Augmented Reality. In <i>Proceedings 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</i> (pp. 77–84). IEEE. https://doi.org/10.1109/ISMAR-Adjunct57072.2022.00025</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Linus",
                    "last_name": "Schaub",
                    "position": 1,
                    "role": "Author",
                    "tid": "330744"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 2,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 3,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": [
                "1875977"
            ]
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "14735",
            "handle": "20.500.12708/14746",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Unterstützung expliziter Articulation Work : Interaktive Externalisierung und Abstimmung mentaler Modelle",
            "keywords": [
                "Articulation Work",
                "Mental Models",
                "Tabletop Interface",
                "Tangible Interface",
                "Topic Map",
                "Modeling",
                "Concept Mapping",
                "Cooperation"
            ],
            "abstract": "Successful cooperative work requires that the involved workers develop a common un- derstanding of the modalities of their interaction.<br />According to Strauss (1985), com- mon understanding emerges from continuously and unconsciously conducted activities for alignment of understanding. In situation perceived to be complex or problematic by the involved persons, Strauss suggests that alignment activities have to triggered and conducted deliberately. Individual perceptions affect both, the identification of the need for alignment and alignment itself (Grudin, 1988). Strauss does not explicit- ly address this aspect in his theory. Approaches that support alignment based upon Strauss' work thus also largely ignore the individual, cognitive dimension of alignment.<br />Accordingly, this work aims at extending the scope of alignment support by explicitly considering the perceptions and needs of individuals. The theory of mental models (Johnson-Laird, 1981) here is used to extend Strauss' concepts and develop effective support for developing a common understanding of work processes.<br />Following the theory of mental model development by (Seel, 1991), the cooperative creation of diagrammatic models as representations of mental models can aid their alignment and the development of a common understanding. Suitable methods for building representations of mental models include structure elaboration techniques and concept mapping (Ifenthaler, 2006). Both methods have properties that are support the cooperative creation of models. In this work, they are integrated to form a method that is useable in the context of the alignment of cooperative work. The main feature for cooperation support is that modeling takes places on a simultaneously accessible and manipulable modeling surface (Dann, 1992). The method thus is  complemented with a tabletop interface - a horizontally mounted interaction surface that is augmented with computer support - to effectively support the alignment of individual views on cooperative work processes.<br />Tangible tokens are used to cooperatively build models on the interaction surface. By physically placing the tokens, the model can be manipulated simultaneously by several people. Token identification is based on visual markers that are tracked by a camera in real time. The gathered information is interpreted by the system to identify modeling activities. Model information is displayed by back-projecting it onto the surface from underneath. An traditional screen is provided as an additional output channel for information that cannot be displayed directly on the interaction surface. Cooperation is further supported by additional features like reconstruction support for former model states.<br />Persistent model representation is based upon the standardized XML Topic Map format, which allows for a reusable, self-contained representation of generic semantic networks.<br />The systems's effectiveness in supporting the alignment of work is tested in an empirical study. In three steps, the system's usability, its effects on the alignment of mental models and the effectiveness in supporting the development of a common understanding of work processes are examined. The results of the study show that the system is comprehensible and useable. Positive effects on both, the cooperation among people during modeling and the alignment of individual views of cooperative work, have been observed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Oppl, S. (2010). <i>Unterstützung expliziter Articulation Work : Interaktive Externalisierung und Abstimmung mentaler Modelle</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-43953</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "29486",
                    "name": "Oppl Stefan - 2010 - Unterstuetzung expliziter Articulation Work Interaktive...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 9169386,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/14746/2/Oppl%20Stefan%20-%202010%20-%20Unterstuetzung%20expliziter%20Articulation%20Work%20Interaktive...pdf"
                },
                {
                    "bsid": "91468",
                    "name": "Oppl Stefan - 2010 - Unterstuetzung expliziter Articulation Work Interaktive...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 1331667,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/14746/5/Oppl%20Stefan%20-%202010%20-%20Unterstuetzung%20expliziter%20Articulation%20Work%20Interaktive...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Stefan",
                    "last_name": "Oppl",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Stary",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "147012"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "147405",
            "handle": "20.500.12708/139729",
            "doi": null,
            "year": 2022,
            "issued": "2022-04-04",
            "issued_on": "2022-04-04",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Novel Redundant Validation IoT System for Affective Learning Based on Facial Expressions and Biological Signals",
            "keywords": [
                "Facial Expression",
                "Humans",
                "Neural Networks, Computer",
                "Photoplethysmography",
                "behavioral analysis",
                "facial expressions",
                "heart rate variability",
                "image databases",
                "neural networks",
                "physiological data",
                "Facial Recognition",
                "Internet of Things"
            ],
            "abstract": "Teaching is an activity that requires understanding the class's reaction to evaluate the teaching methodology effectiveness. This operation can be easy to achieve in small classrooms, while it may be challenging to do in classes of 50 or more students. This paper proposes a novel Internet of Things (IoT) system to aid teachers in their work based on the redundant use of non-invasive techniques such as facial expression recognition and physiological data analysis. Facial expression recognition is performed using a Convolutional Neural Network (CNN), while physiological data are obtained via Photoplethysmography (PPG). By recurring to Russel's model, we grouped the most important Ekman's facial expressions recognized by CNN into active and passive. Then, operations such as thresholding and windowing were performed to make it possible to compare and analyze the results from both sources. Using a window size of 100 samples, both sources have detected a level of attention of about 55.5% for the in-presence lectures tests. By comparing results coming from in-presence and pre-recorded remote lectures, it is possible to note that, thanks to validation with physiological data, facial expressions alone seem useful in determining students' level of attention for in-presence lectures.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Marceddu, A. C., Pugliese, L., Sini, J., Espinosa, G. R., Amel Solouki, M., Chiavassa, P., Giusto, E., Montrucchio, B., Violante, M., &#38; De Pace, F. (2022). A Novel Redundant Validation IoT System for Affective Learning Based on Facial Expressions and Biological Signals. <i>Sensors</i>, <i>22</i>(7), Article 2773. https://doi.org/10.3390/s22072773</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Antonio Costantino",
                    "last_name": "Marceddu",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Luigi",
                    "last_name": "Pugliese",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Jacopo",
                    "last_name": "Sini",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Gustavo Ramirez",
                    "last_name": "Espinosa",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Mohammadreza",
                    "last_name": "Amel Solouki",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Pietro",
                    "last_name": "Chiavassa",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Edoardo",
                    "last_name": "Giusto",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Bartolomeo",
                    "last_name": "Montrucchio",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Massimo",
                    "last_name": "Violante",
                    "position": 9,
                    "role": "Author"
                },
                {
                    "first_name": "Francesco",
                    "last_name": "De Pace",
                    "position": 10,
                    "role": "Author",
                    "tid": "357410"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "147412",
            "handle": "20.500.12708/139742",
            "doi": null,
            "year": 2022,
            "issued": "2022",
            "issued_on": "2022-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "BARI: An Affordable Brain-Augmented Reality Interface to Support Human–Robot Collaboration in Assembly Tasks",
            "keywords": [
                "assembly task",
                "augmented reality",
                "brain interfaces",
                "HoloLens",
                "human–robot collaboration",
                "NextMind",
                "pick and place"
            ],
            "abstract": "Human–robot collaboration (HRC) is a new and challenging discipline that plays a key role in Industry 4.0. Digital transformation of industrial plants aims to introduce flexible production lines able to adapt to different products quickly. In this scenario, HRC can be a booster to support flexible manufacturing, thus introducing new interaction paradigms between humans and machines. Augmented reality (AR) can convey much important information to users: for instance, information related to the status and the intention of the robot/machine the user is collaborating with. On the other hand, traditional input interfaces based on physical devices, gestures, and voice might be precluded in industrial environments. Brain–computer interfaces (BCIs) can be profitably used with AR devices to provide technicians solutions to effectively collaborate with robots. This paper introduces a novel BCI–AR user interface based on the NextMind and the Microsoft Hololens 2. Compared to traditional BCI interfaces, the NextMind provides an intuitive selection mechanism based on visual cortex signals. This interaction paradigm is exploited to guide a collaborative robotic arm for a pick and place selection task. Since the ergonomic design of the NextMind allows its use in combination with the Hololens 2, users can visualize through AR the different parts composing the artifact to be assembled, the visual elements used by the NextMind to enable the selections, and the robot status. In this way, users’ hands are always free, and the focus can be always on the objects to be assembled. Finally, user tests are performed to evaluate the proposed system, assessing both its usability and the task’s workload; preliminary results are very encouraging, and the proposed solution can be considered a starting point to design and develop affordable hybrid-augmented interfaces to foster real-time human–robot collaboration.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sanna, A., Manuri, F., Fiorenza, J., &#38; De Pace, F. (2022). BARI: An Affordable Brain-Augmented Reality Interface to Support Human–Robot Collaboration in Assembly Tasks. <i>Information</i>, <i>13</i>(10), Article 460. https://doi.org/10.3390/info13100460</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Andrea",
                    "last_name": "Sanna",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Federico",
                    "last_name": "Manuri",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Jacopo",
                    "last_name": "Fiorenza",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Francesco",
                    "last_name": "De Pace",
                    "position": 4,
                    "role": "Author",
                    "tid": "357410"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193",
                "E193-03"
            ],
            "pid": "147415",
            "handle": "20.500.12708/187703",
            "doi": null,
            "year": 2022,
            "issued": "2022-05-21",
            "issued_on": "2022-05-21",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Collaborative Environments for Augmented and Virtual Reality Applications",
            "keywords": [
                "Shared digital wokspaces",
                "Share environments"
            ],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Manuri, F., De Pace, F., &#38; Sanna, A. (2022). Collaborative Environments for Augmented and Virtual Reality Applications. In N. Lee (Ed.), <i>Encyclopedia of Computer Graphics and Games</i>. Springer. https://doi.org/10.1007/978-3-319-08234-9_478-1</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Federico",
                    "last_name": "Manuri",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Francesco",
                    "last_name": "De Pace",
                    "position": 2,
                    "role": "Author",
                    "tid": "357410"
                },
                {
                    "first_name": "Andrea",
                    "last_name": "Sanna",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "N.",
                    "last_name": "Lee",
                    "position": 1,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "148050",
            "handle": "20.500.12708/142191",
            "doi": null,
            "year": 2022,
            "issued": "2022",
            "issued_on": "2022-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Fast Distance Transforms in Graphs and in Gmaps",
            "keywords": [
                "nD distance transform",
                "Generalized maps",
                "Irregular pyramids",
                "Parallel processing",
                "Logarithmic complexity",
                "Geodesic distance transform (GDT)"
            ],
            "abstract": "Distance Transform (DT) as a fundamental operation in pattern recognition computes how far inside a shape a point is located. In this paper, at first, a novel method is proposed to compute the DT in a graph. By using the edge classification and a total order, the spanning forest of the foreground is created where distances are propagated through it. Second, in contrast to common linear DT methods, by exploiting the hierarchical structure of the irregular pyramid, the geodesic DT (GDT) is calculated with parallel logarithmic complexity. Third, we introduce the DT in the nD generalized map (n-Gmap) leading to a more precise and smoother DT. Forth, in the n-Gmap we define n different distances and the relation between these distances. Finally, we sketch how the newly introduced concepts can be used to simulate gas propagation in 2D sections of plant leaves.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Banaeyan, M., Carratù, C., Kropatsch, W., &#38; Hladůvka, J. (2022). Fast Distance Transforms in Graphs and in Gmaps. In <i>Structural, Syntactic, and Statistical Pattern Recognition</i> (pp. 193–202). https://doi.org/10.1007/978-3-031-23028-8_20</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Majid",
                    "last_name": "Banaeyan",
                    "position": 1,
                    "role": "Author",
                    "tid": "286018"
                },
                {
                    "first_name": "Carmine",
                    "last_name": "Carratù",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Jiří",
                    "last_name": "Hladůvka",
                    "position": 4,
                    "role": "Author",
                    "tid": "53161"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "148070",
            "handle": "20.500.12708/142194",
            "doi": null,
            "year": 2022,
            "issued": "2022",
            "issued_on": "2022-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Implicit Encoding and Simplification/Reduction of nGmaps",
            "keywords": [
                "Generalized Maps",
                "Implicit representation",
                "Memory savings",
                "nGmaps"
            ],
            "abstract": "This paper aims to present a new method of translating labeled 3D scans of biological tissues into Generalized Maps (nGmaps). Creating such nGmaps from labeled images is a solved problem in 2D and 3D using incremental algorithms. We present a new approach that works in arbitrary dimensions. To achieve this in an effective manner, we perform the necessary operations implicitly using theory rather than explicitly in memory. First we define implicit nGmaps. We then present a scheme to construct said nGmap representing an nD pixel/voxel-grid implicitly. Thirdly we give a description of the process needed to reduce such implicit nGmap. We demonstrate that our implicit approach is able to reduce nGmaps in a fraction of otherwise necessary memory.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Bogner, F., Hladůvka, J., &#38; Kropatsch, W. (2022). Implicit Encoding and Simplification/Reduction of nGmaps. In <i>Discrete Geometry and Mathematical Morphology</i> (pp. 110–122). https://doi.org/10.1007/978-3-031-19897-7_10</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Florian",
                    "last_name": "Bogner",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Jiří",
                    "last_name": "Hladůvka",
                    "position": 2,
                    "role": "Author",
                    "tid": "53161"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "148438",
            "handle": "20.500.12708/139812",
            "doi": null,
            "year": 2022,
            "issued": "2022-10-31",
            "issued_on": "2022-10-31",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Fast Labeled Spanning Tree in Binary Irregular Graph Pyramids",
            "keywords": [
                "spanning trees",
                "irregular graph pyramid",
                "Parallel Processing",
                "redundant information",
                "total order"
            ],
            "abstract": "Irregular Pyramids are powerful hierarchical structures in pattern recognition and image processing. They have high potential of parallel processing that makes them useful in processing of a huge amount of digital data generated every day. This paper presents a fast method for constructing an irregular pyramid over a binary image where the size of the images is more than 2000 in each of 2/3 dimensions. Selecting the contraction kernels (CKs) as the main task in constructing the pyramid is investigated. It is shown that the proposed fast labeled spanning tree (FLST) computes the equivalent contraction kernels (ECKs) in only two steps. To this purpose, first, edges of the corresponding neighborhood graph of the binary input image are classified. Second, by using a total order an efficient function is defined to select the CKs. By defining the redundant edges, further edge classification is performed to partition all the edges in each level of the pyramid. Finally, two important applications are presented : connected component labeling (CCL) and distance transform (DT) with lower parallel complexity 𝒪(𝑙𝑜𝑔(𝛿)) where the 𝛿 is the diameter of the largest connected component in the image.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Banaeyan, M., &#38; Kropatsch, W. (2022). Fast Labeled Spanning Tree in Binary Irregular Graph Pyramids. <i>Journal of Engineering Research and Sciences</i>, <i>1</i>(10), 69–78. https://doi.org/10.55708/js0110009</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Majid",
                    "last_name": "Banaeyan",
                    "position": 1,
                    "role": "Author",
                    "tid": "286018"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": [
                "1754584"
            ]
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "14879",
            "handle": "20.500.12708/14890",
            "doi": "10.34726/hss.2015.33193",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Full body interaction in serious games for rehabilitation",
            "keywords": [
                "Motion Capture",
                "Serious Games",
                "Rehabilitation",
                "Virtual Reality",
                "Augmented Reality"
            ],
            "abstract": "Serious games and especially their employment in healthcare applications are an active and rapidly growing area of research. Video games are expected to increase patient participation through motivational environments and to provide feedback, when repetitive rehabilitation exercises can be incorporated into game interactions. However, wide-spread use is hindered by several challenges regarding the availability and costs of input technologies, the workflow in an every-day clinical environment, the effort of application development and proof of efficacy. This thesis contributes to a solution to these problems in multiple dimensions. An affordable flexible full body Motion Capture (MoCap) system has been developed, providing methods to generate a customized skeleton model for a user and fit it to optical tracking data in real-time. The MoCap data can be used as input to a serious game and to guide the patient in his relearning process (e.g. correcting errors in movement patterns), which is done by a therapist during conventional occupational or physical therapy. Furthermore, the algorithms were integrated in a workflow, which can be handled in an every-day clinical environment. In addition, a low-cost MoCap system has been developed based on RGB-D sensors and evaluated as an alternative input modality for a home-based or telerehabilitation scenario. However, muscle activity not always results in visible motions and might be difficult to track using conventional MoCap devices and therefore biosignal acquisition systems are also used for input. Developing applications and serious games for rehabilitation, especially with a Virtual Reality (VR) setup, requires a lot of time and effort, because in addition to the implementation of game logic and content, often input/output devices, such as the above, have to be  integrated and their data processed. Therefore, for serious games in rehabilitation and other VR applications in research and teaching we have developed a powerful framework - ARTiFICe -, that is lightweight and flexible and easily integrates new devices and technologies. Furthermore, it incorporates modules for interaction, distribution in multi-user scenarios and haptic feedback. Finally, a serious game has been designed and implemented based on the ARTiFICe framework targeting rehabilitation of patients with musculoskeletal chronic pain of the lower back and neck, a group that has previously been neglected by serious games. The game was evaluated in a user study with a sample of ten adults with musculoskeletal pain and showed potential efficacy, clearly motivating patients to perform their exercises.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C. (2015). <i>Full body interaction in serious games for rehabilitation</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.33193</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "29774",
                    "name": "Schoenauer Christian - 2015 - Full body interaction in serious games for...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 49070441,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/14890/2/Schoenauer%20Christian%20-%202015%20-%20Full%20body%20interaction%20in%20serious%20games%20for...pdf"
                },
                {
                    "bsid": "93349",
                    "name": "Schoenauer Christian - 2015 - Full body interaction in serious games for...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 360130,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/14890/5/Schoenauer%20Christian%20-%202015%20-%20Full%20body%20interaction%20in%20serious%20games%20for...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "1496",
            "handle": "20.500.12708/1512",
            "doi": "10.34726/hss.2020.60781",
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Laser projection tracking",
            "keywords": [
                "tracking",
                "inside-out",
                "6dof",
                "infrared",
                "laser projection",
                "pose estimation",
                "virtual reality",
                "augmented reality"
            ],
            "abstract": "This thesis describes the development of a positional tracking library with 6 degrees of freedom (position and orientation). It works by projecting a randomized set of infrared dots on a wall or ceiling. A camera is then attached to the users head-mounted display or any other object that should be tracked (inside-out tracking). By matching the infrared dots captured by the camera with the reference pattern of the infrared dots that got projected on the wall/ceiling, the camera pose can be estimated.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Krier, A. (2020). <i>Laser projection tracking</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2020.60781</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "3011",
                    "name": "Krier Alain - 2020 - Laser projection tracking.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 9707979,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1512/2/Krier%20Alain%20-%202020%20-%20Laser%20projection%20tracking.pdf"
                },
                {
                    "bsid": "73588",
                    "name": "Krier Alain - 2020 - Laser projection tracking.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 189893,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1512/5/Krier%20Alain%20-%202020%20-%20Laser%20projection%20tracking.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Alain",
                    "last_name": "Krier",
                    "position": 1,
                    "role": "Author",
                    "tid": "296989"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "149886",
            "handle": "20.500.12708/139930",
            "doi": "10.34726/hss.2023.108561",
            "year": 2022,
            "issued": "2022",
            "issued_on": "2022-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "From circles to generalized conics: : Enriching the properties of 2D shape representation and description",
            "keywords": [
                "Generalized Conic",
                "Conic",
                "Shape Representation",
                "Shape Description",
                "Distance Field",
                "Distance Transform",
                "Voronoi Diagram",
                "Skeletonization"
            ],
            "abstract": "Like every type of human activity, research is intertwined with knowledge and experience. It aims at systematic exploration of phenomena in order to expand views and possibilities of solving a particular problem. In computer vision, the conventional way of measuring the distance between two objects is to find a pair of closest points belonging to them. When using the Euclidean metric, the equidistant set of a point is a circle. This thesis presents the alternative solutions (with an emphasis on 2D space), involving the implications for shape representation and description. They rely on other types of equidistant sets: conics (ellipse and hyperbola) and generalized conics (multifocal ellipse and hyperbola). The first solution rests on the fact that a circle is a special case of an ellipse, implying a pair of coinciding focal points. Among the variety of ellipse properties, a constant distance sum to the pair of focal points enables defining a metric. It measures the distance between a point and a line segment bounded by the focal points. This metric defines an increment in the line segment length when moving from one focal point to another through the point of interest. The immediate advantages over the classical approach are computational efficiency and independence of the line segment discretization. The second solution exhibits the key property of multifocal ellipse – each of its points has the same distance sum to the set of focal points. This concept alternatively defines the distance from a point to the collection of points. Such an interpretation is valuable in optimization problems, which can, in turn, benefit from efficient image processing techniques for solving their tasks. The third solution reflects a necessity in image processing techniques like skeletonization not only to find the distance to an object but also to find a set of points that are equidistant from a pair of objects. By definition, a multifocal hyperbola contains the points that have a constant difference between the distance sums to the pair of point sets. Assuming the focal point to be any geometric shape, the multifocal hyperbola with the associated zero distance value is an equidistant set to the pair of objects. The central notion behind this thesis is a generalization. Starting with a circle, a special case of an ellipse, it considers a generalized conic – a further conceptual extension. This transformation is reflected in the analysis of the geometric properties of these curves: from the conventional facts to the innovative findings. Such an approach enables explaining the existing and proposed methodologies through a prism of the single theoretical framework.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gabdulkhakova, A. (2022). <i>From circles to generalized conics: : Enriching the properties of 2D shape representation and description</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2023.108561</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "227347",
                    "name": "Gabdulkhakova Aysylu - 2023 - From Circles to Generalized Conics Enriching the...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4107519,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/139930/1/Gabdulkhakova%20Aysylu%20-%202023%20-%20From%20Circles%20to%20Generalized%20Conics%20Enriching%20the...pdf"
                },
                {
                    "bsid": "227426",
                    "name": "Gabdulkhakova Aysylu - 2023 - From Circles to Generalized Conics Enriching the...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 245622,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/139930/4/Gabdulkhakova%20Aysylu%20-%202023%20-%20From%20Circles%20to%20Generalized%20Conics%20Enriching%20the...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Aysylu",
                    "last_name": "Gabdulkhakova",
                    "position": 1,
                    "role": "Author",
                    "tid": "235653"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "150894",
            "handle": "20.500.12708/140902",
            "doi": null,
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "EarVR: Using Ear Haptics in Virtual Reality for Deaf and Hard-of-Hearing People",
            "keywords": [],
            "abstract": "Virtual Reality (VR) has a great potential to improve skills of Deaf and Hard-of-Hearing (DHH) people. Most VR applications and devices are designed for persons without hearing problems. Therefore, DHH persons have many limitations when using VR. Adding special features in a VR environment, such as subtitles, or haptic devices will help them. Previously, it was necessary to design a special VR environment for DHH persons. We introduce and evaluate a new prototype called \"EarVR\" that can be mounted on any desktop or mobile VR Head-Mounted Display (HMD). EarVR analyzes 3D sounds in a VR environment and locates the direction of the sound source that is closest to a user. It notifies the user about the sound direction using two vibro-motors placed on the user's ears. EarVR helps DHH persons to complete sound-based VR tasks in any VR application with 3D audio and a mute option for background music. Therefore, DHH persons can use all VR applications with 3D audio, not only those applications designed for them. Our user study shows that DHH participants were able to complete a simple VR task significantly faster with EarVR than without. The completion time of DHH participants was very close to participants without hearing problems. Also, it shows that DHH participants were able to finish a complex VR task with EarVR, while without it, they could not finish the task even once. Finally, our qualitative and quantitative evaluation among DHH participants indicates that they preferred to use EarVR and it encouraged them to use VR technology more.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mirzaei, M., Kan, P., &#38; Kaufmann, H. (2020). EarVR: Using Ear Haptics in Virtual Reality for Deaf and Hard-of-Hearing People. <i>IEEE Transactions on Visualization and Computer Graphics</i>, <i>26</i>(5), 2084–2093. https://doi.org/10.1109/tvcg.2020.2973441</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mohammadreza",
                    "last_name": "Mirzaei",
                    "position": 1,
                    "role": "Author",
                    "tid": "306356"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 2,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "151584",
            "handle": "20.500.12708/141595",
            "doi": null,
            "year": 2021,
            "issued": "2021-02",
            "issued_on": "2021-02-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Gestalt descriptions for deep image understanding",
            "keywords": [
                "Artificial Intelligence",
                "Image analysis",
                "Computer Vision and Pattern Recognition",
                "Face recognition",
                "Image classification",
                "Deep learning-based methods",
                "Gestalt descriptors",
                "Person identification"
            ],
            "abstract": "In this work, we present a novel visual perception-inspired local description approach as a preprocessing step for deep learning.\r\nWith the ongoing growth of visual data, efficient image descriptor methods are becoming more and more important.\r\nSeveral local point-based description methods were defined in the past decades before the highly accurate and popular deep\r\nlearning methods such as convolutional neural networks (CNNs) emerged. The method presented in this work combines a\r\nnovel local description approach inspired by the Gestalt laws with deep learning, and thereby, it benefits from both worlds.\r\nTo test our method, we conducted several experiments on different datasets of various forensic application domains, e.g.,\r\nmakeup-robust face recognition. Our results show that the proposed approach is robust against overfitting and only little\r\nimage information is necessary to classify the image content with high accuracy. Furthermore, we compared our experimental\r\nresults to state-of-the-art description methods and found that our method is highly competitive. For example it outperforms\r\na conventional CNN in terms of accuracy in the domain of makeup-robust face recognition.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hörhan, M., &#38; Eidenberger, H. (2021). Gestalt descriptions for deep image understanding. <i>Pattern Analysis and Applications</i>, <i>24</i>(1), 89–107. https://doi.org/10.1007/s10044-020-00904-6</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "278604",
                    "name": "Hoerhan-2021-Pattern Analysis and Applications-vor.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3695907,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/141595/3/Hoerhan-2021-Pattern%20Analysis%20and%20Applications-vor.pdf"
                },
                {
                    "bsid": "278651",
                    "name": "Hoerhan-2021-Pattern Analysis and Applications-vor.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 68771,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/141595/5/Hoerhan-2021-Pattern%20Analysis%20and%20Applications-vor.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Hörhan",
                    "position": 1,
                    "role": "Author",
                    "tid": "40036"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 2,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "151647",
            "handle": "20.500.12708/141658",
            "doi": null,
            "year": 2021,
            "issued": "2021-09",
            "issued_on": "2021-09-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Immersive training of first responder squad leaders in untethered virtual reality",
            "keywords": [
                "Software",
                "Human-Computer Interaction",
                "Mixed Reality",
                "Computer Graphics and Computer-Aided Design",
                "Interaction",
                "Virtual Reality",
                "First Responder",
                "Training",
                "Augmented Virtuality",
                "3D Object Interaction"
            ],
            "abstract": "We present the VROnSite platform that supports immersive training of first responder units' on-site squad leaders. Our training platform is fully immersive, entirely untethered to ease use and provides two means of navigation-abstract and natural walking-to simulate stress and exhaustion, two important factors for decision making. With the platform's capabilities, we close a gap in prior art for first responder training. Our research is closely interlocked with stakeholders from multiple fire brigades to gather early feedback in an iterative design process. In this paper, we present the system's design rationale, provide insight into the process of training scenario development and present results of a user study with 41 squad leaders from the firefighting domain. Virtual disaster environments with two different navigation types were evaluated using quantitative and qualitative measures. Participants considered our platform highly suitable for training of decision making in complex first responder scenarios and results show the importance of the provided navigation technologies in this context.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., Schoenauer, C., Froeschl, M., Peer, A., Goellner, J., &#38; Kaufmann, H. (2021). Immersive training of first responder squad leaders in untethered virtual reality. <i>Virtual Reality</i>, <i>25</i>(3), 745–759. https://doi.org/10.1007/s10055-020-00487-x</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "278621",
                    "name": "Mossel-2021-Virtual Reality-vor.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2518857,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/141658/2/Mossel-2021-Virtual%20Reality-vor.pdf"
                },
                {
                    "bsid": "278653",
                    "name": "Mossel-2021-Virtual Reality-vor.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 71670,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/141658/4/Mossel-2021-Virtual%20Reality-vor.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schoenauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Mario",
                    "last_name": "Froeschl",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Peer",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Johannes",
                    "last_name": "Goellner",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 6,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "624284"
            ]
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "151939",
            "handle": "20.500.12708/141961",
            "doi": "10.34726/hss.2023.97160",
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Development of a vision-based foot massage robot",
            "keywords": [
                "Computer Vision",
                "Object Recognition",
                "Roboter Control"
            ],
            "abstract": "So far, many different robots have been built and developed to help people, improve the quality of life or make work easier. Nowadays we live in a society where pressure arises due to work and other factors, causing people to develop stress in their lives. A massage can have a relaxing effect and help alleviate stress, such as a foot massage.This thesis presents a prototype of a vision-based foot massage robot. For this purpose,a hardware and a software setup was created. One part of the hardware setup was themodification of the robotic arm. This involved replacing the claw, which originally served as an end-effector, with a deodorant roller. This deodorant roller is additionally equipped with five buttons, which are supposed to detect the physical contact between the robot and the sole. Furthermore, a platform and a footrest was built.One of the main goals of the software setup is to identify and recognize the sole of thefoot as well as the robot. To achieve this goal, a Mask Region-Based Convolutional Neural Network was trained. This network marks the pixels from the robot and the sole of the foot on the input image. The massage points are manually generated once on a sole mask template and projected on to the mask of the foot during the massage. In addition, these points are loaded into a simulation, where the robotic arm is also mapped into. The rotation angles of each servomotor are calculated in the simulation with inversekinematics, so that the end-effector reaches the massage point. Then these angles aresent to the robot controller, in this case an Arduino, to move the robot. If physical contact is detected, the massage technique frictioning will be performed. Furthermore,some settings of the massage movements, such as the speed, can be changed. For this purpose, three Convolutional Neural Networks were trained, which are used for audio command recognition.Finally, the prototype was tested in a user study with a group of 13 participants. Through the evaluation it was discovered that a robotic arm can perform a state-of-the-art footmassage. The stress level of the participants is lower than before the massage and they reported to be more relaxed. However the robot massages the right foot more accurate than the left one and the robot does not always respond well to the voice commands.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ammer, D. (2023). <i>Development of a vision-based foot massage robot</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2023.97160</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "230514",
                    "name": "Ammer David - 2023 - Development of A Vision-Based Foot Massage Robot.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2565952,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/141961/1/Ammer%20David%20-%202023%20-%20Development%20of%20A%20Vision-Based%20Foot%20Massage%20Robot.pdf"
                },
                {
                    "bsid": "277681",
                    "name": "Ammer David - 2023 - Development of A Vision-Based Foot Massage Robot.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 149667,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/141961/4/Ammer%20David%20-%202023%20-%20Development%20of%20A%20Vision-Based%20Foot%20Massage%20Robot.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "David",
                    "last_name": "Ammer",
                    "position": 1,
                    "role": "Author",
                    "tid": "302732"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "154791",
            "handle": "20.500.12708/144808",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "DARGS: Dynamic AR Guiding System for Indoor Environments",
            "keywords": [],
            "abstract": "Complex public buildings, such as airports, use various systems to guide people to a certain destination. Such approaches are usually implemented by showing a floor plan that has guiding signs or color coded lines on the floor. With a technology that supports six degrees of freedom (6DoF) tracking in indoor environments, it is possible to guide people individually, thereby considering obstacles, path lengths, or pathways for handicapped people. With an augmented reality (AR) device, such as a smart phone or AR glasses, the path can be presented on top of the real environment. In this paper, we present DARGS, an algorithm, which calculates a path through a complex building in real time. Usual path planning algorithms use either shortest paths or dynamic paths for robot interaction. The human factor in a real environment is not considered. The main advantage of DARGS is the incorporation of the current field of view (FOV) of the used device to visualize a more dynamic presentation. Rather than searching for the AR content with a small FOV, with the presented approach the user always gets a meaningful three-dimensional overlay of the path independent of the viewing direction. A detailed user study is performed to prove the applicability of the system. The results indicate that the presented system is especially helpful in the first few important seconds of the guiding process, when the user is still disoriented.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gerstweiler, G., Platzer, K., &#38; Kaufmann, H. (2018). DARGS: Dynamic AR Guiding System for Indoor Environments. <i>Computers</i>, <i>7</i>(1), 5. https://doi.org/10.3390/computers7010005</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 1,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Karl",
                    "last_name": "Platzer",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "716533"
            ]
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-02",
                "E193-06"
            ],
            "pid": "154967",
            "handle": "20.500.12708/144984",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Guided projections for analyzing the structure of high-dimensional data",
            "keywords": [],
            "abstract": "A powerful data transformation method named guided projections is proposed creating new possibilities to reveal the group structure of high-dimensional data in the presence of noise variables. Utilizing projections onto a space spanned by a selection of a small number of observations allows measuring the similarity of other observations to the selection based on orthogonal and score distances. Observations are iteratively exchanged from the selection creating a non-random sequence of projections which we call guided projections. In contrast to conventional projection pursuit methods, which typically identify a low-dimensional projection revealing some interesting features contained in the data, guided projections generate a series of projections that serve as a basis not just for diagnostic plots but to directly investigate the group structure in data. Based on simulated data we identify the strengths and limitations of guided projections in comparison to commonly employed data transformation methods. We further show the relevance of the transformation by applying it to real-world data sets.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ortner, T., Filzmoser, P., Rohm, M., Breiteneder, C., &#38; Brodinova, S. (2018). Guided projections for analyzing the structure of high-dimensional data. <i>Journal of Computational and Graphical Statistics</i>, <i>27</i>(4), 750–762. https://doi.org/10.1080/10618600.2018.1459304</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 1,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 2,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Rohm",
                    "position": 3,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 5,
                    "role": "Author",
                    "tid": "281480"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "155582",
            "handle": "20.500.12708/145598",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Extraction of Structural and Semantic Data from 2D Floor Plans for Interactive and Immersive VR Real Estate Exploration",
            "keywords": [],
            "abstract": "Three-dimensional reconstructions of indoor environments are useful in various augmented and virtual scenarios. Creating a realistic virtual apartment in 3D manually does not only take time, but also needs skilled people for implementation. Analyzing a floor plan is a complicated task. Due to the lack of engineering standards in creating these drawings, they can have multiple different appearances for the same building. This paper proposes multiple models and heuristics which enable fully automated 3D reconstructions out of only a 2D floor plan. Our study focuses on floor plan analysis and definition of special requirements for a 3D building model used in a Virtual Reality (VR) setup. The proposed method automatically analyzes floor plans with a pattern recognition approach, thereby extracting accurate metric information about important components of the building. An algorithm for mesh generation and extracting semantic information such as apartment separation and room type estimation is presented. A novel method for VR interaction with interior design completes the framework. The result of the presented system is intended to be used for presenting a large number of apartments to customers. It can also be used as a base for purposes such as furnishing apartments, realistic occlusions for AR (Augmented Reality) applications such as indoor navigation or analyzing purposes. Finally, a technical evaluation and an interactive user study prove the advantages of the presented system.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gerstweiler, G., Furlan, L., Timofeev, M., &#38; Kaufmann, H. (2018). Extraction of Structural and Semantic Data from 2D Floor Plans for Interactive and Immersive VR Real Estate Exploration. <i>Technologies</i>, <i>6</i>(4), 101. https://doi.org/10.3390/technologies6040101</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 1,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Lukas",
                    "last_name": "Furlan",
                    "position": 2,
                    "role": "Author",
                    "tid": "182083"
                },
                {
                    "first_name": "Mikhail",
                    "last_name": "Timofeev",
                    "position": 3,
                    "role": "Author",
                    "tid": "64984"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "624243"
            ]
        },
        {
            "org_nrs": [
                "E186"
            ],
            "pid": "1557",
            "handle": "20.500.12708/1573",
            "doi": "10.34726/hss.2016.37305",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Detektion und Verfolgung von Zügen in OTDR Signalen : ein robustes Framework zur Zuglokalisierung in Signalen von optischer Zeitbereichsreflektometrie mit GPGPU Beschleunigung",
            "keywords": [
                "OTDR signal processing",
                "Recognition and Tracking of Trains"
            ],
            "abstract": "This thesis investigates the use of an Optical Time Domain Reflectometry (OTDR) device for railway safety improvement. OTDR sensing, often also termed Distributed Acoustical Sensing (DAS), measures the Rayleigh backscattering of a light pulse along an optical fiber. The resulting signal provides information on local acoustic pressure at linearly spaced segments, corresponding to positions, along the fiber. Using optical time-domain reflectometry, vibrations in the ground caused by different sources can be detected with high accuracy in time and space. We propose a novel method for the detection of vibrations caused by trains in an optical fiber buried within a few meters from the railway track. The presented method learns the characteristic pattern in the Fourier domain using a Support Vector Machine (SVM) and it becomes robust to background noise in the signal. We show that using a General Purpose Graphical Processing Unit (GPGPU) it is possible to compute feature values relevant for train detection in real-time. For the tracking of trains, a point-based causal algorithm is presented. The tracking has two stages to minimize the influence of false classifications of the vibration detection and is solved as an optimization problem. While several algorithms have been demonstrated in the literature for train tracking using OTDR signals, they have neither been tested on longer recordings nor with a large number of train samples. In contrast to that, our data contain four railway stations and more than ten train trajectory crossings over two hours under realistic conditions. To our knowledge, the presented algorithm is the first one in the literature which is tested against ground truth of train trajectories from a conventional train tracking system.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Papp, A. (2016). <i>Detektion und Verfolgung von Zügen in OTDR Signalen : ein robustes Framework zur Zuglokalisierung in Signalen von optischer Zeitbereichsreflektometrie mit GPGPU Beschleunigung</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.37305</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "3133",
                    "name": "Papp Adam - 2016 - Detektion und Verfolgung von Zuegen in OTDR Signalen ein...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 7219702,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1573/2/Papp%20Adam%20-%202016%20-%20Detektion%20und%20Verfolgung%20von%20Zuegen%20in%20OTDR%20Signalen%20ein...pdf"
                },
                {
                    "bsid": "74248",
                    "name": "Papp Adam - 2016 - Detektion und Verfolgung von Zuegen in OTDR Signalen ein...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 118930,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1573/5/Papp%20Adam%20-%202016%20-%20Detektion%20und%20Verfolgung%20von%20Zuegen%20in%20OTDR%20Signalen%20ein...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Adam",
                    "last_name": "Papp",
                    "position": 1,
                    "role": "Author",
                    "tid": "275941"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Litzenberger",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "111056"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "157156",
            "handle": "20.500.12708/147191",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Special Issue \"Advances in Graph-based Pattern Recognition\" Editorial",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Liu, C.-L., Luo, B., &#38; Kropatsch, W. (2017). Special Issue “Advances in Graph-based Pattern Recognition” Editorial. <i>Pattern Recognition Letters</i>, <i>87</i>, 1–3. https://doi.org/10.1016/j.patrec.2016.10.008</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Cheng-Lin",
                    "last_name": "Liu",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Bin",
                    "last_name": "Luo",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "157421",
            "handle": "20.500.12708/147454",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": true,
            "title": "Compressing VR: Fitting Large Virtual Environments within Limited Physical Space",
            "keywords": [],
            "abstract": "In practice, real-world physical workspaces and technological limitations restrict the free and unlimited exploration of an arbitrary large-scale virtual environment. This article provides an overview of the existing approaches and techniques for enlarging the walkable virtual space. The authors specifically focus on the methods that use spatial manipulation for spatial compression because it is one of the most promising, but underexplored methods for nonintrusive user redirection in a limited physical space.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vasylevska, K., &#38; Kaufmann, H. (2017). Compressing VR: Fitting Large Virtual Environments within Limited Physical Space. <i>IEEE Computer Graphics and Applications</i>, <i>37</i>(5), 85–91. https://doi.org/10.1109/mcg.2017.3621226</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "157790",
            "handle": "20.500.12708/147823",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Automatic Classification of Functional Gait Disorders",
            "keywords": [],
            "abstract": "This article proposes a comprehensive investigation of the automatic classification of functional gait disorders based solely on ground reaction force (GRF) measurements. The aim of the study is twofold: (1) to investigate the suitability of stateof-the-art GRF parameterization techniques (representations) for the discrimination of functional gait disorders; and (2) to provide a first performance baseline for the automated classification of functional gait disorders for a large-scale dataset. The utilized database comprises GRF measurements from 279 patients with gait disorders (GDs) and data from 161 healthy controls (N). Patients were manually classified into four classes with different functional impairments associated with the \"hip\", \"knee\", \"ankle\", and \"calcaneus\". Different parameterizations are investigated: GRF parameters, global principal component analysis (PCA)-based representations and a combined representation applying PCA on GRF parameters. The discriminative power of each parameterization for different classes is investigated by linear discriminant analysis (LDA). Based on this analysis, two classification experiments are pursued: (1) distinction between healthy and impaired gait (N vs. GD) and (2) multi-class classification between healthy gait and all four GD classes. Experiments show promising results and reveal among others that several factors, such as imbalanced class cardinalities and varying numbers of measurement sessions per patient have a strong impact on the classification accuracy and therefore need to be taken into account. The results represent a promising first step towards the automated classification of gait disorders and a first performance baseline for future developments in this direction.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Slijepcevic, D., Zeppelzauer, M., Gorgas, A.-M., Schwab, C., Schüller, M., Baca, A., Breiteneder, C., &#38; Horsak, B. (2017). Automatic Classification of Functional Gait Disorders. <i>IEEE Journal of Biomedical and Health Informatics</i>, <i>22</i>(5), 1653–1661. https://doi.org/10.1109/jbhi.2017.2785682</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Djordje",
                    "last_name": "Slijepcevic",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Anna-Maria",
                    "last_name": "Gorgas",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Caterine",
                    "last_name": "Schwab",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Schüller",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Arnold",
                    "last_name": "Baca",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 7,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Brian",
                    "last_name": "Horsak",
                    "position": 8,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "158233",
            "handle": "20.500.12708/148294",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "A framework for the extraction of quantitative traits from 2d images of mature Arabidopsis thaliana",
            "keywords": [],
            "abstract": "In this work, we propose an image-based phenotyping framework for the determination of quantitative traits from mature Arabidopsis thaliana plants. Two-dimensional (2D) images taken from the dried and flattened plants are analyzed regarding their geometry as well as their branching topology. The realistic branching architecture is hereby reconstructed from a single 2D image using a tracing approach with a semi-circular search window. The centerline segments of the tracing procedure are subsequently merged and labeled based on a hierarchical approach combining continuity properties with geometrical and topological information determined during tracing. This paper covers a detailed description of the proposed plant phenotyping pipeline from the image acquisition process until the extraction of the quantitative traits. The framework is evaluated using a set of 106 images and compared to a manual phenotyping approach as well as a semi-automatic image-based approach. The most relevant results of this evaluation are presented.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Augustin, M., Haxhimusa, Y., Busch, W., &#38; Kropatsch, W. G. (2016). A framework for the extraction of quantitative traits from 2d images of mature Arabidopsis thaliana. <i>Machine Vision and Applications</i>, <i>27</i>(5), 647–661. https://doi.org/10.1007/s00138-015-0720-z</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Marco",
                    "last_name": "Augustin",
                    "position": 1,
                    "role": "Author",
                    "tid": "59438"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Wolfgang",
                    "last_name": "Busch",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-02"
            ],
            "pid": "158236",
            "handle": "20.500.12708/148297",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Retargeting Approach for Mesopic Vision: Simulation and Compensation",
            "keywords": [],
            "abstract": "Retargeting approaches aim at providing a uni ed framework for image rendering in\r\nwhich both the intended scene luminance and the actual luminance of the display are taken\r\ninto account. At the core of any color retargeting method, a color vision model and its\r\ninverse are employed. Such a color appearance model should be invertible and cover the\r\nentire luminance range of the human visual system. There are not many available models\r\nwhich meet these two conditions. Moreover, most of these models are developed based on\r\npsychophysical experiments over color patches, and many have never been used for complex\r\nimages due to their complexity. In this research, a color retargeting approach based on the\r\nmesopic model of Shin et al. [1] is developed to work with complex images. We propose an\r\ninverse model for complex images to compensate for color appearance changes on dimmed\r\ndisplays viewed in dark environment. Our experimental results using both quantitative and\r\nqualitative evaluations show a discriminative improvement in the perceived color quality for\r\nmesopic vision. The proposed method can be incorporated into image retargeting techniques\r\nand display rendering mechanisms.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Rezagholizadeh, M., Akhavan, T., Soudi, A., Kaufmann, H., &#38; Clark, J. J. (2016). A Retargeting Approach for Mesopic Vision: Simulation and Compensation. <i>JOURNAL OF IMAGING SCIENCE AND TECHNOLOGY</i>, <i>60</i>(1), 10410–10412. https://doi.org/10.2352/j.imagingsci.technol.2016.60.1.010410</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mehdi",
                    "last_name": "Rezagholizadeh",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Tara",
                    "last_name": "Akhavan",
                    "position": 2,
                    "role": "Author",
                    "tid": "247963"
                },
                {
                    "first_name": "Afsoon",
                    "last_name": "Soudi",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "James J.",
                    "last_name": "Clark",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E101-03",
                "E193-03",
                "E384"
            ],
            "pid": "159151",
            "handle": "20.500.12708/149211",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Robustness Diagram with Loop and Time Controls For System Modelling and Scenario Extraction with Energy System Applications",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Malinao, J., Judex, F., Selke, T., Zucker, G., Adorna, H., Caro, J., &#38; Kropatsch, W. (2016). Robustness Diagram with Loop and Time Controls For System Modelling and Scenario Extraction with Energy System Applications. <i>Energy Procedia</i>, <i>88</i>, 537–543. https://doi.org/10.1016/j.egypro.2016.06.075</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Jasmine",
                    "last_name": "Malinao",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Florian",
                    "last_name": "Judex",
                    "position": 2,
                    "role": "Author",
                    "tid": "50717"
                },
                {
                    "first_name": "Tim",
                    "last_name": "Selke",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Gerhard",
                    "last_name": "Zucker",
                    "position": 4,
                    "role": "Author",
                    "tid": "50698"
                },
                {
                    "first_name": "Henry",
                    "last_name": "Adorna",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Jaime",
                    "last_name": "Caro",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 7,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "159153",
            "handle": "20.500.12708/149213",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Topology-based image segmentation using LBP pyramids",
            "keywords": [],
            "abstract": "In this paper, we present a new image segmentation algorithm which is based on local binary patterns (LBPs) and the combinatorial pyramid and which preserves structural correctness and image topology. For this purpose, we define a codification of LBPs using graph pyramids. Since the LBP code characterizes the topological category (local max, min, slope, saddle) of the gray level landscape around the center region, we use it to obtain a \"minimal\" image representation in terms of the topological characterization of a given 2D grayscale image. Based on this idea, we further describe our hierarchical texture aware image segmentation algorithm and compare its segmentation output and the \"minimal\" image representation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Cerman, M., Janusch, I., Gonzalez-Diaz, R., &#38; Kropatsch, W. G. (2016). Topology-based image segmentation using LBP pyramids. <i>Machine Vision and Applications</i>, <i>27</i>(8), 1161–1174. https://doi.org/10.1007/s00138-016-0795-1</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Cerman",
                    "position": 1,
                    "role": "Author",
                    "tid": "43978"
                },
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 2,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Rocio",
                    "last_name": "Gonzalez-Diaz",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "159155",
            "handle": "20.500.12708/149215",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Topological Attribute Patterns for texture recognition",
            "keywords": [],
            "abstract": "An efficient texture modeling framework based on Topological Attribute Patterns (TAP) is presented considering topology related attributes calculated from Local Binary Patterns (LBP). Our main contribution is to introduce new efficient mapping mechanisms that improve some typical mappings for LBP-based operators in texture classification such as rotation invariant patterns (ri), rotation invariant uniform patterns (riu2), and Local Binary Count (LBC). Like them, the proposed approach allows contrast and rotation invariant image description using more compact descriptors by projecting binary patterns to a reduced feature space. However, its expressiveness, and then its discrimination capability, is higher, since it includes additional information, related to the connected components of the binary patterns. The proposed mapping, evaluated and compared with different popular mappings, validates the interest of our approach. We then develop Complemented Patterns of Topological Attributes (CTAP) that generalize TAP model and exploit complemented information to further enhance its discrimination capability, and evaluate it on different texture datasets.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Nguyen, T. P., Manzanera, A., Kropatsch, W. G., &#38; Nguyen, X. S. (2016). Topological Attribute Patterns for texture recognition. <i>Pattern Recognition Letters</i>, <i>80</i>, 91–97. https://doi.org/10.1016/j.patrec.2016.06.003</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thanh Phuong",
                    "last_name": "Nguyen",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Antoine",
                    "last_name": "Manzanera",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Xuan Son",
                    "last_name": "Nguyen",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E362"
            ],
            "pid": "159158",
            "handle": "20.500.12708/149218",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Train Detection and Tracking in Optical Time Domain Reflectometry (OTDR) Signals",
            "keywords": [],
            "abstract": "We propose a novel method for the detection of vibrations\r\ncaused by trains in an optical fiber buried nearby the railway track.\r\nUsing optical time-domain reflectometry vibrations in the ground caused\r\nby different sources can be detected with high accuracy in time and\r\nspace. While several algorithms have been proposed in the literature for\r\ntrain tracking using OTDR signals they have not been tested on longer\r\nrecordings. The presented method learns the characteristic pattern in the\r\nFourier domain using a support vector machine (SVM) and it becomes\r\nmore robust to any kind of noise and artifacts in the signal. The point-\r\nbased causal train tracking has two stages to minimize the influence of\r\nfalse classifications of the vibration detection. Our technical contribution\r\nis the evaluation of the presented algorithm based on two hour long\r\nrecording and demonstration of open problems for commercial usage.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Papp, A., Wiesmeyr, C., Litzenberger, M., Garn, H., &#38; Kropatsch, W. (2016). Train Detection and Tracking in Optical Time Domain Reflectometry (OTDR) Signals. <i>Pattern Recognition</i>, 320–331. https://doi.org/10.1007/978-3-319-45886-1_26</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adam",
                    "last_name": "Papp",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Christoph",
                    "last_name": "Wiesmeyr",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Litzenberger",
                    "position": 3,
                    "role": "Author",
                    "tid": "111056"
                },
                {
                    "first_name": "Heinrich",
                    "last_name": "Garn",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 5,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06"
            ],
            "pid": "159926",
            "handle": "20.500.12708/149989",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Interactive 3D Segmentation of Rock-Art by Enhanced Depth Maps and Gradient Preserving Regularization",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zeppelzauer, M., Poier, G., Seidl, M., Reinbacher, C., Schulter, S., Breiteneder, C., &#38; Bischof, H. (2016). Interactive 3D Segmentation of Rock-Art by Enhanced Depth Maps and Gradient Preserving Regularization. <i>Journal on Computing and Cultural Heritage</i>, <i>9</i>. http://hdl.handle.net/20.500.12708/149989</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Poier",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Seidl",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Reinbacher",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Samuel",
                    "last_name": "Schulter",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 6,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 7,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "160200",
            "handle": "20.500.12708/150382",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": true,
            "title": "Markerless 3D Interaction in an Unconstrained Handheld Mixed Reality Setup",
            "keywords": [],
            "abstract": "In mobile applications, it is crucial to provide intuitive means for \r\n2D and 3D interaction. A large number of techniques exist to \r\nsupport a natural user interface (NUI) by detecting the user's hand \r\nposture in RGB+D (depth) data. Depending on the given \r\ninteraction scenario and its environmental properties, each \r\ntechnique has its advantages and disadvantages regarding \r\naccuracy and the robustness of posture detection. While the \r\ninteraction environment in a desktop setup can be constrained to \r\nmeet certain requirements, a handheld scenario has to deal with \r\nvarying environmental conditions. To evaluate the performance of \r\ntechniques on a mobile device, a powerful software framework \r\nwas developed that is capable of processing and fusing RGB and \r\ndepth data directly on a handheld device. Using this framework, \r\nfive existing hand posture recognition techniques were integrated \r\nand systematically evaluated by comparing their accuracy under \r\nvarying illumination and background. Overall results reveal best \r\nrecognition rate of posture detection for combined RGB+D data at \r\nthe expense of update rate. To support users in choosing the appropriate technique for their specific mobile interaction task, we derived guidelines based on our study. In the last step, an experimental study was conducted using the detected hand \r\npostures to perform the canonical 3D interaction tasks selection and positioning in a mixed reality handheld setup.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fritz, D., Mossel, A., &#38; Kaufmann, H. (2015). Markerless 3D Interaction in an Unconstrained Handheld Mixed Reality Setup. <i>The International Journal of Virtual Reality</i>, <i>15</i>(1), 25–34. http://hdl.handle.net/20.500.12708/150382</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Daniel",
                    "last_name": "Fritz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 2,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "161230",
            "handle": "20.500.12708/151411",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Preface of Special Issue on \"Graph-based Processing for Pattern Recognition",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Artner, N., Haxhimusa, Y., &#38; Jiang, X. (2015). Preface of Special Issue on \"Graph-based Processing for Pattern Recognition. <i>Pattern Recognition</i>, <i>48</i>(2), 289–290. https://doi.org/10.1016/j.patcog.2014.08.018</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Xiaoyi",
                    "last_name": "Jiang",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E101-03",
                "E193-03",
                "E384"
            ],
            "pid": "161231",
            "handle": "20.500.12708/151412",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Pattern mining and fault detection via \\textitCOP_\\textittherm-based profiling with correlation analysis of circuit variables in chiller systems",
            "keywords": [],
            "abstract": "In this paper, we propose methods of handling,\r\nanalyzing, and profiling monitoring data of energy systems\r\nusing their thermal coefficient of performance seen in uneven\r\nsegmentations in their time series databases. Aside from\r\nassessing the performance of chillers using this parameter,\r\nwe dealt with pinpointing different trends that this para-\r\nmeter undergoes through while the systems operate. From\r\nthese results, we identified and cross-validated with domain\r\nexperts outlier behavior which were ultimately identified as\r\nfaulty operation of the chiller. Finally, we establish correla-\r\ntions of the parameter with the other independent variables\r\nacross the different circuits of the machine with or without\r\nthe observed faulty behavior.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Malinao, J., Judex, F., Selke, T., Zucker, G., Caro, J., &#38; Kropatsch, W. (2015). Pattern mining and fault detection via \\textitCOP_\\textittherm-based profiling with correlation analysis of circuit variables in chiller systems. <i>Computer Science - Research and Development</i>, <i>31</i>(1–2), 79–87. https://doi.org/10.1007/s00450-014-0277-5</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Jasmine",
                    "last_name": "Malinao",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Florian",
                    "last_name": "Judex",
                    "position": 2,
                    "role": "Author",
                    "tid": "50717"
                },
                {
                    "first_name": "Tim",
                    "last_name": "Selke",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Gerhard",
                    "last_name": "Zucker",
                    "position": 4,
                    "role": "Author",
                    "tid": "50698"
                },
                {
                    "first_name": "Jaime",
                    "last_name": "Caro",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 6,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "161233",
            "handle": "20.500.12708/151414",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automated Multi-Contrast Brain Pathological Area Extraction from 2D MR Images",
            "keywords": [],
            "abstract": "The aim of this work is to propose the fully automated pathological area extraction from multi-parametric 2D MR images of brain. The proposed method is based on multi-resolution symmetry analysis and automatic thresholding. The proposed algorithm first detects the presence of pathology and then starts its extraction. T2 images are used for the presence detection and the multi-contrast MRI is used for the extraction, concretely T2 and FLAIR images. The extraction is based on thresholding, where Otsu's algorithm is used for the automatic determination of the threshold. Since the method is based on symmetry, it works for both axial and coronal planes. In both these planes of healthy brain, the approximate left-right symmetry exists and it is used as the prior knowledge for searching the approximate pathology location. It is assumed that this area is not located symmetrically in both hemispheres, which is met in most cases. The detection algorithm was tested on 203 T2-weighted images and reached the true positive rate of 87.52% and true negative rate of 93.14%. The extraction algorithm was tested on 357 axial and 443 coronal real images from publicly available BRATS databases containing 3D volumes brain tumor patients. The results were evaluated by Dice Coefficient (axial: 0.85 ± 0.11, coronal 0.82 ± 0.18) and by Accuracy (axial: 0.96 ± 0.05, coronal 0.94 ± 0.09).",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dvořák, P., Bartušek, K., Kropatsch, W. G., &#38; Smékal, Z. (2015). Automated Multi-Contrast Brain Pathological Area Extraction from 2D MR Images. <i>Journal of Applied Research and Technology</i>, <i>13</i>(1), 58–69. https://doi.org/10.1016/s1665-6423(15)30005-5</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "P.",
                    "last_name": "Dvořák",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "K.",
                    "last_name": "Bartušek",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "W.G.",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Z.",
                    "last_name": "Smékal",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "161252",
            "handle": "20.500.12708/151433",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Evaluating and Grading Students in Large-Scale Image Processing Lectures",
            "keywords": [],
            "abstract": "In undergraduate practical courses, it is common to work with groups of 100 or more students. These large-scale courses bring their own challenges. For example, course problems are too small and lack \"the big picture\"; grading becomes burdensome and repetitive for the teaching staff; and it is difficult to detect cheating. Based on their experience with a traditional large-scale practical course in image processing, the authors developed a novel course approach to teaching \"Introduction to Digital Image Processing\" (or EDBV, from the German course title Einführung in die Digitale Bild-Verarbeitung) for all undergraduate students of media informatics and visual computing and medical informatics at the TU Wien.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N. M., Janusch, I., &#38; Kropatsch, W. G. (2015). Evaluating and Grading Students in Large-Scale Image Processing Lectures. <i>IEEE Computer Graphics and Applications</i>, <i>35</i>(5), 101-c3. https://doi.org/10.1109/mcg.2015.107</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole M.",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 2,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "16143",
            "handle": "20.500.12708/15971",
            "doi": "10.34726/hss.2020.84123",
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Novel visual media summarization methods for retrieval applications",
            "keywords": [
                "Image analysis",
                "Deep learning based methods",
                "Gestalt descriptors",
                "Image classi  cation",
                "Face recognition",
                "Person identi  cation"
            ],
            "abstract": "In this work we present novel visual summarization methods, which are based on the visual perception of human beings, in particular on the Gestalt Laws. These laws define theories about how people perceive the world around them and the simplification of the visual stimuli without loss of meaning. To the best of our knowledge, many computer vision methods developed in the past are limited to purely technical aspects and omit psychological theories, such as Gestalt theory. With this work, we want to contribute to counteracting this fact. The most important method that was developed during this dissertation and that can be used for summarizing data is the Gestalt Interest Points (GIP) algorithm. The algorithm is fast and highly effective because it extracts very little but well-selected image information and thereby creates very compact semantic summaries of images. The GIP algorithm was the foundation for the Gestalt Regions of Interest (GROI) method. With the GROI images we improved the accuracy of a CNN for the domain of makeup-robust face recognition.Training a CNN with GROI images clearly outperforms the accuracy of a CNN trained with raw pixel imagesfor the domain of makeup-robust face recognition. Additionally, our presented method is more robust against over-fitting than the conventional approach, training a CNN from raw pixel images. The biggest advantage of the GROI method is that it is possible to summarize the semantic content of images more compactly than from whole images. This is a very important argument in particular in big data domains such as face recognition.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hörhan, M. (2020). <i>Novel visual media summarization methods for retrieval applications</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2020.84123</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "32905",
                    "name": "Hoerhan Markus - 2020 - Novel visual media summarization methods for retrieval...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2772410,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/15971/2/Hoerhan%20Markus%20-%202020%20-%20Novel%20visual%20media%20summarization%20methods%20for%20retrieval...pdf"
                },
                {
                    "bsid": "72207",
                    "name": "Hoerhan Markus - 2020 - Novel Visual Media Summarization Methods for Retrieval...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 357394,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/15971/5/Hoerhan%20Markus%20-%202020%20-%20Novel%20Visual%20Media%20Summarization%20Methods%20for%20Retrieval...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Hörhan",
                    "position": 1,
                    "role": "Author",
                    "tid": "40036"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "161595",
            "handle": "20.500.12708/151776",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Development of Tests to Evaluate the Sensory Abilities of Children with Autism Spectrum Disorder",
            "keywords": [],
            "abstract": "An emerging line of research attempts to reveal underlying mechanisms of Autism Spectrum Disorder (ASD) by studying differences in sensory processing in individuals with ASD. One sense that has not been studied well yet in this context is proprioception, a sensory system that processes information from muscles and joints about body position and force, and is hypothesized to feed into a body schema that is the foundation for motor planning and purposeful action (praxis). In this paper, we introduce new methods to measure proprioceptive functions of children with ASD. The instruments use force, touch and RGB-D sensors to retrieve data in different test scenarios. Data are transferred to a mobile device or PC and analyzed close to real-time with specifically developed software tools. The instruments were pilot tested with typically developing children to test for functionality and usability of the instruments. They will be used in a larger study with children with ASD.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Söchting, E., Hartl, J., Riederer, M., Schönauer, C., Kaufmann, H., &#38; Lamm, C. (2015). Development of Tests to Evaluate the Sensory Abilities of Children with Autism Spectrum Disorder. <i>Procedia Computer Science</i>, <i>67</i>, 193–203. https://doi.org/10.1016/j.procs.2015.09.263</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Elisabeth",
                    "last_name": "Söchting",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Johannes",
                    "last_name": "Hartl",
                    "position": 2,
                    "role": "Author",
                    "tid": "60760"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Riederer",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 4,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 5,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Claus",
                    "last_name": "Lamm",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "162062",
            "handle": "20.500.12708/193204",
            "doi": null,
            "year": 2022,
            "issued": "2022-05-29",
            "issued_on": "2022-05-29",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Parallel O(log(n)) Computation of the Adjacency of Connected Components",
            "keywords": [
                "Combinatorial map",
                "Connected Component Labeling",
                "Irregular graph pyramid",
                "Parallel processing",
                "Pattern recognition"
            ],
            "abstract": "Connected Component Labeling (CCL) is a fundamental task in pattern recognition and image processing algorithms. It groups the pixels into regions, such that adjacent pixels have the same label while pixels belonging to distinct regions have different labels. The common linear-time raster scan CCL techniques have a complexity of O(image- size) in a 2D binary image. To speed up the procedure of the CCL, the paper proposes a new irregular graph pyramid. To construct this pyramid, we use a new formalism [1] that introduces an order of the pixels in the base grid to detect the redundant edges through the hierarchical structure. These redundant edges, unlike the usual methods of constructing the irregular pyramid, are removed before contracting the edges. This not only simplifies the construction processes but may decrease memory consumption by approximately half. To perform the CCL task efficiently the proposed parallel algorithm reduces the complexity to O(log(n) ) where the n is the diameter of the largest connected component in the image. In addition, using an efficient combinatorial structure the topological properties of the connected components including adjacency of CCs, multi-boundaries and inclusions are preserved. Finally, the mathematical proofs provide fully parallel implementations and lead to efficient results in comparison with the state-of-the-art.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Banaeyan, M., &#38; Kropatsch, W. (2022). Parallel O(log(n)) Computation of the Adjacency of Connected Components. In <i>3rd International Conference on Pattern Recognition and Artificial Intelligence (ICPRAI)</i> (pp. 102–113). https://doi.org/10.1007/978-3-031-09282-4_9</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Majid",
                    "last_name": "Banaeyan",
                    "position": 1,
                    "role": "Author",
                    "tid": "286018"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "162064",
            "handle": "20.500.12708/193208",
            "doi": null,
            "year": 2022,
            "issued": "2022-06-17",
            "issued_on": "2022-06-17",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Removing Redundancies in Binary Images",
            "keywords": [
                "Binary image",
                "Combinatorial map",
                "Connected component labeling",
                "Redundant edges"
            ],
            "abstract": "Every day a huge amount of digital data is generated. Processing such big data encourages efficient data structure and parallelized operations. In this regard, this paper proposes a graph-based method reducing the memory requirement of the data storage. Graphs as a versatile representative tool in intelligent systems and pattern recognition may consist of many nonessential edges accumulating memory. This paper defines the structure of such redundant edges in the neighborhood graph of a 2D binary image. We introduce a novel approach for contracting the edges that simultaneously assists in determining the structurally redundant edges. In addition, finding a set of independent edges, the redundant edges are removed in parallel with the complexity O(1 ). Theoretically, we prove that the maximum number of redundant edges is bounded by half of all edges. Practical results show the memory requirement decreases significantly depending on the input data in different categories of binary image data sets. Using the combinatorial map as the data structure, first the topological structure of the graph is preserved. Second, the method can be extended to higher dimensions (nD).",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Banaeyan, M., Batavia, D., &#38; Kropatsch, W. (2022). Removing Redundancies in Binary Images. In <i>2nd International Conference on Intelligent Systems and Patterns Recognition (ISPR)</i> (pp. 221–233). https://doi.org/10.1007/978-3-031-08277-1_19</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Majid",
                    "last_name": "Banaeyan",
                    "position": 1,
                    "role": "Author",
                    "tid": "286018"
                },
                {
                    "first_name": "Darshan",
                    "last_name": "Batavia",
                    "position": 2,
                    "role": "Author",
                    "tid": "317665"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "162660",
            "handle": "20.500.12708/154613",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Improving Data Fusion in Personal Positioning Systems for Outdoor Environments",
            "keywords": [],
            "abstract": "A fault detection and correction methodology for personal positioning systems for outdoor environments is presented. We demonstrate its successful use in a system consisting of a global positioning system receiver and an inertial measurement unit. Localization is based on the dead reckoning algorithm. In order to obtain more reliable information from data fusion, which is carried out with Kalman filtering, the proposed methodology involves: (1) evaluation of the information provided by the sensors and (2) adaptability of the filtering. By carefully analyzing these factors we accomplish fault detection in different sources of information and in filtering. This allows us to apply corrections whenever the system requires it. Hence, our methodology consists of two stages. In the first stage, the evaluation is conducted. We apply the principles of causal diagnosis using possibility theory by defining states for normal behavior and for fault states. When a fault occurs, corrective measures are applied according to empirical knowledge. In the second stage, the consistency test of the filtering is performed. If this is inconsistent, principles of adaptive Kalman filtering are applied, which means the process and measurement noise matrices are\r\n37 tuned. Our results indicate a reasonable improvement of the trajectory obtained. At the same time, we can achieve consistent filtering, to obtain a more robust system and reliable information",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pulido Herrera, E., Kaufmann, H., Secue, J., Quirós, R., &#38; Fabregat, G. (2013). Improving Data Fusion in Personal Positioning Systems for Outdoor Environments. <i>Information Fusion</i>, <i>14</i>(1), 45–56. https://doi.org/10.1016/j.inffus.2012.01.009</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "E.",
                    "last_name": "Pulido Herrera",
                    "position": 1,
                    "role": "Author",
                    "tid": "191422"
                },
                {
                    "first_name": "H.",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "J.",
                    "last_name": "Secue",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "R.",
                    "last_name": "Quirós",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "G.",
                    "last_name": "Fabregat",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "162803",
            "handle": "20.500.12708/154756",
            "doi": null,
            "year": 2013,
            "issued": "2013-02",
            "issued_on": "2013-02-01",
            "type": "Publication",
            "subtype": "Special Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Qualität nach Maß",
            "keywords": [],
            "abstract": "Vertraglich festgelegte Preise können durch das Gewährleistungsrecht in Frage gestellt werden. In der Auftrags-Softwareentwicklung ist ein\r\nGewährleistungsstreit schnell vom Zaun gebrochen, weil es auf den ersten Blick keine allgemein anerkannten Normen für nach dem Stand der Technik mangelfreie Software gibt. Bei näherem Hinsehen lassen sich aber unter anderem verschiedene ISO-Normen zur Beurteilung\r\nder Quellcodequalität heranziehen. Wer seinen Entwicklungsprozess auf Basis der einschlägigen Verfahrensmodelle organisiert, produziert bona fide mangelfreie Software.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2013, February). Qualität nach Maß. <i>IX : Magazin Für Professionelle Informationstechnik</i>, <i>02/2013</i>, 132–136. http://hdl.handle.net/20.500.12708/154756</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "163192",
            "handle": "20.500.12708/155145",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Wide Area Motion Tracking Using Consumer Hardware",
            "keywords": [],
            "abstract": "In this paper, we present a wide area tracking system based on consumer hardware and available motion capture modules and middleware. We are using multiple depth cameras for human pose tracking in order to increase the captured space. Commercially available cameras can capture human movements in a non-intrusive way, while associated software-modules produce pose information of a simplified skeleton model. We calibrate the cameras relatively to each other to seamlessly combine their tracking data. Our design allows an arbitrary number of sensors to be integrated and used in parallel over a local area network. This enables us to capture human movements in a large arbitrarily shaped area. In addition we can improve motion capture data in regions, where the field of view of multiple cameras overlaps, by mutually completing partly occluded poses. In various examples we demonstrate, how human pose data is being merged in order to cover a wide area and how this data can easily be used for character animation in a virtual environment.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., &#38; Kaufmann, H. (2013). Wide Area Motion Tracking Using Consumer Hardware. <i>The International Journal of Virtual Reality</i>, <i>12</i>(1), 1–9. http://hdl.handle.net/20.500.12708/155145</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "163374",
            "handle": "20.500.12708/155327",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Serious Exergame for Patients Suffering from Chronic Musculoskeletal Back and Neck Pain: A Pilot Study",
            "keywords": [],
            "abstract": "Introduction: Over recent years, the popularity of videogames has gone beyond youth and gamers and is slowly entering the field of professional healthcare. Exergames are an attractive alternative to physical therapy. The primary aim of this pilot study was to explore the user experience (usability, satisfaction, level of motivation, and game experience) of the patient with the \"PlayMancer\" exergame. The secondary aim was to explore the progression of the performed motor skills (walking velocity, overhead reach ability, and cervical range of motion) and the clinical changes (to physical condition, disability, and pain intensity) in a group of patients with chronic musculoskeletal pain using an exergame for 4 weeks.\r\n\r\nMaterials and Methods: In the European PlayMancer project, an exergame for physical rehabilitation of chronic pain patients was developed. This exergame is controlled by relevant motions of the patient's body captured by a motion suit and several infrared cameras. In three different integrated minigames, the patient can train the following motor skills: Walking velocity, overhead reaching, and neck mobility.\r\n\r\nResults: Ten patients participated in this study and completed the 4 weeks of gaming. Patients rated the usability of the exergames as good (score of 78.5 [standard deviation 9.7; range, 60.0-97.5]) on the System Usability Scale, and the game motivated all patients to perform their exercises. Patients enjoyed playing and were pleased with both the game environment and the game play. Overall, the patients made a progression in the examined motor skills during the minigames over the 4 weeks of gaming.\r\n\r\nConclusions: The \"PlayMancer\" exergame is a potential tool for achieving physical rehabilitation because it motivates patients to perform their exercises and as a result increases their motor skills and physical condition.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Jansen-Kosterink, S. M., Huis in ’t Veld, R. M. H. A., Schönauer, C., Kaufmann, H., Hermens, H. J., &#38; Vollenbroek-Hutten, M. M. R. (2013). A Serious Exergame for Patients Suffering from Chronic Musculoskeletal Back and Neck Pain: A Pilot Study. <i>Games for Health Journal</i>, <i>2</i>(5), 299–307. https://doi.org/10.1089/g4h.2013.0043</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Stephanie M.",
                    "last_name": "Jansen-Kosterink",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Rianne M.H.A.",
                    "last_name": "Huis in ’t Veld",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 3,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Hermie J.",
                    "last_name": "Hermens",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Miriam M.R.",
                    "last_name": "Vollenbroek-Hutten",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "163481",
            "handle": "20.500.12708/155434",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Crypts detection in microscopic images using hierarchical structures",
            "keywords": [],
            "abstract": "This paper\r\npresents\r\nan extended and improved version of an automatic\r\ntechnique\r\nwhich robustly\r\nidentifies\r\nthe\r\nepithelial nuclei (crypt)\r\nagainst interstitial nuclei\r\nin\r\nmicroscopic images taken from colon\r\ntissues\r\n.\r\nThe detection of the\r\ncrypt inner boundary\r\nis performed\r\nusing the closing morphological hierarchy\r\n.\r\nThe disadvantages of this approach\r\nrelated to the execution time and\r\nthe\r\nused memory\r\nare highlighted and\r\nthe mor\r\nphological pyramid is used instead\r\ndue to its\r\ncomputational efficiency, the\r\nreduced\r\namount of used\r\nmemory\r\nand\r\nthe increase\r\nd\r\nrobustness. A\r\nn analysis of\r\nthe two approaches is\r\nperformed\r\nconsidering the\r\nnumber of processed pixels\r\n, the used memory and the compl\r\nexity\r\n.\r\nThe outer border is\r\ndetermined\r\nby\r\nthe\r\nepithelial nuclei overlapped by\r\nthe maximal isoline of the inner boundary\r\n.\r\nThe percentage of\r\nthe mis\r\n-\r\nsegmented nuclei\r\nagainst\r\nepithelial nuclei\r\nper crypt\r\nis used to\r\nevaluat\r\ne\r\nthe proposed method\r\ns\r\n.\r\nThe limitations\r\nare described\r\nin order\r\nto highlight the situations in which the current approach\r\nes\r\ndo not provide suitable\r\nresults.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Smochina, C., Manta, V., &#38; Kropatsch, W. (2013). Crypts detection in microscopic images using hierarchical structures. <i>Pattern Recognition Letters</i>, <i>34</i>(8), 934–941. http://hdl.handle.net/20.500.12708/155434</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Cristian",
                    "last_name": "Smochina",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Vasile",
                    "last_name": "Manta",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "163485",
            "handle": "20.500.12708/155438",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": true,
            "title": "Special issue on computer analysis of images and patterns",
            "keywords": [],
            "abstract": "Editorial\r\nSpecial issue on computer analysis of images and patterns\r\nThis special issue comprises twelve recent and relevant works\r\nin different aspects of the computer analysis of images and pat-\r\nterns. We cover here the important topics like human walking\r\nmovement, analysis of biomedical images, automatic visual surface\r\ninspection, object segmentation, natural computing, 3D mobile\r\ninteraction, vision-based driver assistance, face recognition, topo-\r\nlogical pattern recognition and estimation of shapes. All the papers\r\nincluded in this SI have been selected via the standard reviewing\r\nprocess of the journal ''Pattern Recognition Letters''. They are ex-\r\ntended versions of the best contributions accepted in the interna-\r\ntional conference CAIP2011 held from 29 up to 31 August 2011\r\nin Seville (Spain).\r\nA brief outline highlights the main achievements for each of the\r\npapers included in the SI. We grouped these papers into four\r\ngroups. Three of them include papers based on methodological\r\nsimilarities and the last one includes papers dealing with six\r\ndifferent applications.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Real, P., &#38; Kropatsch, W. (2013). Special issue on computer analysis of images and patterns. <i>Pattern Recognition Letters</i>, <i>34</i>(8), 831–832. http://hdl.handle.net/20.500.12708/155438</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Pedro",
                    "last_name": "Real",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "163486",
            "handle": "20.500.12708/155439",
            "doi": null,
            "year": 2013,
            "issued": "2013-04",
            "issued_on": "2013-04-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Three-Dimensional surface-imaging systems.",
            "keywords": [],
            "abstract": "In the past decade, advances in optical-based threedimensional\r\nimaging technologies, such as structured\r\nlight1 and stereophotogrammetry,2 have\r\ngained popularity worldwide and offer multiple medical\r\napplications.3 Understanding the science of\r\nthese technologies is critical so that surgeons can\r\nselect the appropriate tool for their clinical practice\r\nor institution.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Tzou, C.-H. J., Artner, N., Kropatsch, W., &#38; Frey, M. (2013). Three-Dimensional surface-imaging systems. <i>Plastic and Reconstructive Surgery</i>, <i>131</i>(4), 668e–670e. https://doi.org/10.1097/prs.0b013e3182827abe</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Chieh-Han John",
                    "last_name": "Tzou",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Manfred",
                    "last_name": "Frey",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "163487",
            "handle": "20.500.12708/155440",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Pathological Area Detection in MR Images of Brain",
            "keywords": [],
            "abstract": "This paper focuses on automatic locating\r\nof pathological areas in brain and its extraction. The\r\nknowledge of properties of healthy brains are used for\r\nlocating the approximate position of a pathological area.\r\nThese areas are found as parts of the brain breaking the\r\nleft-right symmetry. This method works for axial and\r\ncoronal slices and it was tested on T2-weighted images\r\nand FLAIR images in both planes",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dvorak, P., Kropatsch, W., &#38; Bartusek, K. (2013). Pathological Area Detection in MR Images of Brain. <i>Elektrorevue</i>, <i>4</i>(1), 17–21. http://hdl.handle.net/20.500.12708/155440</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Pavel",
                    "last_name": "Dvorak",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Karel",
                    "last_name": "Bartusek",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "16351",
            "handle": "20.500.12708/16171",
            "doi": "10.34726/hss.2020.80642",
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Analysis of dual-sided cycling power in a virtual reality game",
            "keywords": [
                "Virtual Reality",
                "physiotherapy",
                "cycling",
                "power meter",
                "leg power distribution"
            ],
            "abstract": "Virtual Reality is an upcoming topic, in which increasingly better hardware enables the realisation of realistic applications. The virtual 3D environment gives the opportunity for visual feedback. In this diploma thesis, visual feedback should be used in an application to show a user if his right and left leg power is equally distributed. Furthermore, the application should serve to improve a possible asymmetry in the leg power distribution through frequently usage. The application was developed for the use in the field of physiotherapy, especially for people with leg, knee or foot injuries and after surgery in one of these areas, as affected people usually put less strain on their injured leg and therefore a higher risk of an asymmetry consists. A persistent asymmetry in the leg power distribution can lead to premature fatigue, as well as to a worse performance and it furthermore increases the risk of injury. During the test, the user is riding a stationary bike in real world and is wearing a Head Mounted Display (HMD). In the VR environment the user is driving a pedal boat, which is steered by the measured leg power distribution of the right and left leg. The measurements are taken with a pedal-based power meter (Garmin Vector 3). In addition to the actual analysis test, where the user should only drive straight ahead, the application contains a second course, where the user drives a slalom. Different difficulty levels exist for both courses to better meet the users' needs. The bicycle analysis has a duration of ~14 -25 minutes, depending on the performance on the stationary bike. This work includes descriptions of all hardware components and the differences to existing VR bicycle applications. Further parts of this work are the explanation of the software and hardware design, details of the implementation and their visual results, as well as the description and evaluation of a user study. The performed user study showed that asymmetry in the leg power distribution can be recognized through this application. These are mostly discovered by the user himself during the usage. As every participant took the test only once, no statement can be done whether a frequently usage leads to an improvement. The analysis test was described as effective, physically demanding and entertaining.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Niedermayer, M. (2020). <i>Analysis of dual-sided cycling power in a virtual reality game</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2020.80642</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "33221",
                    "name": "Niedermayer Michaela - 2020 - Analysis of dual-sided cycling power in a virtual...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4518289,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16171/2/Niedermayer%20Michaela%20-%202020%20-%20Analysis%20of%20dual-sided%20cycling%20power%20in%20a%20virtual...pdf"
                },
                {
                    "bsid": "73075",
                    "name": "Niedermayer Michaela - 2020 - Analysis of Dual-Sided Cycling Power in a Virtual...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 247179,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16171/5/Niedermayer%20Michaela%20-%202020%20-%20Analysis%20of%20Dual-Sided%20Cycling%20Power%20in%20a%20Virtual...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Michaela",
                    "last_name": "Niedermayer",
                    "position": 1,
                    "role": "Author",
                    "tid": "255210"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "16352",
            "handle": "20.500.12708/16172",
            "doi": "10.34726/hss.2020.67603",
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "In-situ-questionnaires for haptic experience in VR",
            "keywords": [
                "haptics",
                "VR",
                "questionnaire",
                "real objects",
                "virtual models"
            ],
            "abstract": "Real (physical) objects are being used in virtual reality (VR), where users see its 3D model and interact with the real object. However, creating or finding an exact replica from a 3D model can be costly. As users can adapt to small proprioceptive mismatches, the objects used do not have to be identical. There is no questionnaire established in virtual reality, which would focus on object features and would be able to compare objects and their suitability to be used as substitutes. Therefore, our aim was to design a questionnaire, which would distinguish, whether the real object corresponds to a 3D model the user sees in VR based on the evaluation of object properties. To avoid incorrect completion of the questionnaire due to faulty memory, this questionnaire is asked while the users are still in VR and can interact with the object. Therefore, we named it the In-Situ-Questionnaire. To research different aspects of developing a suitable questionnaire, we designed three slightly different questionnaires, where the main change is a scale. To verify validity of the In-Situ-Questionnaires, a simple VR environment was created to test it during user studies. There the users could see their hands and interact with three different chairs. Each chair was evaluated separately. For comparison with the In-Situ-Questionnaires, we also created a Post-Questionnaire, where we used commonly used questionnaires. Analyzing of the responses from the user studies, we concluded that the designed In-Situ-Questionnaires are able to compare different objects and indicate, which object is the most suitable as a substitute.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Varečková, J. (2020). <i>In-situ-questionnaires for haptic experience in VR</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2020.67603</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "33223",
                    "name": "Vareckova Jana - 2020 - In-situ-questionnaires for haptic experience in VR.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 11356340,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16172/2/Vareckova%20Jana%20-%202020%20-%20In-situ-questionnaires%20for%20haptic%20experience%20in%20VR.pdf"
                },
                {
                    "bsid": "72935",
                    "name": "Vareckova Jana - 2020 - In Situ Questionnaires for Haptic Experience in VR.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 227065,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16172/5/Vareckova%20Jana%20-%202020%20-%20In%20Situ%20Questionnaires%20for%20Haptic%20Experience%20in%20VR.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Jana",
                    "last_name": "Varečková",
                    "position": 1,
                    "role": "Author",
                    "tid": "295275"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "40682"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "16356",
            "handle": "20.500.12708/16176",
            "doi": "10.34726/hss.2020.68745",
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "AR-Schulungs-Anwendung und -Editor für 3D-BIM-Visualisierung im Bauingenieurwesen",
            "keywords": [
                "AR",
                "Learning",
                "Construction"
            ],
            "abstract": "The following master's thesis analyzes the visualisation of 3D Building Information Modeling (BIM) data in Augmented Reality (AR) for civil engineering education. Related work, applicable technologies, and relevant standards were analyzed. Based on this knowledge, a framework consisting of a desktop editor and a handheld AR application was designed and implemented utilizing the Unity game engine alongside Google's ARCore. The desktop editor lets lecturers import IFC files, add annotations in the form of images, videos and text to them, and export the resulting projects for the use inside the AR viewer. This development has been supported by members of the Center of Digital Building Process (E234, Faculty of Civil Engineering, TU Wien), who provided real-world requirements and feedback throughout the process. The usability of the two resulting prototypes was evaluated using qualitative and quantitative methods as part of moderated in-person lab sessions. The findings were interpreted and a set of usability problems was identified for each application. Different solutions to these issues were developed alongside implementation ideas and mockups. Lastly, an outlook on the possible future of this project is given.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Höbart, K. (2020). <i>AR-Schulungs-Anwendung und -Editor für 3D-BIM-Visualisierung im Bauingenieurwesen</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2020.68745</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "33231",
                    "name": "Hoebart Konstantin - 2020 - AR-Schulungs-Anwendung und -Editor fuer...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 5293047,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16176/2/Hoebart%20Konstantin%20-%202020%20-%20AR-Schulungs-Anwendung%20und%20-Editor%20fuer...pdf"
                },
                {
                    "bsid": "73076",
                    "name": "Hoebart Konstantin - 2020 - AR Training Application and Authoring Tool for 3D...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 250815,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16176/5/Hoebart%20Konstantin%20-%202020%20-%20AR%20Training%20Application%20and%20Authoring%20Tool%20for%203D...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Konstantin",
                    "last_name": "Höbart",
                    "position": 1,
                    "role": "Author",
                    "tid": "231675"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "163599",
            "handle": "20.500.12708/155552",
            "doi": null,
            "year": 2013,
            "issued": "2013-11-04",
            "issued_on": "2013-11-04",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Structural Cues in 2D Tracking: Edge Lengths vs. Barycentric Coordinates",
            "keywords": [],
            "abstract": "Graph models offer high representational power and useful\r\nstructural cues. Unfortunately, tracking objects by matching graphs over\r\ntime is in general NP-hard. Simple appearance-based trackers are able to\r\nfind temporal correspondences fast and efficient, but often fail to over-\r\ncome challenging situations like occlusions, distractors and noise. This\r\npaper proposes an approach, where an attributed graph is used to rep-\r\nresent the structure of the target object and multiple, simple trackers\r\nin combination with structural cues replace the costly graph matching.\r\nThus, the strengths of both methodologies are combined to overcome\r\ntheir weaknesses. Experiments based on synthetic videos are used to\r\nevaluate two possible structural cues. Results show the superiority of the\r\ncue based on barycentric coordinates and the potential of the proposed\r\ntracking approach in challenging situations",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., &#38; Kropatsch, W. (2013). Structural Cues in 2D Tracking: Edge Lengths vs. Barycentric Coordinates. In <i>Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</i> (pp. 503–512). Springer. https://doi.org/10.1007/978-3-642-41827-3_63</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "163601",
            "handle": "20.500.12708/155554",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automatic Brain Tumor Detection in T2-weighted Magnetic Resonance Images",
            "keywords": [],
            "abstract": "This work focuses on fully automatic detection of brain tumors. The first aim is to determine, whether the image contains a\r\nbrain with a tumor, and if it does, localize it. The goal of this work is not the exact segmentation of tumors, but the localization of\r\ntheir approximate position. The test database contains 203 T2-weighted images of which 131 are images of healthy brain and the\r\nremaining 72 images contain brain with pathological area. The estimation, whether the image shows an afflicted brain and where a\r\npathological area is, is done by multi resolution symmetry analysis. The first goal was tested by five-fold cross-validation technique\r\nwith 100 repetitions to avoid the result dependency on sample order. This part of the proposed method reaches the true positive rate\r\nof 87.52% and the true negative rate of 93.14% for an afflicted brain detection. The evaluation of the second part of the algorithm\r\nwas carried out by comparing the estimated location to the true tumor location. The detection of the tumor location reaches the\r\nrate of 95.83% of correct anomaly detection and the rate 87.5% of correct tumor location.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dvořák, P., Kropatsch, W. G., &#38; Bartušek, K. (2013). Automatic Brain Tumor Detection in T2-weighted Magnetic Resonance Images. <i>Measurement Science Review</i>, <i>13</i>(5), 223–230. https://doi.org/10.2478/msr-2013-0034</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "P.",
                    "last_name": "Dvořák",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "W.G.",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "K.",
                    "last_name": "Bartušek",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "163603",
            "handle": "20.500.12708/155556",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Video Object Segmentation by Salient Segment Chain Composition",
            "keywords": [],
            "abstract": "We present a model for video segmentation, applicable to\r\nRGB (and if available RGB-D) information that constructs\r\nmultiple plausible partitions corresponding to the static and\r\nthe moving objects in the scene: i) we generate multiple\r\nfigure-ground segmentations, in each frame, parametrically,\r\nbased on boundary and optical flow cues, then track, link\r\nand refine the salient segment chains corresponding to the\r\ndifferent objects, over time, using long-range temporal constraints;\r\nii) a video partition is obtained by composing segment\r\nchains into consistent tilings, where the different individual\r\nobject chains explain the video and do not overlap.\r\nSaliency metrics based on figural and motion cues, as\r\nwell as measures learned from human eye movements are\r\nexploited, with substantial gain, at the level of segment generation\r\nand chain construction, in order to produce compact\r\nsets of hypotheses which correctly reflect the qualities\r\nof the different configurations. The model makes it possible\r\nto compute multiple hypotheses over both individual object\r\nsegmentations tracked over time, and for complete video\r\npartitions. We report quantitative, state of the art results in\r\nthe SegTrack single object benchmark, and promising qualitative\r\nand quantitative results in clips filming multiple static\r\nand moving objects collected from Hollywood movies and\r\nfrom the MIT dataset.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Banica, D., Agape, A., &#38; Sminchisescu, C. (2013). Video Object Segmentation by Salient Segment Chain Composition. <i>International Journal of Computer Vision</i>, <i>1</i>, 1–8. http://hdl.handle.net/20.500.12708/155556</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Dan",
                    "last_name": "Banica",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Aelxandru",
                    "last_name": "Agape",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Cristian",
                    "last_name": "Sminchisescu",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "164122",
            "handle": "20.500.12708/156075",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "The effect of airflow on thermographically determined temperature of the distal forelimb of the horse",
            "keywords": [],
            "abstract": "REASONS FOR PERFORMING STUDY:\r\n\r\nCurrent literature suggests that thermographic imaging of horses should be performed in a draught-free room. However, studies on the effect of airflow on determined temperature have not been published.\r\nOBJECTIVES:\r\n\r\nTo investigate effects of airflow on thermographically determined temperature of horses' forelimbs; to assess the relationship of wind velocity, rectal temperature, ambient temperature and humidity.\r\nMETHODS:\r\n\r\nThermographic images were obtained for the forelimbs of 6 horses in a draught-free room. Three replicates (R) with defined wind velocities (R1, 0.5-1.0 m/s; R2, 1.3-2.6 m/s; and R3, 3.0-4.0 m/s) were conducted. Each replicate consisted of a baseline image, a 15 min phase with the wind on and a 15 min phase with the wind off. We exposed only the right leg to airflow and determined the temperature by thermography with the wind on and wind off. Temperature differences between baseline and wind on, between wind on and wind off and between different wind velocities were analysed by a general linear model, Student's paired t test and ANOVA.\r\nRESULTS:\r\n\r\nAfter the onset of wind, the temperature on the right forelimb decreased within 1-3 min (by approximately 0.6°C at R1, 1.5°C at R2 and 2.1°C at R3). With the wind off, the temperature increased within 3 min (by approximately 1.2°C at R1, 1.7°C at R2 and 2.1°C at R3). With increasing wind velocity, the temperature differences between baseline and wind on and between wind on and wind off increased significantly.\r\nCONCLUSIONS:\r\n\r\nBarely noticeable wind velocities caused a decrease in thermographically determined temperatures of the forelimbs of the horse. Further research is required to assess the influence of airflow on other parts of the body and at different ambient temperatures, as well as the effect on horses with inflammatory lesions, especially of the distal limbs.\r\nPOTENTIAL RELEVANCE:\r\n\r\nIt is essential for practitioners to perform thermography on horses in a draught-free environment in order to avoid false-positive or -negative diagnoses.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Westermann, S., Stanek, C., Schramel, J. P., Ion, A., &#38; Buchner, H. H. (2013). The effect of airflow on thermographically determined temperature of the distal forelimb of the horse. <i>Equine Veterinary Journal</i>, <i>45</i>(5), 637–641. http://hdl.handle.net/20.500.12708/156075</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Simone",
                    "last_name": "Westermann",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "C",
                    "last_name": "Stanek",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "J. P.",
                    "last_name": "Schramel",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "H.H.",
                    "last_name": "Buchner",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "164418",
            "handle": "20.500.12708/156371",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": true,
            "title": "SmartCopter - Enabling Autonomous Flight in Indoor Environments with a Smartphone as On-Board Processing Unit",
            "keywords": [],
            "abstract": "We present a low-cost Unmanned Aerial Vehicle (UAV) for autonomous flight and navigation in GPS-denied environments using an off-the-shelf smartphone as its core on-board processing unit. Thereby, our approach is independent from additional ground hardware and the UAV core unit can be easily replaced with more powerful hardware that simplifies setup updates as well as maintenance. The UAV is able to map, locate and navigate in an unknown indoor environment fusing vision based tracking with inertial and attitude measurements. We choose an algorithmic approach for mapping and localization that does not require GPS coverage of the target area, therefore autonomous indoor navigation is made possible. We demonstrate the UAVs capabilities of mapping, localization and navigation in an unknown 2D marker environment. Our promising results enable future research on 3D self-localization and dense mapping using mobile hardware as the only on-board processing unit. Our proposed autonomous flight processing pipeline robustly tracks and maps planar markers that need to be distributed throughout the tracking volume. We provide a low-cost off-the-shelf flight platform that only requires a commercially available mobile device as core processing unit for autonomous flight in GPS denied areas.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., Leichtfried, M., Kaltenriener, C., &#38; Kaufmann, H. (2014). SmartCopter - Enabling Autonomous Flight in Indoor Environments with a Smartphone as On-Board Processing Unit. <i>International Journal of Pervasive Computing and Communications</i>, <i>10</i>(1), 92–114. https://doi.org/10.1108/ijpcc-01-2014-0010</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Leichtfried",
                    "position": 2,
                    "role": "Author",
                    "tid": "183837"
                },
                {
                    "first_name": "Christoph",
                    "last_name": "Kaltenriener",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E120-05",
                "E193-02"
            ],
            "pid": "164463",
            "handle": "20.500.12708/156416",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Vision-Based Long-Range 3D Tracking, applied to Underground Surveying Tasks",
            "keywords": [],
            "abstract": "To address the need of highly automated positioning systems in underground construction, we present a long-range 3D tracking system based on infrared optical markers. It provides continuous 3D position estimation of static or kinematic targets with low latency over a tracking volume of 12x8x70m (width\r\nx height x depth). Over the entire volume, relative 3D point accuracy with a maximal deviation ≤22 mm is ensured with possible target rotations of yaw, pitch = 0 − 45 and roll = 0 − 360 . No preliminary sighting of target(s) is necessary since the system automatically locks onto a target without user intervention and autonomously starts tracking as soon as a target is within the view of the system. The proposed system needs a minimal hardware setup, consisting of two machine vision cameras and a standard workstation for data processing. This allows for quick installation with minimal disturbance of construction work. The data processing pipeline ensures camera calibration and tracking during on-going underground activities. Tests in real underground\r\nscenarios prove the system's capabilities to act as 3D position measurement platform for multiple underground tasks that require long range, low latency and high accuracy. Those tasks include simultaneously tracking of personnel, machines or robots.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., Gerstweiler, G., Vonach, E., Chmelina, K., &#38; Kaufmann, H. (2014). Vision-Based Long-Range 3D Tracking, applied to Underground Surveying Tasks. <i>Journal of Applied Geodesy</i>, <i>8</i>(1). https://doi.org/10.1515/jag-2013-0009</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 2,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 3,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Klaus",
                    "last_name": "Chmelina",
                    "position": 4,
                    "role": "Author",
                    "tid": "127455"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 5,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "165214",
            "handle": "20.500.12708/157168",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Comparison of three-dimensional surface-imaging systems",
            "keywords": [],
            "abstract": "Background: In recent decades, three-dimensional (3D) surface-imaging technologies\r\nhave gained popularity worldwide, but because most published articles that mention them\r\nare technical, clinicians often have difficulties gaining a proper understanding of them. This\r\narticle aims to provide the reader with relevant information on 3D surface-imaging systems.\r\nIn it, we compare the most recent technologies to reveal their differences.\r\nMethods: We have accessed five international companies with the latest technologies in 3D\r\nsurface-imaging systems: 3dMD, Axisthree, Canfield, Crisalix and Dimensional Imaging (Di3D;\r\nin alphabetical order). We evaluated their technical equipment, independent validation\r\nstudies and corporate backgrounds.\r\nResults: The fastest capturing devices are the 3dMD and Di3D systems, capable of capturing\r\nimages within 1.5 and 1 ms, respectively. All companies provide software for tissue modifications.\r\nAdditionally, 3dMD, Canfield and Di3D can fuse computed tomography (CT)/cone-beam\r\ncomputed tomography (CBCT) images into their 3D surface-imaging data. 3dMD and Di3D provide\r\n4D capture systems, which allow capturing the movement of a 3D surface over time. Crisalix\r\ngreatly differs from the other four systems as it is purely web based and realised via cloud\r\ncomputing.\r\nConclusion: 3D surface-imaging systems are becoming important in today's plastic surgical setups,\r\ntaking surgeons to a new level of communication with patients, surgical planning and\r\noutcome evaluation. Technologies used in 3D surface-imaging systems and their intended field",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Tzou, C.-H. J., Artner, N. M., Pona, I., Hold, A., Placheta, E., Kropatsch, W. G., &#38; Frey, M. (2014). Comparison of three-dimensional surface-imaging systems. <i>Journal of Plastic, Reconstructive and Aesthetic Surgery</i>, <i>67</i>(4), 489–497. https://doi.org/10.1016/j.bjps.2014.01.003</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Chieh-Han John",
                    "last_name": "Tzou",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Nicole M.",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Igor",
                    "last_name": "Pona",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Alina",
                    "last_name": "Hold",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Eva",
                    "last_name": "Placheta",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 6,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Manfred",
                    "last_name": "Frey",
                    "position": 7,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "165251",
            "handle": "20.500.12708/157205",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": true,
            "title": "Video analysis of a snooker footage based on a kinematic model",
            "keywords": [],
            "abstract": "Taking an inspiration from psychological studies of visual attention, the contribution of this paper lies in prediction of the critical points of the trajectory using the structure of a scene and physical motion model. On one side, we present our approach for video analysis that differs from traditional tracking techniques by predicting future states of the moving object rather than its next consecutive position using the physically-based motion functionality. On the other side, we propose to use the structure of the scene, which contains the information about the obstacles and space limits, for discovering the critical points of the trajectory. As a proof of concept we developed the use case application for analysing snooker footage.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gabdulkhakova, A., &#38; Kropatsch, W. G. (2014). Video analysis of a snooker footage based on a kinematic model. In <i>Structural, Syntactic, and Statistical Pattern Recognition</i> (pp. 223–232). Springer. https://doi.org/10.1007/978-3-662-44415-3_23</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Aysylu",
                    "last_name": "Gabdulkhakova",
                    "position": 1,
                    "role": "Author",
                    "tid": "235653"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "165645",
            "handle": "20.500.12708/157601",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Graph-based point drift: Graph centrality on the registration of point-sets",
            "keywords": [],
            "abstract": "The problem of point-set registration often arises in Pattern Recognition whenever one needs to match information available in images, such as feature locations, landmarks, or points representing a surface of an object. It is a challenging task and a widely explored topic in stereo vision, image alignment, medical imaging, and other fields. Many of those problems have been addressed using graph theory by taking advantage of the structural information available in graphs. In this paper, graph centralities are explored in the point-set registration problem for the first time. We propose a variant of the Coherent Point Drift (CPD) by integrating the degree, betweenness, closeness, eigenvector, and pagerank centralities. The centrality values bring topological information used during the computation of correspondence between points. We analyse the performance on several datasets and our results indicate that the registration can converge faster when the centrality is combined with the spatial information in the traditional probabilistic framework. Our novel contribution introduces the social network centralities as a good source of prior information for the registration problem and it demonstrates how one can take advantage of such information.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">de Sousa, S., &#38; Kropatsch, W. G. (2014). Graph-based point drift: Graph centrality on the registration of point-sets. <i>Pattern Recognition</i>, <i>48</i>(2), 368–379. https://doi.org/10.1016/j.patcog.2014.06.011</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Samuel",
                    "last_name": "de Sousa",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "166744",
            "handle": "20.500.12708/158753",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Assessing the sensory functionality of children using depth data : Development of a reproducible and stable test for measuring the kinesthesia of children (with ASD)",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hartl, J. (2015). <i>Assessing the sensory functionality of children using depth data : Development of a reproducible and stable test for measuring the kinesthesia of children (with ASD)</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/158753</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Johannes",
                    "last_name": "Hartl",
                    "position": 1,
                    "role": "Author",
                    "tid": "60760"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "166745",
            "handle": "20.500.12708/158754",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Video representation by recurring regions",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Diem, L. (2015). <i>Video representation by recurring regions</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/158754</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Lukas",
                    "last_name": "Diem",
                    "position": 1,
                    "role": "Author",
                    "tid": "52647"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "166753",
            "handle": "20.500.12708/158762",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "On robust homography estimation across the 2D/3D modalities",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Barclay, A. (2015). <i>On robust homography estimation across the 2D/3D modalities</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/158762</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adam",
                    "last_name": "Barclay",
                    "position": 1,
                    "role": "Author",
                    "tid": "54614"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "166758",
            "handle": "20.500.12708/158767",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Kameragestützte Informationsvisualisierung durch Augmented Reality für Fahrerassistenzsysteme",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hofmann-Wellenhof, J. (2015). <i>Kameragestützte Informationsvisualisierung durch Augmented Reality für Fahrerassistenzsysteme</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/158767</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Johannes",
                    "last_name": "Hofmann-Wellenhof",
                    "position": 1,
                    "role": "Author",
                    "tid": "61744"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "166781",
            "handle": "20.500.12708/158790",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Trainingsüberprüfung mittels Methoden aus \"Machine Learning\"",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sedlmayr, J. (2015). <i>Trainingsüberprüfung mittels Methoden aus “Machine Learning”</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/158790</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Jan",
                    "last_name": "Sedlmayr",
                    "position": 1,
                    "role": "Author",
                    "tid": "230663"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "166815",
            "handle": "20.500.12708/158824",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Erkennung von Elephantenlauten anhand lokaler Spektralfeatures",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropshofer, D. (2015). <i>Erkennung von Elephantenlauten anhand lokaler Spektralfeatures</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/158824</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Daniel",
                    "last_name": "Kropshofer",
                    "position": 1,
                    "role": "Author",
                    "tid": "39270"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "168405",
            "handle": "20.500.12708/160414",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Cross-platform tracking of a 6DoF motion controller using computer vision and sensor fusion",
            "keywords": [
                "computer vision",
                "sensor fusion",
                "tracking",
                "calibration",
                "mobile",
                "cross-platform",
                "motion tracking",
                "input",
                "6dof",
                "open source"
            ],
            "abstract": "There is a lack of software for 6DoF (six degrees of freedom) tracking using affordable off-the-shelf hardware. With the introduction of motion controllers in game consoles, the hardware is easily available these days, but no fully-featured software solutions for 6DoF tracking exist.<br />This thesis introduces the PS Move API, a cross-platform open source library for multiple programming languages that can be used to track multiple PS Move Motion Controllers via Bluetooth and a USB 2.0 PS Eye camera. The library implements sensor fusion to track all six degrees of freedom: 3-axis position and 3-axis rotation.<br />The library solves the problems of communicating with the controller via USB and Bluetooth using the HID (Human Interface Device) protocol, pairing the controller with the host computer (for Bluetooth connections) and connecting to the controller in a cross-platform manner.<br />Vision tracking is implemented using the freely available OpenCV framework and a PS Eye camera (other cameras are supported as well). The PS Move Motion Controller has a sphere at its top that can change its color using RGB LEDs - this is used to track the controller and to distinguish between multiple controllers.<br />Orientation tracking is implemented using an open source AHRS (attitude heading reference system) algorithm, integrating inertial sensor readings from accelerometers, gyroscopes and magnetometers into a quaternion representation, which describes rotations in 3D space.<br />Sensor fusion combines data from the visual and orientation tracking to get the controller position and orientation relative to the camera position in world coordinates. This data can then be used for different input mechanisms, such as augmented or virtual reality applications.<br />An easy-to-use API (application programming interface) is provided as part of the library design, allowing quick prototyping and efficient implementation of  solutions incorporating the PS Move Motion Controller.<br />Example applications and integrations into existing frameworks such as TUIO and OpenTracker demonstrate different API use cases and validate the results of the implementation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Perl, T. (2012). <i>Cross-platform tracking of a 6DoF motion controller using computer vision and sensor fusion</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/160414</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Perl",
                    "position": 1,
                    "role": "Author",
                    "tid": "60724"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "168427",
            "handle": "20.500.12708/160436",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Entwicklung eines multitouchfähigen Schaufensters",
            "keywords": [
                "multitouch",
                "touch",
                "shop-window",
                "window",
                "hci"
            ],
            "abstract": "In this thesis a hard- and software setup is presented, which allows to equip any shop window with multitouch functionality. First, this paper summarizes the theoretic fundamentals of optical sensor technology and light sources. Then an overview of the available multitouch-technologies is provided. Thereby we focus on camera-based technologies, which are suitable for self-construction and for the realisation of large-sized displays. Furthermore, miscellaneous software-components, which can be used for the implementation of multitouch-applications, are described. Hereupon the implemented hardware-setup, as well as the used and self-developed software, is presented.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Grünling, M. (2011). <i>Entwicklung eines multitouchfähigen Schaufensters</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/160436</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Manfred",
                    "last_name": "Grünling",
                    "position": 1,
                    "role": "Author",
                    "tid": "43360"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "168483",
            "handle": "20.500.12708/160492",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Emotionsbasierte Videoverfremdung",
            "keywords": [
                "emotion",
                "video obfuscation",
                "anonymization"
            ],
            "abstract": "This master thesis is about obfuscation of persons shown in a video. The obfuscation process is based on an automatic evaluation of emotional speech. On the one hand visible and audible individuals are anonymized by the implemented effects. On the other hand the effects are supposed to reconstruct or even emphasize emotions that are lost during the anonymization process. Many works on emotion recognition focus on distinguishing between the so-called basic emotions proposed by Ekman like joy, sadness, anger, fear, etc. In this thesis, emotions are described in a continuous, three-dimensional space, the coordinate axes of which correspond to the emotion primitives valence, arousal and dominance. The emotion recognition is accomplished by two different machine learning algorithms namely Support Vector Regression and a modified k-Nearest-Neighbor algorithm. The training and test sets for the machine learning process are taken from the German \"Vera am Mittag\" database out of the HUMAINE project. The dataset contains twelve hours of annotated and ready-to-use video and speech. In this work 69 prosodic and spectral features such as pitch, RMS or MFCC are used for emotion recognition. A separate ranking of all features is created for each of the three emotion primitives. Three different visual anonymization effects are implemented: an edge based effect, a symbolic based effect and an effect for a hand-painted look. The emotion primitives act as steering parameters for the effects and thus directly influence their appearance. Voice is anonymized by applying a vocoder-like effect.<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fischl, C. (2011). <i>Emotionsbasierte Videoverfremdung</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/160492</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Fischl",
                    "position": 1,
                    "role": "Author",
                    "tid": "41961"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E186"
            ],
            "pid": "168570",
            "handle": "20.500.12708/160579",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Interactive tracking of markers for facial palsy analysis",
            "keywords": [
                "computer vision",
                "object tracking",
                "facial analysis",
                "medical diagnosis",
                "interactive",
                "particle filtering",
                "Bayes"
            ],
            "abstract": "The human face provides a rich source of information from muscular movement and nerval actuation to properties of skin and facial characteristics. This information can be exploited to diagnose and quantify facial impairments. Facial palsy is one of these impairments, and is caused by restrictions of the nerval actuation of muscles responsible for facial expressions. The main symptoms of this condition are asymmetrical facial movement and partial facial paralysis. To measure its progress and to compare pre-surgical with post-surgical conditions, medical physicians require different clinical measures extracted from those locations of the face which provide most information about the facial expression. These locations are indicated by small artificial markers which are placed on the patient's face before an evaluation session. A video of the patient is then recorded which is used to localize these markers in every frame. This task is currently performed manually by an operator and can take up to five hours for a single video. Object tracking refers to a research field which deals with the estimation of the position of one or many objects from an image sequence. Its methods have been applied successfully to different applications, ranging from video surveillance to robotics. Traditionally, illumination, changes in pose and occlusion are considered as the main problems when tracking artificial objects of interest. While the associated tracking methods proved themselves able to deal with these problems in recent years, tracking objects from the medical perspective are still partly unexplored. Just like all natural objects, the human face has a high potential for deformation and is characterized by an irregular texture. Additionally, not only one, but multiple objects/markers have to be tracked simultaneously, which imposes additional difficulty by ensuring that markers can be uniquely identified in every frame. The  thesis explores the possibility of tracking the artificial facial markers semi-automatically by applying different, state-of-the-art tracking schemes to the presented problem. The tracking schemes are based on a sequential Bayes estimation technique, the so called particle filter, which assesses a set of hypothesis using their congruence with the target model. Hence, the location of each marker can be accurately estimated and occlusions handled efficiently. To improve the accuracy and to reset lost markers, the clinical operator can interact with the tracking system. The results showed that our chosen methods are superior in both the number of interactions and accuracy when compared with traditional trackers which use only a single hypothesis concerning the marker locations.<br />Additionally, it is shown that the evaluated schemes are able to replace the task of manual tracking while preserving a high accuracy. As a result, the time to locate the markers is decreased by around 2/3 with an accuracy of around 3-4 pixels towards the available ground truth.<br />Additionally, only around 2 % of the evaluated frames required operator intervention.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Limbeck, P. (2012). <i>Interactive tracking of markers for facial palsy analysis</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/160579</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philip",
                    "last_name": "Limbeck",
                    "position": 1,
                    "role": "Author",
                    "tid": "47675"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "38472"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "64562"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "168580",
            "handle": "20.500.12708/160589",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automated classification of paintings",
            "keywords": [
                "image processing",
                "media classification",
                "media understanding",
                "machine learning",
                "painting",
                "art"
            ],
            "abstract": "This thesis deals with the automated classification of paintings against the painting styles cubism, expressionism, impressionism, pointillism and renaissance painting by the means of a prototypical proof of concept system. A feature set has been developed which covers a wide range of feature extraction approaches such as edge-based, frequency-based or color-based features. Some approaches have been adopted from other research project where they have been applied on related problems. The classification of a painting for a human is a highly emotional and intuitive process. As a consequence it is difficult to operationalize and it contains a certain amount of unpredictability.<br />And indeed, it has turned out that some of the developed features behave as intended whereas others behave against intuition and expectation. It is, however, just this unpredictability which makes the underlying classification problem interesting. In order to find the best parameter tuple for each feature they have been optimized on a training set of 1000 images. In order to investigate how far the optimized feature set is reducible a principle component analysis has been applied on it.<br />Succeeding the feature optimization stage the classification algorithms C4.5 - decision tree, random forest, naive bayes, multilayer perceptron and k-nearest neighbor have been applied on the feature set. As it has been done in the feature optimization stage, a wide range of parameter tuples have been tested for each classification algorithm whereas a stratified 10 x 10 cross validation on the training set has been used as the measurement for the classification performance. The classifier which has shown the best results in the cross validation test has been used to classify an independent test set of 200 images. By this means it could be shown that the classification task can be solved in this particular domain. Further, it could be shown that a  principle component analysis allows a reduction of the feature space to no more than 24% of the original size while still gaining approximately the same classification performance. Finally, it could be shown that the related retrieval problem is also worth investigating.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gerger, R. (2012). <i>Automated classification of paintings</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/160589</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Roman",
                    "last_name": "Gerger",
                    "position": 1,
                    "role": "Author",
                    "tid": "42652"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "168587",
            "handle": "20.500.12708/160596",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "A virtual dressing room based on depth data",
            "keywords": [
                "Virtual Dressing Room",
                "Depth Data",
                "Kinect",
                "Garment",
                "Body Measurements"
            ],
            "abstract": "Many of the currently existing Virtual Dressing Rooms are based on diverse approaches. Those are mostly virtual avatars or fiducial markers, and they are mainly enhancing the experience of online shops.<br />The main drawbacks are the inaccurate capturing process and the missing possibility of a virtual mirror, which limits the presentation to a virtual avatar. By utilizing depth cameras like the Microsoft Kinect it becomes possible to track the movements of a body, extract body measurements and furthermore create a virtual mirror with the corresponding video stream. The video image can be merged with a piece of clothing frame by frame.<br />The project is implemented in Unity, a programming environment for 3D applications. OpenNI and the NITE middleware are used for various fundamental functions and for the tracking process in combination with the Microsoft Kinect.<br />Taking a closer look at the results, several 3D cloth models were created and textured. The pieces of garment are based on a female 3D model. The clothes are adapted to the body of the user in front of the Kinect during runtime. In addition, cloth physics are taking care of a realistic representation. Furthermore trying on clothes in front of different backgrounds and surroundings (e.g. at night) shall be possible. Also a lot of value is placed on the interaction aspects of the Virtual Dressing Room.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Presle, P. (2012). <i>A virtual dressing room based on depth data</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/160596</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philipp",
                    "last_name": "Presle",
                    "position": 1,
                    "role": "Author",
                    "tid": "49778"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "168596",
            "handle": "20.500.12708/160605",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Physics driven 3D dynamic geometry software for elementary education",
            "keywords": [
                "Dynamic Geometry Software",
                "DGS",
                "Spatial Ability"
            ],
            "abstract": "Spatial abilities are an essential part of human recognition and general intelligence. They represent our ability to perceive spatial patterns, mentally manipulate two- or three-dimensional objects and orient ourselves within our environment. They are also a key factor when it comes to creative thinking and problem solving, and they determine our efficiency in information retrieval. The latter, in turn, is increasingly relevant in modern times as it is closely linked to our ability to navigate large websites or handle complex user interfaces.<br />In consideration of the many ways spatial ability influences our everyday life, the work at hand presents a software intended to support its development in children. In order to achieve this goal, a virtual environment is combined with elements of geometry education, both of which have proven to be useful in improving spatial skills. Furthermore, a physics component is added to the mix, which leaves users with a virtual playground, in which they can construct complex scenes out of basic geometric objects.<br />The software has been developed in Unity 3D and uses a novel motion controller as input device, in order to increase the users' immersion into the virtual world. Embedded into an educational context, with specific instructions and work tasks, the application is believed to have beneficial effects on the development of spatial skills.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zeller, D. (2012). <i>Physics driven 3D dynamic geometry software for elementary education</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/160605</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "David",
                    "last_name": "Zeller",
                    "position": 1,
                    "role": "Author",
                    "tid": "46368"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "168858",
            "handle": "20.500.12708/160867",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Spracherkennung in mobilen Endgeräten",
            "keywords": [
                "automatic speech recognition",
                "mobile phones"
            ],
            "abstract": "In this work, a speech recognition system including a simple word-by-word translation system for mobile devices such as mobile phones and PDAs is planned and ultimately implemented on a target device. In the rst part the theoretical basis of speech production and speech recognition are examined. Subsequently prominent techniques used in speech recognition are presented. Furthermore the potential sources of interferences and restrictions in mobile applications are examined. In the second part the Java ME platform is presented, which serves as the basis for the implementation of the speech recognition system in practice. The development process is explained including the technology selection and the implementation. Optimization steps, problems and solutions in the practical work are illustrated. Finally, the experimental validation provides information and insights on the implemented system.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Morgenbesser, W. (2010). <i>Spracherkennung in mobilen Endgeräten</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/160867</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Wolfgang",
                    "last_name": "Morgenbesser",
                    "position": 1,
                    "role": "Author",
                    "tid": "44489"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "169364",
            "handle": "20.500.12708/161373",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automatische Erkennung von Gewaltszenen in Filmen",
            "keywords": [
                "film",
                "violence",
                "scenes",
                "pattern recognition",
                "multimodal information retrieval",
                "video",
                "audio"
            ],
            "abstract": "The news very often reports on the bad influence that violence in movies has on children and young adults. Frequently, censors are employed to identify and remove such content. This is a time consuming and expensive task and therefore an automatic process would be beneficial.<br />Software which solves this task in a fully automated fashion is not imaginable at this point in time because there are many different forms of violence and people generally disagree on whether a film scene is violent or not. Still, in this work a method is proposed to classify film scenes as either containing or not containing violent content in a computer assisted manner. For this task it is necessary to extract features that are characteristic for violent scenes. Then a classifier labels film scenes in violent and nonviolent scenes with respect to the chosen features.<br />The output of the algorithm are all scenes that contain violence. The research prototype was implemented in the Matlab environment. To realize this prototype several experiments with different retrieval methods, various features and multiple classifiers were made. The tested methods and features that gave the best results were incorporated in the prototype. First results of the prototype evaluation are encouraging.<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hörhan, M. (2010). <i>Automatische Erkennung von Gewaltszenen in Filmen</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/161373</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Hörhan",
                    "position": 1,
                    "role": "Author",
                    "tid": "40036"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "169538",
            "handle": "20.500.12708/161547",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Segmentation von Szenen in historischen Dokumentarfilmen",
            "keywords": [
                "scenes segmentation",
                "digital filmanalysis"
            ],
            "abstract": "The goal of scene segmentation is to split a movie into separate units of action. Current algorithms use features that are not available in historical artistic documentaries either due to film composition or technical aspects. This master thesis introduces a new method that is specifically developed for historical artistic documentaries. The first step of scene segmentation is the detection of shot boundaries. For each shot that is found a keyframe is selected. The keyframes are compared by image features. Two shots that lie in-between a certain time span and exceed a limit of similarity belong, as all the shots that lie between these two shots, to the same scene. The features the algorithm uses to compare the keyframes are SIFT Keypoints, the Edge Change Ratio and block-based histograms.<br />Core scenes are the result of the previous step - the identification of similar shots. Between the core scenes a few shots exist that cannot be assigned. These loose areas are classified by recursively decreasing the similarity thresholds for the respective features. Another part of the thesis is devoted to the evaluation of the features with differing thresholds for a quality assessment of the features. The quality of a feature is defined by a weighted ratio between correctly and falsely classified similar shots. Furthermore, the influence of the keyframe selection strategy is investigated.<br />The implementation of the method is tested with historical artistic documentaries and modern movies. This allows for evaluating the implementation's ability to solve the given problem of scene segmentation in historical artistic documentaries as well as in modern movies. Finally, the proposed method is compared with other state-of-the-art techniques.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hartlieb, S. (2010). <i>Segmentation von Szenen in historischen Dokumentarfilmen</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/161547</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Stefan",
                    "last_name": "Hartlieb",
                    "position": 1,
                    "role": "Author",
                    "tid": "51711"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "169692",
            "handle": "20.500.12708/161689",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Film Analysis of Archive Documentaries",
            "keywords": [],
            "abstract": "Film experts, archives and museums state highly demanding requirements for automated film analysis. The experimental style of archive documentaries and the state of the film material challenge conventional content-based retrieval techniques. In this article, we outline issues in the context of automated film analysis for archive documentaries. We explore examples of the applicability of existing content-based retrieval methods and provide insights into both limitations and potentials. Performed experiments show that the discussed methods bear the potential to improve the understanding, handling, and accessing of archive film material.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaharieva, M., Mitrovic, D., Zeppelzauer, M., &#38; Breiteneder, C. (2011). Film Analysis of Archive Documentaries. <i>IEEE MultiMedia</i>, <i>18</i>(2), 38–47. https://doi.org/10.1109/mmul.2010.67</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 2,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 3,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170517",
            "handle": "20.500.12708/162512",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Multi-scale 2D tracking of articulated objects using hierarchical spring systems",
            "keywords": [],
            "abstract": "This paper presents a flexible framework to build a target-specific, part-based representation for arbitrary articulated or rigid objects. The aim is to successfully track the target object in 2D, through multiple scales and occlusions. This is realized by employing a hierarchical, iterative optimization process on the proposed representation of structure and appearance. Therefore, each rigid part of an object is described by a hierarchical spring system represented by an attributed graph pyramid. Hierarchical spring systems encode the spatial relationships of the features (attributes of the graph pyramid) describing the parts and enforce them by spring-like behavior during tracking. Articulation points connecting the parts of the object allow to transfer position information from reliable to ambiguous parts. Tracking is done in an iterative process by combining the hypotheses of simple trackers with the hypotheses extracted from the hierarchical spring systems.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N. M., Ion, A., &#38; Kropatsch, W. G. (2011). Multi-scale 2D tracking of articulated objects using hierarchical spring systems. <i>Pattern Recognition</i>, <i>44</i>(4), 800–810. https://doi.org/10.1016/j.patcog.2010.10.025</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole M.",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170720",
            "handle": "20.500.12708/162715",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": true,
            "title": "Hierarchical spatio-temporal extraction of models for moving rigid parts",
            "keywords": [],
            "abstract": "This paper presents a method to extract a previous termpartnext term-based previous termmodelnext term of an observed scene from a video sequence. Independent motion is a strong cue that two points belong to different \"previous termrigidnext term\" entities. Conversely, things that move together throughout the whole video belong together and define a \"previous termrigidnext term\" object or previous termpartnext term. Successfully tracked features indicate trajectories of salient points in the scene. A triangulated graph connects the salient points and encodes their local neighborhood in the first frame. The length variation of the triangle edges is used to label them as relevant (on an object) or separating (connecting different objects). A following grouping process uses the motion of the triangles marked as relevant as a cue to identify the \"previous termrigidnext term\" previous termpartsnext term of the foreground or the background. The choice of the motion-based grouping criterion depends on the type of motion: in the image plane or out of the image plane. The result is a previous termhierarchicalnext term description (graph pyramid) of the scene, where each vertex in the top level of the pyramid represents a \"previous termrigidnext term\" previous termpartnext term of the foreground or the background, and encloses to the salient features used to describe it. Promising experimental results show the potential of the approach.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N. M., Ion, A., &#38; Kropatsch, W. G. (2011). Hierarchical spatio-temporal extraction of models for moving rigid parts. <i>Pattern Recognition Letters</i>, <i>32</i>(16), 2239–2249. https://doi.org/10.1016/j.patrec.2011.05.005</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole M.",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170723",
            "handle": "20.500.12708/162718",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": true,
            "title": "Matching 2D and 3D articulated shapes using the eccentricity transform",
            "keywords": [],
            "abstract": "This paper presents a novel method for 2D and 3D shape matching that is insensitive to articulation. It uses the eccentricity transform, which is based on the computation of geodesic distances. Geodesic distances computed over a 2D or 3D shape are articulation insensitive. The eccentricity transform considers the length of the longest geodesics. Histograms of the eccentricity transform characterize the compactness of a shape, in a way insensitive to rotation, scaling, and articulation. To characterize the structure of a shape, a histogram of the connected components of the level-sets of the transform is used. These two histograms make up a highly compact descriptor and the resulting method for shape matching is straightforward. Experimental results on established 2D and 3D benchmarks show results similar to more complex state of the art methods, especially when considering articulation. The connection between the geometrical modification of a shape and the corresponding impact on its histogram representation is explained. The influence of the number of bins in the two histograms and the respective importance of each histogram is studied in detail.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Artner, N. M., Peyré, G., Kropatsch, W. G., &#38; Cohen, L. D. (2011). Matching 2D and 3D articulated shapes using the eccentricity transform. <i>Computer Vision and Image Understanding</i>, <i>115</i>(6), 817–834. https://doi.org/10.1016/j.cviu.2011.02.006</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Nicole M.",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Gabriel",
                    "last_name": "Peyré",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Laurent D.",
                    "last_name": "Cohen",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170745",
            "handle": "20.500.12708/162740",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": true,
            "title": "Convex Deficiencies for Human Action Recognition",
            "keywords": [],
            "abstract": "A human action can be identified by visualizing the sequence of 2D binary projections over time. Here, one of the most representative features is shape and a wide range of algorithms have been proposed using its descriptors. This paper proposes convex deficiencies, the difference between an object and its convex hull, to be considered as a representation for the human action classification problem. A simple description using the centroids of the convex deficiencies over time is presented. Recognition of human actions is done with a fast matching algorithm that considers the spatial distribution of the centroid trajectories and the shape of the clusters in its 2D projection. The proposed representation is robust to deformations, scale, speed of the performed action and to the starting point of the movement sequence. Experiments using the videos of the Weizmann database show promising results demonstrating the effectiveness of the proposed methodology in classifying simple human actions, e.g. walking and running. The new proposed methodology should be extendable to a broader set of actions.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Iglesias-Ham, M., Bartolo García-Reyes, E., Kropatsch, W., &#38; Artner, N. (2011). Convex Deficiencies for Human Action Recognition. <i>Journal of Intelligent and Robotic Systems</i>, <i>64</i>(3), 353–364. http://hdl.handle.net/20.500.12708/162740</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mabel",
                    "last_name": "Iglesias-Ham",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Edel",
                    "last_name": "Bartolo García-Reyes",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 4,
                    "role": "Author",
                    "tid": "37618"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170748",
            "handle": "20.500.12708/162743",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": true,
            "title": "A new algorithm for computing the 2-dimensional matching distance between size functions",
            "keywords": [],
            "abstract": "Size Theory has proven to be a useful geometrical/topological approach to shape comparison. Originally introduced by considering 1-dimensional properties of shapes, described by means of real-valued functions, it has recently been generalized to taking into account multi-dimensional properties coded by functions valued in R^k. This has led to the introduction of a shape descriptor called k-dimensional size function, and the k-dimensional matching distance to compare size functions. This paper presents new theoretical results about the 2-dimensional matching distance, leading to the formulation of an algorithm for its approximation up to an arbitrary error threshold. Experiments on 3D object comparison are shown to discuss the efficacy and effectiveness of the algorithm.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Biasotti, S., Cerri, A., Frosini, P., &#38; Giorgi, D. (2011). A new algorithm for computing the 2-dimensional matching distance between size functions. <i>Pattern Recognition Letters</i>, <i>32</i>(14), 1735–1746. https://doi.org/10.1016/j.patrec.2011.07.014</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Silvia",
                    "last_name": "Biasotti",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Andrea",
                    "last_name": "Cerri",
                    "position": 2,
                    "role": "Author",
                    "tid": "233419"
                },
                {
                    "first_name": "Patrizio",
                    "last_name": "Frosini",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Daniela",
                    "last_name": "Giorgi",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170751",
            "handle": "20.500.12708/162746",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "On certain optimal diffeomorphisms between closed curves",
            "keywords": [],
            "abstract": "Abstract: The concept of natural pseudo-distance has proven to be a powerful tool for measuring the dissimilarity between shape properties of topological spaces, modeled as continuous real-valued functions defined on the spaces themselves. Roughly speaking, the natural pseudo-distance is defined as the infimum of the change of the functions' values, when moving from one space to the other through homeomorphisms, if possible. In this paper, we prove the first available result about the existence of optimal homeomorphisms between closed curves,\r\ni.e. inducing a change of the function that equals the natural pseudo-distance. Moreover, we show that, under our assumptions, this optimal homeomorphism is actually a diffeomorphism.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Cerri, A., &#38; Di Fabio, B. (2011). On certain optimal diffeomorphisms between closed curves. <i>Forum Mathematicum</i>, <i>26</i>(6). https://doi.org/10.1515/form.2011.172</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Andrea",
                    "last_name": "Cerri",
                    "position": 1,
                    "role": "Author",
                    "tid": "233419"
                },
                {
                    "first_name": "Barbara",
                    "last_name": "Di Fabio",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170783",
            "handle": "20.500.12708/162778",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": true,
            "title": "Hardness results for homology localization",
            "keywords": [],
            "abstract": "We address the problem of localizing homology classes, namely, finding\r\nthe cycle representing a given class with the most concise geometric measure. We\r\nstudy the problem with different measures: volume, diameter and radius.\r\nFor volume, that is, the 1-norm of a cycle, two main results are presented. First,\r\nwe prove that the problem is NP-hard to approximate within any constant factor.\r\nSecond, we prove that for homology of dimension two or higher, the problem is\r\nNP-hard to approximate even when the Betti number is O(1). The latter result leads\r\nto the inapproximability of the problem of computing the nonbounding cycle with\r\nthe smallest volume and computing cycles representing a homology basis with the\r\nminimal total volume.\r\nAs for the other two measures defined by pairwise geodesic distance, diameter\r\nand radius, we show that the localization problem is NP-hard for diameter but is\r\npolynomial for radius.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Chen, C., &#38; Freedman, D. (2011). Hardness results for homology localization. <i>Discrete &#38; Computational Geometry</i>, <i>45</i>(3), 425–448. http://hdl.handle.net/20.500.12708/162778</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Chao",
                    "last_name": "Chen",
                    "position": 1,
                    "role": "Author",
                    "tid": "179282"
                },
                {
                    "first_name": "Daniel",
                    "last_name": "Freedman",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170785",
            "handle": "20.500.12708/162780",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": true,
            "title": "Invariant representative cocycles of cohomology generators using irregular graph pyramids",
            "keywords": [],
            "abstract": "Structural pattern recognition describes and classifies data based on the relationships of features and\r\nparts. Topological invariants, like the Euler number, characterize the structure of objects of any dimension.\r\nCohomology can provide more refined algebraic invariants to a topological space than does homology.\r\nIt assigns 'quantities' to the chains used in homology to characterize holes of any dimension. Graph\r\npyramids can be used to describe subdivisions of the same object at multiple levels of detail. This paper\r\npresents cohomology in the context of structural pattern recognition and introduces an algorithm to efficiently\r\ncompute representative cocycles (the basic elements of cohomology) in 2D using a graph pyramid.\r\nAn extension to obtain scanning and rotation invariant cocycles is given.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gonzalez-Diaz, R., Ion, A., Iglesias-Ham, M., &#38; Kropatsch, W. G. (2011). Invariant representative cocycles of cohomology generators using irregular graph pyramids. <i>Computer Vision and Image Understanding</i>, <i>115</i>(7), 1011–1022. https://doi.org/10.1016/j.cviu.2010.12.009</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rocio",
                    "last_name": "Gonzalez-Diaz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Mabel",
                    "last_name": "Iglesias-Ham",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170809",
            "handle": "20.500.12708/162804",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Searching high order invariants in Computer Imagery",
            "keywords": [],
            "abstract": "In this paper, we present a direct computational application of Ho-\r\nmological Perturbation Theory (HPT, for short) to Computer Imagery. More\r\nprecisely, the formulas of the A1{coalgebra maps 2 and 3 using the notion\r\nof AT-model of a digital image, and the HPT technique are implemented. The\r\nmethod has been tested on some speci c examples, showing the usefulness of\r\nthis computational tool for distinguishing 3D digital images.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Berciano, A., Molina-Abril, H., &#38; Real, P. (2012). Searching high order invariants in Computer Imagery. <i>Applicable Algebra in Engineering, Communication and Computing</i>, <i>23</i>(1–2), 17–28. https://doi.org/10.1007/s00200-012-0169-5</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "A.",
                    "last_name": "Berciano",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "H.",
                    "last_name": "Molina-Abril",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "P.",
                    "last_name": "Real",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170830",
            "handle": "20.500.12708/162825",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "On the search of optimal reconstruction resolution",
            "keywords": [],
            "abstract": "In this paper we present a novel algorithm to optimize the reconstruction from non-uniform point sets.\r\nWe introduce a statistically-derived topology-controller for selecting the reconstruction resolution of a\r\ngiven non-uniform point set. Deriving information from homology-based statistics, our topology-controller\r\nensures a stable and sound basis for the analysis process. By analyzing our topology-controller, we\r\nselect an optimal reconstruction resolution which ensures both low reconstruction errors and a topological\r\nstability of the underlying signal. Our approach offers a valuable method for the evaluation of the\r\nreconstruction process without the need of visual inspection of the reconstructed datasets. By means\r\nof qualitative results we show how our proposed topology statistics provides complementary information\r\nin the enhancement of existing reconstruction pipelines in visualization.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vuçini, E., &#38; Kropatsch, W. G. (2012). On the search of optimal reconstruction resolution. <i>Pattern Recognition Letters</i>, <i>33</i>(11), 1460–1467. https://doi.org/10.1016/j.patrec.2011.10.006</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Erald",
                    "last_name": "Vuçini",
                    "position": 1,
                    "role": "Author",
                    "tid": "45707"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170843",
            "handle": "20.500.12708/162838",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Homological optimality in Discrete Morse theory through chain homotopies",
            "keywords": [],
            "abstract": "Morse theory is a fundamental tool for analyzing the geometry and topology\r\nof smooth manifolds. This tool was translated by Forman to discrete\r\nstructures such as cell complexes, by using discrete Morse functions or equivalently\r\ngradient vector fields. Once a discrete gradient vector field has been\r\ndefined on a finite cell complex, information about its homology can be directly\r\ndeduced from it. In this paper we introduce the foundations of a\r\nhomology-based heuristic for finding optimal discrete gradient vector fields\r\non a general finite cell complex K. The method is based on a computational\r\nhomological algebra representation (called homological spanning forest or\r\nHSF, for short) that is an useful framework to design fast and efficient algorithms\r\nfor computing advanced algebraic-topological information (classification\r\nof cycles, cohomology algebra, homology A(∞)-coalgebra, cohomology\r\noperations, homotopy groups,. . . ). Our approach is to consider the optimality\r\nproblem as a homology computation process for a chain complex endowed\r\nwith an extra chain homotopy operator.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Molina-Abril, H., &#38; Real, P. (2012). Homological optimality in Discrete Morse theory through chain homotopies. <i>Pattern Recognition Letters</i>, <i>33</i>(11), 1501–1506. https://doi.org/10.1016/j.patrec.2012.01.014</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Helena",
                    "last_name": "Molina-Abril",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Real",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "170844",
            "handle": "20.500.12708/162839",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Homological Spanning Forest framework for 2D image processing",
            "keywords": [],
            "abstract": "A 2D topology-based digital image processing framework is presented\r\nhere. This framework consists of the computation of a flexible geometric graphbased\r\nstructure, starting from a raster representation of a digital image I. This\r\nstructure is called Homological Spanning Forest (HSF for short), and it is built\r\non a cell complex associated to I. The HSF framework allows an efficient and\r\naccurate topological analysis of regions of interest (ROIs) by using a four-level\r\narchitecture. By topological analysis, we mean not only the computation of Euler\r\ncharacteristic, genus or Betti numbers, but also advanced computational algebraic\r\ntopological information derived from homological classification of cycles. An initial\r\nHSF representation can be modified to obtain a different one, in which ROIs are\r\nalmost isolated and ready to be topologically analyzed. The HSF framework is\r\nsusceptible of being parallelized and generalized to higher dimensions.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Molina-Abril, H., &#38; Real, P. (2012). Homological Spanning Forest framework for 2D image processing. <i>Annals of Mathematics and Artificial Intelligence</i>, <i>64</i>(4), 385–409. https://doi.org/10.1007/s10472-012-9297-7</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Helena",
                    "last_name": "Molina-Abril",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Real",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "170929",
            "handle": "20.500.12708/162924",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Retrieval of motion composition in film",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zeppelzauer, M., Zaharieva, M., Mitrović, D., &#38; Breiteneder, C. (2011). Retrieval of motion composition in film. <i>Digital Creativity</i>, <i>22</i>(4), 219–234. https://doi.org/10.1080/14626268.2011.622282</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 2,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrović",
                    "position": 3,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "170938",
            "handle": "20.500.12708/162933",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Gradual transition detection in historic film material-a systematic study",
            "keywords": [],
            "abstract": "The segmentation of films and videos into shots requires the detection of gradual transitions such as dissolves and fades. There are two types of approaches: unified approaches, that is, one detector for all gradual transition types, and approaches that use specialized detectors for each gradual transition type. We present an overview on existing methods and extend an existing unified approach for the detection of gradual transitions in historic material. In an experimental study, we evaluate the proposed approach on complex and low-quality historic material as well as on contemporary material from the TRECVid evaluation. Additionally, we investigate different features, feature combinations, and fusion strategies. We observe that the historic material requires the use of texture features, in contrast to the contemporary material that, in most of the cases, requires the use of color and luminance features.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Seidl, M., Zeppelzauer, M., Mitrovic, D., &#38; Breiteneder, C. (2011). Gradual transition detection in historic film material-a systematic study. <i>Journal on Computing and Cultural Heritage</i>, <i>4</i>(3). http://hdl.handle.net/20.500.12708/162933</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Seidl",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 3,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "171979",
            "handle": "20.500.12708/163975",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "ARTiFICe - Augmented Reality Framework for Distributed Collaboration",
            "keywords": [],
            "abstract": "This paper introduces a flexible and powerful software framework \r\nbased on an off the shelf game engine which is used to develop \r\ndistributed and collaborative virtual and augmented reality \r\napplications. We describe ARTiFICe's flexible design and \r\nimplementation and demonstrate its use in research and teaching \r\nwhere 97 students in 2 lab courses developed AR applications\r\nwith it. Applications are presented on mobile, desktop and \r\nimmersive systems using low cost 6-DOF input devices \r\n(Microsoft Kinect, Razer Hydra, SpaceNavigator), that we \r\nintegrated into our framework.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., Schönauer, C., Gerstweiler, G., &#38; Kaufmann, H. (2012). ARTiFICe - Augmented Reality Framework for Distributed Collaboration. <i>The International Journal of Virtual Reality</i>, <i>11</i>(3), 1–7. http://hdl.handle.net/20.500.12708/163975</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 3,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "172195",
            "handle": "20.500.12708/164191",
            "doi": null,
            "year": 2012,
            "issued": "2012-08",
            "issued_on": "2012-08-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evolution of the 3-Dimensional Video System for Facial Motion Analysis: Ten Years' Experiences and Recent Developments",
            "keywords": [],
            "abstract": "Since the implementation of the computer-aided system for assessing\r\nfacial palsy in 1999 by Frey et al (Plast Reconstr Surg. 1999;104:2032Y2039),\r\nno similar system that can make an objective, three-dimensional, quantitative\r\nanalysis of facial movements has been marketed.\r\nThis system has been in routine use since its launch, and it has proven\r\nto be reliable, clinically applicable, and therapeutically accurate. With the cooperation\r\nof international partners, more than 200 patients were analyzed. Recent\r\ndevelopments in computer visionVmostly in the area of generative face\r\nmodels, applying active-appearance models (and extensions), optical flow,\r\nand video-trackingVhave been successfully incorporated to automate the prototype\r\nsystem.\r\nFurther market-ready development and a business partner will be needed\r\nto enable the production of this system to enhance clinical methodology in diagnostic\r\nand prognostic accuracy as a personalized therapy concept, leading to\r\nbetter results and higher quality of life for patients with impaired facial function.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Tzou, C.-H. J., Pona, I., Placheta, E., Hold, A., Michaelidou, M., Artner, N., Kropatsch, W., Gerber, H., &#38; Frey, M. (2012). Evolution of the 3-Dimensional Video System for Facial Motion Analysis: Ten Years’ Experiences and Recent Developments. <i>Annals of Plastic Surgery</i>, <i>69</i>(2), 173–185. https://doi.org/10.1097/sap.0b013e3182223d96</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Chieh-Han John",
                    "last_name": "Tzou",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Igor",
                    "last_name": "Pona",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Eva",
                    "last_name": "Placheta",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Alina",
                    "last_name": "Hold",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Maria",
                    "last_name": "Michaelidou",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 6,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 7,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Hans",
                    "last_name": "Gerber",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Manfred",
                    "last_name": "Frey",
                    "position": 9,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "172966",
            "handle": "20.500.12708/164963",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Archive Film Material - A novel Challenge for Automated Film Analysis",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zeppelzauer, M., Mitrovic, D., &#38; Breiteneder, C. (2012). Archive Film Material - A novel Challenge for Automated Film Analysis. <i>The Frames Cinema Journal</i>, <i>1</i>(1). http://hdl.handle.net/20.500.12708/164963</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 2,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 3,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "173300",
            "handle": "20.500.12708/165299",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Approximative Graph Pyramid Solution of the E-TSP",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y., Kropatsch, W., Pizlo, Z., &#38; Ion, A. (2009). Approximative Graph Pyramid Solution of the E-TSP. <i>Image and Vision Computing</i>, <i>27</i>(7), 887–896. http://hdl.handle.net/20.500.12708/165299</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Zygmunt",
                    "last_name": "Pizlo",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "173644",
            "handle": "20.500.12708/165643",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "PlayMancer: Games for Health with Accessibility in Mind",
            "keywords": [],
            "abstract": "The term Serious Games has been used to describe computer and video\r\ngames used as educational technology or as a vehicle for presenting or promoting a point\r\nof view. Serious games can be of any genre and many of them can be considered a kind\r\nof edutainment. Serious games are intended to provide an engaging, self-reinforcing\r\ncontext in which to motivate and educate the players towards knowledgeable processes,\r\nincluding business operations, training, marketing and advertisement. Serious games can\r\nbe compelling, educative, provocative, disruptive and inspirational. The potential of games\r\nfor entertainment and learning has been demonstrated thoroughly from both research and\r\nmarket. Unfortunately, the investments committed to entertainment dwarf what is\r\ncommitted for more serious purposes. In this feature, we will argue that the motives,\r\nincentives and expectations of the computer game industry differ from one cultural and\r\neconomic environment to another. As the game industry is dominated by US companies,\r\ncomputer game products are targeting user groups mostly informed by the marketing\r\ndepartments of those companies. This process creates marginalised user groups and\r\ngame types that are not addressed effectively by the computer game market. Accessible\r\ngames and games for health comprise this underdeveloped niche. Research project\r\nPlayMancer is a multi-partner effort to tackle both of those issues in a coherent way.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kalapanidas, E., Davarakis, C., Fernandez Aranda, F., Jiménez-Murcia, S., Kocsis, O., Ganchev, T., Kaufmann, H., Lam, T., &#38; Konstantas, D. (2009). PlayMancer: Games for Health with Accessibility in Mind. <i>Communications &#38;amp; Strategies</i>, <i>73</i>, 105–120. http://hdl.handle.net/20.500.12708/165643</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Elias",
                    "last_name": "Kalapanidas",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Costas",
                    "last_name": "Davarakis",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Fernando",
                    "last_name": "Fernandez Aranda",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Susanna",
                    "last_name": "Jiménez-Murcia",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Otilia",
                    "last_name": "Kocsis",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Todor",
                    "last_name": "Ganchev",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 7,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Tony",
                    "last_name": "Lam",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Dimitri",
                    "last_name": "Konstantas",
                    "position": 9,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "173645",
            "handle": "20.500.12708/165644",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Dynamic Differential Geometry in Education",
            "keywords": [],
            "abstract": "We present an augmented reality application which introduces differential\r\ngeometry in educational dynamic geometry software. New functionality\r\nsuch as a Frenet frame, center and circle of curvature in arbitrary curve points,\r\nand others were implemented. Dynamic geometry allows to study di erential geometric\r\nproperties under movement. Using this tool we developed examples which\r\nenable teachers and learners to intuitively explore properties of interesting curves,\r\nto visualize contact of higher order between curves and surfaces, to construct\r\nMeusnier's sphere, Dupin's indicatrix and more.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2009). Dynamic Differential Geometry in Education. <i>Journal for Geometry and Graphics</i>, <i>13</i>(2). http://hdl.handle.net/20.500.12708/165644</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "174250",
            "handle": "20.500.12708/166247",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Morphological segmentation on learned boundaries",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A., &#38; Marcotegui, B. (2009). Morphological segmentation on learned boundaries. <i>Image and Vision Computing</i>, <i>27</i>(4), 480–488. http://hdl.handle.net/20.500.12708/166247</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Beatriz",
                    "last_name": "Marcotegui",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "174256",
            "handle": "20.500.12708/166253",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Affine-invariant contours recognition using an incremental hybrid learning approach",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Bandera, A., Marfill, R., &#38; Antunez, E. (2009). Affine-invariant contours recognition using an incremental hybrid learning approach. <i>Pattern Recognition Letters</i>, <i>30</i>(14), 1310–1320. http://hdl.handle.net/20.500.12708/166253</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Antonio",
                    "last_name": "Bandera",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Rebeca",
                    "last_name": "Marfill",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Esther",
                    "last_name": "Antunez",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "174354",
            "handle": "20.500.12708/166351",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Image-Based Retrieval and Identification of Ancient Coins",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kampel, M., Huber-Mörk, R., &#38; Zaharieva, M. (2009). Image-Based Retrieval and Identification of Ancient Coins. <i>IEEE Intelligent Systems</i>, <i>24</i>(2), 26–34. http://hdl.handle.net/20.500.12708/166351</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 1,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Reinhold",
                    "last_name": "Huber-Mörk",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 3,
                    "role": "Author",
                    "tid": "39017"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "17445",
            "handle": "20.500.12708/16782",
            "doi": "10.34726/hss.2021.76463",
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Recommending reviewers for theses using artificial intelligence",
            "keywords": [
                "Natural Language Processing",
                "Deep Learning",
                "Information Retrieval"
            ],
            "abstract": "Peer review in the area of scientific contributions, such as publishing papers to conferences,plays a crucial role to evaluate the integrity and correctness of the respective work. This is also the case for the process of obtaining a Master’s degree at TU Wien, where students must defend their thesis in front of an examination board. The examination board consists of the supervisor and two additional reviewers, which must be selected manually as part of the process. The manual selection of those reviewers can be prone to humanmisjudgment, causing a faulty evaluation during the examination.In this thesis, we aim to develop a recommendation engine driven by state-of-the-artmethodology in the area of artificial intelligence. This process is done in three steps:Firstly, we extract necessary data from TU Wien internal databases. Then, we define aset of different deep learning architectures and train them on our data set. The presented models are inspired by and incorporate the architectures of LSTMs, Autoencoder and Siamese Neural Networks. Each model is trained based on three text embedding modules:BERT, GPT2 and XLNet. Finally, we evaluate the models on the task of ranking potentialr esearch profiles given a thesis as input and compare it with BM25, an established stateof-the-art baseline. Furthermore, a manual evaluation for selected use cases is performed to further validate their respective performances. We conclude that models based ona Siamese Neural Network architecture achieve promising results and, in the setting of neural re-ranking, even out perform BM25. Based on the conducted experiments,we also observe that BERT as embedding module results in the best scores across all architectures. Finally, we come to the conclusion that the matching and selection process of reviewers can be optimised using the above presented state-of-the-art deep learning methods.xi",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Penz, D. (2021). <i>Recommending reviewers for theses using artificial intelligence</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2021.76463</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "69710",
                    "name": "Penz David - 2021 - Recommending reviewers for theses using artificial...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2167317,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16782/1/Penz%20David%20-%202021%20-%20Recommending%20reviewers%20for%20theses%20using%20artificial...pdf"
                },
                {
                    "bsid": "72826",
                    "name": "Penz David - 2021 - Recommending Reviewers for Theses using Artificial...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 155587,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16782/4/Penz%20David%20-%202021%20-%20Recommending%20Reviewers%20for%20Theses%20using%20Artificial...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "David",
                    "last_name": "Penz",
                    "position": 1,
                    "role": "Author",
                    "tid": "299191"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "175052",
            "handle": "20.500.12708/167049",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Virtual Environments for Mathematics and Geometry Education",
            "keywords": [],
            "abstract": "Since ancient times mathematicians and geometricians use visualisations to describe, discuss, to study\r\nand to teach mathematics. In mathematics education visualisations are still used whenever possible to\r\nsupport teaching, to inspire students and feed their need to actually see abstract mathematical facts. In\r\nour times virtual reality presents a fascinating, extremely motivating new tool in teachers' hands which\r\nallows students to see mathematics in three dimensions.\r\nThis chapter gives an overview of various, mainly immersive, virtual environments that have been\r\ndeveloped in the previous 10 years to support mathematics and geometry education. The focus lies on\r\none advanced application for geometry education that has been used and evaluated with over 500\r\nstudents throughout the years. Findings and teaching experiences are described and discussed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2009). Virtual Environments for Mathematics and Geometry Education. <i>Themes in Science and Technology Education</i>, <i>SPECIAL ISSUE: VIRTUAL REALITY IN EDUCATION, VOL. 2</i>(1–2), 131–152. http://hdl.handle.net/20.500.12708/167049</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "175252",
            "handle": "20.500.12708/167249",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Archive Film Comparison",
            "keywords": [],
            "abstract": "In this paper, the authors present an approach for video comparison, in which an instantiated framework allows for the easy comparison of different methods that are required at each step of the comparison process. The authors' approach is evaluated based on a real world scenario of challenging video data of archive documentaries. In this paper, the performed experiments aim at the evaluation of the performance of established shot boundary detection algorithms, the influence of keyframe selection, and feature representation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaharieva, M., Zeppelzauer, M., Mitrovic, D., &#38; Breiteneder, C. (2010). Archive Film Comparison. <i>International Journal of Multimedia Data Engineering and Management</i>, <i>1</i>(3), 41–56. http://hdl.handle.net/20.500.12708/167249</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 3,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "175453",
            "handle": "20.500.12708/167450",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "New approach to the perception of 3D shape based on veridicality, complexity, symmetry and volume",
            "keywords": [],
            "abstract": "This paper reviews recent progress towards understanding 3D shape perception made possible by appreciating\r\nthe significant role that veridicality and complexity play in the natural visual environment. The\r\nability to see objects as they really are ''out there\" is derived from the complexity inherent in the 3D\r\nobject's shape. The importance of both veridicality and complexity was ignored in most prior research.\r\nAppreciating their importance made it possible to devise a computational model that recovers the 3D\r\nshape of an object from only one of its 2D images. This model uses a simplicity principle consisting of\r\nonly four a priori constraints representing properties of 3D shapes, primarily their symmetry and volume.\r\nThe model recovers 3D shapes from a single 2D image as well, and sometimes even better, than a human\r\nbeing. In the rare recoveries in which errors are observed, the errors made by the model and human subjects\r\nare very similar. The model makes no use of depth, surfaces or learning. Recent elaborations of this\r\nmodel include: (i) the recovery of the shapes of natural objects, including human and animal bodies with\r\nlimbs in varying positions (ii) providing the model with two input images that allowed it to achieve virtually\r\nperfect shape constancy from almost all viewing directions. The review concludes with a comparison\r\nof some of the highlights of our novel, successful approach to the recovery of 3D shape from a 2D\r\nimage with prior, less successful approaches.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pizlo, Z., Sawada, T., Li, Y., Kropatsch, W., &#38; Steinman, R. (2010). New approach to the perception of 3D shape based on veridicality, complexity, symmetry and volume. <i>Vision Research</i>, <i>50</i>, 1–11. http://hdl.handle.net/20.500.12708/167450</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Zygmunt",
                    "last_name": "Pizlo",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Tadamasa",
                    "last_name": "Sawada",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Yunfeng",
                    "last_name": "Li",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Steinman",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "175458",
            "handle": "20.500.12708/167455",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Robust and efficient analysis of signals and images",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., &#38; Ruiz-Shulcloper, J. (2010). Robust and efficient analysis of signals and images. <i>Pattern Recognition Letters</i>, <i>31</i>(6), 445–446. http://hdl.handle.net/20.500.12708/167455</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Jose",
                    "last_name": "Ruiz-Shulcloper",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "175566",
            "handle": "20.500.12708/167563",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Multi-scale 2D tracking of articulated objects using hierarchical spring systems",
            "keywords": [],
            "abstract": "This paper presents a flexible framework to build a target-specific, part-based representation for arbitrary\r\n\r\narticulated or rigid objects. The aim is to successfully track the target object in 2D, through multiple scales\r\n\r\nand occlusions. This is realized by employing a hierarchical, iterative optimization process on the\r\n\r\nproposed representation of structure and appearance. Therefore, each rigid part of an object is described\r\n\r\nby a hierarchical spring system represented by an attributed graph pyramid. Hierarchical spring systems\r\n\r\nencode the spatial relationships of the features (attributes of the graph pyramid) describing the parts and\r\n\r\nenforce them by spring-like behavior during tracking. Articulation points connecting the parts of the\r\n\r\nobject allow to transfer position information from reliable to ambiguous parts. Tracking is done in an\r\n\r\niterative process by combining the hypotheses of simple trackers with the hypotheses extracted from the\r\n\r\nhierarchical spring systems.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., Ion, A., &#38; Kropatsch, W. (2010). Multi-scale 2D tracking of articulated objects using hierarchical spring systems. <i>Pattern Recognition</i>, <i>44</i>(4), 800–810. http://hdl.handle.net/20.500.12708/167563</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "176295",
            "handle": "20.500.12708/168290",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Use of strategy in a 3-dimensional spatial ability test",
            "keywords": [],
            "abstract": "Use of strategy was investigated using a new spatial test in which items are presented in three-dimensional space and solutions are actively constructed rather than selected from alternatives. As the final test also comprises a training module, the focus of a first evaluation study was on the strategies participants use and their relationship to performance. Participants were interviewed after completing the test. The number of strategies reported and two specific strategies were significantly correlated to the test score. Implications of the findings for strategy assessment and test design are discussed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Strasser, I., Koller, I., Strauß, S., Csisinko, M., Kaufmann, H., &#38; Glück, J. (2010). Use of strategy in a 3-dimensional spatial ability test. <i>Journal of Individual Differences</i>, <i>31</i>(2), 74–77. https://doi.org/10.1027/1614-0001/a000013</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Irene",
                    "last_name": "Strasser",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Ingrid",
                    "last_name": "Koller",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Sabine",
                    "last_name": "Strauß",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 4,
                    "role": "Author",
                    "tid": "47153"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 5,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Judith",
                    "last_name": "Glück",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-03"
            ],
            "pid": "176305",
            "handle": "20.500.12708/168300",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "On Visualization and Reconstruction from Non-Uniform Point Sets using B-splines",
            "keywords": [],
            "abstract": "In this paper we present a novel framework for the visualization and reconstruction from non-uniform point sets. We adopt a variational method for the reconstruction of 3D non-uniform data to a uniform grid of chosen resolution. We will extend this reconstruction to an efficient multi-resolution uniform representation of the underlying data. Our multi-resolution representation includes a traditional bottom-up multi-resolution approach and a novel top-down hierarchy for adaptive hierarchical reconstruction. Using a hybrid regularization functional we can improve the reconstruction results. Finally, we discuss further application scenarios and show rendering results to emphasize the effectiveness and quality of our proposed framework. By means of qualitative results and error comparisons we demonstrate superiority of our method compared to competing methods",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vuçini, E., Möller, T., &#38; Gröller, M. E. (2009). On Visualization and Reconstruction from Non-Uniform Point Sets using B-splines. <i>Computer Graphics Forum</i>, <i>28</i>(3), 1007–1014. https://doi.org/10.1111/j.1467-8659.2009.01447.x</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Erald",
                    "last_name": "Vuçini",
                    "position": 1,
                    "role": "Author",
                    "tid": "45707"
                },
                {
                    "first_name": "Torsten",
                    "last_name": "Möller",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "M. Eduard",
                    "last_name": "Gröller",
                    "position": 3,
                    "role": "Author",
                    "tid": "143572"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "1775",
            "handle": "20.500.12708/1791",
            "doi": "10.34726/hss.2018.43827",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Replication in loosely coupled systems : simulation and evaluation of replication strategies for their use in loosely coupled IT systems",
            "keywords": [
                "Replication",
                "Messaging",
                "Distributed Systems"
            ],
            "abstract": "Modern IT systems are often implemented as loosely coupled systems, consisting of independently developed and deployed components. These systems require a high degree of communication within and can be optimized by the replication of data. Actual research on replication mainly focuses on the data layer, which leads to tight coupling and is therefore not suitable for loosely coupled systems. This thesis identified replication strategies which may be used for loosely coupled systems by a bottom-up approach (based on general characteristics of replication mechanisms) and evaluates their use in respect to qualitative and quantitative indicators. The considered replication strategies mainly build on Event-Driven Architectures with Messaging and Atom-based Web Feeds. Data was gathered by an analytical evaluation and a simulation. Qualitative indicators were solely examined analytically, in accordance with the quality indicators of ISO 25010. Quantitative indicators are the latency of a client-side read operation, the latency of a server-side update operation, the time until all replicas are in a consistent state and the consumed bandwidth. These indicators were examined analytically and measured during a simulation in a custom-built simulation environment with implementation of the replication strategies in Java EE and Ruby on Rails. To emulate the behavior of a real IT system, the simulation uses several synthetic workloads which are based on real-world observations and assumptions made in related research. The evaluation concludes that there is no single best replication strategy for all cases of applications. Replication strategies which use Atom-based Web Feeds are generally not recommended because of the insufficient availability of tools in this area. The evaluation indicates that replication by push-based notifications of the full state of a data item with a publish-subscribe message queue can be used for most cases of applications.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schenk, A. (2018). <i>Replication in loosely coupled systems : simulation and evaluation of replication strategies for their use in loosely coupled IT systems</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.43827</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "3569",
                    "name": "Schenk Alexander - 2018 - Replication in loosely coupled systems simulation and...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1408881,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1791/2/Schenk%20Alexander%20-%202018%20-%20Replication%20in%20loosely%20coupled%20systems%20simulation%20and...pdf"
                },
                {
                    "bsid": "73919",
                    "name": "Schenk Alexander - 2018 - Replication in loosely coupled systems simulation and...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 292488,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1791/5/Schenk%20Alexander%20-%202018%20-%20Replication%20in%20loosely%20coupled%20systems%20simulation%20and...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Alexander",
                    "last_name": "Schenk",
                    "position": 1,
                    "role": "Author",
                    "tid": "61560"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "177687",
            "handle": "20.500.12708/169629",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Special Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Handy sapiens",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2007). Handy sapiens. <i>IX : Magazin Für Professionelle Informationstechnik</i>, <i>2007</i>(2), 95–99. http://hdl.handle.net/20.500.12708/169629</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "177706",
            "handle": "20.500.12708/169648",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Evaluation of content-based image descriptors by statistical methods",
            "keywords": [],
            "abstract": "Evaluation of visual information retrieval systems is usually performed by\r\nexecuting test queries and computing recall- and precision-like measures based on\r\npredefined media collections and ground truth information. This process is complex and\r\ntime consuming. For the evaluation of feature transformations (transformation of visual\r\nmedia objects to feature vectors) it would be desirable to have simpler methods available as\r\nwell. In this paper we introduce a supplementary evaluation procedure for features that is\r\nfounded on statistical data analysis. A second novelty is that we make use of the existing\r\nvisual MPEG-7 descriptors to judge the characteristics of feature transformations. The\r\nproposed procedure is divided into four steps: (1) feature extraction, (2) merging with\r\nMPEG-7 data and normalisation, (3) statistical data analysis and (4) visualisation and\r\ninterpretation. Three types of statistical methods are used for evaluation: (1) univariate\r\ndescription (moments, etc.), (2) identification of similarities between feature elements (e.g.\r\ncluster analysis) and (3) identification of dependencies between variables (e.g. factor\r\nanalysis). Statistical analysis provides beneficial insights into the structure of features that\r\ncan be exploited for feature redesign. Application and advantages of the proposed approach\r\nare shown in a number of toy examples.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2007). Evaluation of content-based image descriptors by statistical methods. <i>Multimedia Tools and Applications</i>, <i>5</i>(3), 241–258. http://hdl.handle.net/20.500.12708/169648</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "177712",
            "handle": "20.500.12708/169654",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Warning: Subtle Aspects of Strategy Assessment may Affect Correlations Among Spatial Tests",
            "keywords": [],
            "abstract": "In this study, preliminary to a larger experiment, 42 participants completed four different spatial tests and, after each test, a strategy questionnaire. For half of the participants, visualizational strategies were presented first in this questionnaire, and for the other half, analytical strategies. The order of strategy descriptions had effects on the strategies reported and on the intercorrelations among the spatial tests and between the spatial tests and an inductive-reasoning test. In the group first presented with visualizational strategies, intercorrelations among the spatial tests were higher and correlations with the reasoning tests were lower than in the group first presented with analytical strategies. Bootstrap analyses with 100 random splits of the sample confirmed the result. The findings are interpreted as indications of a priming effect by the strategy descriptions which affected the way participants dealt with subsequent tests. Implications for strategy assessment are discussed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Glück, J., Dünser, A., Steinbügl, K., &#38; Kaufmann, H. (2007). Warning: Subtle Aspects of Strategy Assessment may Affect Correlations Among Spatial Tests. <i>Perceptual and Motor Skills</i>, <i>104</i>, 123–140. http://hdl.handle.net/20.500.12708/169654</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Judith",
                    "last_name": "Glück",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Dünser",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Karin",
                    "last_name": "Steinbügl",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "177714",
            "handle": "20.500.12708/169656",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Multiple Head Mounted Displays in Virtual and Augmented Reality Applications",
            "keywords": [],
            "abstract": "With the recent introduction of low cost head mounted displays (HMDs), prices of HMD-based virtual reality setups dropped considerably. In various application areas personal head mounted displays can be utilized for groups of users to deliver different context sensitive information to individual users. We present a hardware setup that allows to attach 12 or more HMDs to a single PC. Finally we demonstrate how a collaborative, educational, augmented reality application is used by six students wearing HMDs on a single PC simultaneously with interactive framerates.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., &#38; Csisinko, M. (2007). Multiple Head Mounted Displays in Virtual and Augmented Reality Applications. <i>The International Journal of Virtual Reality</i>, <i>6</i>(2), 43–50. http://hdl.handle.net/20.500.12708/169656</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 2,
                    "role": "Author",
                    "tid": "47153"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "177730",
            "handle": "20.500.12708/169672",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Identification of Drawing Tools by Classification of Textural and Boundary Features of Strokes",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kammerer, P., Lettner, M., Zolda, E., &#38; Sablatnig, R. (2007). Identification of Drawing Tools by Classification of Textural and Boundary Features of Strokes. <i>Pattern Recognition Letters</i>, <i>28</i>(16), 710–718. http://hdl.handle.net/20.500.12708/169672</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Paul",
                    "last_name": "Kammerer",
                    "position": 1,
                    "role": "Author",
                    "tid": "49696"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Lettner",
                    "position": 2,
                    "role": "Author",
                    "tid": "48706"
                },
                {
                    "first_name": "Ernestine",
                    "last_name": "Zolda",
                    "position": 3,
                    "role": "Author",
                    "tid": "229392"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 4,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "177743",
            "handle": "20.500.12708/169685",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Improved motion segmentation based on shadow detection",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kampel, M., Wildenauer, H., Blauensteiner, P., &#38; Hanbury, A. (2007). Improved motion segmentation based on shadow detection. <i>Electronic Letters on Computer Vision and Image Analysis</i>, <i>6</i>(3), 12. http://hdl.handle.net/20.500.12708/169685</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 1,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Wildenauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "51087"
                },
                {
                    "first_name": "Philipp",
                    "last_name": "Blauensteiner",
                    "position": 3,
                    "role": "Author",
                    "tid": "53592"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 4,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "17820",
            "handle": "20.500.12708/16961",
            "doi": "10.34726/hss.2021.79041",
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Open set classification in the domain of license plate type images using deep learning",
            "keywords": [
                "Deep Learning",
                "Open Set Classification",
                "Open Set Recognition",
                "Embedding Space",
                "Saliency Map",
                "Neural Network",
                "License Plate",
                "License Plate Type"
            ],
            "abstract": "License plate type recognition is a classification problem on an open set of classes. Its purpose is to distinguish between different backgrounds of license plates mounted on vehicles. Knowing the license plate type allows to identify the country in which the respective license plate is registered. Modern approaches which solve open set classification problems make use of deep learning to train an intermediate embedding space in which a subsequent classification is performed. This embedding space essentially represents the result of a non-linear feature reduction. Within it, each class is represented by a compact cluster which is separable from the other classes. The embedding space is only trained with a subset of the known classes. The remaining known classes are incorporated without retraining the network. Instances of classes which are unknown to the system are identified using thresholding. The main contribution of this work is the analysis of the class distributions within the embedding space, which has been largely neglected by previous work. For this purpose, new benchmarks which allow a qualitative comparison between different learned embedding spaces are introduced. The identifying characters of a license plate are challenging for license plate type recognition because the model tends towards learning specific characters as features. To overcome this problem, randomized masking of the characters during training is proposed. Furthermore, investigations about which features the trained network is sensitive to are carried out using gradient-based saliency map techniques. A commonly used embedding space classifier assumes class distributions of homogeneous variance as a prior under the open set restriction. The analysis of the class distributions in the embedding space shows that this prior is not met for the used license plate type dataset. The experiments reveal that the class distributions even show a significant difference from multivariate Gaussians, which are capable of modeling more complex distributions in the shape of hyper-ellipsoids. The computed saliency maps visualize that the model learns reasonable features for most of the license plate types. However, for a few of them, primitive features which a human would not consider for classification are exploited.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haushofer, S. (2021). <i>Open set classification in the domain of license plate type images using deep learning</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2021.79041</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "70563",
                    "name": "Haushofer Sebastian - 2021 - Open set classification in the domain of license...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3248749,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16961/1/Haushofer%20Sebastian%20-%202021%20-%20Open%20set%20classification%20in%20the%20domain%20of%20license...pdf"
                },
                {
                    "bsid": "72614",
                    "name": "Haushofer Sebastian - 2021 - Open Set Classification in the Domain of License...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 217640,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16961/4/Haushofer%20Sebastian%20-%202021%20-%20Open%20Set%20Classification%20in%20the%20Domain%20of%20License...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Sebastian",
                    "last_name": "Haushofer",
                    "position": 1,
                    "role": "Author",
                    "tid": "291526"
                },
                {
                    "first_name": "Jiri",
                    "last_name": "Hladuvka",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "53161"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "17832",
            "handle": "20.500.12708/16973",
            "doi": "10.34726/hss.2021.28546",
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Interactive 3D dense surface exploration in immersive virtual reality",
            "keywords": [
                "Virtual Reality",
                "3D Interaction",
                "Oclulus Rift",
                "Virtualizer",
                "Interactive Scene Exploration"
            ],
            "abstract": "Dense 3D reconstructions of real-world environments become wide spread and are foreseen to act as data base to solve real world problems, such as remote inspections. Therefore not only scene viewing is required but also the ability to interact with the environment,such as selection of a user-defined part of the reconstruction for later usage. However, inter-object occlusion is inherent to large dense 3D reconstructions, due to scene geometry or reconstruction artifacts that might result in object containment. Since prior art lacks approaches for occlusion management in environments that consist of one or multiple(large) continuous surfaces, we propose the novel technique Large Scale Cut Plane that enables segmentation and subsequent selection of visible, partly or fully occluded patches within a large 3D reconstruction, even at far distance. An immersive Virtual reality setup consisting of a Head-Mounted Display, a locomotion device (omni-directional treadmill)and a 6DOF-hand-tracking device are combined with the Large Scale Cut Plane technique to foster 3D scene understanding and natural user interactions. We furthermore present results from a user study where we investigate performance and usability of our proposed technique compared to a baseline technique. Our results indicate Large Scale Cut Plane to be superior in terms of speed and precision, while we found need of improvement of the user interface.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kössler, C. (2021). <i>Interactive 3D dense surface exploration in immersive virtual reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2021.28546</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "70599",
                    "name": "Koessler Christian - 2021 - Interactive 3D dense surface exploration in...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2008553,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16973/1/Koessler%20Christian%20-%202021%20-%20Interactive%203D%20dense%20surface%20exploration%20in...pdf"
                },
                {
                    "bsid": "72276",
                    "name": "Koessler Christian - 2021 - Interactive 3D Mesh Exploration in Immersive Virtual...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 141454,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/16973/4/Koessler%20Christian%20-%202021%20-%20Interactive%203D%20Mesh%20Exploration%20in%20Immersive%20Virtual...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Kössler",
                    "position": 1,
                    "role": "Author",
                    "tid": "185061"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-03"
            ],
            "pid": "178588",
            "handle": "20.500.12708/170530",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Efficient Reconstruction from Non-uniform Point Sets",
            "keywords": [],
            "abstract": "We propose a method for non-uniform reconstruction of 3D scalar data. Typically, radial basis functions, trigonometric polynomials or shift-invariant functions are used in the functional approximation of 3D data. We adopt a variational approach for the reconstruction and rendering of 3D data. The principle idea is based on data fitting via thin-plate splines. An approximation by B-splines offers more compact support for fast reconstruction. We adopt this method for large datasets by introducing a block-based reconstruction approach. This makes the method practical for large data sets. Our reconstruction will be smooth across blocks. We give reconstruction measurements as error estimations based on different parameter settings and also an insight on the computational effort. We show that the block size used in reconstruction has a negligible effect on the reconstruction error. Finally we show rendering results to emphasize the quality of this 3D reconstruction technique.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vucini, E., Möller, T., &#38; Gröller, E. (2008). Efficient Reconstruction from Non-uniform Point Sets. <i>Visual Computer</i>, <i>24</i>(7–9), 555–563. http://hdl.handle.net/20.500.12708/170530</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Erald",
                    "last_name": "Vucini",
                    "position": 1,
                    "role": "Author",
                    "tid": "45707"
                },
                {
                    "first_name": "Torsten",
                    "last_name": "Möller",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "position": 3,
                    "role": "Author",
                    "tid": "143572"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "4043"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "178858",
            "handle": "20.500.12708/170800",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Representations for Cognitive Vision: A Review of Appearance-Based, Spatio-Temporal, and Graph-Based Approaches",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pinz, A., Bischof, H., Kropatsch, W., Schweighofer, G., Haxhimusa, Y., Opelt, A., &#38; Ion, A. (2008). Representations for Cognitive Vision: A Review of Appearance-Based, Spatio-Temporal, and Graph-Based Approaches. <i>Electronic Letters on Computer Vision and Image Analysis</i>, <i>7</i>(2), 27. http://hdl.handle.net/20.500.12708/170800</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Axel",
                    "last_name": "Pinz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Gerald",
                    "last_name": "Schweighofer",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 5,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Opelt",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 7,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "178875",
            "handle": "20.500.12708/170817",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Constructing cylindrical coordinate colour spaces",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A. (2008). Constructing cylindrical coordinate colour spaces. <i>Pattern Recognition Letters</i>, <i>29</i>, 494–500. http://hdl.handle.net/20.500.12708/170817</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "178876",
            "handle": "20.500.12708/170818",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "A survey of methods for image annotation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A. (2008). A survey of methods for image annotation. <i>Journal of Visual Languages and Computing</i>, <i>19</i>, 617–627. http://hdl.handle.net/20.500.12708/170818</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "178929",
            "handle": "20.500.12708/170871",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Special Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Strömungslehre",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2008). Strömungslehre. <i>IX : Magazin Für Professionelle Informationstechnik</i>, <i>2008</i>(11), 114–116. http://hdl.handle.net/20.500.12708/170871</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "178930",
            "handle": "20.500.12708/170872",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Special Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Werkzeuge für Medien-Streaming",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2008). Werkzeuge für Medien-Streaming. <i>IX : Magazin Für Professionelle Informationstechnik</i>, <i>2008</i>(11), 135. http://hdl.handle.net/20.500.12708/170872</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "178931",
            "handle": "20.500.12708/170873",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Special Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Frameworks für Medienverarbeitung",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2008). Frameworks für Medienverarbeitung. <i>IX : Magazin Für Professionelle Informationstechnik</i>, <i>2008</i>(12), 122–125. http://hdl.handle.net/20.500.12708/170873</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "178932",
            "handle": "20.500.12708/170874",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Special Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Review von Wiki-Engines",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2008). Review von Wiki-Engines. <i>IX : Magazin Für Professionelle Informationstechnik</i>, <i>2008</i>(12), 50–60. http://hdl.handle.net/20.500.12708/170874</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "179161",
            "handle": "20.500.12708/171103",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Messen statt scoren in der Rheumatologie; Neue quantitative Bildgebungs- und -verarbeitungsmethoden der Radiologie",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Peloschek, P. L., Müller-Mang, C., Friedrich, K., Langs, G., Donner, R., Valentinitsch, A., &#38; Kainberger, F. (2008). Messen statt scoren in der Rheumatologie; Neue quantitative Bildgebungs- und -verarbeitungsmethoden der Radiologie. <i>Zeitschrift Für Rheumatologie</i>, <i>1</i>, 51–58. http://hdl.handle.net/20.500.12708/171103</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "C.",
                    "last_name": "Müller-Mang",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Klaus",
                    "last_name": "Friedrich",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 4,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 5,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "A.",
                    "last_name": "Valentinitsch",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "F",
                    "last_name": "Kainberger",
                    "position": 7,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "181379",
            "handle": "20.500.12708/173324",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Vision Pyramids that do not Grow too High",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Haxhimusa, Y., Pizlo, Z., &#38; Langs, G. (2005). Vision Pyramids that do not Grow too High. <i>Pattern Recognition Letters</i>, <i>26</i>(3), 319–337. http://hdl.handle.net/20.500.12708/173324</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Zygmunt",
                    "last_name": "Pizlo",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 4,
                    "role": "Author",
                    "tid": "69204"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "181391",
            "handle": "20.500.12708/173336",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Special Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Visual Information Retrieval",
            "keywords": [],
            "abstract": "Grundlagen des Visual Information Retrieval",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2005). Visual Information Retrieval. <i>IX : Magazin Für Professionelle Informationstechnik</i>, <i>2005</i>(2), 114–117. http://hdl.handle.net/20.500.12708/173336</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06",
                "E194-01"
            ],
            "pid": "181392",
            "handle": "20.500.12708/173337",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Special Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Artificial Intelligence and Query Execution Methods in the VizIR Framework",
            "keywords": [],
            "abstract": "The article introduces the architecture of the querying components of the visual information retrieval framework VizIR. A major design goal was to assure adaptability and extensibility in manifold ways. VizIR components can be arbitrarily combined to build extensive applications. The framework provides various visual content descriptors, similarity measures and query models. Moreover, the platform can be extended by visual features or similarity measures as well as entirely novel query paradigms. VizIR introduces an approach for Video Browsing based on MPEG-7 visual features and Self- Organizing Maps for clustering. Furthermore, a recently proposed approach for integrating browsing and retrieval techniques is presented. Following this approach, the user is enabled to interact with the querying system in several ways which improves retrieval quality and performance considerably.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Divotkey, D., Eidenberger, H., &#38; Divotkey, R. (2005). Artificial Intelligence and Query Execution Methods in the VizIR Framework. <i>ÖGAI Journal</i>, <i>24</i>(2), 17–27. http://hdl.handle.net/20.500.12708/173337</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Doris",
                    "last_name": "Divotkey",
                    "position": 1,
                    "role": "Author",
                    "tid": "93452"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 2,
                    "role": "Author",
                    "tid": "97117"
                },
                {
                    "first_name": "Roman",
                    "last_name": "Divotkey",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01",
                "E194-03"
            ],
            "pid": "181393",
            "handle": "20.500.12708/173338",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Special Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Web automatization with Mechanize",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H., &#38; Michlmayr, E. (2005). Web automatization with Mechanize. <i>IX : Magazin Für Professionelle Informationstechnik</i>, <i>2005</i>(7), 142–145. http://hdl.handle.net/20.500.12708/173338</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                },
                {
                    "first_name": "Elke",
                    "last_name": "Michlmayr",
                    "position": 2,
                    "role": "Author",
                    "tid": "49487"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "181431",
            "handle": "20.500.12708/173376",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Structure from Motion with Wide Circular Field of View Cameras",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Micusik, B., &#38; Pajdla, T. (2006). Structure from Motion with Wide Circular Field of View Cameras. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, <i>28</i>(7), 1135–1149. http://hdl.handle.net/20.500.12708/173376</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 1,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Tomas",
                    "last_name": "Pajdla",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "181432",
            "handle": "20.500.12708/173377",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Geometrie und Raumvorstellung - Psychologische Perspektiven",
            "keywords": [],
            "abstract": "In diesem Beitrag soll zunächst ein kurzer Überblick über den psychologischen Wissensstand zum räumlichen Vorstellungsvermögen gegeben werden, wobei speziell auf die Trainierbarkeit dieser Leistungsdimension eingegangen wird, da diese Thematik in Zusammenhang mit Geometrieunterricht von besonderem Interesse ist. Anschließend wird ein laufendes Trainingsprojekt beschrieben, in dem - in interdisziplinärer Zusammenarbeit zwischen der Fakultät für Psychologie der Universität Wien und dem Institut für Softwaretechnik und Interaktive Systeme der TU Wien - die Auswirkungen eines Trainings mit Augmented Reality auf die Raumvorstellungsleistung untersucht werden.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Glück, J., Kaufmann, H., Dünser, A., &#38; Steinbügl, K. (2005). Geometrie und Raumvorstellung - Psychologische Perspektiven. <i>Informationsblätter Der Geometrie (IBDG)</i>, <i>24</i>(1), 4–11. http://hdl.handle.net/20.500.12708/173377</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Judith",
                    "last_name": "Glück",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Dünser",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Karin",
                    "last_name": "Steinbügl",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "181433",
            "handle": "20.500.12708/173378",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": true,
            "title": "3D-Vision Applied in Archaeology",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mara, H., &#38; Sablatnig, R. (2005). 3D-Vision Applied in Archaeology. <i>Forum Archaeologiae</i>, <i>HTTP://FARCH.NET</i>. http://hdl.handle.net/20.500.12708/173378</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 1,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "181434",
            "handle": "20.500.12708/173379",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": true,
            "title": "Analysis of Geometry and Documentation of NASCA Ceramics using 3D-Acquisiton",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mara, H., &#38; Hecht, N. (2005). Analysis of Geometry and Documentation of NASCA Ceramics using 3D-Acquisiton. <i>Forum Archaeologiae</i>, <i>HTTP://FARCH.NET</i>. http://hdl.handle.net/20.500.12708/173379</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 1,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "Niels",
                    "last_name": "Hecht",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "181439",
            "handle": "20.500.12708/173384",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Evaluation and Analysis of Similarity Measures for Content-based Visual Information Retrieval",
            "keywords": [],
            "abstract": "The selection of appropriate proximity measures is one of the crucial success factors of content-based visual information retrieval. In this area of research, proximity measures are used to estimate the similarity of media objects by the distance of feature vectors. The research focus of this work is the identification of proximity measures that perform better than the usual choices (e.g. Minkowski metrics). We evaluate a catalogue of 37 measures that are picked from various areas (psychology, sociology, economics, etc.). The evaluation is based on content-based MPEG-7 descriptions of carefully selected media collections. Unfortunately, some proximity measures are only defined on predicates (e.g. most psychological measures). One major contribution of this paper is a model that allows for the application of such measures on continuous feature data. The evaluation results uncover proximity measures that perform better than others on content-based features. Some predicate-based measures clearly outperform the frequently used distance norms. Eventually, the discussion of the evaluation leads to a catalogue of mathematical terms of successful retrieval and browsing measures.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2006). Evaluation and Analysis of Similarity Measures for Content-based Visual Information Retrieval. <i>Multimedia Systems</i>, <i>3</i>. http://hdl.handle.net/20.500.12708/173384</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "181462",
            "handle": "20.500.12708/173407",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Contains and Inside Relationships within Combinatorial Pyramids",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brun, L., &#38; Kropatsch, W. (2006). Contains and Inside Relationships within Combinatorial Pyramids. <i>Pattern Recognition</i>, <i>39</i>(4), 515–526. http://hdl.handle.net/20.500.12708/173407</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Luc",
                    "last_name": "Brun",
                    "position": 1,
                    "role": "Author",
                    "tid": "227964"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "181466",
            "handle": "20.500.12708/173411",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Fast Active Appearance Model Search Using Canonical Correlation Analysis",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Donner, R., Reiter, M., Langs, G., Peloschek, P. L., &#38; Bischof, H. (2006). Fast Active Appearance Model Search Using Canonical Correlation Analysis. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, <i>28</i>(10), 1690–1694. http://hdl.handle.net/20.500.12708/173411</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 1,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Reiter",
                    "position": 2,
                    "role": "Author",
                    "tid": "42378"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 3,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 5,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "181468",
            "handle": "20.500.12708/173413",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Special Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "The 7th European Research Framework",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2006). The 7th European Research Framework. <i>IX : Magazin Für Professionelle Informationstechnik</i>, <i>2006</i>(9), 114–117. http://hdl.handle.net/20.500.12708/173413</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "181469",
            "handle": "20.500.12708/173414",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Special Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Quaero (me) ante portas",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2006). Quaero (me) ante portas. <i>IX : Magazin Für Professionelle Informationstechnik</i>, <i>2006</i>(11), 118–122. http://hdl.handle.net/20.500.12708/173414</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "181487",
            "handle": "20.500.12708/173432",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Traveling Salesman Problem: A Foveating Pyramid Model",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pizlo, Z., Stefanov, E., Saalweachter, J., Haxhimusa, Y., &#38; Kropatsch, W. (2006). Traveling Salesman Problem: A Foveating Pyramid Model. <i>The Journal of Problem Solving</i>, <i>1</i>(1), 83–101. http://hdl.handle.net/20.500.12708/173432</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Zygmunt",
                    "last_name": "Pizlo",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Emil",
                    "last_name": "Stefanov",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "John",
                    "last_name": "Saalweachter",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 4,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 5,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "182870",
            "handle": "20.500.12708/174796",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Receptive fields within the Combinatorial Pyramid Framework",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brun, L., &#38; Kropatsch, W. (2003). Receptive fields within the Combinatorial Pyramid Framework. <i>Graphical Models</i>, <i>65</i>(1–3), 23–42. http://hdl.handle.net/20.500.12708/174796</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Luc",
                    "last_name": "Brun",
                    "position": 1,
                    "role": "Author",
                    "tid": "227964"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "182906",
            "handle": "20.500.12708/174832",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": false,
            "invited": false,
            "title": "Augmented Reality für die Geometrie",
            "keywords": [],
            "abstract": "Hannes Kaufmann ist Doktorand an der TU Wien. \r\nSein Arbeitsgebiet: Augmented Reality (\"AR\") für den Mathematik- und Geometrieunterricht. AR bedeutet eine Ergänzung unserer Wirklichkeit mit Werkzeugen, die der Computer uns bietet. Auf der Imagina - dem alljährlichen Festival für das Digitale Bild in Monaco www.imagina.mc - sprach Michael Reiter für TELL & Call mit Hannes Kaufmann über sein Projekt Construct3D.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2003). Augmented Reality für die Geometrie. <i>Tell&#38;amp;Call</i>, <i>03</i>(3), 35. http://hdl.handle.net/20.500.12708/174832</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "18421",
            "handle": "20.500.12708/17530",
            "doi": "10.34726/hss.2021.62304",
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Mobile collaborating robots for direct haptics in mixed reality",
            "keywords": [
                "virtual reality",
                "haptics",
                "collaborate",
                "robot",
                "rb-kairos",
                "ur-10",
                "vive",
                "robot operating system",
                "ros",
                "steamvr"
            ],
            "abstract": "After technological advancements in computer graphics and miniaturization of electric circuits, virtual reality has finally found its way into the consumer market. Commercial VR systems like HTC ’s Vive allow their wearers to experience virtual worlds realistically enough to feel audio-visually immersed. However, when interacting with the simulated environment, the limitations of such a system become apparent quickly. They offer no haptic capabilities or feedback beyond what is integrated in their hand-held input devices. Additional body-worn equipment, like haptic suits or exoskeletons, deliver only rudimentary haptic experiences or encumber the user’s ease of movement with excessive weight. Haptic hardware of the ‘encounter’ type are often constrained to a specific location within the simulation area or deliver only soft touching sensations because of their highly mobile but fragile architecture. Therfore, this thesis covers the topic of creating a VR system with haptic feedback and describes its design and implementation in a room sized setup. The paper shows how a mobile manipulator, like the RB-Kairos, can be combined with a virtual reality headset, like the Vive, to deliver real world props into the hands of users to enhance their virtual experience. To track the manipulator’s position with the same accuracy of the VR headset, the Vive’s Lighthouse tracking solution is integrated into the robot. On the software side, the system takes advantage of the Robot Operating System (ROS), which is already configured to control the robot’s basic functionality and is extended to include new modules handling the deliverance of haptic sensations. The simulation of the visual part of this project is handled by the gaming engine Unity, which features a variety of plugins suitable to create basic VR applications with minimal effort. The communication between VR application, RB-Kairos and user is handled wirelessly via radio signals which allows unrestricted mobility for participants and robots within the simulation area. The subsequent technical evaluation offers insights to operating parameters and lists potential enhancement and upgrade possibilities.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sprung, R. (2021). <i>Mobile collaborating robots for direct haptics in mixed reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2021.62304</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "72102",
                    "name": "Sprung Reinhard - 2021 - Mobile collaborating robots for direct haptics in mixed...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4902200,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/17530/1/Sprung%20Reinhard%20-%202021%20-%20Mobile%20collaborating%20robots%20for%20direct%20haptics%20in%20mixed...pdf"
                },
                {
                    "bsid": "73017",
                    "name": "Sprung Reinhard - 2021 - Mobile Kollaborative Roboter fuer Direktes Haptisches...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 235802,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/17530/4/Sprung%20Reinhard%20-%202021%20-%20Mobile%20Kollaborative%20Roboter%20fuer%20Direktes%20Haptisches...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Reinhard",
                    "last_name": "Sprung",
                    "position": 1,
                    "role": "Author",
                    "tid": "61466"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "40682"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "184386",
            "handle": "20.500.12708/176131",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Collaborative Augmented Reality in Education",
            "keywords": [],
            "abstract": "Technological advances enable the use of innovative learning tools for education. This work gives a brief insight into the potential and challenges of using collaborative Augmented Reality (AR) in education within the greater context of immersive virtual learning environments. As an example the experiences made during the development of a collaborative AR application specifically designed for mathematics and geometry education called Construct3D are summarized. Construct3D is based on the mobile collaborative AR system \"Studierstube\". We describe our efforts in developing a system for the improvement of spatial abilities and maximization of transfer of learning. Means of application and integration in mathematics and geometry education at high school as well as university level are being discussed. Anecdotal evidence supports our claim that Construct3D is easy to learn, encourages experimentation with geometric constructions and improves spatial skills.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2003). Collaborative Augmented Reality in Education. In <i>Imagina Conference 2003</i>. Imagina, Austria. Monaco Mediax. http://hdl.handle.net/20.500.12708/176131</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "184388",
            "handle": "20.500.12708/176133",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Improving Spatial Abilities by Geometry Education in Augmented Reality - Application and Evaluation Design",
            "keywords": [],
            "abstract": "In the first part we summarize our development of a system that uses collaborative augmented reality as a medium for teaching, and uses 3D dynamic geometry to facilitate mathematics and geometry education. Geometry education has proven as one powerful means of improving spatial abilities, an important component of human intelligence. A number of training studies have shown the usefulness of virtual reality (VR) in training spatial ability. Our immersive collaborative educational application, specifically developed for geometry education, serves as the basis of a comprehensive evaluation study regarding its efficacy in training spatial abilities.\r\nThe main contribution of this paper is the description of evaluation design including the test instruments, learning tasks and practical experiences with using our system for actual teaching of high school students. Preliminary results of spatial ability tests in high schools are presented. They point to an interesting aspect of gender differences which has not been reported in literature before.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., Steinbügl, K., Dünser, A., &#38; Glück, J. (2005). Improving Spatial Abilities by Geometry Education in Augmented Reality - Application and Evaluation Design. In <i>VRIC Laval Virtual 2005 Proceedings</i> (pp. 25–34). IEEE. http://hdl.handle.net/20.500.12708/176133</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Karin",
                    "last_name": "Steinbügl",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Dünser",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Judith",
                    "last_name": "Glück",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "184389",
            "handle": "20.500.12708/176134",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Designing Immersive Virtual Reality for Geometry Education",
            "keywords": [],
            "abstract": "Our work introduces immersive collaborative learning to\r\ngeometry education. More specifically, we present a system that\r\nuses collaborative augmented reality as a medium for teaching,\r\nand uses 3D dynamic geometry to facilitate mathematics and\r\ngeometry education. Both these aspects are novel to geometry\r\neducation. We describe improvements in the user interface and\r\nvisual design of such an application. We also report on practical\r\nexperiences with using our system for actual teaching of high\r\nschool students, and present initial quantitative data on the\r\neducational value of such an approach.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., &#38; Schmalstieg, D. (2006). Designing Immersive Virtual Reality for Geometry Education. In <i>Proceedings of IEEE Virtual Reality Conference 2006</i> (pp. 51–58). IEEE Computer Society. http://hdl.handle.net/20.500.12708/176134</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Dieter",
                    "last_name": "Schmalstieg",
                    "position": 2,
                    "role": "Author",
                    "tid": "110794"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "184390",
            "handle": "20.500.12708/176135",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Learning Objects for Education with Augmented Reality",
            "keywords": [],
            "abstract": "To fill the gap of next-generation user interfaces for mathematics and geometry education\r\nConstruct3D is presented, a three-dimensional dynamic geometry construction tool that can be used in\r\nhigh school and university education. This system uses Augmented Reality (AR) to provide a natural\r\nsetting for face-to-face collaboration of teachers and students. The main advantage of using AR is that\r\nstudents actually see three dimensional objects which they until now had to calculate and construct\r\nwith traditional (mostly pen and paper) methods (Figure 1). By working directly in 3D space, complex\r\nspatial problems and spatial relationships may be comprehended better and faster than with traditional\r\nmethods. This paper summarizes the development of learning objects and content specifically\r\ndesigned for teaching dynamic 3D geometry in Augmented Reality. It concentrates on pedagogical\r\nfindings while working with teachers and students in more than 500 teaching lessons.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., &#38; Papp, M. (2006). Learning Objects for Education with Augmented Reality. In <i>Proceedings of EDEN 2006 Conference</i> (pp. 160–166). European Distance and E-Learning Network. http://hdl.handle.net/20.500.12708/176135</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Marion",
                    "last_name": "Papp",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "184391",
            "handle": "20.500.12708/176136",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Virtual and Augmented Reality as Spatial Ability Training Tools",
            "keywords": [],
            "abstract": "Virtual reality (VR) and augmented reality (AR - overlaying virtual objects onto the real world) offer interesting and wide spread possibilities to study different components of human behaviour and cognitive processes. One aspect of human cognition that has been frequently studied using VR technology is spatial ability. Research ranges from training studies that investigate whether and/or how spatial ability can be improved by using these new technologies to studies that focus on specific aspects of spatial ability for which VR is an efficient investigational tool. In this paper we first review studies that used VR technologies to study different aspects of spatial ability. Then results and findings will be presented from one of the first large-scale studies (215 students) that investigated the potential of an AR application to train spatial ability.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dünser, A., Kaufmann, H., Steinbügl, K., &#38; Glück, J. (2006). Virtual and Augmented Reality as Spatial Ability Training Tools. In <i>Proceedings of the CHINZ 2006</i> (pp. 125–132). ACM Press. http://hdl.handle.net/20.500.12708/176136</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Andreas",
                    "last_name": "Dünser",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Karin",
                    "last_name": "Steinbügl",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Judith",
                    "last_name": "Glück",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "184763",
            "handle": "20.500.12708/176506",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Rigid-Body Target Design Methodology for Optical Pose-Tracking Systems",
            "keywords": [],
            "abstract": "The standard method for estimating the rigid-body motion of arbitrary interaction devices with an infrared-optical tracking system involves attaching pre-defined geometric constellations of retro-reflective or light-emitting markers, commonly referred to as \"targets\", to all tracked objects. Optical markers of the same type are typically indistinguishable from each other, requiring the tracking system to establish their identities through known spatial relationships. Consequently, the specific geometric arrangement of markers across multiple targets has a considerable impact on the system's overall performance and robustness. In this paper, we propose a simple new methodology for constructing optically tracked rigid-body targets. Our practically-oriented approach employs an optimization heuristic to compute near-optimal marker arrangements. Using prefabricated mounting fixtures, the assembly step requires only basic hobbyist tools and skills.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pintaric, T., &#38; Kaufmann, H. (2008). A Rigid-Body Target Design Methodology for Optical Pose-Tracking Systems. In <i>Proceedings of the 2008 ACM symposium on Virtual reality software and technology</i>. 15th ACM Symposium on Virtual Reality Software and Technology (VRST 2008), Bordeaux, France, EU. ACM Press. https://doi.org/10.1145/1450579.1450594</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Pintaric",
                    "position": 1,
                    "role": "Author",
                    "tid": "39909"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "4151"
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "186166",
            "handle": "20.500.12708/177329",
            "doi": "10.34726/hss.2023.94903",
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Sketch based L-systems for tree modeling in virtual reality",
            "keywords": [
                "virtual reality",
                "tree modeling",
                "l-systems",
                "sketch-based modeling"
            ],
            "abstract": "This thesis proposes a new method for creating tree assets in virtual reality for digital content that combines a sketch-based approach, with a procedural branching system.Sketch-based modeling in virtual reality offers the possibility of quick iteration andcreation of models while a procedural system can aid realism but also provide a large variety of assets from one model. To demonstrate this concept, a virtual reality prototype for tree modeling that combines sketch-based modeling with L-Systems to generate atree’s branching structure is presented. To achieve the combined approach of sketch-basedmodeling and procedural generation of trees, a 2OL-System, a specialized Lindenmayer System that includes context sensitive rules and parameters, was implemented and adapted to react to the user’s input. This work explores the challenges of implementing an interactive L-System by including the user’s choices as parameters into the contextsensitive and parameterized generation rules of the system. Additionally the benefits and pitfalls are discussed that come with a sketch based interface in virtual reality. A user study was conducted to compare the usability of the prototype with a widely used tree modeling tool, SpeedTree. The results showed that the prototype is comparable to the level of usability as SpeedTree, but with the added benefit of quickly generating tree shapes through sketching. This research contributes to the fields of virtual reality, tree modeling, and sketch-based modeling by introducing a new approach utilizing L-Systems for the quick generation of parameterized tree models.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Steiner, M. (2023). <i>Sketch based L-systems for tree modeling in virtual reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2023.94903</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "278531",
                    "name": "Steiner Maximilian - 2023 - Sketch based L-Systems for Tree Modeling in Virtual...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2356015,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/177329/1/Steiner%20Maximilian%20-%202023%20-%20Sketch%20based%20L-Systems%20for%20Tree%20Modeling%20in%20Virtual...pdf"
                },
                {
                    "bsid": "278569",
                    "name": "Steiner Maximilian - 2023 - Sketch based L-Systems for Tree Modeling in Virtual...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 112121,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/177329/4/Steiner%20Maximilian%20-%202023%20-%20Sketch%20based%20L-Systems%20for%20Tree%20Modeling%20in%20Virtual...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Maximilian",
                    "last_name": "Steiner",
                    "position": 1,
                    "role": "Author",
                    "tid": "328144"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "46406"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "231177"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "186167",
            "handle": "20.500.12708/177330",
            "doi": "10.34726/hss.2023.102261",
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Safer Internet Chatbot",
            "keywords": [],
            "abstract": "Virtual assistants or conversational agents - widely known as chatbots - are becomingan increasingly pervasive part of our modern society, and are already widely used to take on tasks where permanent accessibility is beneficial. In our particular use case,an already operational German language chatbot is used to answer children’s questionsregarding chain letters. It is not a conversational chatbot, but serves a particular goal.In a typical scenario, a child asks a question and sends the received message, for which itwants to know whether it should be taken seriously or whether the message can be safely ignored. The chatbot’s task is to recognise the intent of the question, detect whether the received message represents a chain letter, and to respond appropriately and steer the conversation such that potential fears are alleviated and advice is given on how to proceed further. Throughout this work, we have improved the current German language chatbotby (i) providing an implementation based on open-source technologies, and conducteda (ii) quantitative evaluation of 120 different approaches based on machine learning,where we have combined a variety of algorithms, neural network architectures and textembedding methods. By applying transfer learning based on the BERT language model,we were able to achieve a classification performance of 0.9 for both metrics F-Score and accuracy. We have also examined (iii) the influence of emojis on the overall classification performance, where to our surprise we could not identify any clear effect. Furthermore,we have conducted (iv) a qualitative evaluation of our implementation and have therefo recompiled a questionnaire regarding the expected and actual results concerning the system behavior and performance. The feedback received through the questionnaire has beenvery positive, and it showed that we were able to increase the perceived quality of (v)intent recognition, (vi) chain letter detection and (vii) response generation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kasal, K. (2023). <i>Safer Internet Chatbot</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2023.102261</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "278534",
                    "name": "Kasal Kresimir - 2023 - Safer Internet Chatbot.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3267286,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/177330/1/Kasal%20Kresimir%20-%202023%20-%20Safer%20Internet%20Chatbot.pdf"
                },
                {
                    "bsid": "278566",
                    "name": "Kasal Kresimir - 2023 - Safer Internet Chatbot.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 183458,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/177330/4/Kasal%20Kresimir%20-%202023%20-%20Safer%20Internet%20Chatbot.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Kresimir",
                    "last_name": "Kasal",
                    "position": 1,
                    "role": "Author",
                    "tid": "48325"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "187779",
            "handle": "20.500.12708/178663",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Template-based face detection in videos",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Meyer, H. (2006). <i>Template-based face detection in videos</i> [Master Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/178663</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Harald",
                    "last_name": "Meyer",
                    "position": 1,
                    "role": "Author",
                    "tid": "39488"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E183"
            ],
            "pid": "187961",
            "handle": "20.500.12708/178845",
            "doi": null,
            "year": 2002,
            "issued": "2002",
            "issued_on": "2002-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Lagebestimmung des Kopfes mittels Kernel Cannonical Correlation Analysis",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Federspiel, T. (2002). <i>Lagebestimmung des Kopfes mittels Kernel Cannonical Correlation Analysis</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/178845</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Till",
                    "last_name": "Federspiel",
                    "position": 1,
                    "role": "Author",
                    "tid": "111033"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "188428",
            "handle": "20.500.12708/179312",
            "doi": null,
            "year": 2001,
            "issued": "2001",
            "issued_on": "2001-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "ZYX - towards flexible multimedia document models for reuse and adaption",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Boll, S. (2001). <i>ZYX - towards flexible multimedia document models for reuse and adaption</i> [Dissertation, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/179312</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Susanne",
                    "last_name": "Boll",
                    "position": 1,
                    "role": "Author",
                    "tid": "126031"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "188675",
            "handle": "20.500.12708/179559",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Visualization of complex function graphs in augmented reality",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Liebo, R. (2006). <i>Visualization of complex function graphs in augmented reality</i> [Master Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/179559</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Robert",
                    "last_name": "Liebo",
                    "position": 1,
                    "role": "Author",
                    "tid": "86575"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "188683",
            "handle": "20.500.12708/179567",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automatische Erkennung von Text in Videosequenzen",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brandejsky, M. (2006). <i>Automatische Erkennung von Text in Videosequenzen</i> [Master Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/179567</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Brandejsky",
                    "position": 1,
                    "role": "Author",
                    "tid": "83653"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "189040",
            "handle": "20.500.12708/179924",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "A Java-based streaming media server",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Psaier, H. (2005). <i>A Java-based streaming media server</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/179924</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Harald",
                    "last_name": "Psaier",
                    "position": 1,
                    "role": "Author",
                    "tid": "52911"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E183"
            ],
            "pid": "189997",
            "handle": "20.500.12708/180878",
            "doi": null,
            "year": 2002,
            "issued": "2002",
            "issued_on": "2002-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Robust analysis of spot array images",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brändle, N. (2002). <i>Robust analysis of spot array images</i> [Dissertation, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/180878</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Norbert",
                    "last_name": "Brändle",
                    "position": 1,
                    "role": "Author",
                    "tid": "116137"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "19006",
            "handle": "20.500.12708/18086",
            "doi": "10.34726/hss.2021.72021",
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Evaluation of immersion in externally directed VR",
            "keywords": [
                "Virtual Reality",
                "Game Design"
            ],
            "abstract": "In this thesis, the impact of humanly controlled, externally triggered effects on immersioninside a virtual reality simulation is researched. The VR simulation is based on theVirtual Jump project [EM15] which not only serves as physical foundation for creating an immersive VR experience, but also provides the original VR content that this thesis build supon. The software for the external interaction console is created as a completely separateapplication from the ground up. By using one of the interaction console applications the user can trigger actions within the VR simulation, either by using traditional input devices like a mouse or by performing hand movements that are captured with the LeapMotion Controller. Both the original Virtual Jump content, as well as the newly created interaction console application are based on the Unity game engine and can be run as stand-alone applications. The interaction between these two applications is realized witha custom, message-based communication approach, that transfer event triggers over a traditional Ethernet connection.With a prototype implementation of the remote interaction console, as well as with a modified version of the original Virtual Jump content, a small-scale user study was carried out. In order to allow the necessary test jumps to be performed with a maximum of safety in mind, even in light of the COVID-19 pandemic, some concessions to the software implementation as well as a variety of changes to the testing procedure andnumber of test subjects had to be made.The majority of my participants, although not being representative due to sample size,claimed that externally triggered effects did in fact have an influence on immersion whenbeing in a VR simulation. Surprisingly, these effects seem to be positive rather than negative. The user study also covers the inter-person interaction between jumpers and controllers, as well as power dynamics that could be observed during the test jumps.To summarize, my user study, although not representative, showed that the degree ofimmersion of my test subjects in VR was mostly affected positively by humanly-controlled,externally triggered effects. It also revealed that the power dynamics between participants within the two roles highly depended on the individual users. Nevertheless, among my participants, both roles seem to have been enjoyed during the test jumps, with the role of the controller seeming to be the more desirable one.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Farda, S. (2021). <i>Evaluation of immersion in externally directed VR</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2021.72021</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "77366",
                    "name": "Farda Silvester - 2021 - Evaluation of immersion in externally directed VR.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3156733,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18086/1/Farda%20Silvester%20-%202021%20-%20Evaluation%20of%20immersion%20in%20externally%20directed%20VR.pdf"
                },
                {
                    "bsid": "77444",
                    "name": "Farda Silvester - 2021 - Evaluation of Immersion in Externally Directed VR.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 232824,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18086/4/Farda%20Silvester%20-%202021%20-%20Evaluation%20of%20Immersion%20in%20Externally%20Directed%20VR.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Silvester",
                    "last_name": "Farda",
                    "position": 1,
                    "role": "Author",
                    "tid": "59510"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "19007",
            "handle": "20.500.12708/18087",
            "doi": "10.34726/hss.2021.83740",
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Game design in virtual reality",
            "keywords": [
                "Virtual Reality",
                "Game Design"
            ],
            "abstract": "The last decade showed immense improvements in Virtual Reality (VR) technology. VRis used in many different areas other than gaming such as in the medical area. As an example, surgeons can learn how to perform certain complicated surgeries over and overin VR without risking anyone’s life. The possibilities such as this example widened its application areas significantly as the leading sector, gaming, became more appealing to consumers. Many game development companies started to produce immersive experiences which led to strengthening their position on the market by attracting new users. However,VR gaming still falls behind the traditional video gaming experience mainly due to the lack of quality content and the additional costs that come with VR devices. The main reason behind the scarcity of quality content is that creating VR games still involves trying out new ideas by using the general knowledge of game design inherited from developing video games. The fact that the player is present inside VR games should change the perspective of the game designers while building the experience. They need to adapt game design elements such as aesthetics, challenge, and replayability and change them in such a way so the VR can be still compelling to players. To address the secontent-related issues, this thesis investigates the applicability of different digital gamedesign elements onto VR games by developing a VR game called Astroclimb. Astroclimbis a science fiction and space-themed game that revolves around climbing a celestial wallin outer space. The climbing part of the game also takes place in reality thanks to theVreeclimber project of TU Wien, which is a virtual reality climbing system that enablesan infinitely rolling climbing wall.During the development stage of Astroclimb, improving the quality of user experience and the level of immersion were the two main goals for Astroclimb. The final version of the game was evaluated by a small test group with questionnaires and optional short unstructured interviews as well as observing the player during the playthroughs. The evaluation showed that the application of different game design elements successfully increased the immersion level in the game and made the game more compelling to players.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gürcay, O. (2021). <i>Game design in virtual reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2021.83740</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "77369",
                    "name": "Guercay Onur - 2021 - Game design in virtual reality.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4102245,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18087/1/Guercay%20Onur%20-%202021%20-%20Game%20design%20in%20virtual%20reality.pdf"
                },
                {
                    "bsid": "77437",
                    "name": "Guercay Onur - 2021 - Game Design in Virtual Reality.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 236344,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18087/4/Guercay%20Onur%20-%202021%20-%20Game%20Design%20in%20Virtual%20Reality.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Onur",
                    "last_name": "Gürcay",
                    "position": 1,
                    "role": "Author",
                    "tid": "66748"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E183"
            ],
            "pid": "190844",
            "handle": "20.500.12708/181720",
            "doi": null,
            "year": 2002,
            "issued": "2002",
            "issued_on": "2002-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Generalized canonical correlation analysis for object recognition",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Melzer, T. (2002). <i>Generalized canonical correlation analysis for object recognition</i> [Dissertation, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/181720</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Melzer",
                    "position": 1,
                    "role": "Author",
                    "tid": "46466"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E183"
            ],
            "pid": "192169",
            "handle": "20.500.12708/183037",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Cue integration techniques for robust feature tracking",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ayromlou, M. (2003). <i>Cue integration techniques for robust feature tracking</i> [Dissertation, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/183037</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Minu",
                    "last_name": "Ayromlou",
                    "position": 1,
                    "role": "Author",
                    "tid": "112671"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E183"
            ],
            "pid": "192220",
            "handle": "20.500.12708/183088",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automatic quantification of destructive changes caused by rheumatoid arthritis",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langs, G. (2003). <i>Automatic quantification of destructive changes caused by rheumatoid arthritis</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/183088</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 1,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "1930",
            "handle": "20.500.12708/1946",
            "doi": "10.34726/hss.2018.57641",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Outdoor inside-out linear image sensor tracking",
            "keywords": [
                "Tracking",
                "Linear Image Sensor",
                "Virtual Reality",
                "Outdoor Tracking"
            ],
            "abstract": "Tracking systems are a widely used technology in physical computing. Without a positioning system, many applications like robotic systems could not function properly. Especially Virtual Reality / Augmented Reality headsets rely heavily on the position data of such systems. For this emerging technology, accurate and fast position data is crucial, in order to prevent side effects like motion sickness. Furthermore, tracking systems have to be cheap and portable to be competitive on the mass market. In this thesis, a new kind of tracking system is presented, based on the refraction of light and optical focus. The main part of the system forms a linear image sensor together with a 4-piece lens system. Light originates from Light Emitting Diodes (LEDs) placed in the tracking environment. These LEDs are controlled by a microcontroller one at a time. The LED light cones need to be overlapping, in order to get at least two intensity measurements by different light sources. Light rays travel through the lens system, which focuses light on the sensor. When the sensor moves, the angle of the refracted light changes, thus reaching a different part of the sensor. A microcontroller can then compute the tracking systems change of position. The project can be split into two parts: The first part consists of a tracking system simulation. The simulation makes use of Ray Transfer Matrix Analysis, a ray tracing technique utilized in the design of optical systems. Using the simulation, the size of the tracking area has been calculated and the right lenses with correct distances to each other have been chosen. The second part consists of the actual implementation. A functional prototype has been developed, to turn the insights of the simulation into a real-life solution. A microcontroller is used to read the linear image sensor and control the LEDs. The intensity values are sent to a Raspberry Pi, which calculates the position change of the tracker. With only one linear image sensor, the system is able to track translations in two dimensions. However, through adding an additional sensor with corresponding LEDs, tracking of a third axis is possible in theory. Finally, the accuracy and latency of the tracking system prototype have been evaluated.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Prossenitsch, C. (2018). <i>Outdoor inside-out linear image sensor tracking</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.57641</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "3879",
                    "name": "Prossenitsch Christian - 2018 - Outdoor inside-out linear image sensor tracking.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 20665309,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1946/2/Prossenitsch%20Christian%20-%202018%20-%20Outdoor%20inside-out%20linear%20image%20sensor%20tracking.pdf"
                },
                {
                    "bsid": "74129",
                    "name": "Prossenitsch Christian - 2018 - Outdoor inside-out linear image sensor tracking.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 153661,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/1946/5/Prossenitsch%20Christian%20-%202018%20-%20Outdoor%20inside-out%20linear%20image%20sensor%20tracking.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Prossenitsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "177289"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E183"
            ],
            "pid": "193566",
            "handle": "20.500.12708/184412",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Minimal combinatorial maps for analyzing 3D data",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Illetschko, T. (2006). <i>Minimal combinatorial maps for analyzing 3D data</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/184412</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Illetschko",
                    "position": 1,
                    "role": "Author",
                    "tid": "97458"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E183"
            ],
            "pid": "194385",
            "handle": "20.500.12708/185219",
            "doi": null,
            "year": 2002,
            "issued": "2002",
            "issued_on": "2002-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Visuelle Personenverfolgung",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Steindl, C. (2002). <i>Visuelle Personenverfolgung</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/185219</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Steindl",
                    "position": 1,
                    "role": "Author",
                    "tid": "97965"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "194813",
            "handle": "20.500.12708/185648",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Musik-Genreklassifikation mit Hilfe von Hidden Markov Modellen",
            "keywords": [
                "Music",
                "Hidden Markov Models",
                "Classification",
                "Acoustic Features",
                "Features",
                "Music genre",
                "Information Retrieval",
                "Musik Information Retrieval"
            ],
            "abstract": "Music is part of our everyday life. With the help of car radios and mobile devices we listen to music from early in the morning until late in the evening, not only with on the way to work and school, but also during exercise, in restaurants and on vacation. Music information retrieval as a sub domain of information retrieval gains its popularity with the growth of the internet and the spread of music audio formats like MP3. In the last few years we have faced a lot of scientific work that has been published in this area and another kind of work which concentrates in development of efficient algorithms and databases for management of large music collections. A popular form of music organization is the classification of genres. The idea of automatic music classification has been born because of the need for this kind of organization and costs involved by manual classification.<br /> In this work we inspect an often used method for music genre classification. Besides that, we try to give an overview of the music information retrieval, used techniques and also point to the sources of important information, data and existing implementations. The goal of this work is to study the music genre classification with help of Hidden Markov Models and the application of this to classify music provided at International Symposium for Music Information Retrieval (ISMIR) in the year 2004. The 30 seconds of music has been converted from MP3 compressed to wave audio format at CD-Quality. To extract the features we use the MIRToolbox implemented by the University of Jyväskyläda (Finland) - Department of Music as an installable toolbox for MATLAB.<br />Out of six extracted features, as a time-dependent characteristic trait of an audio signal, we build 63 different combinations, which are then trained and classified with help from Kevin Murphy's \"Hidden Markov Toolbox\" for MATLAB. Because we trained the models with three different  training sets (33%, 50%, 100%), we could see how the size of the training set influences the classification with HMMs. The results here show, as expected, that a well chosen combination of features leads to better classification results then any feature individually and that Hidden Markov Models face the classification of music genres very well.<br />In this work we reached the classification accuracy of 75%.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Filipović, A. (2008). <i>Musik-Genreklassifikation mit Hilfe von Hidden Markov Modellen</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/185648</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Aksel",
                    "last_name": "Filipović",
                    "position": 1,
                    "role": "Author",
                    "tid": "53088"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "159676"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "195200",
            "handle": "20.500.12708/186034",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Kinetosis / motion sickness : a new approach for adaptation in virtual reality",
            "keywords": [
                "Kinetosis",
                "virtual environment",
                "virtual reality",
                "motion sickness",
                "cybersickness"
            ],
            "abstract": "Kinetosis is the medical expression for motion sickness. It comes from the Greek word \"kinein\" which means \"to move\". In this master's thesis the effects of motion sickness and cybersickness are described. This topic is first discussed on the basis of existing works in which former approaches are demonstrated. Motion sickness occurs when the information of all parts of the equilibrium organ - ears, muscles and eyes - do not send consistent information to the brain. There exist three main theories which all try to describe the mechanism of becoming motion or cybersick.<br />Besides this there are also other factors which cause cybersickness. The symptoms of the illness are very different and there are many of them, for example eye strain, headache and nausea. The first part of the thesis describes these medical foundations in detail.<br />The practical part of this work tests the hypothesis if a short time training programme is useful for people who suffer from cybersickness.<br />To prove this a virtual environment was built to train test persons within four days. The training itself required them to watch a replay of a racing game for up to 25 minutes per session. On each of the four days exactly one session was held. A questionnaire was created to quantify the results of this training. The sickness scores were analyzed and due to this data the conclusion was drawn. At the end a small outlook to the future of cybersickness research is given.<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kölndorfer, P. (2009). <i>Kinetosis / motion sickness : a new approach for adaptation in virtual reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/186034</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Petra",
                    "last_name": "Kölndorfer",
                    "position": 1,
                    "role": "Author",
                    "tid": "39257"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "195261",
            "handle": "20.500.12708/186095",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Schreiben mit dem Brain-Computer-Interface",
            "keywords": [
                "brain-computer interface",
                "brain-machine interface",
                "P300",
                "P3",
                "speller"
            ],
            "abstract": "A Brain-Computer Interface (BCI) enables a user to control a computer and other electronic devices solely with the power of her thoughts, without any muscle activity. It provides a new interface between the human brain and a computer and is sometimes the only communica-tion channel left of a completely paralyzed patient to its environment. We give an overview of several approaches that enable such a communication. Then the EEG-based P300 approach is discussed in detail. A P300 speller application was developed that enables a user to write symbols, words, and complete sentences on a computer without motor activity. We discuss the employed methods and processing techniques as well as an approach for automatic de-termination of the number of trials to use for each target item. This approach also allows de-tection of the user's activity and inactivity, respectively. We tested the developed speller with seven healthy test persons. An averaged information transfer rate of 5.84 symbols/min with an accuracy of 81.8% was achieved.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zeintlinger, M. (2009). <i>Schreiben mit dem Brain-Computer-Interface</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/186095</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Matthias",
                    "last_name": "Zeintlinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "56628"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E183"
            ],
            "pid": "195358",
            "handle": "20.500.12708/186192",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Multi-view tracking methods",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Bikov, T. (2003). <i>Multi-view tracking methods</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/186192</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Tsvetan",
                    "last_name": "Bikov",
                    "position": 1,
                    "role": "Author",
                    "tid": "96580"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E183"
            ],
            "pid": "196050",
            "handle": "20.500.12708/186888",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Structurally optimal dual graph pyramid and its application in image partitioning",
            "keywords": [
                "Hierarchical image representation",
                "irregular image pyramid",
                "irregular dual graph pyramid",
                "dual graph contraction",
                "graph decimation strategies",
                "minimun spanning tree based image segmentation",
                "segmentation evaluation"
            ],
            "abstract": "The goal of computer vision is to make machines see, or at least to perform vision tasks with the same quality, quantity and speed as humans and animals.Humans and animals are able to delineate, detect and recognize objects in complex scenes 'in no time'. One of the most valuable and critical resources in human visual processing is time, therefore a highly parallel model is the biological answer dealing satisfactorily with this resource. Hierarchical representation and hierarchical processing in computer vision systems are the credible approach to address space and performance constraints, observed in human and animal visual systems. A widely used hierarchical representation in many areas of computer vision and pattern recognition is the (regular) image pyramid, which employs both coarse to fine and fine to coarse processing strategies. The main advantage of regular pyramids is rapid computation of global information in a recursive manner, due to logarithmic height with respect to the size of the input, making algorithms running in this data structure having a logarithmic time complexity. However, regular image pyramids lack shift invariance and do not preserve object connectivity as a result of the fixed vertical neighborhood. Thus they should be abandoned as general segmentation methods. In order to cope with shift invariance, among others, new hierarchical structures, the so called irregular pyramids, should be used. However, the logarithmic height of irregular pyramids is lost, as well as the computational efficiency. The non-logarithmic height is the main drawback of the irregular structures. Employing graph theoretical formalisms to describe irregular hierarchical structures allows an easy analysis of irregular graph pyramids. In order to be able to encode multiple boundaries between regions we use dual graphs, and the build pyramid is called dual graph pyramid.In this thesis we mainly deal with dual graph  pyramids and their application in image partitioning. We introduce two new graph concepts and show that the presented methods, maximal independent edge set (MIES) and maximal independent directed edge set (MIDES) used for the construction of stochastic irregular pyramids, bound logarithmically the height of the pyramid. We show that the two widely used stochastic decimation strategies, the maximal independent vertex set (MIS) and data driven decimation process (D3P) do not lead to logarithmic tapering graph pyramids. After studying different techniques to build the structure of the pyramid, we are motivated to use these structures in a framework for bottom-up processing. Thus into this irregular graph pyramid framework, we introduce a time efficient image partitioning method based on Boruvka's minimum spanning tree principle (MST)(BoruSeg). Although the goal of image segmentation is a single partitioning of the image, and not necessarily an image hierarchy, a hierarchical representation is needed, especially if the image context is not taken into consideration.<br />Even thought this method makes greedy decisions during the merging process it is able to capture important perceptually groupings. We evaluate the quality of the segmentation results of the MIES, MIS and D3P versions of this MST based method with respect to humans and to other graph-based methods. The evaluation shows that in image segmentation, the different stochastic decimations used in this MST image partitioning method produce comparable results. We summarize the major contribution of this thesis and discuss about the computationally hard problem of graph matching and the applicability of hierarchical approaches in dealing with its complexity.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y. (2006). <i>Structurally optimal dual graph pyramid and its application in image partitioning</i> [Dissertation, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/186888</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Luc",
                    "last_name": "Brun",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "227964"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "19635",
            "handle": "20.500.12708/18444",
            "doi": "10.34726/hss.2021.71386",
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Cryptocurrencies: Deep learning for sentiment & market analysis",
            "keywords": [
                "Deep Learning of Time Series",
                "Sentiment Analysis",
                "Cryptocurrencies"
            ],
            "abstract": "The advances of blockchain technology and its global recognition over the last years enabled the rise of digital currencies, also known as cryptocurrencies. These currencies are characterized by unique properties like pseudonymity, low trading fees, and minimal barriers of entry which made them increasingly interesting as investment opportunities.Additionally, these currencies are considered highly volatile and the efficient market hypothesis does currently not hold true according to researchers, making them the ideal target for automated analysis and trading.At the same time, deep neural networks and novel neural-network architectures have been producing promising research results for time-series predictions and sentiment-analyses.However, research on the combination of deep-learning, technical indicators, and financialsentiment analysis in the field of cryptocurrency market predictions is still scarce. Theaim of this thesis is to explore this research gap and provide answers to the complexquestions associated with it.To reach that goal we developed and evaluated multiple deep neural networks to generate trading strategies for the Bitcoin cryptocurrency. Specifically, we focused on the optimization of the structure and hyper parameters of the neural networks, explored the space of risk adjustable target values, tested alternative input sources, and developed a simulation engine for the generated trading strategies.The findings of our experiments confirm the hypothesis that cryptocurrencies open vastopportunities for profitable automated trading. Our experiments show that AI-basedtrading can significantly improve profitability compared to a buy-and-hold strategy while simultaneously reducing the risk associated with it.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Muhm, T. (2021). <i>Cryptocurrencies: Deep learning for sentiment &#38; market analysis</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2021.71386</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "78651",
                    "name": "Muhm Thomas - 2021 - Cryptocurrencies Deep learning for sentiment market...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2773619,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18444/1/Muhm%20Thomas%20-%202021%20-%20Cryptocurrencies%20Deep%20learning%20for%20sentiment%20market...pdf"
                },
                {
                    "bsid": "78665",
                    "name": "Muhm Thomas - 2021 - Cryptocurrencies Deep Learning for Sentiment Market...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 234559,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18444/4/Muhm%20Thomas%20-%202021%20-%20Cryptocurrencies%20Deep%20Learning%20for%20Sentiment%20Market...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Muhm",
                    "position": 1,
                    "role": "Author",
                    "tid": "270511"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "196677",
            "handle": "20.500.12708/187405",
            "doi": "10.34726/hss.2023.113962",
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Auditory sensory substitution in virtual reality : for people with hearing impairments",
            "keywords": [
                "Virtual Reality",
                "Haptic Feedback",
                "Sound Source Localization",
                "Information Presentation",
                "Visualization Techniques",
                "Spatial Object Localization",
                "Human-Computer Interaction",
                "Deaf and Hard-of-Hearing",
                "Computer Vision",
                "3D Audio"
            ],
            "abstract": "The research presented in this dissertation focuses on sound localization and audio visualization methods in Virtual Reality (VR) for Deaf and Hard-of-Hearing (DHH) persons. Most VR applications and devices are designed for hearing persons, making it harder for DHH persons to use VR. This dissertation starts with a brief overview of the sensory substitution systems and the importance of audio visualization in DHH persons’ daily lives. A background survey is conducted to understand DHH persons’ requirements in VR and address some challenges encountered at the early stages of designing an assistive haptic VR system for DHH persons. We continue by describing different development phases of our proposed VR assistive systems, including hardware and software designs. We evaluate a haptic VR suit that helps deaf persons complete sound-related VR tasks. Following the results, we introduce a new novel portable system for deaf persons called \"EarVR\" that analyzes 3D sounds in a VR environment and locates the direction of the closest sound to the user in real-time using two vibro-motors placed on the users’ ears. Then, we propose a novel audio visualization method in VR called “Omni-directional particle visualization” to investigate deaf persons’ reaction times to visual stimuli and compare the result with other methods. Finally, we introduce a new system called \"EarVR+,\" which is an upgraded version of the EarVR system by adding two LEDs to demonstrate visual feedback. Our methods enhance traditional VR devices with additional haptic and visual feedback, which aids spatial sound localization in VR for deaf persons.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mirzaei, M. (2023). <i>Auditory sensory substitution in virtual reality : for people with hearing impairments</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2023.113962</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "290121",
                    "name": "Mirzaei Mohammadreza - 2023 - Auditory Sensory Substitution in Virtual Reality...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 6989458,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/187405/1/Mirzaei%20Mohammadreza%20-%202023%20-%20Auditory%20Sensory%20Substitution%20in%20Virtual%20Reality...pdf"
                },
                {
                    "bsid": "290217",
                    "name": "Mirzaei Mohammadreza - 2023 - Auditory Sensory Substitution in Virtual Reality...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 384910,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/187405/4/Mirzaei%20Mohammadreza%20-%202023%20-%20Auditory%20Sensory%20Substitution%20in%20Virtual%20Reality...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Mohammadreza",
                    "last_name": "Mirzaei",
                    "position": 1,
                    "role": "Author",
                    "tid": "306356"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193",
                "E193-03"
            ],
            "pid": "196858",
            "handle": "20.500.12708/187704",
            "doi": null,
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluation and improvement of HMD-based and RGB-based hand tracking solutions in VR",
            "keywords": [
                "hand tracking",
                "virtual reality",
                "Evaluation"
            ],
            "abstract": "Hand tracking has become a state-of-the-art technology in the modern generation of consumer VR devices. However, off-the-shelf solutions do not support hand detection for more than two hands at the same time at distances beyond arm’s length. The possibility to track multiple hands at larger distances would be beneficial for colocated multi-user VR scenarios, allowing user-worn devices to track the hands of other users and therefore reducing motion artifacts caused by hand tracking loss. With the global focus of enabling natural hand interactions in colocated multi-user VR, we propose an RGB image input-based hand tracking method, built upon the MediaPipe framework, that can track multiple hands at once at distances of up to 3 m. We compared our method’s accuracy to that of Oculus Quest and Leap Motion, at different distances from the tracking device and in static and dynamic settings. The results of our evaluation show that our method provides only slightly less accurate results than Oculus Quest or Leap motion in the near range (with median errors below 1.75 cm at distances below 75 cm); at larger distances, its accuracy remains stable (with a median error of 4.7 cm at the distance of 2.75 m) while Leap Motion and Oculus Quest either loose tracking or produce very inaccurate results. Taking into account the broad choice of suitable hardware (any RGB camera) and the ease of setup, our method can be directly applied to colocated multi-user VR scenarios.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reimer, D., Podkosova, I., Scherzer, D., &#38; Kaufmann, H. (2023). Evaluation and improvement of HMD-based and RGB-based hand tracking solutions in VR. <i>Frontiers in Virtual Reality</i>, <i>4</i>. https://doi.org/10.3389/frvir.2023.1169313</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "291104",
                    "name": "Reimer-2023-Frontiers in Virtual Reality-vor.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2548355,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/187704/2/Reimer-2023-Frontiers%20in%20Virtual%20Reality-vor.pdf"
                },
                {
                    "bsid": "291128",
                    "name": "Reimer-2023-Frontiers in Virtual Reality-vor.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 76070,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/187704/5/Reimer-2023-Frontiers%20in%20Virtual%20Reality-vor.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Dennis",
                    "last_name": "Reimer",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 2,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Daniel",
                    "last_name": "Scherzer",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193",
                "E193-03"
            ],
            "pid": "196969",
            "handle": "20.500.12708/189953",
            "doi": "10.34726/5241",
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "CoboDeck: a large-scale haptic VR system using a collaborative mobile robot",
            "keywords": [
                "Computer systems organization",
                "Haptic devices",
                "Interaction paradigms",
                "Robotics",
                "Embedded and cyberphysical systems",
                "Human computer interaction (HCI)",
                "Human-centered computing",
                "Interaction devices",
                "Virtual reality"
            ],
            "abstract": "We present CoboDeck - our proof-of-concept immersive virtual reality haptic system with free walking support. It provides prop-based encountered-type haptic feedback with a mobile robotic platform. Intended for use as a design tool for architects, it enables the user to directly and intuitively interact with virtual objects like walls, doors, or furniture. A collaborative robotic arm mounted on an omnidirectional mobile platform can present a physical prop that matches the position and orientation of a virtual counterpart anywhere in large virtual and real environments. We describe the concept, hardware, and software architecture of our system. Furthermore, we present the first behavioral algorithm tailored for the unique challenges of safe human-robot haptic interaction in VR, explicitly targeting availability and safety while the user is unaware of the robot and can change trajectory at any time. We explain our high-level state machine that controls the robot to follow a user closely and rapidly escape from him as required by the situation. We present our technical evaluation. The results suggest that our chasing approach saves time, decreases the travel distance and thus battery usage, compared to more traditional approaches for mobile platforms assuming a fixed parking position between interactions. We also show that the robot can escape from the user and prevent a possible collision within a mean time of 1.62 s. Finally, we confirm the validity of our approach in a practical validation and discuss the potential of the proposed system.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mortezapoor, S., Vasylevska, K., Vonach, E., &#38; Kaufmann, H. (2023). CoboDeck: a large-scale haptic VR system using a collaborative mobile robot. In <i>VR 2023. Proceedings. 2023 IEEE Conference Virtual Reality and 3D User Interfaces</i> (pp. 297–307). Institute of Electrical and Electronics Engineers (IEEE). https://doi.org/10.34726/5241</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "291352",
                    "name": "Mortezapoor-2023-CoboDeck A Large-Scale Haptic VR System Using a Collabora...-am.pdf",
                    "description": "© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
                    "type": "application/pdf",
                    "size": 18192451,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/189953/1/Mortezapoor-2023-CoboDeck%20A%20Large-Scale%20Haptic%20VR%20System%20Using%20a%20Collabora...-am.pdf"
                },
                {
                    "bsid": "297391",
                    "name": "Mortezapoor-2023-CoboDeck A Large-Scale Haptic VR System Using a Collabora...-am.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 82129,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/189953/6/Mortezapoor-2023-CoboDeck%20A%20Large-Scale%20Haptic%20VR%20System%20Using%20a%20Collabora...-am.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "position": 1,
                    "role": "Author",
                    "tid": "252414"
                },
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 2,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 3,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": [
                "1745908"
            ]
        },
        {
            "org_nrs": [
                "E104",
                "E104-03",
                "E193",
                "E193-03"
            ],
            "pid": "196991",
            "handle": "20.500.12708/189348",
            "doi": "10.34726/5145",
            "year": 2023,
            "issued": "2023-07",
            "issued_on": "2023-07-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Action-Origami Inspired Haptic Devices for Virtual Reality",
            "keywords": [
                "Origami",
                "Virtual Reality",
                "Haptic Feedback"
            ],
            "abstract": "Origami offers an innovative way to implement haptic interaction with minimum actuation, particularly in immersive encountered-type haptics and robotics. This paper presents two novel action-origami-inspired haptic devices for Virtual Reality (VR). The Zipper Flower Tube is a rigid-foldable origami structure that can provide different stiffness sensations to simulate the elastic response of a material. The Shiftly is a shape-shifting haptic display that employs origami to enable a real-time experience of different shapes and edges of virtual objects or the softness of materials. The modular approach of our action origami haptic devices provides a high-fidelity, energy-efficient and low-cost solution for interacting with virtual materials and objects in VR.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vasylevska, K., Batik, T., Brument, H., Sharifmoghaddam, K., Nawratil, G., Vonach, E., Mortezapoor, S., &#38; Kaufmann, H. (2023). Action-Origami Inspired Haptic Devices for Virtual Reality. In E. Brunvand &#38; M. Glencross (Eds.), <i>Proceedings. SIGGRAPH 2023 Emerging Technologies</i> (pp. 1–2). Association for Computing Machinery. https://doi.org/10.34726/5145</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "292312",
                    "name": "Vasylevska-2023-Action-Origami Inspired Haptic Devices for Virtual Reality-smur.pdf",
                    "description": "Author Copy",
                    "type": "application/pdf",
                    "size": 3093913,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/189348/1/Vasylevska-2023-Action-Origami%20Inspired%20Haptic%20Devices%20for%20Virtual%20Reality-smur.pdf"
                },
                {
                    "bsid": "295499",
                    "name": "Vasylevska-2023-Action-Origami Inspired Haptic Devices for Virtual Reality-smur.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 9647,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/189348/4/Vasylevska-2023-Action-Origami%20Inspired%20Haptic%20Devices%20for%20Virtual%20Reality-smur.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Tobias",
                    "last_name": "Batik",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "position": 3,
                    "role": "Author",
                    "tid": "359272"
                },
                {
                    "first_name": "Kiumars",
                    "last_name": "Sharifmoghaddam",
                    "position": 4,
                    "role": "Author",
                    "tid": "335518"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Nawratil",
                    "position": 5,
                    "role": "Author",
                    "tid": "53597"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 6,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "position": 7,
                    "role": "Author",
                    "tid": "252414"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 8,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Erik",
                    "last_name": "Brunvand",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Mashhuda",
                    "last_name": "Glencross",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": [
                "1745908"
            ]
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "198350",
            "handle": "20.500.12708/188593",
            "doi": "10.34726/hss.2023.102262",
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Digital (Micro-) Credentials for Teaching demonstrated by a prototype for the TU-Vienna",
            "keywords": [
                "Digital Credentials",
                "Blockchain Technologies"
            ],
            "abstract": "Digitalisation has advantageously impacted various sectors and industries in recent decades. However, despite the widespread integration of digital technologies, highereducation institutions (HEIs) still often rely on paper-based methods for the issuance of credentials, such as diplomas and certificates. This is despite the fact that digital credentials promise to be more secure, reduce costs through automation, and improve privacy.In this thesis, we built a digital credentialing prototype for TU Wien to demonstrate and evaluate digital credentials as a replacement for paper-based credentials. The implementation is based on work published by the Digital Credentials Consortium (DCC),a collaboration of universities that aim to build the digital credentialing infrastructure of the future. We extended and adapted their reference implementations to add missing features, improve trustability, and fulfil the requirements of TU Wien.The prototype itself consists of various heterogeneous services and libraries that together form the digital credentialing system. The chosen modular system architecture combined with the Verifiable Credentials Data Model standard by the W3C ensures flexibility,extendibility, and interoperability. Two added and integral parts of the prototype useblockchain technology to store critical trustable data publicly, such as credential statusinformation and trustable issuer identifiers. To interact with the prototype, each actor has its own specially tailored service. Issuers (e.g. universities) have a web interface to issue and update credentials; holders (e.g. students) have a wallet phone app to store and share their credentials; and verifiers (e.g. employers) have a command-line tool tocheck the validity of a digital credential.Then, a group of experts evaluated the developed prototype in the form of an expert discussion from a technical and an organisational perspective. The results indicated thatthe digital credentialing prototype improves security, enables new use cases, and allows automation, which can lead to a reduction in overheads and therefore cost. However, dueto the immaturity of current digital credential standards and a lack of adoption by otheractors, it is too early to adopt digital credentials for production use and as replacements for paper-based credentials.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schwarzhans, M. (2023). <i>Digital (Micro-) Credentials for Teaching demonstrated by a prototype for the TU-Vienna</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2023.102262</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "293465",
                    "name": "Schwarzhans Mathias - 2023 - Digital Micro- Credentials for Teaching...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1491400,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/188593/1/Schwarzhans%20Mathias%20-%202023%20-%20Digital%20Micro-%20Credentials%20for%20Teaching...pdf"
                },
                {
                    "bsid": "293544",
                    "name": "Schwarzhans Mathias - 2023 - Digital Micro- Credentials for Teaching...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 185114,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/188593/4/Schwarzhans%20Mathias%20-%202023%20-%20Digital%20Micro-%20Credentials%20for%20Teaching...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Mathias",
                    "last_name": "Schwarzhans",
                    "position": 1,
                    "role": "Author",
                    "tid": "303982"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "198802",
            "handle": "20.500.12708/189001",
            "doi": "10.34726/hss.2023.106180",
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Embodied Conversational Agents with Situation Awareness",
            "keywords": [
                "Virtual Agent",
                "Intelligent Embodied Agent",
                "Virtual Reality",
                "Artificial Intelligence",
                "Natural Language Processing"
            ],
            "abstract": "Virtual worlds offer unlimited possibilities for creating educational training scenarios in various domains. These worlds are often enriched with embodied agents to simulate human behavior in various interactions between human users and virtual agents. However, these agents usually only have limited knowledge and behavior and their communication skills are usually predefined or they are not able to communicate at all. In this thesis, we investigate the impact of embodied agents with conversational abilities and situation awareness in a first responder training scenario on human perception and performance. We present a novel solution to enabling situation awareness for embodied agents which allows them to capture and react to changes in their environment and their own state. The agents are capable of conveying this captured knowledge through full conversational capabilities by utilizing a combination of novel methods from NVIDIA for automatic speech recognition and speech synthesis and the industry proven conversational Artificial Intelligence (AI) Rasa. To evaluate our conversational agents, we conducted a between-groups user study with 24 participants in a Virtual Reality (VR) training application in the Unity game engine and investigated the differences between agents with full conversational capabilities and agents with scripted audio. During the study we measured several quantitative metrics including presence, co-presence, task performance, realism, learning outcome, information presentation, agents interaction and training duration as well as qualitative measurements in the form of open questions. While our quantitative results did not indicate significant differences in all measured metrics, we found a significant difference in favor of agents with full conversational capabilities in the metric co-presence. In addition, we discovered significant differences between genders in the metrics subjective task performance and training duration. Finally, we discussed user feedback on our conversational enabled agents and derived guidelines for future research and development of training applications with embodied conversational agents with situation awareness in VR from our qualitative results.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Rumpelnik, M. (2023). <i>Embodied Conversational Agents with Situation Awareness</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2023.106180</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "294391",
                    "name": "Rumpelnik Martin - 2023 - Embodied Conversational Agents with Situation...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 9422808,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/189001/1/Rumpelnik%20Martin%20-%202023%20-%20Embodied%20Conversational%20Agents%20with%20Situation...pdf"
                },
                {
                    "bsid": "294462",
                    "name": "Rumpelnik Martin - 2023 - Embodied Conversational Agents with Situation...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 186992,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/189001/4/Rumpelnik%20Martin%20-%202023%20-%20Embodied%20Conversational%20Agents%20with%20Situation...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Rumpelnik",
                    "position": 1,
                    "role": "Author",
                    "tid": "303530"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "231177"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "199002",
            "handle": "20.500.12708/189347",
            "doi": null,
            "year": 2023,
            "issued": "2023-10-16",
            "issued_on": "2023-10-16",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "A systematic evaluation of an RTK-GPS device for wearable augmented reality",
            "keywords": [
                "Augmented reality",
                "Outdoor tracking",
                "Real-time kinematic",
                "GPS"
            ],
            "abstract": "Global Positioning Satellite (GPS) systems sample points on the Earth’s surface with meter accuracy. Real-Time Kinematic (RTK) devices improve GPS performances by providing real-time correction data from ground stations, achieving centimeter accuracy. Reliable tracking approaches are essential for Augmented Reality (AR) applications, especially for outdoor scenarios, which still present unsolved challenges. AR handheld tracking capabilities have been greatly improved by integrating visual tracking approaches with RTK devices, whereas little is known about combining wearable AR interfaces with RTK systems. Although wearable AR devices are intrinsically designed for AR applications, their performance dramatically reduces in large outdoor areas, comprising the user experience. Hence, this paper provides a rigorous evaluation of a small-size RTK device that does not need any additional software integration to collect positional data. The main goal of the assessment is to verify whether its integration with a wearable AR device is advantageous or not. The evaluation has been performed considering both static and dynamic scenarios in open-sky and urban areas. The results show that the RTK device can achieve 1 cm accuracy when used in open-sky areas. In contrast, its accuracy dramatically reduces in the proximity of buildings and obstacles, showing average errors ranging from 76 to 2561%. Since wearable AR devices have an average accuracy of 2 cm, the outcomes indicate that RTK devices should be combined with wearable AR devices only when the RTK device is far from obstacles. On the contrary, the positional data should be completely avoided when barriers surround the RTK device.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">De Pace, F., &#38; Kaufmann, H. (2023). A systematic evaluation of an RTK-GPS device for wearable augmented reality. <i>Virtual Reality</i>. https://doi.org/10.1007/s10055-023-00863-3</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "294821",
                    "name": "De Pace-2023-Virtual Reality-vor.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3393905,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/189347/1/De%20Pace-2023-Virtual%20Reality-vor.pdf"
                },
                {
                    "bsid": "295480",
                    "name": "De Pace-2023-Virtual Reality-vor.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 60989,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/189347/4/De%20Pace-2023-Virtual%20Reality-vor.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Francesco",
                    "last_name": "De Pace",
                    "position": 1,
                    "role": "Author",
                    "tid": "357410"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": [
                "1923969"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "199077",
            "handle": "20.500.12708/190026",
            "doi": null,
            "year": 2023,
            "issued": "2023-10-13",
            "issued_on": "2023-10-13",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Joint Action in Collaborative Mixed Reality: Effects of Immersion Type and Physical Location",
            "keywords": [
                "Human-centered computing",
                "Mixed / augmented reality",
                "Joint Action",
                "Collaboration"
            ],
            "abstract": "Understanding how people effectively perform actions together is fundamental when designing Collaborative Mixed Reality (CMR) applications. While most of the studies on CMR mostly considered either how users are immersed in the CMR (e.g., in virtual or augmented reality), or how the physical workspace is shared by users (i.e., distributed or collocated), little is known about how their combination could influence user’s interaction in CMR. In this paper, we present a user study (n=23) that investigates the effect of the mixed reality setup on the user’s immersion and spatial interaction during a joint-action task. Groups of two participants had to perform two types of joint actions while carrying a virtual rope to maintain a certain distance: (1) Gate, where participants had to pass through a virtual aperture together and (2) Fruit, where participants had to use a rope to slice a virtual fruit moving in the CMR. Users were either in a distributed or collocated setup, and either immersed in virtual or augmented reality. Our results showed that users’ proxemics was altered by the immersion type and location setup, but also the user’s subjective experience. These results contribute to the understanding of joint action in CMR and they are discussed to improve the design of CMR applications.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Podkosova, I., De Pace, F., &#38; Brument, H. (2023). Joint Action in Collaborative Mixed Reality: Effects of Immersion Type and Physical Location. In T. Huang, M. Sra, F. Argelaguet, P. Lopes, &#38; M. D. Barrera Machuca (Eds.), <i>Proceedings SUI 2023 ACM : Symposium on Spatial User Interaction</i>. Association for Computing Machinery. https://doi.org/10.1145/3607822.3614541</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "294967",
                    "name": "Podkosova-2023-Joint Action in Collaborative Mixed Reality Effects of Imm...-vor.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2993679,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/190026/1/Podkosova-2023-Joint%20Action%20in%20Collaborative%20Mixed%20Reality%20Effects%20of%20Imm...-vor.pdf"
                },
                {
                    "bsid": "297624",
                    "name": "Podkosova-2023-Joint Action in Collaborative Mixed Reality Effects of Imm...-vor.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 75497,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/190026/6/Podkosova-2023-Joint%20Action%20in%20Collaborative%20Mixed%20Reality%20Effects%20of%20Imm...-vor.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Francesco",
                    "last_name": "De Pace",
                    "position": 2,
                    "role": "Author",
                    "tid": "357410"
                },
                {
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "position": 3,
                    "role": "Author",
                    "tid": "359272"
                },
                {
                    "first_name": "Tony",
                    "last_name": "Huang",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Misha",
                    "last_name": "Sra",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Ferran",
                    "last_name": "Argelaguet",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Lopes",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Mayra Donaji",
                    "last_name": "Barrera Machuca",
                    "position": 5,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "199080",
            "handle": "20.500.12708/191419",
            "doi": null,
            "year": 2022,
            "issued": "2022",
            "issued_on": "2022-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Comparison of Three Different NeuroTag Visualization Media: Brain Visual Stimuli by Monitor, Augmented and Virtual Reality Devices",
            "keywords": [
                "Brain Computer Interface",
                "NextMind",
                "Virtual reality",
                "Augmented reality"
            ],
            "abstract": "Brain Computer Interfaces (BCIs) proved to overcome some limitations of other input modes (e.g., gestures, voice, haptic, etc.). BCIs are able to detect the brain activity, thus identifying searched patterns. When a specific brain activity is detected, a well-defined action can be triggered, thus implementing a human-machine interaction paradigm. BCIs can be used in different domains ranging from industry to services for impaired people.Small and ergonomics devices, such as the NextMind (https://www.next-mind.com/) are the result of recent technological advances; these new devices allow to support users in everyday life, thus bringing the design of BCIs into a new dimension well beyond the scope of laboratory tests.In particular, The NextMind is a device able to detect and classify signals coming from the visual cortex. Visual stimuli are blinking/flickering textures that are associated with objects called NeuroTags (see Figure 1). An event is triggered when the user focuses on the same NeuroTag for a given amount of time. This paradigm can replace selection methods based on keyboard, mouse, gesture, touch, voice, and gaze.This paper compares and assesses three different interfaces that share the same input device (the NextMind) to detect the brain activity and differ in the medium to convey to the user the visual stimuli. A monitor, an Augmented Reality (AR) device (the Microsoft HoloLens), and a Virtual Reality (VR) device (the Oculus Rift) are considered. The aim of this work is to assess any difference in the three visualization media when displaying NeuroTags. User tests have been performed in order to evaluate the usability of the three different solutions. After each test, users were asked for filling out the System Usability Scale (SUS) questionnaire and the SUS scores have been used for statistical analysis.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Manuri, F., Sanna, A., Bosco, M., &#38; De Pace, F. (2022). A Comparison of Three Different NeuroTag Visualization Media: Brain Visual Stimuli by Monitor, Augmented and Virtual Reality Devices. In T. Ahram &#38; R. Taiar (Eds.), <i>Human Interaction &#38; Emerging Technologies (IHIET 2022): Artificial Intelligence &#38; Future Applications : Proceedings of the 8th International Conference on Human Interaction &#38; Emerging Technologies</i> (pp. 150–158). AHFE International. https://doi.org/10.54941/ahfe1002726</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "294975",
                    "name": "Manuri-2022-A Comparison of Three Different NeuroTag Visualization Media ...-vor.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 689948,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/191419/1/Manuri-2022-A%20Comparison%20of%20Three%20Different%20NeuroTag%20Visualization%20Media%20...-vor.pdf"
                },
                {
                    "bsid": "301815",
                    "name": "Manuri-2022-A Comparison of Three Different NeuroTag Visualization Media ...-vor.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 23206,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/191419/6/Manuri-2022-A%20Comparison%20of%20Three%20Different%20NeuroTag%20Visualization%20Media%20...-vor.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Federico",
                    "last_name": "Manuri",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Andrea",
                    "last_name": "Sanna",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Matteo",
                    "last_name": "Bosco",
                    "position": 3,
                    "role": "Author",
                    "tid": "368723"
                },
                {
                    "first_name": "Francesco",
                    "last_name": "De Pace",
                    "position": 4,
                    "role": "Author",
                    "tid": "357410"
                },
                {
                    "first_name": "Tareq",
                    "last_name": "Ahram",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Redha",
                    "last_name": "Taiar",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "199165",
            "handle": "20.500.12708/189279",
            "doi": "10.34726/hss.2023.54624",
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Tracking Filter Framework",
            "keywords": [
                "Double Exponential Filter",
                "Kalman Filter",
                "Virtual Reality",
                "Optical Tracking",
                "Inertial Tracking",
                "libsurvive",
                "Unreal Engine",
                "SteamVR",
                "Lighthouse"
            ],
            "abstract": "Virtual reality is a computer simulation wherein a system tracks the user’s pose and replaces sensory feedback for one or more senses to place the user into a Virtual environment (VE). In order to immerse users into this environment, the system needs to be accurate and fast in capturing their actions and feed their representations back to the senses. Measurement and pose estimation errors are a common problem for such systems but can be mitigated through the use of filter algorithms. This work documents the design and implementation of a Tracking Filter Framework (TFF), and evaluates its ability to reduce tracking errors and enhance the user experience. The TFF applies filter algorithms to tracking data and provides the result as an output. The Lighthouse Tracking System (LHTS) with Valve’s Index is used as tracking data source, it supports six Degrees Of Freedom (DOF) to track the user’s pose and uses optical and inertial tracking. The experimental library libsurvive is used to access the inertial tracking data. The Double Exponential Smoothed Prediction (DESP), a double exponential filter, and the Error-state Kalman Filter (ESKF), an error-state Kalman filter capable of fusing optical and inertial data, are introduced. The user acceptance of the framework is evaluated by conducting a user study within a VE, using a within-subject design. The results show that the provided tracking data by libsurvive with disabled optimizations turned out to be too noisy and unstable for the introduced filters. The filters cannot compensate for the occurring tracking errors to the degree that would have been necessary for a Virtual Reality Application (VRA). The DESP causes a significant delay when trying to filter noisy tracking data, which is not acceptable for a VRA. The ESKF provides a significant improvement with simulated tracking data. However, it falls short with libsurvive tracking data because of its noisy and unstable nature, as the results of the user study show.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gavornik, M. (2023). <i>Tracking Filter Framework</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2023.54624</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "295204",
                    "name": "Gavornik Manuel - 2023 - Tracking Filter Framework.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3163823,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/189279/1/Gavornik%20Manuel%20-%202023%20-%20Tracking%20Filter%20Framework.pdf"
                },
                {
                    "bsid": "295245",
                    "name": "Gavornik Manuel - 2023 - Tracking Filter Framework.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 260740,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/189279/4/Gavornik%20Manuel%20-%202023%20-%20Tracking%20Filter%20Framework.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Manuel",
                    "last_name": "Gavornik",
                    "position": 1,
                    "role": "Author",
                    "tid": "46107"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "200042",
            "handle": "20.500.12708/190035",
            "doi": null,
            "year": 2023,
            "issued": "2023-11-07",
            "issued_on": "2023-11-07",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Proceedings of the 5th International Workshop on Reading Music Systems",
            "keywords": [
                "Computer Vision and Pattern Recognition",
                "Optical Music Recognition",
                "Deep Learning"
            ],
            "abstract": "The International Workshop on Reading Music Systems (WoRMS) is a workshop that tries to connect researchers who develop systems for reading music, such as in the field of Optical Music Recognition, with other researchers and practitioners that could benefit from such systems, like librarians or musicologists. The relevant topics of interest for the workshop include, but are not limited to: Music reading systems; Optical music recognition; Datasets and performance evaluation; Image processing on music scores; Writer identification; Authoring, editing, storing and presentation systems for music scores; Multi-modal systems; Novel input-methods for music to produce written music; Web-based Music Information Retrieval services; Applications and projects; Use-cases related to written music. These are the proceedings of the 5th International Workshop on Reading Music Systems, held in Milan, Italy on Nov. 4th 2023.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Calvo-Zaragoza, J., Pacha, A., &#38; Shatri, E. (Eds.). (2023). <i>Proceedings of the 5th International Workshop on Reading Music Systems</i>. https://doi.org/10.48550/arXiv.2311.04091</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "296586",
                    "name": "Calvo-Zaragoza-2023-Proceedings of the 5th International Workshop on Read...-vor.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 7559667,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/190035/1/Calvo-Zaragoza-2023-Proceedings%20of%20the%205th%20International%20Workshop%20on%20Read...-vor.pdf"
                },
                {
                    "bsid": "297634",
                    "name": "Calvo-Zaragoza-2023-Proceedings of the 5th International Workshop on Read...-vor.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 203151,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/190035/4/Calvo-Zaragoza-2023-Proceedings%20of%20the%205th%20International%20Workshop%20on%20Read...-vor.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Jorge",
                    "last_name": "Calvo-Zaragoza",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Alexander",
                    "last_name": "Pacha",
                    "position": 2,
                    "role": "Editor",
                    "tid": "68727"
                },
                {
                    "first_name": "Elona",
                    "last_name": "Shatri",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "200288",
            "handle": "20.500.12708/190039",
            "doi": null,
            "year": 2023,
            "issued": "2023-08-01",
            "issued_on": "2023-08-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Temporal Alignment of Human Motion Data: A Geometric Point of View",
            "keywords": [
                "Dynamic Time Warping",
                "Geometric Green Learning",
                "keyframe correspondence"
            ],
            "abstract": "Temporal alignment is an inherent task in most applications dealing with videos: action recognition, motion transfer, virtual trainers, rehabilitation, etc. In this paper we dive into the understanding of this task from a geometric point of view: in particular, we show that the basic properties that are expected from a temporal alignment procedure imply that the set of aligned motions to a template form a slice to a principal fiber bundle for the group of temporal reparameterizations. A temporal alignment procedure provides a reparameterization invariant projection onto this particular slice. This geometric presentation allows us to elaborate a consistency check for testing the accuracy of any temporal alignment procedure. We apply this consistency check to some alignment procedures from the literature based on dynamic programming for the task of aligning motions of tennis players. The comparison of the obtained results leads us to propose a version of dynamic programming that incorporates keyframe correspondences. The temporal alignment procedures produced are not only more accurate, but also computationally more efficient.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Tumpach, A. B., &#38; Kán, P. (2023). Temporal Alignment of Human Motion Data: A Geometric Point of View. In F. Nielsen &#38; F. Barbaresco (Eds.), <i>Geometric Science of Information : 6th International Conference, GSI 2023, St. Malo, France, August 30 – September 1, 2023, Proceedings, Part II</i> (pp. 541–550). Springer. https://doi.org/10.1007/978-3-031-38299-4_56</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Alice Barbora",
                    "last_name": "Tumpach",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 2,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Frank",
                    "last_name": "Nielsen",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Frédéric",
                    "last_name": "Barbaresco",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": [
                "1745908"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "200991",
            "handle": "20.500.12708/190610",
            "doi": null,
            "year": 2023,
            "issued": "2023-05-01",
            "issued_on": "2023-05-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Creating Informal Learning and First Responder Training XR Experiences with the ImmersiveDeck",
            "keywords": [
                "Human-centered computing",
                "First responder training",
                "Informal Learning",
                "extended reality (XR)",
                "Visualization design and evaluation methods"
            ],
            "abstract": "In recent years eXtended Reality (XR) technologies have matured and have become affordable, yet creating XR experiences for training and learning in many cases is still a time-consuming and costly process, hindering widespread adoption. One factor driving effort is that content and features commonly required by many applications get re-implemented for each experience, instead of sharing and reusing these resources by means of a common platform. In this paper we present two XR experiences in the context of informal learning and first responder training along with the shared platform they have been created with and the creation process. Furthermore, we have technically evaluated relevant parts of the platform for feasibility of use with experience requirements and confirmed ap-plicability. Finally, we present an informal expert evaluation of the content creation process's user experience for the informal learning experience along with guidelines derived from the findings.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., Kaufmann, H., Roussou, M., Rüggeberg, J., Rüggeberg, J., Katsikaris, L., Rogkas, S., &#38; Christopoulos, D. (2023). Creating Informal Learning and First Responder Training XR Experiences with the ImmersiveDeck. In <i>2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops</i> (pp. 53–60). IEEE. https://doi.org/10.1109/VRW58643.2023.00016</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Maria",
                    "last_name": "Roussou",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Julien",
                    "last_name": "Rüggeberg",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Jim",
                    "last_name": "Rüggeberg",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Lucas",
                    "last_name": "Katsikaris",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Sakis",
                    "last_name": "Rogkas",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Dimitris",
                    "last_name": "Christopoulos",
                    "position": 8,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": [
                "1875977"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "201183",
            "handle": "20.500.12708/191421",
            "doi": null,
            "year": 2023,
            "issued": "2023-12",
            "issued_on": "2023-12-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Embodied Conversational Agents with Situation Awareness for Training in Virtual Reality",
            "keywords": [
                "Virtual Reality",
                "Conversational Agents",
                "User Study"
            ],
            "abstract": "Embodied conversational agents have a great potential in virtual reality training applications. This paper investigates the impact of conversational agents on users in a first responder training scenario. We integrated methods for automatic speech recognition and speech synthesis with natural language processing into a VR training application in the Unity game engine. Additionally, we present a method for enabling situation awareness for agents in a virtual environment. Finally, we conducted a between-subject lab experiment with 24 participants which investigated  differences between conversational agents and agents with pre-scripted audio. Several metrics were measured in the experiment including presence, subjective task performance, learning outcome, interaction quality, quality of information presentation, perceived realism, co-presence, and training task duration. Our results suggest that users trying our conversational agents condition experienced significantly higher level of co-presence than users with pre-scripted audio. Additionally, significant differences in subjective task performance and training duration were discovered between genders. Based on the results of our qualitative analysis, we provide guidelines that can facilitate future design of VR training applications and research studies with embodied conversational agents.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kan, P., Rumpelnik, M., &#38; Kaufmann, H. (2023). Embodied Conversational Agents with Situation Awareness for Training in Virtual Reality. In <i>ICAT-EGVE 2023 - International Conference on Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments</i>. ICAT-EGVE 2023 - International Conference on Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments, Dublin, Ireland. Eurographics Association. https://doi.org/10.2312/egve.20231310</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "301723",
                    "name": "Kan-2023-Embodied Conversational Agents with Situation Awareness for Trai...-vor.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2117314,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/191421/2/Kan-2023-Embodied%20Conversational%20Agents%20with%20Situation%20Awareness%20for%20Trai...-vor.pdf"
                },
                {
                    "bsid": "301828",
                    "name": "Kan-2023-Embodied Conversational Agents with Situation Awareness for Trai...-vor.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 58354,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/191421/5/Kan-2023-Embodied%20Conversational%20Agents%20with%20Situation%20Awareness%20for%20Trai...-vor.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Rumpelnik",
                    "position": 2,
                    "role": "Author",
                    "tid": "303530"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": [
                "1745908",
                "2232316"
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "2014",
            "handle": "20.500.12708/2030",
            "doi": "10.34726/hss.2018.60860",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Walkable multi-user VR : the effects of physical and virtual colocation",
            "keywords": [
                "Virtual Reality",
                "Multi-User VR",
                "User Studies",
                "Walking Navigation"
            ],
            "abstract": "The research presented in this dissertation focuses on multi-user VR, where multiple immersed users navigate the virtual world by physically walking in a large tracking area. In such a setup, different combinations of user colocation within the physical and the virtual space are possible. We consider a setup to be multi-user if at least one of these two spaces is shared. The dissertation starts with the classification of combinations of physical and virtual colocation. Four such combinations are defined: colocated shared VR, colocated non-shared VR, distributed shared VR and shared VR with mixed colocation. The characteristics of each of these four setups are discussed and the resulting problems and research questions outlined. The dissertation continues with the description of ImmersiveDeck - a large-scale multi-user VR platform that enables navigation by walking and natural interaction. Then, four experiments on multiuser walkable VR developed with the use of ImmersiveDeck are described. The first two experiments are set in colocated non-shared VR where walking users share a tracking space while being immersed into separate virtual worlds. We investigate users mutual awareness in this setup and explore methods of preventing mutual collisions between walking users. The following two experiments study shared VR scenarios in situations of varied physical colocation. We investigate the effects that different modes of physical colocation have on locomotion, collision avoidance and proxemics patterns exhibited by walking users. The sense of copresence and social presence within the virtual world reported by users is investigated as well.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Podkosova, I. (2018). <i>Walkable multi-user VR : the effects of physical and virtual colocation</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.60860</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "4047",
                    "name": "Podkosova Iana - 2018 - Walkable multi-user VR the effects of physical and...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 41850848,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2030/2/Podkosova%20Iana%20-%202018%20-%20Walkable%20multi-user%20VR%20the%20effects%20of%20physical%20and...pdf"
                },
                {
                    "bsid": "74805",
                    "name": "Podkosova Iana - 2018 - Walkable multi-user VR the effects of physical and...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 349397,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2030/5/Podkosova%20Iana%20-%202018%20-%20Walkable%20multi-user%20VR%20the%20effects%20of%20physical%20and...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Ferschin",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "135460"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "20143",
            "handle": "20.500.12708/18653",
            "doi": "10.34726/hss.2021.80341",
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Using natural language processing to measure the consistency of opinions expressed by politicians",
            "keywords": [
                "Sentiment Analysis",
                "Natural Language Processing",
                "Opinion Mining"
            ],
            "abstract": "This experimental study implements a solution for extracting opinions from written text with the help of supervised machine learning methods to visualize their consistencyover time. We examine the practical feasibility and the usefulness of the implemented approach.We gathered speech transcripts of the Austrian Parliament to create two datasets on topics concerning measures against the spread of the Coronavirus. We split the raw text around sentence boundaries into dataset records and used a keyword search to select relevant sentences. Then, we manually assigned opinion labels and used two statistical machine learning algorithms and three deep learning models to predict the labels. We used Monte Carlo cross-validation to evaluate classification performance. Subsequently,we used the predictions of the best-performing algorithm to plot the general sentiment stoward the topic and the consistencies of expressed opinions over time.On the larger dataset (around 5000 records), a BERT network achieved the best accuracy (70%), followed by an LSTM network (68%), an MNB classifier (67%), a Bag-of-Wordsnetwork (62%), and a BM25 document ranking classifier (42%). On the smaller dataset (around 500 records), BERT also performed best (56%), followed by the MNB (53%), theLSTM (51%), the BM25 approach (47%), and the Bag-of-Words network (42%). The biggest challenge to practical feasibility was the manual annotation effort and choosing a topic for which enough training samples are available. Thus, the approach is best suited if the intention is to monitor a small selection of topics over a long period. We showed that the usefulness of the predicted opinion consistency values depends on the accuracy of the underlying opinion predictions. By comparing the graphs from actual opinion data to graphs of predicted data, we gathered that a model with 70% accuracy is sufficient to produce a representative impression of the overall sentiment towards a topic. On the other hand, visualizing the consistency of opinions requires a higher classification accuracy to be useful.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaruba, S. (2021). <i>Using natural language processing to measure the consistency of opinions expressed by politicians</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2021.80341</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "80567",
                    "name": "Zaruba Stefan - 2021 - Using natural language processing to measure the...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2629453,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18653/1/Zaruba%20Stefan%20-%202021%20-%20Using%20natural%20language%20processing%20to%20measure%20the...pdf"
                },
                {
                    "bsid": "94219",
                    "name": "Zaruba Stefan - 2021 - Using Natural Language Processing to Measure the...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 249843,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18653/4/Zaruba%20Stefan%20-%202021%20-%20Using%20Natural%20Language%20Processing%20to%20Measure%20the...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Stefan",
                    "last_name": "Zaruba",
                    "position": 1,
                    "role": "Author",
                    "tid": "264799"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "2017",
            "handle": "20.500.12708/2033",
            "doi": "10.34726/hss.2019.63126",
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "An EEG- and ERP-based image ranking application",
            "keywords": [
                "EEG",
                "BCI",
                "Event-Related Potential",
                "P300",
                "Interstimulus Interval"
            ],
            "abstract": "An electroencephalogram (EEG) is a measurement to record the electrical potentials in the brain, also referred to as brain activity. For some time now, researchers use this EEG signal to create a brain-computer interfaces (BCI), which allows users to manipulate the computer just with their thoughts. This application ranks images of a picture set via their elicit responses in the EEG signal. The goal of this thesis is to check the functionality of the image ranking application with 2 different BCI devices and optimize the application run for image ranking. The first part includes criteria about signal processing, e.g. sample rate. Here, the timing in which the EEG signal is sampled and sent to the application is analysed. After, the reliability of the recorded EEG signal the interstimulus interval (ISI) can be optimized. The ISI consist of three parameters: the time an image is displayed on the screen, the time between two images and the number of times each image (flashes) has to be shown. This three parameters have to be tuned in a way that the accuracy is increased and the time for one application run is decreased. Additionally, a ranking with different subjects should be created to depict if certain images are always ranked in the first few positions and are independent of the subject. The results show that the interaction between application and the two BCI devices work as expected, short of some minor issues. Thus, the applicability of the application was given and the tuning of the ISI parameters could be started. For the ISI the best accuracy was achieved when the images were displayed on the screen for 100 ms and 75 ms was used between two images. Furthermore, each image was flashed 20 or 5 times for the classifier or ranking, respectively. For the group rankings, the results indicated, that images of faces rank in average higher than the rest. Finally, the median of all rankings for one image should be used as the ranking parameter.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Adelberger, P. (2019). <i>An EEG- and ERP-based image ranking application</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2019.63126</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "4053",
                    "name": "Adelberger Patrick - 2019 - An EEG- and ERP-based image ranking application.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 15419130,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2033/2/Adelberger%20Patrick%20-%202019%20-%20An%20EEG-%20and%20ERP-based%20image%20ranking%20application.pdf"
                },
                {
                    "bsid": "74152",
                    "name": "Adelberger Patrick - 2019 - An EEG- and ERP-based image ranking application.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 109730,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2033/5/Adelberger%20Patrick%20-%202019%20-%20An%20EEG-%20and%20ERP-based%20image%20ranking%20application.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Patrick",
                    "last_name": "Adelberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "245809"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "2021",
            "handle": "20.500.12708/2037",
            "doi": "10.34726/hss.2017.47659",
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Mapping of realism in rendering onto perception of presence in augmented reality",
            "keywords": [
                "Augmented Reality",
                "Presence",
                "Virtual Avatars",
                "Uncanny Valley"
            ],
            "abstract": "Augmented Reality (AR) is about seamless integration of virtual computer-generated objects into the real-world view. Ideally, virtual objects should blend into the real world so that the user feels like the virtual objects are \"here\". The sense of something being ''here'' is also known as the concept of presence. Presence is especially important if an AR application is using virtual humans to interact with the user. This thesis examines if the visual realism is essential to achieve the highest possible presence in an AR application. Two hypotheses were posed to examine the effect of realism on the perception of presence and the convenience of users within AR applications. H1: ''Increasing the level of realism increases the sense of presence and convenience of users.''. H2: ''The Uncanny Valley effect can be observed within the experiment.''. The approach of this thesis to examine these hypotheses was to conduct a user study in which the participants experienced a virtual human with a specific visual realism levels. Each visual realism level differs in geometry, texture and lights. The developed AR application included a rendering system which allows the levels of realism of the virtual human to be set. The results partially supported the first hypothesis (H1) and indicated that visual realism is an important factor to achieve a higher sense of presence within an AR application. The second hypothesis (H2) was not supported, most probably due to technical limitations which did not allow such a realistic virtual representation of a human so that the participant would believe it could be a real person. The main novelty of this thesis is its focus on the presence of virtual humans within AR. Recent studies showed that the influence of visual realism on the sense of presence if different in the field of AR than in VR. Future presence demanding AR application can take the results of this thesis of basis to achieve a higher sense of presence. Especially, if virtual humans are used to interact with users.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schüller-Reichl, D. (2017). <i>Mapping of realism in rendering onto perception of presence in augmented reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2017.47659</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "4061",
                    "name": "Schueller-Reichl David - 2017 - Mapping of realism in rendering onto perception...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4232204,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2037/2/Schueller-Reichl%20David%20-%202017%20-%20Mapping%20of%20realism%20in%20rendering%20onto%20perception...pdf"
                },
                {
                    "bsid": "75271",
                    "name": "Schueller-Reichl David - 2017 - Mapping of realism in rendering onto perception...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 157856,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2037/5/Schueller-Reichl%20David%20-%202017%20-%20Mapping%20of%20realism%20in%20rendering%20onto%20perception...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "David",
                    "last_name": "Schüller-Reichl",
                    "position": 1,
                    "role": "Author",
                    "tid": "62392"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E210-01",
                "E210-03"
            ],
            "pid": "202317",
            "handle": "20.500.12708/191383",
            "doi": null,
            "year": 2023,
            "issued": "2023-12-04",
            "issued_on": "2023-12-04",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluation of Virtual Reality for Early-Stage Structure and Production Planning for Industrial Buildings",
            "keywords": [
                "Virtual Reality",
                "Building Information Modeling",
                "Human-Computer Interaction (HCI)",
                "User Evaluation",
                "Multi-User"
            ],
            "abstract": "Efficient communication between stakeholders is key to successful project planning in the AEC industry. The workflow proposed in the research project BIMFlexi enables interdisciplinary exchange already at the early stages of industrial building planning by using Building Information Modeling for integral design of the future production and the building structure. Communication in BIMFlexi is supported by a Virtual Reality platform, BIMFlexi-VR, that provides a multi-user virtual environment in which architects, structural engineers, production planners and production owners can collaboratively change parameters of the planned building and see the resulting changes in the building geometry and various fitness metrics in real-time. This paper discusses the results of the evaluation of BIMFlexi-VR by interdisciplinary teams of professionals from the AEC industry, which identified a strong potential of our platform for interdisciplinary presentation and communication.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Podkosova, I., Reisinger, J., Zahlbruckner, M. A., Kovacic, I., &#38; Kaufmann, H. (2023). Evaluation of Virtual Reality for Early-Stage Structure and Production Planning for Industrial Buildings. In <i>2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</i> (pp. 159–166). IEEE. https://doi.org/10.1109/ISMAR-Adjunct60411.2023.00040</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Julia",
                    "last_name": "Reisinger",
                    "position": 2,
                    "role": "Author",
                    "tid": "230671"
                },
                {
                    "first_name": "Maria Antonia",
                    "last_name": "Zahlbruckner",
                    "position": 3,
                    "role": "Author",
                    "tid": "324294"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 4,
                    "role": "Author",
                    "tid": "42543"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 5,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E057-16",
                "E193-03",
                "E259-01"
            ],
            "pid": "202567",
            "handle": "20.500.12708/191533",
            "doi": null,
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "MR.Sketch. Immediate 3D Sketching via Mixed Reality Drawing Canvases",
            "keywords": [
                "Applied computing",
                "Architecture (buildings)",
                "Arts and humanities",
                "Human-centered computing",
                "Interaction paradigms",
                "Mixed / augmented reality"
            ],
            "abstract": "Sketching is a fundamental technique for early design and form finding. Digital 3D sketching can improve the early design phase by improving spatial understanding and enriching the design with additional information. However, the tools used for sketching should not hinder the expression and thought process inherent in form finding. Methods already exist for using 2D pen-on-tablet input for 3D sketching via stroke projection onto 3D drawing canvases. However, positioning the canvas and sketching lines are separate work steps. This breaks the flow of the designer's thought process. We propose a novel technique for mixed reality 3D sketching that involves the use of viewport-attached drawing canvases, spatial meshing and intersection canvas visualisation. By combining the inside-out tracking capabilities of current portable consumer devices with stylus-on-tablet freehand drawing input, we transform 2D to 3D projective sketching into a more seamless experience. Results of a pilot user study with 16 participants show significant user preference for our technique, as well as increased sketching speed and immediacy.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kovacs, B. I., Erb, I., Kaufmann, H., &#38; Ferschin, P. (2023). MR.Sketch. Immediate 3D Sketching via Mixed Reality Drawing Canvases. In <i>Proceedings of the 2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</i> (pp. 10–19). https://doi.org/10.1109/ISMAR59233.2023.00015</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Balint Istvan",
                    "last_name": "Kovacs",
                    "position": 1,
                    "role": "Author",
                    "tid": "262858"
                },
                {
                    "first_name": "Ingrid",
                    "last_name": "Erb",
                    "position": 2,
                    "role": "Author",
                    "tid": "251601"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Ferschin",
                    "position": 4,
                    "role": "Author",
                    "tid": "135460"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "20259",
            "handle": "20.500.12708/18679",
            "doi": "10.34726/hss.2021.91667",
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Design and development of an immersive collaborative geographical environment for tactical decision-making",
            "keywords": [
                "Virtual Reality (VR)",
                "Geographic Information System (GIS)",
                "Collaborative Virtual Environment (CVE)",
                "Staff Training"
            ],
            "abstract": "Planning tactical and strategical military operations is a well-structured and complex process and involves interdisciplinary expertise. A crucial part of the decision-making process of planning tasks that contains geographical data, is familiarizing professionals with the terrain and surrounding infrastructure. This is commonly operated by analog planning tools such as terrain models, sand tables, and standard 2D paper maps. The shortcoming of traditional equipment for tactical analysis is the lack of intuitive transfer of spatial relationships and geographical structures for visibility tasks and height judgment of ground elements. Immersive Virtual Geographical Environments (VGEs) provide advantageous perspectives for rapid decision-making and reasoning in spatial structures. 2D displays offer a 2D impression of a reduced 3D environment, while immersive displays transfer true depth information. Further, digital planning tools support remote collaboration using a virtual task space for the planning process. A virtual task space has several benefits compared to analog equivalents, such as the option to save planning states, visualize a common mental concept, no physical boundaries, and increased engagement.In this thesis, we set out to investigate how to utilize the benefits of immersive virtual spaces for tactical planning and decision-making in the context of a military staff exercise, to overcome the limitations of current analog and 2D digital planning tools. We design and implement a collaborative Virtual Reality (VR) prototype based on requirements derived from observations of an on-site military staff training and unstructured interviews with consultants from the Austrian Institute for Military Geography (IMG). The key contribution of this thesis is the design, implementation, and evaluation of a VR prototype that supports the decision-making and mission planning for officers in training and commanders during a military staff training. The development of our prototype represents a case study for comparable planning tasks in other domains, such as space missions or disaster prevention management.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schlager, B. (2021). <i>Design and development of an immersive collaborative geographical environment for tactical decision-making</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2021.91667</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "80755",
                    "name": "Schlager Bettina - 2021 - Design and development of an immersive collaborative...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 12703717,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18679/1/Schlager%20Bettina%20-%202021%20-%20Design%20and%20development%20of%20an%20immersive%20collaborative...pdf"
                },
                {
                    "bsid": "94224",
                    "name": "Schlager Bettina - 2021 - Design and Development of an Immersive Collaborative...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 224231,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18679/5/Schlager%20Bettina%20-%202021%20-%20Design%20and%20Development%20of%20an%20Immersive%20Collaborative...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Bettina",
                    "last_name": "Schlager",
                    "position": 1,
                    "role": "Author",
                    "tid": "275773"
                },
                {
                    "first_name": "Anton",
                    "last_name": "Fuhrmann",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "132838"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "203368",
            "handle": "20.500.12708/193750",
            "doi": null,
            "year": 2023,
            "issued": "2023-12",
            "issued_on": "2023-12-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Ownership Estimation for Tracked Hands in a Colocated VR Environment",
            "keywords": [
                "Virtual reality",
                "Mixed / augmented reality"
            ],
            "abstract": "Hand tracking systems play a crucial role in virtual reality (VR) applications, typically focusing on tracking the hands of the user who is using the system. Consequently, most existing systems are designed to track a maximum of two hands simultaneously. However, in certain colocated multi-user VR scenarios, it becomes necessary to track more than two hands simultaneously, such as to eliminate blind spots in individual tracking systems. In such scenarios, accurately assigning the tracked hands to the corresponding users using only the hand locations relative to the users becomes essential. This paper introduces and evaluates various methods for efficiently assigning hands to users in such scenarios. Additionally, we propose an algorithm that leverages past assignments to enhance the robustness and effectiveness of future assignments. Our experimental results demonstrate that this algorithm significantly improves upon existing methods. Furthermore, when combined with an assignment algorithm based on reinforcement learning AI agents, we achieve a remarkable 99% accuracy in hand assignments. As a result, we present an assignment algorithm specifically tailored for colocated VR scenarios, utilizing only the hand and user locations within the scene, making it directly applicable in the aforementioned contexts.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reimer, D., Scherzer, D., &#38; Kaufmann, H. (2023). Ownership Estimation for Tracked Hands in a Colocated VR Environment. In J.-M. Normand, M. Sugimoto, &#38; V. Sundstedt (Eds.), <i>ICAT-EGVE 2023 - International Conference on Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments</i> (pp. 105–114). Eurographics Association. https://doi.org/10.2312/egve.20231318</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "301600",
                    "name": "Reimer-2023-Ownership Estimation for Tracked Hands in a Colocated VR Envi...-vor.pdf",
                    "description": "Main Paper",
                    "type": "application/pdf",
                    "size": 668168,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/193750/1/Reimer-2023-Ownership%20Estimation%20for%20Tracked%20Hands%20in%20a%20Colocated%20VR%20Envi...-vor.pdf"
                },
                {
                    "bsid": "301601",
                    "name": "tables-and-figures-203368.pdf",
                    "description": "Supplementary Material",
                    "type": "application/pdf",
                    "size": 1032179,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/193750/2/tables-and-figures-203368.pdf"
                },
                {
                    "bsid": "305604",
                    "name": "Reimer-2023-Ownership Estimation for Tracked Hands in a Colocated VR Envi...-vor.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 55792,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/193750/5/Reimer-2023-Ownership%20Estimation%20for%20Tracked%20Hands%20in%20a%20Colocated%20VR%20Envi...-vor.pdf.txt"
                },
                {
                    "bsid": "305606",
                    "name": "tables-and-figures-203368.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 2534,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/193750/7/tables-and-figures-203368.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Dennis",
                    "last_name": "Reimer",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Daniel",
                    "last_name": "Scherzer",
                    "position": 2,
                    "role": "Author",
                    "tid": "50997"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Jean-Marie",
                    "last_name": "Normand",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Maki",
                    "last_name": "Sugimoto",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Veronica",
                    "last_name": "Sundstedt",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "205357",
            "handle": "20.500.12708/192387",
            "doi": "10.34726/hss.2023.106787",
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Low-cost Motion Capture Suit using Inertial Sensors",
            "keywords": [
                "tracking",
                "virtual reality",
                "inertial tracking",
                "inertial measurement unit"
            ],
            "abstract": "Motion capture technology has now existed for several decades and has been used in many different fields for a variety of purposes. In the entertainment industry, it has facilitated the creation of realistic and complex animations that, if created manually, would be too difficult and time-consuming. Different motion capture technologies have been invented over the years and they all have their strengths and weaknesses. Inertial motion capture is a low-cost alternative that relies on inertial sensors to estimate the orientation and position of a tracked object in three-dimensional space. Inertial sensors are a combination of a three-axis gyroscope and a three-axis accelerometer and are often contained in devices called inertial measurement units. In recent years they have become smaller, more lightweight, cheaper, less power-consuming and offer high sampling rates which makes them ideal for building a motion capture system. However, the outputted measurements from these sensors suffer from distortion which means that there needs to be a calibration procedure in place in order to minimize these distortions from the measurements. In this work, I developed a completely wireless configurable inertial motion capture solution with a robot-assisted calibration procedure. This motion capture solution con-sists of multiple motion trackers that are attached to the capture subject and wirelessly transmit the motion data to a receiving computer. I implemented a quaternion-based Extended Kalman filter as a sensor fusion method that uses the inertial data to estimate the orientation of the motion tracker. Due to the absence of a magnetometer sensor in this tracking solution, it is difficult to maintain a good enough accuracy when estimating the yaw angles of the motion trackers which leads to accumulated drifting errors over time. Therefore, this solution is only suitable for recording short animations for humanoid 3D characters. Furthermore, I compared my developed motion trackers to an existing commercially available tracking solution and the results indicate, with the exception of the low accuracy of the yaw angle estimation, acceptable orientation estimates.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Othman, A. (2023). <i>Low-cost Motion Capture Suit using Inertial Sensors</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2023.106787</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "303963",
                    "name": "Othman Ahmed - 2023 - Low-cost Motion Capture Suit using Inertial Sensors.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3872400,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/192387/1/Othman%20Ahmed%20-%202023%20-%20Low-cost%20Motion%20Capture%20Suit%20using%20Inertial%20Sensors.pdf"
                },
                {
                    "bsid": "304055",
                    "name": "Othman Ahmed - 2023 - Low-cost Motion Capture Suit using Inertial Sensors.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 200546,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/192387/4/Othman%20Ahmed%20-%202023%20-%20Low-cost%20Motion%20Capture%20Suit%20using%20Inertial%20Sensors.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Ahmed",
                    "last_name": "Othman",
                    "position": 1,
                    "role": "Author",
                    "tid": "273492"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "205364",
            "handle": "20.500.12708/193630",
            "doi": null,
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Unraveling the secrets of cell motility: from images to -omics with the help of bioinformatics",
            "keywords": [
                "Mitochondrial dynamics",
                "Pattern recognition"
            ],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Jobst, M., Gruber, L., Jiří Hladůvka, Gerner, C., &#38; Del Favero, G. (2023). Unraveling the secrets of cell motility: from images to -omics with the help of bioinformatics. In <i>11th Visegrad symposium on biomolecular interactions</i> (pp. 8–8).</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maximilian",
                    "last_name": "Jobst",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Livia",
                    "last_name": "Gruber",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Jiří Hladůvka",
                    "last_name": "Jiří Hladůvka",
                    "position": 3,
                    "role": "Author",
                    "tid": "53161"
                },
                {
                    "first_name": "Christopher",
                    "last_name": "Gerner",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Giorgia",
                    "last_name": "Del Favero",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "2056",
            "handle": "20.500.12708/2072",
            "doi": "10.34726/hss.2015.24574",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Design and implementation of an automatic tourist guide",
            "keywords": [
                "Natural Language Processing",
                "Location-Based Services",
                "Mobile Tourist Guide"
            ],
            "abstract": "Over the last years, smartphones have evolved to an important computing platform. Cities therefore offer mobile applications to provide visitors with guidance and support to boost the tourism industry. There are a lot of available tourist applications for Vienna, but they tend to focus on accommodation, navigation, social services or marketing instead of providing information for sightseeing. This master thesis focuses on the sightseeing aspect for tourists and presents an application for the Android platform which offers location-based information on sights in Austria, Germany, Switzerland and Liechtenstein. The content is automatically gathered fromWikipedia by a backend web application which provides an interface for the front-end to obtain the data with an update. This enables an offline usage of the Android application to avoid roaming charges. The main feature of the Automatic Tourist Guide (ATG) is the automatic guide which makes it possible for the user to walk through the city enjoying the view while the application delivers the relevant information via text-to-speech whenever a point of interest (POI) is passed. This is achieved by determining the exact location of the user and then computing the nearby POIs using a distance calculation algorithm. Furthermore, the ATG provides a map with all available sights and the route to the nearest attraction. The user is also able to choose categories in which he is interested in and will be presented only with content regarding his preferred subjects. In addition, there are three different versions of the content available: the unabridged text, a summarization of the most valuable information and a brief overview with only one paragraph.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Weilharter, M. (2015). <i>Design and implementation of an automatic tourist guide</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.24574</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "4131",
                    "name": "Weilharter Manuela - 2015 - Design and implementation of an automatic tourist...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4952694,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2072/2/Weilharter%20Manuela%20-%202015%20-%20Design%20and%20implementation%20of%20an%20automatic%20tourist...pdf"
                },
                {
                    "bsid": "78994",
                    "name": "Weilharter Manuela - 2015 - Design and implementation of an automatic tourist...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 163848,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2072/5/Weilharter%20Manuela%20-%202015%20-%20Design%20and%20implementation%20of%20an%20automatic%20tourist...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Manuela",
                    "last_name": "Weilharter",
                    "position": 1,
                    "role": "Author",
                    "tid": "62258"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "205717",
            "handle": "20.500.12708/192844",
            "doi": "10.34726/hss.2023.111843",
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Design and Evaluation of a Novel Shape Changing Haptic Device for Virtual Reality",
            "keywords": [
                "haptic feedback",
                "virtual reality",
                "origami"
            ],
            "abstract": "Virtual Reality (VR) is an immersive multisensory experience. Visual and auditory feedback in VR has improved significantly in the last decades. However, despite its great potential, the sense of touch is not satisfactorily served in today's virtual reality systems. Therefore, this thesis develops a novel shape-shifting haptic device named Shiftly, which renders plausible haptic feedback when touching virtual objects in VR. By changing its shape, Shiftly approximates the geometry of a virtual object touched by the user and provides haptic feedback for the hand. The device uses curved origami that is programmatically folded and unfolded to create a shape-changing touch surface that can be transformed from flat to curved. In this thesis, the design of Shiftly is described, a fully functional prototype is fabricated, a VR application is implemented, and Shiftly is evaluated in two user studies. Shiftly can render realistic haptic feedback for flat surfaces, convex shapes of different curvatures, shapes with edges, and, to some extent, concave surfaces and objects with small details. The device achieves this using only three actuators – a considerably smaller number than the comparable state-of-the-art.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Batik, T. (2023). <i>Design and Evaluation of a Novel Shape Changing Haptic Device for Virtual Reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2023.111843</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "304479",
                    "name": "Batik Tobias - 2023 - Design and Evaluation of a Novel Shape Changing Haptic...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3088735,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/192844/1/Batik%20Tobias%20-%202023%20-%20Design%20and%20Evaluation%20of%20a%20Novel%20Shape%20Changing%20Haptic...pdf"
                },
                {
                    "bsid": "304578",
                    "name": "Batik Tobias - 2023 - Design and Evaluation of a Novel Shape Changing Haptic...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 183940,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/192844/4/Batik%20Tobias%20-%202023%20-%20Design%20and%20Evaluation%20of%20a%20Novel%20Shape%20Changing%20Haptic...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Tobias",
                    "last_name": "Batik",
                    "position": 1,
                    "role": "Author",
                    "tid": "310413"
                },
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "232367"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "205759",
            "handle": "20.500.12708/192965",
            "doi": "10.34726/hss.2023.110784",
            "year": 2023,
            "issued": "2023",
            "issued_on": "2023-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Projectverse: A web-based Virtual Reality Platform for Visualizing Research Projects",
            "keywords": [
                "WebXR",
                "Virtual Reality",
                "Augmented Reality",
                "Unity",
                "Learning Applications"
            ],
            "abstract": "This thesis addresses the design, implementation and evaluation of Projectverse, a web-based Virtual Reality (VR) platform created with the intent to provide a better mediumfor those who wish to inform themselves about the work of the TU Wien VR group.Projectverse offers a multi-user environment in which research projects of the VR groupare visualized as stars in star constellations on a night sky. Users can interact with these by shooting them using a laser gun. Upon shooting a star, further information about the project it represents is shown to the user. Finally, users can transition into prototype applications of research projects and gain first-hand experience with these applications in Projectverse.Projectverse makes use of the WebXR Device API for communicating with VR devices.The platform was developed in the Unity game engine, therefore we also used the UnityWebXR plugin, which allows us to use WebXR data in Unity. Furthermore, we use WebXR exporter, another Unity plugin, to build Projectverse in a state that allows it to be executed in web browsers. Multiuser sessions and networking features were implemented using Photon PUN 2 and Photon Voice 2. We chose these frameworks dueto Photon being one of the largest providers of networking solutions for Unity.Projectverse is extensible in the sense that researchers can adapt their own research applications to the platform’s demands - most importantly: browser-readiness - and subsequently deploy them to it. Such extensions to the projects available on Projectverseare included by the platform automatically generating a new visualization once the projects change.To simplify and accelerate the adaptation of project applications to the demands of Projectverse, we provide a developer toolkit for Unity that includes fundamental VR functionality such as grabbing, locomotion and UI interaction as well as prefabs crucialto using VR devices with WebXR.Our evaluation shows that Projectverse provides an engaging, motivating and fun addition to the VR group website to those who wish to gain a deeper impression of the research conducted by the VR group.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Lautenbach, T. (2023). <i>Projectverse: A web-based Virtual Reality Platform for Visualizing Research Projects</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2023.110784</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "304611",
                    "name": "Lautenbach Tom - 2023 - Projectverse A web-based Virtual Reality Platform for...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2910740,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/192965/1/Lautenbach%20Tom%20-%202023%20-%20Projectverse%20A%20web-based%20Virtual%20Reality%20Platform%20for...pdf"
                },
                {
                    "bsid": "304702",
                    "name": "Lautenbach Tom - 2023 - Projectverse A web-based Virtual Reality Platform for...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 179958,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/192965/4/Lautenbach%20Tom%20-%202023%20-%20Projectverse%20A%20web-based%20Virtual%20Reality%20Platform%20for...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Tom",
                    "last_name": "Lautenbach",
                    "position": 1,
                    "role": "Author",
                    "tid": "340876"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "205807",
            "handle": "20.500.12708/193783",
            "doi": null,
            "year": 2023,
            "issued": "2023-02-24",
            "issued_on": "2023-02-24",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Distance Transform in Parallel Logarithmic Complexity",
            "keywords": [
                "Distance Transform",
                "Irregular Pyramids",
                "Logarithmic Complexity",
                "Parallel Processing"
            ],
            "abstract": "Nowadays a huge amount of digital data are generated every moment in a broad spectrum of application domains such as biomedical imaging, document processing, geosciences, remote sensing, video surveillance, etc. Processing such big data requires an efficient data structure, encouraging the algorithms with lower complexity and parallel operations. In this paper, first, a new method for computing the distance transform (DT) as the fundamental operation in binary images is presented. The method computes the DT with the parallel logarithmic complexity O(log(n)) where n is the maximum diameter of the largest foreground region in the 2D binary image. Second, we define the DT in the combinatorial map (CM) structure. In the CM, by replacing each edge with two darts a smoother DT with the double resolution is derived. Moreover, we compute n different distances for the nD-map. Both methods use the hierarchical irregular pyramid structure and have the advantage of preserving topological information between regions. The operations of the proposed algorithms are totally local and lead to parallel implementations. The GPU implementation of the algorithm has high performance while the bottleneck is the bandwidth of the memory or equivalently the number of available independent processing elements. Finally, the logarithmic complexity of the algorithm speeds up the execution and suits it. particularly for large images.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Banaeyan, M., &#38; Kropatsch, W. (2023). Distance Transform in Parallel Logarithmic Complexity. In <i>Proceedings of the 12th International Conference on Pattern Recognition Applications and Methods - ICPRAM 2023</i> (pp. 115–123). SciTePress, Science and Technology Publications. https://doi.org/10.5220/0011681500003411</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "304770",
                    "name": "Banaeyan-2023-Distance Transform in Parallel Logarithmic Complexity-vor.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4823245,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/193783/1/Banaeyan-2023-Distance%20Transform%20in%20Parallel%20Logarithmic%20Complexity-vor.pdf"
                },
                {
                    "bsid": "305729",
                    "name": "Banaeyan-2023-Distance Transform in Parallel Logarithmic Complexity-vor.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 25029,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/193783/4/Banaeyan-2023-Distance%20Transform%20in%20Parallel%20Logarithmic%20Complexity-vor.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Majid",
                    "last_name": "Banaeyan",
                    "position": 1,
                    "role": "Author",
                    "tid": "286018"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": [
                "1754584"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "206023",
            "handle": "20.500.12708/193784",
            "doi": null,
            "year": 2024,
            "issued": "2024-02-02",
            "issued_on": "2024-02-02",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Enhancing human-robot collaboration: Augmented reality interfaces for smarter path planning",
            "keywords": [
                "Augmented Reality",
                "Human-Robot Interaction",
                "human-robot collaboration (HRC)",
                "Path Planning"
            ],
            "abstract": "This chapter explores the challenges and opportunities of human-robot collaboration (HRC) in the context of Industry 4.0. The fourth industrial revolution has brought forth the concept of smart factories, where autonomous and intelligent systems play a vital role. Augmented reality (AR) emerges as a key technology in enhancing HRC, enabling effective interfaces for highly automated and AI–driven collaborative robots. Path planning, a significant activity involving humans and robots sharing physical space, is examined in the context of AR interfaces. The proposed work presents a system architecture for creating these interfaces, considering handheld and wearable AR devices. Usability analysis is conducted using the System Usability Scale, revealing both interfaces as suitable for generating virtual robot paths, with a slight preference for the wearable device. However, further testing is recommended to establish a conclusive comparison. This chapter provides valuable insights into the development of innovative interfaces that enhance HRC while ensuring human safety, addressing the evolving needs of the manufacturing industry in the era of Industry 4.0.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Manuri, F., Sanna, A., De Pace, F., Belcamino, V., &#38; Forteleoni, P. (2024). Enhancing human-robot collaboration: Augmented reality interfaces for smarter path planning. In K. Subburaj, S. Singh, &#38; S. Cukovic (Eds.), <i>Smart VR/AR/MR Systems for Professionals</i>. CRC Press. https://doi.org/10.1201/9781003306078-7</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Federico",
                    "last_name": "Manuri",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Andrea",
                    "last_name": "Sanna",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Francesco",
                    "last_name": "De Pace",
                    "position": 3,
                    "role": "Author",
                    "tid": "357410"
                },
                {
                    "first_name": "Valerio",
                    "last_name": "Belcamino",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Paolo",
                    "last_name": "Forteleoni",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Karupppasamy",
                    "last_name": "Subburaj",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Sunpreet",
                    "last_name": "Singh",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Sasa",
                    "last_name": "Cukovic",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "206024",
            "handle": "20.500.12708/193786",
            "doi": null,
            "year": 2024,
            "issued": "2024-02-01",
            "issued_on": "2024-02-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Storytelling in the Metaverse: From Desktop to Immersive Virtual Reality Storyboarding",
            "keywords": [
                "Virtual Reality",
                "Storyboarding",
                "Metaverse",
                "Storytelling"
            ],
            "abstract": "Creatives from the animation and film industries have always been experimenting with innovative tools and methodologies to improve the creation of prototypes of their visual sequences before bringing them to life. In recent years, as realistic real-time rendering techniques have emerged, the increasing popularity of virtual reality (VR) can lead to new approaches and solutions, leveraging the immersive and interactive features provided by 3D immersive experiences. A 3D desktop application and a novel storyboarding pipeline, which can automatically generate a storyboard including camera details and a textual description of the actions performed in three-dimensional environments, have already been investigated in previous work. The aim was to exploit new technologies to improve existing 3D storytelling approaches, thus providing a software solution for expert and novice storyboarders. This research investigates 3D storyboarding in immersive virtual reality (IVR) to move toward a new storyboarding paradigm. IVR systems provide peculiarities such as body-controlled exploration of the 3D scene and a head-dependant camera view that can extend features of traditional storyboarding tools. The proposed system enables users to set up the virtual stage, adding elements to the scene and exploring the environment as they build it. After that, users can select the available characters or the camera, control them in first person, position them in the scene, and perform actions selecting from a list of options, each paired with a corresponding animation. Relying on the concept of state-machine, the system can automatically generate the list of available actions depending on the context. Finally, the descriptions for each storyboard panel are automatically generated based on the history of activities performed. The proposed application maintains all the functionalities of the desktop version and can be effectively used to create storyboards in immersive virtual environments.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Manuri, F., Sanna, A., &#38; De Pace, F. (2024). Storytelling in the Metaverse: From Desktop to Immersive Virtual Reality Storyboarding. In <i>2023 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)</i> (pp. 28–33). IEEE. https://doi.org/10.1109/MetroXRAINE58569.2023.10405763</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Federico",
                    "last_name": "Manuri",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Andrea",
                    "last_name": "Sanna",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Francesco",
                    "last_name": "De Pace",
                    "position": 3,
                    "role": "Author",
                    "tid": "357410"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "20668",
            "handle": "20.500.12708/18848",
            "doi": "10.34726/hss.2021.77646",
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Foot tracking in virtual reality",
            "keywords": [
                "Virtual Reality",
                "Tracking",
                "Immersion"
            ],
            "abstract": "The visualisation of limbs in Virtual Reality (VR) helps to get a better immersion in the virtual world and it creates better confidence in movement. Sadly a lot of VR applications omit the visualisation of limbs. One reason lies in technical difficulties with bigger scale VR environments and multi-user VR environments where you can not rely on outside-in tracking methods because of the size and possible occlusion that hinders accurate tracking data. Another reason is that developers do not want to exclude parts of their already small user base by demanding special hardware for foot tracking that costs as much as the hand controllers but is only usable in a small number of applications. This thesis tackles these problems by generating a lightweight tracking system that only relies on the correct tracking of the head position so that either inside-out or outside-in tracking can be used with it. To achieve this, a RGB depth camera is mounted on the VR headset. A combination of fiducial marker tracking, depth tracking and inertial measurement units (IMUs) are used to track the user’s feet. These individual tracking signals are then fused to one signal that combines the advantages of the single tracking systems. This tracking information can then be used to animate the feet of a virtual avatar with an Inverse Kinematics (IK) algorithm.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Bayer, A. (2021). <i>Foot tracking in virtual reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2021.77646</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "81602",
                    "name": "Bayer Alexander - 2021 - Foot tracking in virtual reality.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2054976,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18848/1/Bayer%20Alexander%20-%202021%20-%20Foot%20tracking%20in%20virtual%20reality.pdf"
                },
                {
                    "bsid": "94183",
                    "name": "Bayer Alexander - 2021 - Foot Tracking in Virtual Reality.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 134636,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/18848/4/Bayer%20Alexander%20-%202021%20-%20Foot%20Tracking%20in%20Virtual%20Reality.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Alexander",
                    "last_name": "Bayer",
                    "position": 1,
                    "role": "Author",
                    "tid": "61023"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "208308",
            "handle": "20.500.12708/195525",
            "doi": "10.34726/hss.2024.106526",
            "year": 2024,
            "issued": "2024",
            "issued_on": "2024-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Plant Detection and State Classification with Machine Learning",
            "keywords": [
                "Computer Vision",
                "Machine Learning"
            ],
            "abstract": "Water deficiency in household plants can adversely affect growth. Existing solutions to monitor water stress are primarily intended for agricultural contexts where only asmall selection of plants are of interest. To date, there has been no research in household settings where the variety of plants is considerably higher and it is thus more difficult to obtain accurate measures of water stress. Furthermore, current approaches either do not detect plants in images first or use traditional feature extraction for plant detection. We develop a prototype to detect plants and classify them into water-stressed or not using deep learning based methods exclusively.Our two-stage approach consists of a detection and a classification step. In the detectionstep, plants are identified and cut out from the original image. The cut outs are passed to the classifier which outputs a probability for water stress. We use transfer learning to start from a robust base and fine-tune both models for their respective tasks. Each model is optimized using hyperparameter optimization and first evaluated individually and then in aggregate on a custom dataset. We deploy both models to an Nvidia Jetson Nano which is able to survey plants autonomously via an attached camera. The results of the pipeline are published continuously via an API. Downstream watering systems canuse the water stress predictions to water the plants without human intervention.The two models in aggregate achieve a mAP of 0.3581 for the non-optimized version.Both constituent models have robust feature extraction capabilities and are able to cope with various lighting conditions, different angles and a wide variety of household plants. The optimized pipeline achieves a mAP of 0.3838 on unseen images with higher precision for the non-stressed but lower precision for the stressed class. Recall for thenon-stressed class remains at the same level compared to the non-optimized baseline butis 12.1 percentage points higher for the stressed class. The weighted F1-score across both classes was improved by 2.4 percentage points. These results show that our two-stage approach is viable and a promising first step for plant state classification for household plants.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidelpes, T. (2024). <i>Plant Detection and State Classification with Machine Learning</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2024.106526</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "308529",
                    "name": "Eidelpes Tobias - 2024 - Plant Detection and State Classification with Machine...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1526385,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/195525/1/Eidelpes%20Tobias%20-%202024%20-%20Plant%20Detection%20and%20State%20Classification%20with%20Machine...pdf"
                },
                {
                    "bsid": "308594",
                    "name": "Eidelpes Tobias - 2024 - Plant Detection and State Classification with Machine...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 206811,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/195525/4/Eidelpes%20Tobias%20-%202024%20-%20Plant%20Detection%20and%20State%20Classification%20with%20Machine...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Tobias",
                    "last_name": "Eidelpes",
                    "position": 1,
                    "role": "Author",
                    "tid": "293146"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "208523",
            "handle": "20.500.12708/195851",
            "doi": "10.34726/hss.2024.119367",
            "year": 2024,
            "issued": "2024",
            "issued_on": "2024-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "3D Head Tracking and Gesture Recognition using an 8-by-8 Array of Infrared Sensors",
            "keywords": [
                "Image processing with 8x8 infrared sensor",
                "head detection",
                "gesture recognition"
            ],
            "abstract": "Human head tracking and gesture recognition are both known problems with solutions using RGB-cameras or an infrared emitter/receiver setup. In this thesis, we propose a method for head tracking and gesture detection using an 8-by-8 infrared sensor array. For this, a novel time-of-flight infrared sensor array is employed, which is both financially and computationally in expensive, while also alleviating privacy concerns due to the very low resolution of the array. The method is split into two parts: first, a human head is detected using circle detection on the filtered combination of depth and amplitude images. If no circle is detected, shape information is used to estimate the position of the head. To reduce false detection and outliers, the movement of the head is tracked over time. Using the depth value of the detected centroid, gesture detection then looks for movement in the given space between the sensor and detected centroid depth (gesture space) and tracks it over five frames. If the major movement direction exceeds a speed of four pixels per second, a gesture is detected. The experiments are split up into field (sensor on a desk in a living room with a window behind the person, daylight) and laboratory (sensor on a turntable in a photography light tent in a lab with artificial ceiling lighting).",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ismail, O. (2024). <i>3D Head Tracking and Gesture Recognition using an 8-by-8 Array of Infrared Sensors</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2024.119367</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "309130",
                    "name": "Ismail Omar - 2024 - 3D Head Tracking and Gesture Recognition using an 8-by-8...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1874573,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/195851/1/Ismail%20Omar%20-%202024%20-%203D%20Head%20Tracking%20and%20Gesture%20Recognition%20using%20an%208-by-8...pdf"
                },
                {
                    "bsid": "309192",
                    "name": "Ismail Omar - 2024 - 3D Head Tracking and Gesture Recognition using an 8-by-8...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 145730,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/195851/4/Ismail%20Omar%20-%202024%20-%203D%20Head%20Tracking%20and%20Gesture%20Recognition%20using%20an%208-by-8...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Omar",
                    "last_name": "Ismail",
                    "position": 1,
                    "role": "Author",
                    "tid": "276493"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E186"
            ],
            "pid": "2174",
            "handle": "20.500.12708/2190",
            "doi": "10.34726/hss.2017.43523",
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "On building multidimensional workflow models for complex systems modelling",
            "keywords": [
                "Complex Systems",
                "Energy Systems",
                "Modelling",
                "Model Verification",
                "Workflows"
            ],
            "abstract": "From well-known, classical workflows such as Petri nets to one of the recent developments of modelling frameworks such as the Business Process Modelling Notation, the development of system representations has long been established and improved through the years. The common goal of such frameworks is to produce traceable, effective, and well-understood functional and nonfunctional specifications of business and scientific systems. This goal addresses issues from workflow design, verification, control and monitoring, and continual improvement as systems evolve. Through the years, these frameworks had been constructed, enabled, deployed and used under a singular or dual perspective of modelling and verification relating to workflow dimensions, i.e. process, resource, case. In literature, there is a huge gap for the support and enactment of all three dimensions into one model for system representation and verification. That is, these undertakings are mainly either process- or resource-centric. In terms of modelling with all three dimensions in place, some support is observed in the Robustness Diagram(RD) of the ICONIX framework. Because of its notational backbone, it was posed to serve as a bridge for requirements traceability when using other workflows that focus solely on either structure or behavior of system representations. It has potential in providing support from requirements capture to testing to redesign of models. However, this diagram has been underdeveloped with respect to these potentials in modelling and verification especially for complex systems. In this research, the Robustness Diagram with Loop and Time Controls(RDLT) was introduced to support modelling and verification of complex systems. It is an extension based from RDs. Building on RDs, we propose formalizations of RDLTs with consideration for  the use and representation of all three workflow dimensions in one model. Additionally, by accounting the requirements of volatility, persistence, multi-state configuration, and hierarchical structures and relationships present in complex systems, the concept of a reset-bound subsystem(RBS) in RDLTs was also formulated. RBS enacts capabilities of cancellation regions in well-known workflows in literature. However, RBS enforces topological and behavioral requirements to perform resets in values in models. Additionally, this research addressed the problems of explicitness and effectiveness of representing data, control flow patterns(e.g. sequential, parallel, splits, n-out-of-k joins, iteration, cancellation regions, etc.), and multi-level and multi-participant interactions. These problems were also dealt under the specifications of all three workflow dimensions, persistent and volatile structures and behavior in models. In particular, this research introduced attribute-driven typing of vertices, arcs, and substructures to enforce structural and functional specifications in RDLTs. The values of the attributes and the resulting types of RDLT components directly influence their usage and groupings for the execution of activities in models. Furthermore, they also influence mechanisms for encapsulation of data and control flows in RDLTs. We proposed the concept of Points-of-Interests(POIs) in RDLTs that also rely on these information. POIs are then used to establish special regions in the model. We formally defined these regions and characterize their neighborhood structures. We established encapsulation rules for RBS and add its information to the characterized neighborhoods to determine metrics for the analysis of RDLTs. In this research, the metrics that were developed mainly focus on topological- and type-bound  reachability, delays that cause bottlenecks, task synchronicity, and activity completion. They are computed with consideration of the presence of maximal substructures in RDLTs. Each substructure supports the execution of an activity profile for the completion of a case in a multi-activity RDLTs. Among the model properties in literature for workflows, this research adopted and introduced soundness and free-choice for RDLTs. (We focus on these two properties because many other properties can be implied from them.) Noting the types of components and substructures and the presence of RBS in them, a behavioral profile of RDLTs that satisfy the first property is initially provided. This profile is produced by the use of the results from our proposed algorithm for activity extraction. However, this research devised two independent approaches to prove this property through structural views that account the types and the presence of RBS in RDLTs. The first approach was constraint-driven. RLDT attributes that pertain to the enforcement of splits and joins of type-alike components were used. We developed the concepts of an extended RDLT and a vertex-simplified RDLT to support this approach. Meanwhile, the second approach was component use-driven. Attributes that pertain to repeatability of task execution were checked in type-alike/mixed-type components and hierarchical structures and interactions in RDLTs. This approach used our proposed encapsulation rules for RBS. Collectively, both approaches proved soundness based on statically-verifiable information in RDLTs. Meanwhile, the free-choice property for RDLTs was proposed and built from using thePOI-driven metrics for reachability, boundedness, and synchronicity. In literature, this property focused on place-task relationships in workflows. In this research, it is extended  by including combinations of topology-, constraint-, and type-driven free-choiceness in models. There are two RDLT configurations that were proposed for these combinations. One adopts the well-known view of free-choiceness while the latter its extension. However, both configurations consider all the workflow dimensions in modelling RDLTs. In addition, this research provides efficient verification schemes for soundness, freechoiceness, and other proposed model properties for RDLTs. Real-world examples of RDLT models, including that of a complex energy system, was provided to illustrate how their structural and behavioral profiles were captured where these properties are checked. Finally, this research proved the relationships and hierarchies of RDLTs based on the properties that can be verified from them.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Malinao, J. (2017). <i>On building multidimensional workflow models for complex systems modelling</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2017.43523</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "4367",
                    "name": "Malinao Jasmine - 2017 - On building multidimensional workflow models for...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3719827,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2190/2/Malinao%20Jasmine%20-%202017%20-%20On%20building%20multidimensional%20workflow%20models%20for...pdf"
                },
                {
                    "bsid": "79064",
                    "name": "Malinao Jasmine - 2017 - On building multidimensional workflow models for...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 310164,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2190/5/Malinao%20Jasmine%20-%202017%20-%20On%20building%20multidimensional%20workflow%20models%20for...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Jasmine",
                    "last_name": "Malinao",
                    "position": 1,
                    "role": "Author",
                    "tid": "287309"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "23364",
            "handle": "20.500.12708/20241",
            "doi": "10.34726/hss.2022.88909",
            "year": 2022,
            "issued": "2022",
            "issued_on": "2022-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Towards real-time video inpainting",
            "keywords": [
                "Video Inpainting",
                "Image Processing",
                "Video Processing",
                "Machine Learning",
                "Deep Learning"
            ],
            "abstract": "Image inpainting refers to the task of restoring missing information in an image in a way such that a human observer is unable to tell if the image has been manipulated or not. It can be used, for example, to restore old images and photographs by removing scratches or other signs of wear and tear. However, it can also be used to get rid of superimposed text or to remove certain objects or subjects from a video. Video inpainting is an extension of image inpainting into a third, temporal dimension. On the one hand this extension provides additional information for the inpainting of a single frame but on the other hand introduces new challenges such as the need to restore the missing information in a temporally coherent way. Video inpainting can be used in various non-performance critical scenarios such as film editing or film restoration. However, there are also scenarios in which a video must be processed in real-time, which imposes certain constraints on the video inpainting method. Examples of such scenarios are live TV broadcasting or the usage in surveillance systems. This work proposes a modular real-time video inpainting method which is based on a regular video inpainting method and evaluates it with respect to different inpainting scenarios and compares it to the original method. Because of the additionally imposed constraints the results produced by the proposed method have a lower visual quality compared to the results of the original method. However, if the actual use cases of the method are known beforehand, the proposed method can be adapted and further optimized, for example, by relaxing some of the constraints.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ingruber, A. (2022). <i>Towards real-time video inpainting</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2022.88909</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "98287",
                    "name": "Ingruber Aron - 2022 - Towards real-time video inpainting.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2447537,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/20241/1/Ingruber%20Aron%20-%202022%20-%20Towards%20real-time%20video%20inpainting.pdf"
                },
                {
                    "bsid": "98300",
                    "name": "Ingruber Aron - 2022 - Towards Real-Time Video Inpainting.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 218149,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/20241/4/Ingruber%20Aron%20-%202022%20-%20Towards%20Real-Time%20Video%20Inpainting.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Aron",
                    "last_name": "Ingruber",
                    "position": 1,
                    "role": "Author",
                    "tid": "303816"
                },
                {
                    "first_name": "Jiri",
                    "last_name": "Hladuvka",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "53161"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "23366",
            "handle": "20.500.12708/20243",
            "doi": "10.34726/hss.2022.80767",
            "year": 2022,
            "issued": "2022",
            "issued_on": "2022-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Many-objective optimization for maximum flexibility in industrial building design",
            "keywords": [
                "Many-objective optimization",
                "Multi-objective optimization",
                "Evolutionary algorithm",
                "Automated structural layout generation",
                "Integrated industrial building design",
                "Flexible industrial building"
            ],
            "abstract": "Industrial buildings often have a very short lifespan due to inflexible design of load bearing structures. Frequently changing production processes often lead to demolition of industrial buildings because these buildings cannot be adapted to the new requirements. This work is part of the BIMFlexi project, whose goal is to develop an integrated Building Information Modeling (BIM) based platform to connect all stakeholders in a building planning process to design flexible and sustainable industrial buildings. In this work a many-objective optimization tool is presented to support decision makers during the design phase. The tool is built on top of a parametric framework for load bearing structure generation. By presenting multiple optimized load bearing structures with different properties decision makers can make informed decisions about trade-offs between cost, environmental impacts and flexibility of a load bearing structure. The tool has been studied in two different ways. A user study was conducted to verify its usabilityand usefulness. A second study compared three different evolutionary algorithms to find the best fitting algorithm for industrial building optimization.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Wang-Sukalia, X. (2022). <i>Many-objective optimization for maximum flexibility in industrial building design</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2022.80767</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "98293",
                    "name": "Wang-Sukalia Xi - 2022 - Many-objective optimization for maximum flexibility in...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2808088,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/20243/1/Wang-Sukalia%20Xi%20-%202022%20-%20Many-objective%20optimization%20for%20maximum%20flexibility%20in...pdf"
                },
                {
                    "bsid": "98301",
                    "name": "Wang-Sukalia Xi - 2022 - Many-Objective Optimization for Maximum Flexibility in...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 140450,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/20243/4/Wang-Sukalia%20Xi%20-%202022%20-%20Many-Objective%20Optimization%20for%20Maximum%20Flexibility%20in...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Xi",
                    "last_name": "Wang-Sukalia",
                    "position": 1,
                    "role": "Author",
                    "tid": "253667"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "23816",
            "handle": "20.500.12708/78073",
            "doi": null,
            "year": 2022,
            "issued": "2022-04-20",
            "issued_on": "2022-04-20",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Photogrammabot: An Autonomous ROS-Based Mobile Photography Robot for Precise 3D Reconstruction and Mapping of Large Indoor Spaces for Mixed Reality",
            "keywords": [
                "3D reconstruction",
                "Artificial intelligence",
                "automation",
                "autonomous mobile robot",
                "Computer systems organization",
                "Computer vision",
                "Computing methodologies",
                "Embedded and cyberphysical systems",
                "fiducial marker tracking",
                "Human computer interaction (HCI)",
                "Human computer interaction (HCI)",
                "Human-centered computing",
                "Human-centered computing",
                "Interaction paradigms",
                "mixed-reality",
                "Mixed/augmented reality",
                "Photogrammetry",
                "Reconstruction",
                "Robotic autonomy",
                "Robotics",
                "ROS",
                "Virtual reality"
            ],
            "abstract": "Precise 3D reconstruction of environments and real objects for Mixed-Reality applications can be burdensome. Photogrammetry can help to create accurate representations of actual objects in the virtual world using a high number of photos of a subject or an environment. Photogrammabot is an affordable mobile robot that facilitates photogrammetry and 3D reconstruction by autonomously and systematically capturing images. It explores an unknown indoor environment and uses map-based localization and navigation to maintain camera direction at different shooting points. Photogrammabot employs a Raspberry Pi 4B and Robot Operating System (ROS) to control the exploration and capturing processes. The photos are taken using a point-and-shoot camera mounted on a 2-DOF micro turret to enable photography from different angles and compensate for possible robot orientation errors to ensure parallel photos. Photogrammabot has been designed as a general solution to facilitate precise 3D reconstruction of unknown environments. In addition we developed tools to integrate it with and extend the Immersive Deck™ MR system [23], where it aids the setup of the system in new locations.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mortezapoor, S., Schönauer, C., Rüggeberg, J., &#38; Kaufmann, H. (2022). Photogrammabot: An Autonomous ROS-Based Mobile Photography Robot for Precise 3D Reconstruction and Mapping of Large Indoor Spaces for Mixed Reality. In <i>Proceedings of 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</i> (pp. 101–107). IEEE. https://doi.org/10.1109/VRW55335.2022.00033</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "position": 1,
                    "role": "Author",
                    "tid": "252414"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Julien",
                    "last_name": "Rüggeberg",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": [
                "1745908"
            ]
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "253",
            "handle": "20.500.12708/269",
            "doi": null,
            "year": 2018,
            "issued": "2018-11",
            "issued_on": "2018-11-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Smell and touch in the Virtual Jumpcube",
            "keywords": [
                "Virtual reality",
                "Olfactory stimuli",
                "Haptic stimuli",
                "Force simulation",
                "User-based evaluation"
            ],
            "abstract": "The Virtual Jumpcube is a virtual reality setup from 2015 that allows for jumping and flying in audiovisual virtual environments. Recently, we have included several haptic and olfactory stimuli that should further increase the degree of immersion in the experienced virtuality. These additional media channels were tested by the participants of several events and the feedback of 196 jumpers was gathered in a questionnaire. In this paper, we describe the stimulation hardware and software as well as the performed experiment and we present the major findings of the evaluation. It shows that if employed correctly, haptic and olfactory stimuli can enhance immersion and user experience significantly. Major success factors appear to be the amplitude and frequency of stimulation as well as the temporal synchronization with the other media channels, in particular the visual stimuli.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2018). Smell and touch in the Virtual Jumpcube. <i>Multimedia Systems</i>. https://doi.org/10.1007/s00530-018-0592-y</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "525",
                    "name": "Eidenberger Horst - 2018 - Smell and touch in the Virtual Jumpcube.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2044728,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/269/2/Eidenberger%20Horst%20-%202018%20-%20Smell%20and%20touch%20in%20the%20Virtual%20Jumpcube.pdf"
                },
                {
                    "bsid": "45443",
                    "name": "Smell and touch in the Virtual Jumpcube.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 68094,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/269/8/Smell%20and%20touch%20in%20the%20Virtual%20Jumpcube.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01",
                "E194-03"
            ],
            "pid": "25545",
            "handle": "20.500.12708/22283",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Book",
            "peer_reviewed": false,
            "invited": false,
            "title": "Perl. Einführung in die Programmierung und Problemlösen",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H., &#38; Michlmayr, E. (2005). <i>Perl. Einführung in die Programmierung und Problemlösen</i>. dpunkt.verlag. http://hdl.handle.net/20.500.12708/22283</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                },
                {
                    "first_name": "Elke",
                    "last_name": "Michlmayr",
                    "position": 2,
                    "role": "Author",
                    "tid": "49487"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "25563",
            "handle": "20.500.12708/22301",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Pattern Recognition: 27th DAGM Symposium",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Sablatnig, R., &#38; Hanbury, A. (Eds.). (2005). <i>Pattern Recognition: 27th DAGM Symposium</i>. Springer, LNCS. http://hdl.handle.net/20.500.12708/22301</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Editor",
                    "tid": "133566"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Editor",
                    "tid": "48222"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "25564",
            "handle": "20.500.12708/22302",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Proceedings of the 10th Computer Vision Winter Workshop CVWW 2005",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A., &#38; Bischof, H. (Eds.). (2005). <i>Proceedings of the 10th Computer Vision Winter Workshop CVWW 2005</i>. Eigenverlag. http://hdl.handle.net/20.500.12708/22302</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Editor",
                    "tid": "48222"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Editor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "25571",
            "handle": "20.500.12708/22309",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Digital Cultural Heritage - Essential for Tourism, Proc. of 1st. EVA 2006 Vienna Conference",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sablatnig, R., Hemsley, J., Kammerer, P., Zolda, E., &#38; Stockinger, J. (Eds.). (2006). <i>Digital Cultural Heritage - Essential for Tourism, Proc. of 1st. EVA 2006 Vienna Conference</i>. OCG Schriftenreihe, Österreichische Arbeitsgemeinschaft für Mustererkennung. http://hdl.handle.net/20.500.12708/22309</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 1,
                    "role": "Editor",
                    "tid": "133566"
                },
                {
                    "first_name": "James",
                    "last_name": "Hemsley",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Paul",
                    "last_name": "Kammerer",
                    "position": 3,
                    "role": "Editor",
                    "tid": "49696"
                },
                {
                    "first_name": "Ernestine",
                    "last_name": "Zolda",
                    "position": 4,
                    "role": "Editor",
                    "tid": "229392"
                },
                {
                    "first_name": "Johann",
                    "last_name": "Stockinger",
                    "position": 5,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "25610",
            "handle": "20.500.12708/22348",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Book",
            "peer_reviewed": false,
            "invited": false,
            "title": "The Structurally Optimal Dual Graph Pyramid and its Application in Image Partitioning",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y. (Ed.). (2007). <i>The Structurally Optimal Dual Graph Pyramid and its Application in Image Partitioning</i>. IOS Press and Akadademische Verlagsgesellschaft AKA GmbH. http://hdl.handle.net/20.500.12708/22348</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Editor",
                    "tid": "64562"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "25613",
            "handle": "20.500.12708/22351",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Computer Analysis of Images and Patterns, 12th International Conference, CAIP 2007",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Kampel, M., &#38; Hanbury, A. (Eds.). (2007). <i>Computer Analysis of Images and Patterns, 12th International Conference, CAIP 2007</i>. Springer LNCS. http://hdl.handle.net/20.500.12708/22351</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 2,
                    "role": "Editor",
                    "tid": "49535"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Editor",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "26059",
            "handle": "20.500.12708/22796",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Proceedings of the Computer Vision Winter Workshop 2009",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., &#38; Kropatsch, W. (Eds.). (2009). <i>Proceedings of the Computer Vision Winter Workshop 2009</i>. PRIP, TU Wien. http://hdl.handle.net/20.500.12708/22796</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Editor",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "26101",
            "handle": "20.500.12708/22838",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Book",
            "peer_reviewed": false,
            "invited": false,
            "title": "Progress in Pattern Recognition, Image Analysis and Applications",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ruiz-Shulcloper, J., &#38; Kropatsch, W. (Eds.). (2008). <i>Progress in Pattern Recognition, Image Analysis and Applications</i>. Springer LNCS. http://hdl.handle.net/20.500.12708/22838</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Jose",
                    "last_name": "Ruiz-Shulcloper",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Editor",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "26271",
            "handle": "20.500.12708/23008",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Workshop on Computational Topology in Image Context 2009",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Molina Abril, H., &#38; Ion, A. (Eds.). (2009). <i>Workshop on Computational Topology in Image Context 2009</i>. PRIP, TU Wien. http://hdl.handle.net/20.500.12708/23008</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Helena",
                    "last_name": "Molina Abril",
                    "position": 2,
                    "role": "Editor",
                    "tid": "55000"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "26650",
            "handle": "20.500.12708/23387",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "The 14th International Conference on Computer Analysis of Images and Patterns",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Berciano, A., Diaz-Pernil, D., Kropatsch, W., Molina-Abril, H., &#38; Real, P. (Eds.). (2011). <i>The 14th International Conference on Computer Analysis of Images and Patterns</i>. Springer. http://hdl.handle.net/20.500.12708/23387</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ainhoa",
                    "last_name": "Berciano",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Daniel",
                    "last_name": "Diaz-Pernil",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Helena",
                    "last_name": "Molina-Abril",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Real",
                    "position": 5,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "26651",
            "handle": "20.500.12708/23388",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "The 14th International Conference on Computer Analysis of Images and Patterns (CAIP), Part II",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Berciano, A., Diaz-Pernil, D., Kropatsch, W., Molina-Abril, H., &#38; Real, P. (Eds.). (2011). <i>The 14th International Conference on Computer Analysis of Images and Patterns (CAIP), Part II</i>. Springer-Verlag. http://hdl.handle.net/20.500.12708/23388</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ainhoa",
                    "last_name": "Berciano",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Daniel",
                    "last_name": "Diaz-Pernil",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Helena",
                    "last_name": "Molina-Abril",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Real",
                    "position": 5,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "26743",
            "handle": "20.500.12708/23480",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Book",
            "peer_reviewed": false,
            "invited": false,
            "title": "Professional Media Understanding",
            "keywords": [],
            "abstract": "Media understanding is the science/art of identifying semantic structures in digital media objects such as audio, biosignals, images, text and videos. This volume continues the work started in &quot;Fundamental Media Understanding&quot; (atpress, 2011). It covers methods employed in professional multimedia information retrieval such as audiovisual feature transformations based on discrete transforms, wavelet transforms, local interest points methods such as SIFT and MSER, flow-based motion description, information filtering by singular value decomposition, feature selection, principles of human learning and machine learning, categorization by risk minimization and kernel-based learning, dynamic optimization, mixture models, and evaluation based on cross validation and receiver operating characteristic curves. In contrast to related publications, this book does not focus on one type of media but considers all the above-named as well as a few others. The author endeavors to identify similarities between the methods employed in audio retrieval, image understanding, text summarization and many other research domains. It turns out that a number of significant parallels do exist. Structuring the methods along common criteria and discussing their similarities and differences breaks the ground for a new research discipline: true computational understanding of multimedia content.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2012). <i>Professional Media Understanding</i>. atpress. http://hdl.handle.net/20.500.12708/23480</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "26805",
            "handle": "20.500.12708/23541",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Book",
            "peer_reviewed": false,
            "invited": false,
            "title": "Frontiers of Media Understanding",
            "keywords": [],
            "abstract": "Media understanding is the science/art of identifying semantic structures in digital media objects such as audio, biosignals, images, text and videos. Computational media understanding should do what our senses and cognition do: immediate understanding of events as diverse as watching a bird and listening to a speech. This book introduces the reader with the state-of-the-art methods applied today for media summarization and for the categorization of events. In contrast to related publications, it does not focus on one type of media but considers all the above-named as well as a few others. The author endeavors to identify similarities between the methods employed in audio retrieval, image understanding, text summarization and many other research domains. It turns out that a number of significant parallels do exist. Structuring the methods along common criteria and discussing their similarities and differences breaks the ground for a new research discipline: true computational understanding of multimedia content.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2012). <i>Frontiers of Media Understanding</i>. atpress. http://hdl.handle.net/20.500.12708/23541</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "26806",
            "handle": "20.500.12708/23542",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Book",
            "peer_reviewed": false,
            "invited": false,
            "title": "Handbook of Multimedia Information Retrieval",
            "keywords": [],
            "abstract": "Multimedia information retrieval: That is the desire to make computers see, hear and understand like humans do. Is it possible to give perception to machines, to make them understand facial expressions, hummed melodies, stock charts and ECG curves? If yes, the computer would become an even more valuable companion in business and private life. Think of the possibilities in, for example, healthcare, home security, online customer support or market analysis. This book explains what is possible in multimedia information retrieval today and what is not. We introduce the basic concepts, explain why the first step is always summarization and the second classification, which is essentially applying human understanding of some context on the summary. We group and discuss the various methods that have been proposed for the summarization of audio, visual and other media information. In classification, we build on today's psychological understanding of human cognition. Successfully, we transfer concepts of human similarity perception on machine classification. We cluster machine learning methods by their approach, model and process. On top of that, we link back from the state of the art methods of multimedia information retrieval to human cognition: We propose artificial neural structures for the building blocks of media summarization and classification. The result is a balanced introduction into the field that starts from graduate IT knowledge and ends at the current frontiers of multimedia research.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2012). <i>Handbook of Multimedia Information Retrieval</i>. atpress. http://hdl.handle.net/20.500.12708/23542</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06"
            ],
            "pid": "26917",
            "handle": "20.500.12708/23653",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Book",
            "peer_reviewed": false,
            "invited": false,
            "title": "Advances in Multimedia Modeling",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Advances in Multimedia Modeling. (2012). In K. Schoeffmann, B. Merialdo, A. G. Hauptmann, C.-W. Ngo, Y. Andreopoulos, &#38; C. Breiteneder (Eds.), <i>Lecture Notes in Computer Science</i>. Springer LNCS. https://doi.org/10.1007/978-3-642-27355-1</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Klaus",
                    "last_name": "Schoeffmann",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Bernald",
                    "last_name": "Merialdo",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Alexander G.",
                    "last_name": "Hauptmann",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Chong-Wah",
                    "last_name": "Ngo",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Yiannis",
                    "last_name": "Andreopoulos",
                    "position": 5,
                    "role": "Editor"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 6,
                    "role": "Editor",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E389-02"
            ],
            "pid": "26976",
            "handle": "20.500.12708/23712",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Proceedings of the 18th Computer Vision Winter Workshop 2013",
            "keywords": [],
            "abstract": "The Computer Vision Winter Workshop is an annual meeting formed around re-\r\nsearch groups from the Vienna University of Technology (PRIP), Graz University of\r\nTechnology (ICG), Czech Technical University in Prague (CMP), and the Univer-\r\nsity of Ljubljana (CVG, MVG, ViCoS). The goal of the workshop is to communicate\r\nfresh ideas between the four groups and to provide conference experience to students.\r\nThe workshop is open to students worldwide.\r\nThe 18th edition of this workshop was organized by the Pattern Recognition and\r\nImage Processing Group (PRIP), and was held in Hernstein, Austria, February 4-6,\r\n2013.\r\n32 Papers were submitted to the workshop this year. Each of them was reviewed\r\nby at least 2 members of the Program Committee, and 28 were selected for oral\r\npresentation at the workshop. A few authors wished that their paper is not included\r\nin these proceedings. According to the general philosophy of the CVWW series,\r\nthese wishes have been accepted. 16 papers were included in the proceedings of the\r\nworkshop.\r\nBesides the submitted papers, one invited talk was included. We would like to\r\nthank Werner Purgathofer (Institute of Computer Graphics and Algorithms, Vienna\r\nUniversity of Technology) for his valuable contribution.\r\nLast but not least, we would like to thank the members of the Program Committee\r\nfor their careful reviews, and wish the participants many new contacts and fresh\r\nideas.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Torres Garcia, F., &#38; Ramachandran, G. (Eds.). (2013). <i>Proceedings of the 18th Computer Vision Winter Workshop 2013</i>. PRIP, TU Wien. http://hdl.handle.net/20.500.12708/23712</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Fuensanta",
                    "last_name": "Torres Garcia",
                    "position": 2,
                    "role": "Editor",
                    "tid": "235851"
                },
                {
                    "first_name": "Geetha",
                    "last_name": "Ramachandran",
                    "position": 3,
                    "role": "Editor",
                    "tid": "195689"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "26987",
            "handle": "20.500.12708/23722",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Graph-Based Representations in Pattern Recognition",
            "keywords": [],
            "abstract": "These proceedings present the papers a\r\nccepted for the 9th IAPR-TC-15 Work-\r\nshop on Graph-based Representations in Pattern Recognition (GbR) 2013. For\r\nmore than 15 years, GbR has been providing a forum for researchers from the\r\nfields of pattern recognition, image processing, and computer vision who build\r\ntheir works on the basis of graph theory. This year it was a great pleasure for us\r\nto organize the GbR 2013 in the heart of Europe - Vienna, Austria.\r\nThe Technical Committee 15 (TC15) of the International Association for\r\nPattern Recognition (IAPR) was created in 1996. It encourages elaboration of\r\ngraph-based research works, is an integral partner in organizing biennial GbR\r\nworkshops, sponsors related special sessions at conferences, and promotes special\r\nissues in journals.\r\nTraditionally the work presented at GbR covers a wide range of topics. The\r\nscope of the papers varies from theoretical contributions to applications, from\r\ndiscovering the new properties of a single graph (graph edit distance, maximum\r\ncut, graph characteris\r\ntics derived from Schr &#776;\r\nodinger equation) to developing al-\r\ngorithms for sets of graphs, maximum subgraph problems and graph matching.\r\nA great interest was shown in the problems of graph kernels and topology.\r\nBesides the regular research papers, this workshop featured two highlights:\r\nthe IAPR distinguished speakers Mario Vento and Herbert Edelsbrunner. Mario\r\nVento was among the founders of TC15 s\r\nome 16 years ago. He summarized the\r\ndevelopment of our field starting with the original motivation. It allowed the\r\nyounger generation of our community to compare the goals and expectations\r\nof the early years with the current state. Herbert Edelsbrunner created a new\r\nbridge from TC15 to the area of topology and persistence.\r\nOverall, GbR 2013 attracted 27 submissions from 10 countries. Each paper\r\nwent through a critical reviewing proces\r\ns by at least two members of the inter-\r\nnational Program Committee. Finally, 24 papers including the contributions of\r\nthe invited speakers were accepted for oral presentation and publication in these\r\nproceedings.\r\nOn behalf of the organizers, we would like to thank the members of the\r\nProgram Committee for their timely and co\r\nmpetent reviews; the authors of the\r\nsubmitted papers for their work and their abidance to all deadlines. Finally,\r\nwe would like to thank the IAPR for sponsoring our workshop and the IAPR\r\ndistinguished speakers for their contributions.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Graph-Based Representations in Pattern Recognition. (2013). In W. Kropatsch, N. Artner, Y. Haxhimusa, &#38; X. Jiang (Eds.), <i>Lecture Notes in Computer Science</i>. Springer-Verlag Berlin Heidelberg. https://doi.org/10.1007/978-3-642-38221-5</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Editor",
                    "tid": "37618"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Editor",
                    "tid": "64562"
                },
                {
                    "first_name": "Xiaoyi",
                    "last_name": "Jiang",
                    "position": 4,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E104-03",
                "E104-04",
                "E193-02"
            ],
            "pid": "27084",
            "handle": "20.500.12708/23818",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Book",
            "peer_reviewed": false,
            "invited": false,
            "title": "GeodiKon. Die Lernmaterialien",
            "keywords": [],
            "abstract": "Das Buch enthält mehr als 100 ausgewählte, erprobte und nach speziellen Kriterien zusammengestellte Raumvorstellungsübungen, durch die die Raumintelligenz trainiert und gefördert wird. Es beschreibt eingangs kompakt Tipps und Hinweise, wie die Lernmaterialien im Unterricht eingesetzt werden können. Die Aufgaben sind eine ideale Unterstützung für den Geometrie- und Mathematikunterricht und können auch in weiteren Gegenständen, die sich mit Raum und Raumvorstellung beschäftigen, eingesetzt werden. Die einzelnen Übungen können direkt im Buch bearbeitet werden, wodurch in einem ausgewogenen Verhältnis die vier oben genannten Faktoren der Raumintelligenz gezielt trainiert werden.\r\nZusätzlich zum Übungsteil sind die Lösungen sämtlicher Aufgaben angegeben. Dies ermöglicht die selbständige Kontrolle der Korrektheit der Bearbeitung.\r\n\r\nDiese Sammlung von geometrischen Aufgaben wurde für das Forschungsprojekt GeodiKon (Entwicklung eines didaktischen Konzepts für den Einsatz von zeitgemäßen Lernmaterialien im Geometrie-Unterricht der Sekundarstufe I mit speziellem Fokus auf die individualisierte Förderung des Raumvorstellungsvermögens durch Schulung der Faktoren der Raumintelligenz und durch Bewusstmachung eines breiten Strategierepertoires zur Lösung von raumgeometrischen Aufgaben) des österreichischen Bildungsministeriums und der Pädagogischen Hochschule Salzburg zusammengestellt.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Asperl, A., Feßl, C., Gems, W., Kaufmann, H., Leopoldseder, S., Maresch, G., Miestinger, D., Müller, T., Luksch, K., Redl, G., Scheiber, K., Schilling, G., Schmied, H., Slepcevic, H., &#38; Wischounig, M. (2014). <i>GeodiKon. Die Lernmaterialien</i>. StudienVerlag. http://hdl.handle.net/20.500.12708/23818</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Andreas",
                    "last_name": "Asperl",
                    "position": 1,
                    "role": "Author",
                    "tid": "145232"
                },
                {
                    "first_name": "Christoph",
                    "last_name": "Feßl",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Werner",
                    "last_name": "Gems",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Stefan",
                    "last_name": "Leopoldseder",
                    "position": 5,
                    "role": "Author",
                    "tid": "69168"
                },
                {
                    "first_name": "Günter",
                    "last_name": "Maresch",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Doris",
                    "last_name": "Miestinger",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Müller",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Katharina",
                    "last_name": "Luksch",
                    "position": 9,
                    "role": "Author",
                    "tid": "96193"
                },
                {
                    "first_name": "Günter",
                    "last_name": "Redl",
                    "position": 10,
                    "role": "Author"
                },
                {
                    "first_name": "Klaus",
                    "last_name": "Scheiber",
                    "position": 11,
                    "role": "Author"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Schilling",
                    "position": 12,
                    "role": "Author"
                },
                {
                    "first_name": "Hans",
                    "last_name": "Schmied",
                    "position": 13,
                    "role": "Author"
                },
                {
                    "first_name": "Heinz",
                    "last_name": "Slepcevic",
                    "position": 14,
                    "role": "Author"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Wischounig",
                    "position": 15,
                    "role": "Author",
                    "tid": "54038"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "27108",
            "handle": "20.500.12708/23842",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Book",
            "peer_reviewed": false,
            "invited": false,
            "title": "Applications of Mixed Reality",
            "keywords": [],
            "abstract": "This thesis gives an overview of the author´s scientific work in previous\r\nyears. It reflects the author´s ambition to develop applications of mixed\r\nreality which are beneficial to society as a whole or to specific groups of\r\npeople.\r\nProviding and deploying high-end mixed reality hardware and software\r\napplications to multiple users and larger target groups finally raises questions\r\nof scalability, robustness, design and affordability of the technology involved.\r\nThey trigger scientific questions and developments in return. All of these\r\naspects will be touched in this work.\r\nThe first part of the introduction defines the scientific domain and various\r\nproblems therein followed by a discussion of the author´s contribution in\r\nthis area. The individual publications that constitute the remainder of the\r\nthesis are discussed and put in context.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2010). <i>Applications of Mixed Reality</i>. Holzhausen. http://hdl.handle.net/20.500.12708/23842</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "27313",
            "handle": "20.500.12708/24047",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Graph-Based Representations in Pattern Recognition",
            "keywords": [],
            "abstract": "This book constitutes the refereed proceedings of the 10th IAPR-TC-15 International Workshop on Graph-Based Representations in Pattern Recognition, GbRPR 2015, held in Beijing, China, in May 2015. The 36 papers presented in this volume were carefully reviewed and selected from 53 submissions. The accepted papers cover diverse issues of graph-based methods and applications, with 7 in graph representation, 15 in graph matching, 7 in graph clustering and classification, and 7 in graph-based applications.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Graph-Based Representations in Pattern Recognition. (2015). In C.-L. Liu, B. Luo, W. Kropatsch, &#38; J. Cheng (Eds.), <i>Lecture Notes in Computer Science</i>. Springer International Publishing. https://doi.org/10.1007/978-3-319-18224-7</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Cheng-Lin",
                    "last_name": "Liu",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Bin",
                    "last_name": "Luo",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Jian",
                    "last_name": "Cheng",
                    "position": 4,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "27620",
            "handle": "20.500.12708/24354",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Proceedings of the 22nd Computer Vision Winter Workshop 2017",
            "keywords": [],
            "abstract": "The 22nd Computer Winter Workshop was organized by the PRIP Club and the PRIP Group at the Institute of Computer Graphics\r\nand Algorithms at TU Wien. It took place at Symposium Hotel Althof in Retz, Austria, from February 6th till February 8th, 2017.\r\n\r\nThe Computer Vision Winter Workshop (CVWW) is the annual meeting of several computer vision research groups located in Graz,\r\nLjubljana, Prague, and Vienna. We want to communicate fresh ideas within the groups, and provide conference experience to\r\nstudents. However, the workshop is open to everyone.\r\n\r\n26 papers have been submitted to the workshop. Each of them was reviewed by at least 3 members of the program committee. 25\r\npapers have been accepted, where 17 are oral presentations and 8 are posters.\r\n\r\nA &quot;Best Paper Award&quot; with the prize money of Euro 500 (in cooperation with the Austrian Computer Society, OCG) was handed over to\r\nthe paper nominated based on the presentation during the workshop and the written paper in the proceedings. In addition, there\r\nwas a &quot;Best Reviewer Voting&quot; for the first time in the history of CVWW. The result was a ranking of the members of the program\r\ncommittee, which should be updated and continued in future workshops.\r\n\r\nThe co-chairs of the workshop would like to thank the local organizing committee and the members of the program committee for\r\ntheir work, support and feedback.\r\n\r\nWalter G. Kropatsch\r\nInes Janusch\r\nNicole M. Artner\r\n\r\nCVWW co-chairs",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., Janusch, I., &#38; Kropatsch, W. (Eds.). (2017). <i>Proceedings of the 22nd Computer Vision Winter Workshop 2017</i>. PRIP, TU Wien. http://hdl.handle.net/20.500.12708/24354</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Editor",
                    "tid": "37618"
                },
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 2,
                    "role": "Editor",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Editor",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "27621",
            "handle": "20.500.12708/24355",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Proceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Discrete Geometry for Computer Imagery",
            "keywords": [
                "applied computing",
                "image analysis",
                "discrete modeling and visualization"
            ],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Artner, N., &#38; Janusch, I. (Eds.). (2017). <i>Discrete Geometry for Computer Imagery</i>. Springer International Publishing. https://doi.org/10.1007/978-3-319-66272-5</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Editor",
                    "tid": "37618"
                },
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 3,
                    "role": "Editor",
                    "tid": "44201"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "28615",
            "handle": "20.500.12708/25343",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Mathematical Morphology Applied to Circular Data",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A. (2003). Mathematical Morphology Applied to Circular Data. In <i>Advances in Imaging and Electron Physics</i> (pp. 123–204). ACM Press. http://hdl.handle.net/20.500.12708/25343</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "28649",
            "handle": "20.500.12708/25377",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "General Training of Spatial Abilities by Geometry Education in Augmented Reality",
            "keywords": [],
            "abstract": "Geometry education has proven as one powerful means of improving spatial abilities, an important component of human intelligence. In the first part of this paper we summarize our development of a system that uses collaborative augmented reality as a medium for teaching, and uses 3D dynamic geometry to facilitate mathematics and geometry education. Our immersive collaborative educational application, specifically developed for geometry education, serves as the basis of a comprehensive evaluation study regarding its efficacy in training spatial abilities. The main contribution is the description of evaluation design including the test instruments, learning tasks and practical experiences with using our system for actual training of high school students. Results of a pre-study with spatial ability tests in high schools are presented. They point to interesting gender-specific differences of strategies when solving spatial ability tests, which have not been reported in literature before.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., Steinbügl, K., Dünser, A., &#38; Glück, J. (2005). General Training of Spatial Abilities by Geometry Education in Augmented Reality. In <i>Annual Review of CyberTherapy and Telemedicine: A Decade of VR, vol. 3</i> (pp. 65–76). Annual Review of CyberTherapy and Telemedicine. http://hdl.handle.net/20.500.12708/25377</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Karin",
                    "last_name": "Steinbügl",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Dünser",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Judith",
                    "last_name": "Glück",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "28656",
            "handle": "20.500.12708/25384",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Long Distance Distribution of Educational Augmented Reality Applications",
            "keywords": [],
            "abstract": "For distance education utilizing shared Virtual or Augmented Reality (VR/AR) applications, reliable network distribution\r\nof educational content is of prime importance. In this paper we summarize the development of software\r\ncomponents enabling stable and reliable distribution of an existing educational AR application for geometry education.\r\nOur efforts focus on three main areas: (1) For long distance distribution of Open Inventor scene graphs,\r\nthroughout a wide area IP network, a TCP based network protocol was implemented in Distributed Open Inventor.\r\n(2) A tracking middleware was extended to support sending tracking data unicast instead or in addition to sending\r\nmulticast messages. (3) Multiple adaptations in our geometry application were required to improve scalability, robustness\r\nand reliability. We present an early evaluation with high school students in a distant learning, distributed\r\nHMD setup and highlight final results.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., Csisinko, M., &#38; Totter, A. (2006). Long Distance Distribution of Educational Augmented Reality Applications. In <i>Educational Papers, Eurographics 2006</i> (pp. 23–33). Eurographics Association. http://hdl.handle.net/20.500.12708/25384</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 2,
                    "role": "Author",
                    "tid": "47153"
                },
                {
                    "first_name": "Alexandra",
                    "last_name": "Totter",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "28660",
            "handle": "20.500.12708/25388",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": true,
            "invited": false,
            "title": "An Event-Driven, Stochastic, Undirected Narrative (EDSUN) Framework for Interactive Contents",
            "keywords": [
                "augmented reality",
                "event driven",
                "stochastic transitions",
                "undirected narrative",
                "content",
                "framework",
                "grammar",
                "story telling"
            ],
            "abstract": "In this paper, we present an extensible framework for interactive multimodal contents, with emphasis on augmented reality applications. The proposed framework, EDSUN, enables concurrent and variable narrative structures as well as content reusability and dynamic yet natural experience generation. EDSUN's main components include a canonical specification of 5-state lexical syntax and grammar, stochastic state transitions, and extensions for hierarchical grammars to represent complex behavioral and multimodal interactions. The benefits of EDSUN in enabling classical contents to support the affordances of AR environments and in complementing recent published works are also discussed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Barclay, A. M., &#38; Kaufmann, H. (2006). An Event-Driven, Stochastic, Undirected Narrative (EDSUN) Framework for Interactive Contents. In S. Göbel, R. Malkewitz, &#38; I. Iurgel (Eds.), <i>Technologies for Interactive Digital Storytelling and Entertainment</i> (Vol. 4326, pp. 13–24). Springer-Verlag. https://doi.org/10.1007/11944577_2</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adam Madera",
                    "last_name": "Barclay",
                    "position": 1,
                    "role": "Author",
                    "tid": "54614"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Stefan",
                    "last_name": "Göbel",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Rainer",
                    "last_name": "Malkewitz",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Ido",
                    "last_name": "Iurgel",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "28664",
            "handle": "20.500.12708/25392",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "13. Hiearchies relating Topology and Geometry",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Haxhimusa, Y., &#38; Lienhardt, P. (2006). 13. Hiearchies relating Topology and Geometry. In <i>Cognitive Vision Systems: Sampling the Spectrum of Approaches</i> (pp. 199–220). Springer LNCS. http://hdl.handle.net/20.500.12708/25392</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Pascal",
                    "last_name": "Lienhardt",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E194-01"
            ],
            "pid": "28679",
            "handle": "20.500.12708/25407",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Semantics in Content-based Multimedia Retrieval",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H., &#38; Zaharieva, M. (2007). Semantics in Content-based Multimedia Retrieval. In <i>Multimedia Semantics - The Role of Metadata</i> (pp. 151–174). Springer. http://hdl.handle.net/20.500.12708/25407</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 2,
                    "role": "Author",
                    "tid": "39017"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "28681",
            "handle": "20.500.12708/25409",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": true,
            "invited": false,
            "title": "Multiresolution Image Segmentations in Graph Pyramids",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Haxhimusa, Y., &#38; Ion, A. (2007). Multiresolution Image Segmentations in Graph Pyramids. In A. Kandel, H. Bunke, &#38; M. Last (Eds.), <i>Applied Graph Theory in Computer Vision and Pattern Recognition</i> (pp. 3–41). Springer. http://hdl.handle.net/20.500.12708/25409</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Abraham",
                    "last_name": "Kandel",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bunke",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Last",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "2895",
            "handle": "20.500.12708/2911",
            "doi": "10.34726/hss.2016.40502",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Computational analysis of petroglyphs",
            "keywords": [
                "Computer Vision",
                "Segmentation",
                "Pixel Classification",
                "Shape Descriptors",
                "Surface Classification",
                "Surface Description",
                "Petroglyphs",
                "Computing and Cultural Heritage",
                "Valcamonica"
            ],
            "abstract": "Numerous petroglyphs have been pecked, scratched and carved into rock surfaces in the northern Italian valley Valcamonica. The classic documentation work carried out by archaeologists is a massively time-consuming process. The rising availability of digital images and 3D scans of petroglyphs facilitates digital workflows which can improve the documentation process. In this thesis, we aim at supporting the classic documentation pipeline for petroglyphs. The first step of the pipeline is the determination of the boundaries and spatial locations of petroglyphs on a rock surface. This is usually done by time-consuming manual contact tracing. Then, the found figures are classified according to their shapes and pecking styles. The large number of petroglyphs (Valcamonica contains up to 300.000 figures) demands large efforts for manual classification. The investigation of pecking styles is often impossible based on the contact tracings and thus requires researchers to return to the rocks frequently. Following the classic documentation pipeline for petroglyphs, we propose and evaluate novel methods. To determine the positions and shapes of petroglyphs on a rock panel we approach segmentation of 2D and 3D petroglyph images in pecked regions and natural rock surface. Furthermore, we use 3D scans to investigate the similarity of pecking styles, i.e. the shape, size, depth and spatial distribution of the peck marks a figure consists of. Finally, we develop a petroglyph shape descriptor which allows the classification of petroglyphs. Our tasks are challenging. The figures have been pecked over thousands of years. The rocks are subject to weathering and abrasion. Therefore, the visual and tactile appearance of the petroglyphs varies greatly. Figures have often been superimposed over existing figures. Consequently, many merged  and partial figures exist. Contrary to previous work by others, we show that the segmentation of 2D images of rock surfaces is feasible. The employment of illumination-independent high-resolution 3D data of the surfaces- microtopographies clearly improves results. We facilitate the investigation of pecking styles by modeling their similarity with 3D surface descriptors. The shape classification of a dataset containing more than thousand petroglyphs yields very good results with a combination of skeleton-, boundary-, and regionbased shape descriptors. Our results can be useful for rock art researchers. Furthermore, we suggest how they can be applied in other domains.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Seidl, M. (2016). <i>Computational analysis of petroglyphs</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.40502</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "5809",
                    "name": "Seidl Markus - 2016 - Computational analysis of petroglyphs.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 11532759,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2911/2/Seidl%20Markus%20-%202016%20-%20Computational%20analysis%20of%20petroglyphs.pdf"
                },
                {
                    "bsid": "79689",
                    "name": "Seidl Markus - 2016 - Computational analysis of petroglyphs.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 338215,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2911/5/Seidl%20Markus%20-%202016%20-%20Computational%20analysis%20of%20petroglyphs.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Seidl",
                    "position": 1,
                    "role": "Author",
                    "tid": "43980"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E120-06",
                "E193-03"
            ],
            "pid": "29466",
            "handle": "20.500.12708/26194",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": true,
            "invited": false,
            "title": "Pedestrian Behaviour Monitoring: Methods and Experiences",
            "keywords": [],
            "abstract": "Modern societies are characterised by a clear tendency towards individualism and\r\nindependence. Strategies for promoting independence and quality of life for people of every age are\r\nespecially important in the field of mobility. Mobility allows people to perform essential functions,\r\nincluding engaging in social and recreational activities when desired and reaching business and social\r\nservices when needed. Especially people who are restricted in physical mobility due to physicalneuromuscular\r\nhandicaps or handicaps caused by limited or missing sensory perception need special\r\nsupport. Pedestrians without physical constraints can also benefit from navigational and environmental\r\ninformation services when walking through unfamiliar environments. In this respect applied research\r\nhas produced a number of emerging technologies and technological services such as e.g. navigation\r\naids implemented on mobile devices that respect individual needs, in order to support self-determined\r\nmobility for completing basic daily tasks without personal assistance.\r\nAdvances in this field are strongly dependent on broad knowledge about people's behaviour with\r\nregard to motion, decisive decision processes and related influencing factors. Efficient assistance and\r\ntechnological services can only be developed on the basis of comprehensive investigation of spatiotemporal\r\nbehaviour and underlying determinants. Researchers of different disciplines, e.g. sociology,\r\ntourism and travel behaviour research, artificial intelligence, or ubiquitous geotechnology and\r\ngeoinformation, are therefore applying various methods in order to examine, analyse and interpret\r\npedestrian behaviour.\r\nThis contribution provides an overview about common methods for monitoring and analysing human\r\nspatio-temporal behaviour. Based on a detailed problem description and definition of related terms and\r\nexpressions the chapter comprises two main sections. The first part focuses on dataset generation.\r\nCommonly used methods for data collection are presented and discussed with respect to specific\r\nstrengths and limitations. Empirical methods presented in this section include e.g. vision-based object\r\ntracking, laser scanning, land-based localisation techniques (e.g. GSM, RFID, Bluetooth), satellitebased\r\nlocalisation (GPS), shadowing and observation methods (unobtrusive observation, participatory\r\nobservation), and interview survey techniques (questionnaires, narrative interviews, trip diaries). The\r\nsecond part focuses on data analysis. Methods for analysing specific datasets are discussed, including\r\ne.g. stop detection, velocity histograms, use of space (density maps), cluster analysis, and descriptive\r\nand inferential statistics.\r\nThe chapter concludes with a discussion of the applicability of the presented datasets and related\r\nempirical methods for selected research foci on human spatio-temporal behaviour, e.g. travel\r\nbehaviour research, tourism research, crowd dynamics, or the development of agent-based simulation\r\nmodels.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Millonig, A., Brändle, N., Ray, M., Bauer, D., &#38; van der Spek, S. (2009). Pedestrian Behaviour Monitoring: Methods and Experiences. In B. Gottfried &#38; H. Aghajan (Eds.), <i>Behaviour Monitoring and Interpretation -- BMI: Smart Environments</i> (Vol. 3, pp. 11–42). IOS Press. https://doi.org/10.3233/978-1-60750-048-3-11</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Alexandra",
                    "last_name": "Millonig",
                    "position": 1,
                    "role": "Author",
                    "tid": "50370"
                },
                {
                    "first_name": "Norbert",
                    "last_name": "Brändle",
                    "position": 2,
                    "role": "Author",
                    "tid": "116137"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Ray",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Dietmar",
                    "last_name": "Bauer",
                    "position": 4,
                    "role": "Author",
                    "tid": "37141"
                },
                {
                    "first_name": "Stefan",
                    "last_name": "van der Spek",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Björn",
                    "last_name": "Gottfried",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Hamid",
                    "last_name": "Aghajan",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "29593",
            "handle": "20.500.12708/26321",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Pottery Plotted by Laser - 3D Acquisition for Documentation and Analysis of Symmetry of Ancient Ceramics",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mara, H. (2009). Pottery Plotted by Laser - 3D Acquisition for Documentation and Analysis of Symmetry of Ancient Ceramics. In M. Reindel &#38; G. Wagner (Eds.), <i>New Technologies for Archaeology</i> (pp. 379–390). Springer-Verlag. http://hdl.handle.net/20.500.12708/26321</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 1,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Reindel",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Günther",
                    "last_name": "Wagner",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "2962",
            "handle": "20.500.12708/2978",
            "doi": "10.34726/hss.2017.47643",
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Virtual Reality zum Anfassen : VR-System mit robotergestütztem haptischem Feedback : VR system with robotic haptic feedback",
            "keywords": [
                "Virtual Reality",
                "Haptic Feedback",
                "Framework",
                "Robot"
            ],
            "abstract": "Continuously improving hardware increases the realism in VR However, the immersion is often disturbed due to the lack of appropriate haptic feedback when interacting with virtual objects. Existing approaches like haptic input devices or exoskeletons cannot fully solve this problem, as the provided feedback is either too simple or user movement is restricted. Simulators can provide suitable haptic feedback but they cannot be used for different applications. There are special haptic feedback systems that do not suffer most of these constraints. They use varying methods, e.g. physical props, to simulate the sensation during the interaction with an object. The main drawback of existing feedback systems is that the required physical space increases with larger virtual worlds. This thesis describes the design and the implementation of an immersive VR system that provides haptic feedback by using a robotic arm to position physical props around the user. As a basis for the system, a framework was developed that combines various out-of-the-box VR components with a custom-built robotic arm. A locomotion platform enables virtual worlds of unlimited size on a constant physical space. The system provides an easy to use Unity3D framework wrapping the required software libraries and a robotics system. This framework builds the basis for creating new VR applications with robotic haptic feedback. The framework allows to use only a subset of the supported VR devices without changing the application and also enables later modifications on the robotic arm. The performed technical evaluation and user study prove the functional capability of the system and indicate positive effects of haptic feedback on the VR experience. Safety guidelines for the use of the systems are specified to minimize the risk when using the system. The user study proves that these guidelines are satisfactory.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gatterer, C. (2017). <i>Virtual Reality zum Anfassen : VR-System mit robotergestütztem haptischem Feedback : VR system with robotic haptic feedback</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2017.47643</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "5943",
                    "name": "Gatterer Clemens - 2017 - Virtual Reality zum Anfassen VR-System mit...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 13969683,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2978/2/Gatterer%20Clemens%20-%202017%20-%20Virtual%20Reality%20zum%20Anfassen%20VR-System%20mit...pdf"
                },
                {
                    "bsid": "79170",
                    "name": "Gatterer Clemens - 2017 - Virtual Reality zum Anfassen VR-System mit...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 221671,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2978/5/Gatterer%20Clemens%20-%202017%20-%20Virtual%20Reality%20zum%20Anfassen%20VR-System%20mit...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Clemens",
                    "last_name": "Gatterer",
                    "position": 1,
                    "role": "Author",
                    "tid": "56871"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "40682"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "29665",
            "handle": "20.500.12708/26393",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Playmancer Project: A Serious Videogame as Additional Therapy Tool for Eating and Impulse Control Disorders",
            "keywords": [],
            "abstract": "Reviews and few non-controlled studies showed the effectiveness of\r\nseveral specific designed computer video-games as an additional form of treatment\r\nin several areas. However, there is a lack in the literature of specially designed\r\nserious-games for treating mental disorders. Playmancer (ICT European initiative)\r\naims to develop and assess a serious videogame that may help to treat underlying\r\nprocesses (e.g. lack of self-control strategies) in Eating and Impulse control\r\ndisorders. Preliminary data will be shown.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Jiménez-Murcia, S., Fernandez Aranda, F., Kalapanidas, E., Konstantas, D., Ganchev, T., Kocsis, O., Lam, T., Santamaría, J., Raguin, T., Breiteneder, C., Kaufmann, H., &#38; Davarakis, C. (2009). Playmancer Project: A Serious Videogame as Additional Therapy Tool for Eating and Impulse Control Disorders. In B. Wiederhold &#38; G. Riva (Eds.), <i>Annual Review of Cybertherapy and Telemedicine 2009: Advanced Technologies in the Behavioral, Social and Neurosciences</i> (pp. 163–166). IOS Press. http://hdl.handle.net/20.500.12708/26393</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Susanna",
                    "last_name": "Jiménez-Murcia",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Fernando",
                    "last_name": "Fernandez Aranda",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Elias",
                    "last_name": "Kalapanidas",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Dimitri",
                    "last_name": "Konstantas",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Todor",
                    "last_name": "Ganchev",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Otilia",
                    "last_name": "Kocsis",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Tony",
                    "last_name": "Lam",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Juanjo",
                    "last_name": "Santamaría",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Thierry",
                    "last_name": "Raguin",
                    "position": 9,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 10,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 11,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Costas",
                    "last_name": "Davarakis",
                    "position": 12,
                    "role": "Author"
                },
                {
                    "first_name": "Brenda",
                    "last_name": "Wiederhold",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Guiseppe",
                    "last_name": "Riva",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "2971",
            "handle": "20.500.12708/2987",
            "doi": "10.34726/hss.2017.51247",
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Weakly-supervised learning of visual object models",
            "keywords": [
                "Machine Learning",
                "Supervised Learning"
            ],
            "abstract": "This master’s thesis investigates the weakly-supervised visual object detection from a given set of images. The main goal is set as obtaining an optimal object model for any selected visual object by learning from positive- and negative-labeled images. To this end, an analysis process is proposed that gathers segments of the target visual object from positive training images. A more common object model is built by determining the most discriminative detected object segments. This ultimate form of the object model is employed by a binary classifier in order to detect segments of a target visual object from test images. The proposed approach for the recovery of an optimal object model comprises of four major processing steps: segmentation, feature extraction, similarity measurement, and learning. For each of these steps, the suitability of different techniques is evaluated. Firstly, an evaluation with respect to segmentation is made with mean shift segmentation and a simpler and faster sliding window approach. Secondly, different types of features (color histograms, dense SIFT descriptors, PHOW descriptors, VLAD descriptors, CEDD and MPEG-7 color descriptors) are evaluated for the description of the segments obtained in the first step. Thirdly, in similarity measurement an evaluation involves different distance and similarity functions. Lastly, in the learning step a non-parametric discriminative learning scheme based on information gain is employed. The result of learning is a ranking that expresses the distinctiveness of each candidate segment. In addition, MPEG video encoding is investigated as an alternative technique for both feature extraction and similarity measurement. For this purpose, an approach originating from texture classification is extended to color image segments. The experimental results demonstrate lower computational complexity for all combinations of investigated feature descriptors and distance functions compared to the MPEG video encoding-based approach. Furthermore, the accuracy of visual object models obtained by MPEG video encoding is lower than those presented in some of the proposed approaches employing separate processing steps for feature extraction and similarity measurement. These master thesis results suggest using the feature descriptors obtained from VLAD descriptors and one of three distance functions: chi square statistics, diffusion distance, and Euclidean distance. Moreover, an evaluation of target object detectors is performed by selecting one of the recovered object models for each evaluation. The target object detector is an SVM classifier, a Naïve Bayes classifier, and an alternative approach employing information gain to learn decision thresholds.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ince, S. A. (2017). <i>Weakly-supervised learning of visual object models</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2017.51247</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "5961",
                    "name": "Ince Sami Alper - 2017 - Weakly-supervised learning of visual object models.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 18632173,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2987/2/Ince%20Sami%20Alper%20-%202017%20-%20Weakly-supervised%20learning%20of%20visual%20object%20models.pdf"
                },
                {
                    "bsid": "74374",
                    "name": "Ince Sami Alper - 2017 - Weakly-supervised learning of visual object models.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 361867,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/2987/5/Ince%20Sami%20Alper%20-%202017%20-%20Weakly-supervised%20learning%20of%20visual%20object%20models.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Sami Alper",
                    "last_name": "Ince",
                    "position": 1,
                    "role": "Author",
                    "tid": "187284"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "159676"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "53598"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E180"
            ],
            "pid": "2994",
            "handle": "20.500.12708/3010",
            "doi": "10.34726/hss.2017.46560",
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "3D Interaction within a multi-user distributed untethered virtual reality training simulation",
            "keywords": [
                "Virtual Reality",
                "Multi-User",
                "3D Interaction",
                "Distributed VR Spaces",
                "First Responder",
                "Training"
            ],
            "abstract": "Catastrophes happen worldwide, in some areas more often than in others. The first priority is to circumvent these catastrophes. When a catastrophe is not preventable or out of human influence, preparations need to be made. Not only are materials reserves and structural measures required, but also the personnel in the disaster relief management needs to be prepared. The squad leaders, in particular, have to be prepared, to make the right decisions, which influence the outcome of the relief operation tremendously. Therefore, the squad leaders need to be either experienced or are well trained. However, to effectively train squad leader, simulation exercises need to be executed to compensate the experience gap. To arrange large simulation exercises, many relief units, a lot of materials, a suitable location, and role play actors are required. Additionally, the government and the local communities must endorse ambitious simulations. In order to create an alternative to real simulation exercises, the FFG funded project, \"VROnSite\" was started in the year 2015. The aim of this project was to develop and evaluate the first fully mobile, generic, multi-user immersive virtual reality platform  to train squad leaders of first responder units, such as fire brigades, paramedics, police forces and other disaster relief units. The goal of this master thesis was to extend the VROnSite project by developing a distributed 3D interaction system to allow for immersive training of multiple first responders, supervised by a trainer, within a virtual reality environment. For the visualization part of the scenario, mobile head-mounted displays are used. To navigate through and interact within the virtual reality, gamepads and omnidirectional treadmills are used. To make the system able to perform the training simulation with multiple users, a distributed layer had to be integrated. The main intention of the system is to train squad leaders upon arrival at the disaster site. To enable simulation and training of the entire command cycle, distributed 3D interactions are going to be developed, including 3D selection and manipulation tasks employed by the participants within the immersive simulation, and 3D manipulation tasks by the trainer, using desktop input. The innovative modules will be tested with actual stakeholders, such as fire brigade and paramedic training centers.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Perndorfer, R. (2017). <i>3D Interaction within a multi-user distributed untethered virtual reality training simulation</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2017.46560</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "6007",
                    "name": "Perndorfer Rafael - 2017 - 3D Interaction within a multi-user distributed...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 7865383,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3010/2/Perndorfer%20Rafael%20-%202017%20-%203D%20Interaction%20within%20a%20multi-user%20distributed...pdf"
                },
                {
                    "bsid": "75793",
                    "name": "Perndorfer Rafael - 2017 - 3D Interaction within a multi-user distributed...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 200710,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3010/5/Perndorfer%20Rafael%20-%202017%20-%203D%20Interaction%20within%20a%20multi-user%20distributed...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Rafael",
                    "last_name": "Perndorfer",
                    "position": 1,
                    "role": "Author",
                    "tid": "203013"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "30100",
            "handle": "20.500.12708/26827",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": true,
            "invited": false,
            "title": "Features for Content-Based Audio Retrieval",
            "keywords": [],
            "abstract": "Today, a large number of audio features exists in audio retrieval for ifferent purposes, such as automatic speech recognition, music information retrieval, audio segmentation, and environmental sound retrieval. The goal of this paper is to review latest research in the context of audio feature extraction and to give an application-independent overview of the most important existing techniques. We survey state-of-the-art features from various domains and propose a novel taxonomy for the organization of audio features. Additionally, we identify the building blocks of audio features and propose a scheme that allows for the description of arbitrary features. We present an extensive literature survey and provide more than 150 references to relevant high quality publications.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mitrovic, D., Zeppelzauer, M., &#38; Breiteneder, C. (2010). Features for Content-Based Audio Retrieval. In <i>Advances in Computers Volume 78 Improving the Web</i> (pp. 71–150). Elsevier. http://hdl.handle.net/20.500.12708/26827</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 1,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 3,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "3045",
            "handle": "20.500.12708/3061",
            "doi": "10.34726/hss.2014.25872",
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Interaction in dense one-handed handheld augmented reality",
            "keywords": [
                "Handheld Augmented Reality",
                "3D Interaction Techniques",
                "3D Manipulation",
                "3D Selection",
                "3D Polygonal Modeling",
                "Shape Detection",
                "Dense Virtual Environments"
            ],
            "abstract": "The rapid improvement of handheld hardware devices enable real-time rendering of a large number of 3D models in handheld augmented reality (AR) environments. Although there are already many AR applications available for handheld devices, they often remain limited regarding their degree of interactivity compared to desktop AR applications. Important interactions in AR environments are the effortless creation of 3D models, precise selection and six-degree-of-freedom (6DOF) manipulation of virtual objects. Due to the imprecise input of the touch interface, the small screen size and complex touch gestures, all three interactions suffer in one-handed handheld AR setups of different limitations. In this thesis novel one-handed handheld interaction techniques for modeling, selection and manipulation of virtual content are introduced. A simple polygonal modeling technique is presented that uses real-time shape detection to collect handdrawn shapes that are used for Perspective Driven Modeling by extrusion or lathing in an AR environment. The novel selection technique DrillSample, designed particularly for the precise selection and disambiguation of virtual objects, is evaluated against state of the selection techniques. DrillSample is inspired of taking a core sample, e.g. of earth sediments, and designed with a focus on simple touch input and virtual context preservation. Two new and competing manipulation techniques, 3DTouch and HOMER-S, for the intuitive translation, rotation and scaling of virtual objects are introduced. 3DTouch focuses on simple touch input and degree of freedom separation, down to a single DOF, making the different DOF easily accessible from different view perspectives. HOMER-S on the other hand provides integral manipulation up to 6DOF and avoids touch input completely by mapping the handheld-s  device pose to the manipulated object. Both techniques are evalulated in a thorough user study followed by a statistical evaluation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Venditti, B. (2014). <i>Interaction in dense one-handed handheld augmented reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2014.25872</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "6109",
                    "name": "Venditti Benjamin - 2014 - Interaction in dense one-handed handheld augmented...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 19182015,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3061/2/Venditti%20Benjamin%20-%202014%20-%20Interaction%20in%20dense%20one-handed%20handheld%20augmented...pdf"
                },
                {
                    "bsid": "79118",
                    "name": "Venditti Benjamin - 2014 - Interaction in dense one-handed handheld augmented...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 237791,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3061/5/Venditti%20Benjamin%20-%202014%20-%20Interaction%20in%20dense%20one-handed%20handheld%20augmented...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Benjamin",
                    "last_name": "Venditti",
                    "position": 1,
                    "role": "Author",
                    "tid": "182097"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "30713",
            "handle": "20.500.12708/27439",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Wireless Displays in Educational Augmented Reality Applications",
            "keywords": [],
            "abstract": "Augmented Reality (AR) as defined by Azuma [1] does not pose restrictions on\r\noutput devices to be used for AR. Starting with light-weight notebooks and ultra\r\nmobile PCs, recently smartphones became favorite AR output devices. They represent\r\na class of self contained computing units, providing (usually limited) computing\r\npower as well as input and output peripherals - all in one device.\r\nIn contrast to that are output devices without a general computing processing unit\r\nwhich rely on external source to create and transfer image data. The latter are of\r\ninterest in this work.\r\nIn this chapter we present wireless technologies that can be used to transmit uncompressed\r\nstereoscopic video signals to wireless displays in real time. We introduce\r\ntwo output devices, a stereoscopic head mounted display (HMD) and a TFT\r\ndisplay module. Both of them have been adapted to act as wireless receivers in order\r\nto display wirelessly streamed AR content. Next we focus on advantages of\r\nwireless displays for educational AR applications. By way of example two educational\r\nAR applications are presented which were used to demonstrate and test\r\nwireless displays. A number of teaching scenarios are described where teachers\r\nand students greatly benefit from the use of wireless displays. We briefly summarize\r\nthe results of our observations while developing and evaluating these displays.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., &#38; Csisinko, M. (2011). Wireless Displays in Educational Augmented Reality Applications. In <i>Handbook of Augmented Reality: Technologies and Applications</i> (pp. 157–175). Springer. http://hdl.handle.net/20.500.12708/27439</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 2,
                    "role": "Author",
                    "tid": "47153"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "30982",
            "handle": "20.500.12708/27708",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": true,
            "invited": false,
            "title": "Image Processing and Analysis with Graphs: Theory and Practice",
            "keywords": [],
            "abstract": "Introduction\r\n\r\nRegular Pyramids\r\n\r\nIrregular Pyramids Parallel construction schemes\r\n\r\nIrregular Pyramids and Image properties",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brun, L., &#38; Kropatsch, W. (2012). Image Processing and Analysis with Graphs: Theory and Practice. In O. Lézoray &#38; L. Grady (Eds.), <i>Image Processing and Analysis with Graphs: Theory and Practice Image Processing and Analysis with Graphs: Theory and Practice</i> (pp. 305–349). CRC Press. http://hdl.handle.net/20.500.12708/27708</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Luc",
                    "last_name": "Brun",
                    "position": 1,
                    "role": "Author",
                    "tid": "227964"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Olivier",
                    "last_name": "Lézoray",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Leo",
                    "last_name": "Grady",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "30983",
            "handle": "20.500.12708/27709",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": true,
            "invited": false,
            "title": "Artificial Visual Attention Using Combinatorial Pyramids",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Antunez, E., Haxhimusa, Y., Marfill, R., Kropatsch, W., &#38; Banderas, A. J. (2012). Artificial Visual Attention Using Combinatorial Pyramids. In J. Rodriguez-Garcia &#38; M. Cazorla (Eds.), <i>Robotic Vision: Technologies for Machine Learning and Vision Applications</i> (pp. 437–455). IGI Global. https://doi.org/10.4018/978-1-4666-2672-0.ch022</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Esther",
                    "last_name": "Antunez",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Rebeca",
                    "last_name": "Marfill",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Antonio J.",
                    "last_name": "Banderas",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Jose",
                    "last_name": "Rodriguez-Garcia",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Miguel",
                    "last_name": "Cazorla",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "31760",
            "handle": "20.500.12708/28485",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Walking in Virtual Reality: Flexible Spaces and Other Techniques",
            "keywords": [],
            "abstract": "In many virtual reality applications the virtual world is larger than the available physical workspace. Multiple mechanical solutions have been developed to support the exploration of large virtual environments. However real walking is still the most immersive way of supporting locomotion in a virtual environment. Redirected walking techniques enable natural locomotion through large scale virtual worlds.\r\nIn this chapter we briefly discuss some of the existing interfaces for walking and focus on existing approaches for redirected walking. We will focus specifically on spatial manipulations techniques and introduce a novel approach for their use - flexible spaces. This is a novel redirection technique that enables infinite real walking in virtual environments that do not require the replication of real world layouts. This approach allows designers of virtual environments to focus on the content of the virtual world independent of the implementation details imposed by real walking, thereby making spatial manipulation techniques more practical for use in a variety of application domains.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vasylevska, K., Podkosova, I., &#38; Kaufmann, H. (2015). Walking in Virtual Reality: Flexible Spaces and Other Techniques. In L. Cocchiarella (Ed.), <i>The Visual Language of Technique</i> (pp. 81–97). Springer International Publishing. https://doi.org/10.1007/978-3-319-05341-7_7</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 2,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Luigi",
                    "last_name": "Cocchiarella",
                    "position": 1,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06"
            ],
            "pid": "32215",
            "handle": "20.500.12708/28940",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Book Contribution",
            "peer_reviewed": false,
            "invited": false,
            "title": "Towards a Taxonomy of Display Styles for Ubiquitious Multimedia",
            "keywords": [],
            "abstract": "In this article, a domain independent taxonomy of sign functions, rooted in an analysis of physical signs found in public space, is presented. This knowledge is necessary for the construction of future multimedia systems that are capable of automatically generating complex yet legible graphical responses from an underlying abstract information space such as a semantic network. The authors take the presence of a sign in the real world as indication for a demand for the information encoded in that sign, and identify the fundamental types of information that are needed to fulfill various tasks. For the information types listed in the taxonomy, strategies for rendering the information to the user in digital, mobile multimedia systems are discussed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ledermann, F., &#38; Breiteneder, C. (2006). Towards a Taxonomy of Display Styles for Ubiquitious Multimedia. In <i>Handbook of Research on Mobile Multimedia</i> (pp. 383–398). IGI Global. https://doi.org/10.4018/978-1-59140-866-6.ch026</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Florian",
                    "last_name": "Ledermann",
                    "position": 1,
                    "role": "Author",
                    "tid": "52487"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 2,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E186"
            ],
            "pid": "3278",
            "handle": "20.500.12708/3294",
            "doi": "10.34726/hss.2016.33981",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Gesichtserkennung in uneingeschränkten Video streams",
            "keywords": [
                "Face recognition",
                "video search"
            ],
            "abstract": "Face recognition is a long-standing topic in machine learning. Recent advances promise successful applications in large-scale video surveillance. Recognizing millions of people in thousands of streaming cameras, however, implies significant costs in terms of computations and communication bandwidth. This work aims to reduce the complexity of face recognition in video and pursues two complementary strategies. We first investigate face recognition in the whitened descriptor space and improve the complexity of the state-of-the-art Matched Background Similarity (MBGS) approach. Second, we study frame selection and demonstrate robust recognition performance from a small fraction of video frames. For a better understanding of the advancing Deep Convolution Neural Networks field, we give a thorough introduction to the topic and briefly summarize the recent development with a focus on the ImageNet classification challenge. We evaluate our methods and present competitive results for the YTF and TSFT benchmarks while significantly reducing the complexity of previous methods.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Omenitsch, P. (2016). <i>Gesichtserkennung in uneingeschränkten Video streams</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.33981</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "6575",
                    "name": "Omenitsch Philipp - 2016 - Gesichtserkennung in uneingeschraenkten Video streams.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 11967160,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3294/2/Omenitsch%20Philipp%20-%202016%20-%20Gesichtserkennung%20in%20uneingeschraenkten%20Video%20streams.pdf"
                },
                {
                    "bsid": "75403",
                    "name": "Omenitsch Philipp - 2016 - Gesichtserkennung in uneingeschraenkten Video streams.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 132704,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3294/5/Omenitsch%20Philipp%20-%202016%20-%20Gesichtserkennung%20in%20uneingeschraenkten%20Video%20streams.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Philipp",
                    "last_name": "Omenitsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "202710"
                },
                {
                    "first_name": "Ivan",
                    "last_name": "Laptev",
                    "position": 1,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "3304",
            "handle": "20.500.12708/3320",
            "doi": "10.34726/hss.2016.35420",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Speckle Tracking : ACTO Tracking durch optische Maussensoren",
            "keywords": [
                "Tangible User Interfaces",
                "Tracking",
                "optical sensors",
                "Mixed Reality"
            ],
            "abstract": "The use of Tangible User Interfaces (TUIs) for medical and entertainment purposes seems promising because of their mostly very intuitive handling and playful aspect (computer mouse, steering wheel, etc.). TUIs can be supplemented with elements of mobile robots to allow independent movement as feedback. The Actuated Tangible User Interface Object-project (ACTO [3]) is focused on these actuated TUIs, resulting in cubic robots called ACTOs. Every ACTO is modularly composed and, in addition to the movement unit, can be equipped with a variety of input and output extensions. To control and coordinate the movement of the ACTOs, their position needs to be retrieved in real time. Every method of determining an ACTO-s position has its own advantages and disadvantages. Until now, the position was obtained by the optical tracking of a marker at the ACTO-s bottom. This tracking method provides precise, absolute tracking data but cannot cope with poor illumination or movement which is too fast. To compensate for these shortcomings, this thesis introduces an extension which supplements the existing tracking system by another tracking method based on two optical mouse sensors. In this process the translation along the X- and Y-axis is read from both sensors. Based on these four translation values, the ACTO-s translation and rotation and therefore its new position is computed. The thesis discusses the ideas leading to the design and construction of this relative tracking system and therefore elaborates on the development of the hard- and software, as well as the mathematical background. In the end it is shown that the supplement of the absolute tracking by the relative tracking based on two optical mouse sensors provides considerable advantages in terms of precision and robustness. The system is composed of low-priced,  off-the-shelf components and is easily integrated into the existing ACTO system. Thus, the use of this system seems promising in various research areas. During the work on several prototypes and the coding of the software, great potential for more modifications and adaptions to various environments was revealed which is discussed in the last part of this thesis.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Bernhardt, D. (2016). <i>Speckle Tracking : ACTO Tracking durch optische Maussensoren</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.35420</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "6627",
                    "name": "Bernhardt Daniel - 2016 - Speckle Tracking ACTO Tracking durch optische...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 5940673,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3320/2/Bernhardt%20Daniel%20-%202016%20-%20Speckle%20Tracking%20ACTO%20Tracking%20durch%20optische...pdf"
                },
                {
                    "bsid": "79438",
                    "name": "Bernhardt Daniel - 2016 - Speckle Tracking ACTO Tracking durch optische...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 195913,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3320/5/Bernhardt%20Daniel%20-%202016%20-%20Speckle%20Tracking%20ACTO%20Tracking%20durch%20optische...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Daniel",
                    "last_name": "Bernhardt",
                    "position": 1,
                    "role": "Author",
                    "tid": "61966"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "40682"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "3305",
            "handle": "20.500.12708/3321",
            "doi": "10.34726/hss.2016.39671",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Entwicklung eines autonomen Überwachungssystems",
            "keywords": [
                "Sensorensystem",
                "Multisensorsystem",
                "Embedded Systems",
                "Eingebettete Systeme",
                "ARM-Prozessoren BeagleBone Raspberry Pii Systems Eingebettete Systeme ARM-Prozessoren BeagleBone Raspberry Pi"
            ],
            "abstract": "Video and audio monitoring systems are used across the globe to observe certain events and processes. Usually, such systems are connected to a communications network and electricity grid and are not equipped with any kind of processing logic (\"intelligence\") other than data compression. How can such systems be used if there is no electricity nor a broadband communications network available? The aim of this thesis is to develop an autonomous monitoring system built entirely from commercially-available standard components, which can operate independently. The first step to achieve this is to conduct a pre-evaluation of existing hardware and software components according to various criteria. The next step involves the development of a hardware prototype as well as the implementation of an efficient and modular software framework that can be adapted for different applications. The system should support policy-based recording modes such as modes controlled by input data from defined sensors, like motion and light sensors, that make autonomous decisions about the use of available resources. The software framework should be designed to support the integration of image processing algorithms to allow pre-processing of recorded (video) data on board in order to save bandwidth. In the final prototype, both the recording of audio and video (among other) data was realized to the desired extent. Using an application, the recording mode can be set easily using a XML configuration file. The power consumption of the build system is, depending on used sensors and executed tasks, between 0.68 to 1.75 watts and lies within the range of comparable commercial solutions, such as \"leanXcam\".",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Buturovic, A. (2016). <i>Entwicklung eines autonomen Überwachungssystems</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.39671</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "6629",
                    "name": "Buturovic Adis - 2016 - Entwicklung eines autonomen UEberwachungssystems.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 5527563,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3321/2/Buturovic%20Adis%20-%202016%20-%20Entwicklung%20eines%20autonomen%20UEberwachungssystems.pdf"
                },
                {
                    "bsid": "75411",
                    "name": "Buturovic Adis - 2016 - Entwicklung eines autonomen UEberwachungssystems.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 153034,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3321/5/Buturovic%20Adis%20-%202016%20-%20Entwicklung%20eines%20autonomen%20UEberwachungssystems.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Adis",
                    "last_name": "Buturovic",
                    "position": 1,
                    "role": "Author",
                    "tid": "40042"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "3307",
            "handle": "20.500.12708/3323",
            "doi": "10.34726/hss.2016.37468",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Real-time filtering of Monte Carlo noise on GPU",
            "keywords": [],
            "abstract": "Producing photo-realistic images, hardly distinguishable from the real photos, is one of the most important problems in computer graphics. Physically based rendering and particularly Monte Carlo path tracing is able to produce images of such quality by performing excessive sampling during Monte Carlo integration. The problem of Monte Carlo Integration is a high variance at low sampling rate. This variance appears as a noise in final image. In order to address such problem high-dimensional filtering is used. In this thesis we inspect the applicability of the Genetic Programming for the search of new high-dimensional filtering expressions and present three novel expressions generated by our method. Our method consists of iterative random changes of initial expressions until the finishing criterion is met and the comparison of the filtered pixel values, obtained with newly generated expressions, with the ground truth of the training scenes. The resulting expressions perform better than cross-bilateral filter with constant parameters. Additionally, our GPU implementation of identified expressions allows fast filtering of Monte Carlo noise with computational time of less than a second.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Davletaliyev, M. (2016). <i>Real-time filtering of Monte Carlo noise on GPU</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.37468</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "6633",
                    "name": "Davletaliyev Maxim - 2016 - Real-time filtering of Monte Carlo noise on GPU.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 16429183,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3323/2/Davletaliyev%20Maxim%20-%202016%20-%20Real-time%20filtering%20of%20Monte%20Carlo%20noise%20on%20GPU.pdf"
                },
                {
                    "bsid": "79439",
                    "name": "Davletaliyev Maxim - 2016 - Real-time filtering of Monte Carlo noise on GPU.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 116368,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3323/5/Davletaliyev%20Maxim%20-%202016%20-%20Real-time%20filtering%20of%20Monte%20Carlo%20noise%20on%20GPU.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Maxim",
                    "last_name": "Davletaliyev",
                    "position": 1,
                    "role": "Author",
                    "tid": "270629"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "3316",
            "handle": "20.500.12708/3332",
            "doi": "10.34726/hss.2016.37547",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Virtual Reality Menü Interaktion mit einer Smartwatch",
            "keywords": [
                "Smartwatch",
                "3D Interaction",
                "Virtual Reality"
            ],
            "abstract": "In immersive virtual environments users are placed inside virtual reality (VR) simulations. In such systems, menus are needed to change the system state, change tools or issue commands. However interacting with these menus in VR can be cumbersome, since input devices are not seen by the user due to the Head Mounted Displays. Therefore interaction needs to be simple and easy to adopt to. This diploma thesis presents a possibility to navigate VR menus with the help of a smartwatch over a WLAN. For this, a plugin for the UE4 game engine is implemented which contains components to create a smartwatch controllable menu. Furthermore an Android Wear application is designed and implemented to enable plug-and-play navigation of the menu via touch interaction. The resulting applications and the plugin are then tested to detect any issues regarding performance impact, with the help of one of four created example menus. The results reveal that performance is not affected considerably.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schuster, F. (2016). <i>Virtual Reality Menü Interaktion mit einer Smartwatch</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.37547</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "6651",
                    "name": "Schuster Florian - 2016 - Virtual Reality Menue Interaktion mit einer Smartwatch.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 15169259,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3332/2/Schuster%20Florian%20-%202016%20-%20Virtual%20Reality%20Menue%20Interaktion%20mit%20einer%20Smartwatch.pdf"
                },
                {
                    "bsid": "79487",
                    "name": "Schuster Florian - 2016 - Virtual Reality Menue Interaktion mit einer Smartwatch.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 153847,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3332/5/Schuster%20Florian%20-%202016%20-%20Virtual%20Reality%20Menue%20Interaktion%20mit%20einer%20Smartwatch.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Florian",
                    "last_name": "Schuster",
                    "position": 1,
                    "role": "Author",
                    "tid": "203020"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "3461",
            "handle": "20.500.12708/3477",
            "doi": "10.34726/hss.2018.48141",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Graph-based checkerboard indexing and its applications in camera calibration and multi-view processing",
            "keywords": [
                "Multi-Image calibration",
                "Image stitching"
            ],
            "abstract": "In this thesis, a new method for checkerboard indexing is presented. A graph-based approach is used to capture the topology of the checkerboard pattern. The regions and crosspoints of the pattern are identified using image segmentation and represented by a graph. At this point, no explicit mathematical model is assumed which describes the distortion in the checkerboard image. A special calibration pattern design allows the identification of a unique reference point in the pattern. This way, the crosspoints can be interpreted as the grid points of a target coordinate system. Possible applications in the fields of camera calibration and multi-view processing are discussed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Huber, H. (2018). <i>Graph-based checkerboard indexing and its applications in camera calibration and multi-view processing</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.48141</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "6941",
                    "name": "Huber Hanna - 2018 - Graph-based checkerboard indexing and its applications in...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 9533536,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3477/2/Huber%20Hanna%20-%202018%20-%20Graph-based%20checkerboard%20indexing%20and%20its%20applications%20in...pdf"
                },
                {
                    "bsid": "79136",
                    "name": "Huber Hanna - 2018 - Graph-based checkerboard indexing and its applications in...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 129542,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3477/5/Huber%20Hanna%20-%202018%20-%20Graph-based%20checkerboard%20indexing%20and%20its%20applications%20in...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Hanna",
                    "last_name": "Huber",
                    "position": 1,
                    "role": "Author",
                    "tid": "176956"
                },
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E104-04",
                "E193-03"
            ],
            "pid": "35151",
            "handle": "20.500.12708/31621",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "The isophotic metric and its application to feature sensitive morphology on surfaces",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pottmann, H., Steiner, T., Hofer, M., Haider, C., &#38; Hanbury, A. (2003). <i>The isophotic metric and its application to feature sensitive morphology on surfaces</i>. http://hdl.handle.net/20.500.12708/31621</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Helmut",
                    "last_name": "Pottmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "149417"
                },
                {
                    "first_name": "Tibor",
                    "last_name": "Steiner",
                    "position": 2,
                    "role": "Author",
                    "tid": "229509"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Hofer",
                    "position": 3,
                    "role": "Author",
                    "tid": "69129"
                },
                {
                    "first_name": "C.",
                    "last_name": "Haider",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 5,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "36340",
            "handle": "20.500.12708/32810",
            "doi": null,
            "year": 2002,
            "issued": "2002",
            "issued_on": "2002-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Reduction Factors of Pyramids on Undirected and Directed Graphs",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y., &#38; Kropatsch, W. (2002). <i>Reduction Factors of Pyramids on Undirected and Directed Graphs</i>. http://hdl.handle.net/20.500.12708/32810</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "36341",
            "handle": "20.500.12708/32811",
            "doi": null,
            "year": 2002,
            "issued": "2002",
            "issued_on": "2002-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Irregular Eigenimage Pyramids and Robust Appearance-Based Object Recognition",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langs, G., Bischof, H., &#38; Kropatsch, W. (2002). <i>Irregular Eigenimage Pyramids and Robust Appearance-Based Object Recognition</i>. http://hdl.handle.net/20.500.12708/32811</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 1,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "36449",
            "handle": "20.500.12708/32919",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Experimental Results of MIS, MIES, MIDES and D3P",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y., &#38; Kropatsch, W. (2003). <i>Experimental Results of MIS, MIES, MIDES and D3P</i>. http://hdl.handle.net/20.500.12708/32919</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "36450",
            "handle": "20.500.12708/32920",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Quality Control of Agro-alimentarxy Products",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Belbachir, A. N. (2003). <i>Quality Control of Agro-alimentarxy Products</i>. http://hdl.handle.net/20.500.12708/32920</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 1,
                    "role": "Author",
                    "tid": "217349"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "36451",
            "handle": "20.500.12708/32921",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "On Board Data Compression: Noise and Compexity Related Aspects",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Belbachir, A. N. (2003). <i>On Board Data Compression: Noise and Compexity Related Aspects</i>. http://hdl.handle.net/20.500.12708/32921</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 1,
                    "role": "Author",
                    "tid": "217349"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "36452",
            "handle": "20.500.12708/32922",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Image Compression using Hartley Transform",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Belbachir, A. N., &#38; Bischof, H. (2003). <i>Image Compression using Hartley Transform</i>. http://hdl.handle.net/20.500.12708/32922</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 1,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "36453",
            "handle": "20.500.12708/32923",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Hierarchical Image Partitioning with Dual Graph Contraction",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y., &#38; Kropatsch, W. (2003). <i>Hierarchical Image Partitioning with Dual Graph Contraction</i>. http://hdl.handle.net/20.500.12708/32923</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01"
            ],
            "pid": "36454",
            "handle": "20.500.12708/32924",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automatic Quantification of Destructive Changes caused by Rheumatoid Arthritis",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langs, G., Bischof, H., &#38; Peloschek, P. L. (2003). <i>Automatic Quantification of Destructive Changes caused by Rheumatoid Arthritis</i>. http://hdl.handle.net/20.500.12708/32924</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 1,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "36455",
            "handle": "20.500.12708/32925",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automated Profile Extraction of Archaeological Fragments",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mara, H. (2003). <i>Automated Profile Extraction of Archaeological Fragments</i>. http://hdl.handle.net/20.500.12708/32925</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 1,
                    "role": "Author",
                    "tid": "51366"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "36551",
            "handle": "20.500.12708/33021",
            "doi": null,
            "year": 2004,
            "issued": "2004",
            "issued_on": "2004-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "On-Ground Astronomical Data Processing",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hirz, E., Belbachir, A. N., &#38; Sablatnig, R. (2004). <i>On-Ground Astronomical Data Processing</i>. http://hdl.handle.net/20.500.12708/33021</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ernst",
                    "last_name": "Hirz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 2,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 3,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "36575",
            "handle": "20.500.12708/33045",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Integral trees: Subtree depth and diameter, PRIP TR-092",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Haxhimusa, Y., &#38; Pizlo, Z. (2005). <i>Integral trees: Subtree depth and diameter, PRIP TR-092</i>. http://hdl.handle.net/20.500.12708/33045</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Zygmunt",
                    "last_name": "Pizlo",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "3915",
            "handle": "20.500.12708/3931",
            "doi": "10.34726/hss.2014.25871",
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Robust wide-area tracking and intuitive 3D interaction for mixed reality environments",
            "keywords": [
                "Wide area tracking",
                "3d interaction",
                "mixed reality"
            ],
            "abstract": "Mixed reality has been a focus of research for many years and has recently gained particular importance with the emergence of powerful, low-cost input- and output devices as well as processing platforms that foster the applicability of virtual simulations for everyday usage. However, this leads to signicant challenges since the creation of compelling mixed reality environments requires knowledge and robust techniques in the areas tracking, visualization, interaction and in the non-obligatory areas distribution and authoring. This thesis focuses on the development of novel techniques and algorithms to contribute to the solution of fundamental problems in the areas of tracking, interaction, and application development of mixed reality systems. Firstly, a novel system for wide-area optical tracking in unconstrained indoor environments is presented that is capable of stereo camera calibration and model-based tracking of rigid-body targets in environments with poor illumination, static and moving ambient light sources, occlusions and harsh conditions, such as fog. The experimental results demonstrate the system's capabilities to track targets up to 90m and its applicability to act as a mixed reality tracking system as well as a general purpose measurement tool for future (underground) surveying tasks, such as autonomous machine guidance. Secondly, we investigated concepts for intuitive 3D interaction in virtual environments, specically in one-handed handheld mixed reality. To address the shortcomings of state-of-the art 3D selection and manipulation techniques, the novel algorithms DrillSample for selection, and 3DTouch and HOMER-S for manipulation are proposed. All three approaches aim at reducing the necessary input through the user's ngers to provide easy to understand and straightforward interaction. Therefore,  they incorporate the 6-degree-of-freedom pose that is obtained through optical tracking, resulting in a one-finger interaction for precise selection of partly or fully occluded objects with high visual similarity. Thirdly, the novel software framework ARTIFICe is presented that facilitates the development of compelling mixed reality environments. It aims at minimizing the initial hurdles of application development as it is inexpensive and provides a powerful graphical interface to easily access and author tracking, interaction, visualization and distribution. With the presented contribution, we aim at leveraging the applicability of mixed reality into unconstrained everyday environments that are used by non-experts.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A. (2014). <i>Robust wide-area tracking and intuitive 3D interaction for mixed reality environments</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2014.25871</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "7849",
                    "name": "Mossel Annette - 2014 - Robust wide-area tracking and intuitive 3D interaction...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 14425807,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3931/2/Mossel%20Annette%20-%202014%20-%20Robust%20wide-area%20tracking%20and%20intuitive%203D%20interaction...pdf"
                },
                {
                    "bsid": "77165",
                    "name": "Mossel Annette - 2014 - Robust wide-area tracking and intuitive 3D interaction...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 459318,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/3931/5/Mossel%20Annette%20-%202014%20-%20Robust%20wide-area%20tracking%20and%20intuitive%203D%20interaction...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "4063",
            "handle": "20.500.12708/4079",
            "doi": "10.34726/hss.2015.30570",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Development and evaluation of a virtual reality adventure game for the virtualizer",
            "keywords": [
                "Immersive Virtual Reality"
            ],
            "abstract": "This thesis describes the design, implementation and evaluation of a virtual reality (VR) adventure game for the Virtualizer, the PrioVR and the Oculus Rift. First an overview is given about the state-of-the art in terms of head-mounted displays (HMDs), motion capture suits, game motion controllers and technologies for walking in virtual reality. Further, an overview is given about already developed VR games. Cybersickness is a major problem in VR games. There are three theories that try to explain the process of becoming cybersick, which are described in this thesis. Further, factors that can influence the possibility of getting cybersick are mentioned and approaches that can reduce the occurrence of cybersickness are described. In VR it is necessary to be able to interact in the virtual world like in the real world as well as seeing your own virtual body that follows your real-world movements to create a feeling of presence in the virtual world. Recent technologies enable this kind of interaction for the casual player at home. Up to now, no game has been developed that uses the combination of the Oculus Rift, the PrioVR and the Virtualizer. Therefore this thesis describes the development of a VR game for these technologies. User tests were conducted to find out about how good the interaction with these technologies is working and if the participants get cybersick. Further, design guidelines are discussed and technical problems that occurred during the user tests are described.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Broneder, E. (2015). <i>Development and evaluation of a virtual reality adventure game for the virtualizer</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.30570</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "8145",
                    "name": "Broneder Elisabeth - 2015 - Development and evaluation of a virtual reality...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4469460,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4079/2/Broneder%20Elisabeth%20-%202015%20-%20Development%20and%20evaluation%20of%20a%20virtual%20reality...pdf"
                },
                {
                    "bsid": "79593",
                    "name": "Broneder Elisabeth - 2015 - Development and evaluation of a virtual reality...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 202953,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4079/5/Broneder%20Elisabeth%20-%202015%20-%20Development%20and%20evaluation%20of%20a%20virtual%20reality...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Elisabeth",
                    "last_name": "Broneder",
                    "position": 1,
                    "role": "Author",
                    "tid": "57709"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "40645",
            "handle": "20.500.12708/37106",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Skewed Coordinate System for Dense Point Correspondences Inside Articulated Shapes",
            "keywords": [],
            "abstract": "This report considers using a non-rigid coordinate system to find corresponding points in\r\ndifferent poses of the same articulated 2D shape. The shape-centered coordinate system\r\nis mapped on top of the eccentricity transform of the shape, which uses maximal geodesic\r\ndistances and is bounded under articulation. The isolines of the eccentricity transform\r\nare used as one of the coordinates, the radial-like, and the other one, the angular-like, is\r\nstretched to compensate for changes in the widths of parts. The polar-like coordinate system\r\nis first computed on inter-pixel isolines and then mapped to the pixels. The angular-like\r\ncoordinates are aligned using the 1D signals of the eccentricity values along the boundaries\r\nof the two shapes. Correspondences between points are established by minimizing the\r\ndifference of their coordinates. Detecting failed correspondences is done using an adaptive\r\nthreshold which adjusts to the changing local variation of the coordinates. Experimental\r\nresults are shown on a set of hand poses, ranging from minor movement to touching or\r\nmissing fingers.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Haxhimusa, Y., &#38; Kropatsch, W. (2011). <i>Skewed Coordinate System for Dense Point Correspondences Inside Articulated Shapes</i> (PRIP-TR-126). http://hdl.handle.net/20.500.12708/37106</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "4067",
            "handle": "20.500.12708/4083",
            "doi": "10.34726/hss.2015.29501",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Virtual reality content creator : a generic content creation and placement tool for simulations and storytelling in unity 3D",
            "keywords": [
                "Virtual Reality",
                "Unity3D",
                "Path System",
                "Multimedia",
                "Simulation"
            ],
            "abstract": "In the recent years, applications in the area of virtual reality have become very popular. This is due to the fact that researchers are developing ever more efficient hardware and 3D-glasses. We assume that this trend will keep up for the next years. Likewise there is a trend for the research of virtual simulations at Universities. Therefore it has become necessary to develop a tool, which can be used to efficently create virtual reality simulations. The system described in this work was created in the developement environment Unity3D. Besides a system which can handle the creation of a smooth path in a threedimensional space, a method is being introduced to fill gameobjects with multimedia-based content and to place them on the path in such a way that a bypassing user can perceive all the information without having to concentrate on more than one object at a time. Additionally the gameobjects can be equipped with different types of behaviour required by the current simulation. This system has been under developement for several months and was regularly tested and improved in an existing simulation in which users can perform a virtual parachute jump. Finally, the system was introduced to developers who were familiar with Unity3D. It was tested, how well the developers could handle the system and if it was intuitive. The results showed that the developers were able to create a new simulation in an existing environment in a short amount of time. The system allows developers to easily create a variety of simulations in which the user follows a predefined path while perceiving information.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Froihofer, T. (2015). <i>Virtual reality content creator : a generic content creation and placement tool for simulations and storytelling in unity 3D</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.29501</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "8153",
                    "name": "Froihofer Tobias - 2015 - Virtual reality content creator a generic content...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 15878056,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4083/2/Froihofer%20Tobias%20-%202015%20-%20Virtual%20reality%20content%20creator%20a%20generic%20content...pdf"
                },
                {
                    "bsid": "75851",
                    "name": "Froihofer Tobias - 2015 - Virtual reality content creator a generic content...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 149955,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4083/5/Froihofer%20Tobias%20-%202015%20-%20Virtual%20reality%20content%20creator%20a%20generic%20content...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Tobias",
                    "last_name": "Froihofer",
                    "position": 1,
                    "role": "Author",
                    "tid": "68729"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "40991",
            "handle": "20.500.12708/37452",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Analyse der Machbarkeit und des Innovationspotentials der Anwendung der Technologie des Optical Real-Time Trackings für Aufgaben der Tunnelvortriebsvermessung",
            "keywords": [],
            "abstract": "Innerhalb dieser Studie wird untersucht, ob die Technologie des &quot;Optical Real-Time Trackings&quot; für Aufgaben der Tunnenvortriebsvermessung geeignet und wie hoch das Innovationspotential ist. Hierfür werden erreichbare Genauigkeiten und Reichweiten praktisch evaluiert, mögliche Fehler- und Störeinflüsse identifiziert und beschrieben, auf Basis von Simulationen die Genauigkeitsschranken für die 3D-Positionsmessung theoretisch abgeschätzt und ein denkbarer Systemaufbau und Messablauf für das Anwendungsgebiet erörtert. Abschließend wird das Innovationspotential des Systems diskutiert.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., Pintaric, T., &#38; Kaufmann, H. (2008). <i>Analyse der Machbarkeit und des Innovationspotentials der Anwendung der Technologie des Optical Real-Time Trackings für Aufgaben der Tunnelvortriebsvermessung</i>. http://hdl.handle.net/20.500.12708/37452</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Pintaric",
                    "position": 2,
                    "role": "Author",
                    "tid": "39909"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "40994",
            "handle": "20.500.12708/37455",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Analyzing the Feasibility of PTAM based Localization Algorithms in Large Environments on Mobile Devices",
            "keywords": [],
            "abstract": "Augmented reality (AR) is a young field in science and industry. In AR we interact in a real environment with virtual content. AR has the potential to change how people interact and how they experience their surrounding environment. By combining it with mobile computing, using the power of sophisticated mobile devices, AR will impact almost every industry, particularly retail, entertainment, travel, design, art and advertising. For providing accurate augmentation, precise user localisation is required. This project aims at providing precise localisation of the user in a large indoor environment. We use a real- time tracking and mapping system, which provides an accuracy and robustness comparable to model based methods. The possibility to run tracking and mapping in parallel threads, separately from each other, allows to utilize the power of modern multi-core processors. The resulting system produces detailed maps with thousands of landmarks which can be tracked at interactive frame-rates.\r\nThe map and camera pose tracking can be used for determining precise user location, for adding augmented content, for tracking a user´s path, for searching for various objects in the environment and for other applications.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kosyreva, O., Schönauer, C., Kaufmann, H., Vonach, E., &#38; Gerstweiler, G. (2012). <i>Analyzing the Feasibility of PTAM based Localization Algorithms in Large Environments on Mobile Devices</i>. http://hdl.handle.net/20.500.12708/37455</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Olga",
                    "last_name": "Kosyreva",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 4,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 5,
                    "role": "Author",
                    "tid": "40923"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "9895"
            ]
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "4192",
            "handle": "20.500.12708/4208",
            "doi": "10.34726/hss.2013.22563",
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Visualizing and interactive modelling of process hierarchies",
            "keywords": [],
            "abstract": "This study deals with the visualization and modeling of protocol-based treatment plans that are composed from hierarchically structured skeletal-plans, and defined in the Asbru language. For this it introduces PlanStrips, a tree-map based visualization, designed for ease of use and simplicity. To test the PlanStrips visualization a prototype of the same name was developed. PlanStrips main characteristics are a dense presentation of treatment plans and qualitative presentation of plan execution order. The prototype also displays other guideline features such as conditions. After an introduction and short description of the projects background, the thesis takes a look at well-known visualization approaches for hierarchically structured data, as well as existing tools for Asbru guideline visualization and editing. In a discussion aspects/features of these tools are juxtaposed with PlanStrips. A user study was conducted to evaluate the PlanStrips prototype and verify its claims of simplicity and easy of use. The evaluation included three physicians, two domain experts, and a software engineer. The participants' knowledge in the fields of medical guidelines, guideline definition languages and general computer knowledge varied from novice to expert level. The results of the evaluation are presented and discussed. Both a qualitative and, to a lesser degree, quantitative evaluation of the findings are given.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hirsch, H. (2013). <i>Visualizing and interactive modelling of process hierarchies</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2013.22563</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "8403",
                    "name": "Hirsch Hubert - 2013 - Visualizing and interactive modelling of process...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4551273,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4208/2/Hirsch%20Hubert%20-%202013%20-%20Visualizing%20and%20interactive%20modelling%20of%20process...pdf"
                },
                {
                    "bsid": "74635",
                    "name": "Hirsch Hubert - 2013 - Visualizing and interactive modelling of process...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 127323,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4208/5/Hirsch%20Hubert%20-%202013%20-%20Visualizing%20and%20interactive%20modelling%20of%20process...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Hirsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "45301"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-02",
                "E193-06"
            ],
            "pid": "42083",
            "handle": "20.500.12708/38539",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Evaluation of Robust PCA for Supervised Audio Outlier Detection",
            "keywords": [],
            "abstract": "Outliers often reveal crucial information about the underlying data such as the presence of unusual observations that require for in-depth analysis. The detection of outliers is especially challenging in real-world application scenarios dealing with high-dimensional and flat data bearing different subpopulations of potentially varying data distributions. In the context of high-dimensional data, PCA-based methods are commonly applied in order to reduce dimensionality and to reveal outliers. In this paper, we perform a thorough empirical evaluation of well-establish PCA-based methods for the detection of outliers in a challenging audio data set. In this evaluation we focus on various experimental data settings motivated by the requirements of real-world scenarios, such as varying number of outliers, available training data, and data characteristics in terms of potential subpopulations.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brodinova, S., Ortner, T., Filzmoser, P., Zaharieva, M., &#38; Breiteneder, C. (2015). <i>Evaluation of Robust PCA for Supervised Audio Outlier Detection</i> (CS-2015-2). http://hdl.handle.net/20.500.12708/38539</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 1,
                    "role": "Author",
                    "tid": "281480"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 2,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 3,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 4,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 5,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "220239"
            ]
        },
        {
            "org_nrs": [
                "E186"
            ],
            "pid": "4230",
            "handle": "20.500.12708/4246",
            "doi": "10.34726/hss.2013.22627",
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Tracking related multiple targets in videos",
            "keywords": [
                "Tracking with Structure"
            ],
            "abstract": "This cumulative thesis presents research in the field of tracking. Tracking is one of the most thoroughly researched problems in computer vision. The aim of tracking is to follow an object of interest (target) in a video. In this thesis, I focus on a special problem: tracking related multiple targets. Two important questions in tracking are: What is the target? and Where is the target? The core contributions of this thesis answer these two questions with the help of graph-based representations and methods. The first core contribution is a fully automatic initialization for target models (What?), based on the principal that things which move together belong together. The input of the approach is a video showing the targets in motion. In this video a set of salient points is tracked to extract the necessary motion information in the form of trajectories. A triangulated graph is built based on the initial positions of the tracked points (i.e. 2D positions in the first frame). Then, the triangulated graph is deformed based on the motion encoded in the trajectories. This deformation of the triangulation over time is the input of a hierarchical grouping process, which is realized by an irregular dual graph pyramid. In the top level of the resulting pyramid the rigid entities (e.g. body parts of a human body) are identified. Finally, the motion of these rigid entities is analyzed to find possible points of articulation connecting them (e.g. upper and lower arm of a human). The second core contribution is a novel approach for finding temporal correspondences of multiple related targets (Where?). This thesis proposes to represent the targets by a graph model, where each target is represented by a vertex and their relationships are encoded by edges. The traditional solution to find the temporal correspondences of a graph  model is graph matching. In contrast to that, this thesis proposes a novel approach, which finds the correspondence of each vertex (target) by combining the appearance cue of a simple tracker with the structural cue deduced from a graph model. These two cues are combined in an iterative process inspired by the well-known Mean Shift algorithm. The outcome are correspondences for all vertices and edges in the graph, which locally maximize the similarity in appearance and locally minimize the deviation from the structure encoded in the model. Finally, the main goal of this thesis is to show the potential of graph-based representations and methods in tracking. This goal has been achieved through these two core contributions.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N. M. (2013). <i>Tracking related multiple targets in videos</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2013.22627</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "8479",
                    "name": "Artner Nicole M - 2013 - Tracking related multiple targets in videos.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 15337693,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4246/2/Artner%20Nicole%20M%20-%202013%20-%20Tracking%20related%20multiple%20targets%20in%20videos.pdf"
                },
                {
                    "bsid": "75475",
                    "name": "Artner Nicole M - 2013 - Tracking related multiple targets in videos.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 153488,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4246/5/Artner%20Nicole%20M%20-%202013%20-%20Tracking%20related%20multiple%20targets%20in%20videos.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Nicole M.",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "4323",
            "handle": "20.500.12708/4339",
            "doi": "10.34726/hss.2018.25683",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Inhaltsbasierte Suchmaschine für Videos von Parlamentssitzungen",
            "keywords": [
                "Content-Based Video Retrieval",
                "Audio Analysis"
            ],
            "abstract": "Making big, unstructured video data collections searchable fully automated and efficiently is a scientific task whose solution would be of big interest. Many data collections like film archives, online media centres, video surveillance archives and online learning platforms depend on an efficient search structure. It is common to use manually generated content indices for this purpose. These indices are produced by saving video frames including metadata like keywords or textual annotations. This task is extremely time-consuming. The produced indices are mostly inexact and incomplete. Huge amounts of information are lost for search and retrieval by this approach. Therefore the possibilities of the more current concept of „content based“ data search are investigated with this master’s thesis, as an example of using this approach for segmentation and classification of videos from Austrian parliament sessions. The aim is the automated and multimodal extraction of audio and image features for training appropriate classifiers in order to use them for classification of audio events and persons. The main focus of the classification lies in the detection of scenes where the atmosphere in the parliament chamber is different from the classical speech-atmosphere, which would be an evidence of interesting events during the sessions. The  recognition of acting parliamentarians - including their facial expression - is the second big focus of this work. This paper starts with an overview of the basic principles of “content based” video retrieval including its subsections: video segmentation, feature extraction from image and audio data and classification. Furthermore, methods for the statistical evaluation of the results will be presented, followed by an overview of related research papers. Afterwards, an explanation of the implemented prototype on the basis of the chosen features and classification methods is given. Finally, the statistical evaluation of the classification results is introduced, which show that the „content based“ approach for feature extraction and classification is definitely appropriate for the detection of relevant events and persons in videos of parliament sessions without the need for complex, manual indexing in advance. It is shown that, in the case of parliament session videos, audio features are more significant than visual features. Focussing on the detection of audio events for the identification of relevant scenes has proved to be right for this reason. Especially the classification of facial expression has turned out to be problematic, because in many cases the expression is not distinctive enough for a correct evaluation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Straka, K. (2018). <i>Inhaltsbasierte Suchmaschine für Videos von Parlamentssitzungen</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.25683</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "8665",
                    "name": "Straka Karin - 2018 - Inhaltsbasierte Suchmaschine fuer Videos von...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4756702,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4339/2/Straka%20Karin%20-%202018%20-%20Inhaltsbasierte%20Suchmaschine%20fuer%20Videos%20von...pdf"
                },
                {
                    "bsid": "75883",
                    "name": "Straka Karin - 2018 - Inhaltsbasierte Suchmaschine fuer Videos von...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 201052,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4339/5/Straka%20Karin%20-%202018%20-%20Inhaltsbasierte%20Suchmaschine%20fuer%20Videos%20von...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Karin",
                    "last_name": "Straka",
                    "position": 1,
                    "role": "Author",
                    "tid": "40907"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "43335",
            "handle": "20.500.12708/39791",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "The effect of motion blur on the ManakinTracker",
            "keywords": [],
            "abstract": "The Manakin Tracker was developed to track a small tropical bird, called golden-collaredmanakin, which performs its courtship dance at a very high speed. The fast movement leads to strong motion blur. The ManakinTracker is based on a Convolutional Neural Network(CNN) as well as blob detection through background subtraction based on a Mixture of Gaussians model. The CNN was trained on images cropped from frames in a data set of videos depicting the golden-collared manakin displaying its courtship dance. In our experiments, we pre-processed (simulated motion blur, rotated, deblurred) the videos toassess how motion blur affects the performance of the ManakinTracker. We found that when we simulated motion blur, less motion blur lead to an increased robustness of the ManakinTracker.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gostler, A. (2019). <i>The effect of motion blur on the ManakinTracker</i>. http://hdl.handle.net/20.500.12708/39791</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Anna",
                    "last_name": "Gostler",
                    "position": 1,
                    "role": "Author",
                    "tid": "249859"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "43336",
            "handle": "20.500.12708/39792",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Elliptical Line Voronoi Skeletons on the GPU",
            "keywords": [],
            "abstract": "Calculation of a shape skeleton can be slow, therefore an implementation on the GPU is beneficial. This report covers the algorithm and implementation of an Elliptical Voronoi Skeleton and compares implementations on the CPU and GPU. Additionally, a speed im-proved CPU version using Python´s Numba library is compared. An advanced residual calculation on a per pixel level is introduced and theoretical, although computationally expensive, elliptical residual calculations are given. A short comparision of Elliptical Voronoi and Classical Line Voronoi is given.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langer, M. (2019). <i>Elliptical Line Voronoi Skeletons on the GPU</i>. http://hdl.handle.net/20.500.12708/39792</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maximilian",
                    "last_name": "Langer",
                    "position": 1,
                    "role": "Author",
                    "tid": "254918"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "43337",
            "handle": "20.500.12708/39793",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Interactive segmentation and contour extractionfor automatic phenotyping of Lipizzaner horses",
            "keywords": [],
            "abstract": "In this paper we introduce a new custom software that was developed to support researchers in their study of exterior movement characteristics of the Lipizzaner horses. It uses an interactive image segmentation algorithm to segment a single horse in a given image from the background. The interactive algorithms offer a good tradeoff between accuracy and user interaction and allows faster segmentation than manual tools. Based on the resulting foreground mask of the horse, the whole contour or a segment of it can be exported for further analysis.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pucher, D., &#38; Kropatsch, W. (2019). <i>Interactive segmentation and contour extractionfor automatic phenotyping of Lipizzaner horses</i>. http://hdl.handle.net/20.500.12708/39793</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Daniel",
                    "last_name": "Pucher",
                    "position": 1,
                    "role": "Author",
                    "tid": "255108"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "43339",
            "handle": "20.500.12708/39795",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Quality Assessment of Color Fundusand Fluorescein Angiography Images",
            "keywords": [],
            "abstract": "Color Fundus and Fluorescein Angiography are medical procedures used for eye diagnosis. In Fluorescein Angiography a series of photographs is taken to analyze the blood flow in the back of the eyes. Color Fundus records color images of the interior surface of the eye to monitor and document disorders. As with every photograph, certain image distortions occur, making analysis difficult. The aim of this theses is to find a way to measure these distortions and ideally, flagbad photographs so that the analysis process can be accelerated.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">König, M., &#38; Ismail, O. (2019). <i>Quality Assessment of Color Fundusand Fluorescein Angiography Images</i>. http://hdl.handle.net/20.500.12708/39795</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "König",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Omar",
                    "last_name": "Ismail",
                    "position": 2,
                    "role": "Author",
                    "tid": "276493"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "43787",
            "handle": "20.500.12708/40238",
            "doi": null,
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "Motion Similarity Modeling - A State of the Art Report",
            "keywords": [],
            "abstract": "The analysis of human motion opens up a wide range of possibilities, such as realistic training simulations or authentic motions in robotics or animation. One of the problems underlying motion analysis is the meaningful comparison of actions based on similarity measures. Since the motion analysis is application-dependent, it is essential to find the appropriate motion similarity method for the particular use case. This state of the art report provides an overview of human motion analysis and different similarity modeling methods, while mainly focusing on approaches that work with 3D motion data. The survey summarizes various similarity aspects and features of motion and describes approaches to measuring the similarity between two actions.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sebernegg, A., Kán, P., &#38; Kaufmann, H. (2020). <i>Motion Similarity Modeling - A State of the Art Report</i>. http://hdl.handle.net/20.500.12708/40238</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Anna",
                    "last_name": "Sebernegg",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 2,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E226-02",
                "E234-02",
                "E259-01",
                "E259-03"
            ],
            "pid": "43927",
            "handle": "20.500.12708/40378",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "SCI_BIM Scanning and data capturing for Integrated Resources and Energy Assessment using Building Information Modelling",
            "keywords": [],
            "abstract": "Due to the rapidly increasing consumption of resources and land worldwide, as well as the growing generation of waste, the building stock plays a crucial role not only for the reduction of the energy \r\nconsumption, but also as a future source of materials (urban mining). However, there is a lack of information on the detailed material composition of the building stock, which is the main obstacle for \r\nmodelling and predicting its future use. Therefore, the main research question is whether the use of the digital technologies \"Laser Scanning\" and \"Ground Penetrating Radar\" (GPR) as well as a \r\ngamification concept, enable to develop and maintain a digital twin (BIM model) which serves as a basis for urban mining.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Honic, M., Kovacic, I., Sreckovic, M., Gourlis, G., Rechberger, H., Aschenbrenner, P., Fellner, J., Gilmutdinov, I., Wimmer, M., Pont, U., Mahdavi, A., Schuss, M., Ferschin, P., &#38; Kán, P. (2021). <i>SCI_BIM Scanning and data capturing for Integrated Resources and Energy Assessment using Building Information Modelling</i> (Schriftenreihe 21/2021). http://hdl.handle.net/20.500.12708/40378</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Meliha",
                    "last_name": "Honic",
                    "position": 1,
                    "role": "Author",
                    "tid": "178974"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 2,
                    "role": "Author",
                    "tid": "42543"
                },
                {
                    "first_name": "Marijana",
                    "last_name": "Sreckovic",
                    "position": 3,
                    "role": "Author",
                    "tid": "41722"
                },
                {
                    "first_name": "Georgios",
                    "last_name": "Gourlis",
                    "position": 4,
                    "role": "Author",
                    "tid": "253075"
                },
                {
                    "first_name": "Helmut",
                    "last_name": "Rechberger",
                    "position": 5,
                    "role": "Author",
                    "tid": "45055"
                },
                {
                    "first_name": "Philipp",
                    "last_name": "Aschenbrenner",
                    "position": 6,
                    "role": "Author",
                    "tid": "58309"
                },
                {
                    "first_name": "Johann",
                    "last_name": "Fellner",
                    "position": 7,
                    "role": "Author",
                    "tid": "49459"
                },
                {
                    "first_name": "Ildar",
                    "last_name": "Gilmutdinov",
                    "position": 8,
                    "role": "Author",
                    "tid": "315281"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Wimmer",
                    "position": 9,
                    "role": "Author",
                    "tid": "40666"
                },
                {
                    "first_name": "Ulrich",
                    "last_name": "Pont",
                    "position": 10,
                    "role": "Author",
                    "tid": "53337"
                },
                {
                    "first_name": "Ardeshir",
                    "last_name": "Mahdavi",
                    "position": 11,
                    "role": "Author",
                    "tid": "40365"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Schuss",
                    "position": 12,
                    "role": "Author",
                    "tid": "64017"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Ferschin",
                    "position": 13,
                    "role": "Author",
                    "tid": "135460"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 14,
                    "role": "Author",
                    "tid": "231177"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E234-01"
            ],
            "pid": "44060",
            "handle": "20.500.12708/40511",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Report",
            "peer_reviewed": false,
            "invited": false,
            "title": "AR-AQ-Bau − Einsatz von Augmented Reality zur Abnahme und Qualitätssicherung auf Baustellen",
            "keywords": [],
            "abstract": "Starting point / Motivation\r\nTo date, the construction industry has been one of the sectors least affected by digitization. At present, construction progress control, function checks, and inventories are mostly still carried out manually. A special case is building services (HVAC) in construction; it has become increasingly complex and is responsible for up to 35 % of construction costs. With the increasing use of Building Information Modeling (BIM), other technologies are following (step by step), such as augmented reality (AR), virtual reality (VR), artificial intelligence (AI), etc. (also called Construction 4.0). Many studies already highlight the potential added value of using AR. Therefore, the project AR-AQ-Bau focuses on the acceptance control and remote support of HVAC systems using augmented reality.\r\nFor widespread use of AR on construction sites, there was a need for research among the available AR systems in 2018. There were a few reasons for this. At that time, data could be transferred from the BIM model to the AR model, but not back again. This is especially important for quality and progress control. Conditions on construction sites (sunlight, dust, same-colored surfaces with few recognition features) make it difficult to locate the position (tracking) of the AR glasses as well as the virtual AR model more accurately over longer distances. There are still few user interfaces specialized for construction - and these only for tablets and not for AR glasses, which are more suitable for construction site use due to their better usability for this purpose.\r\n\r\nContents and goals\r\nThe project AR-AQ-Bau is developing an advanced AR system for the acceptance control of HVAC systems in the field of building services. In construction, the BIM model is the central data model; all information of the BIM model should be available for all parties involved in the construction process and, for the first time, be kept continuously up-to-date in a closed-loop approach. This so-called &quot;closed-loop data communication&quot; enables construction progress and as-built inventories to be marked in the AR model on the construction site and, thus, kept up to date. The project focuses on interaction options for transferring comments, images, and new component information to the AR model and then feeding them back into the BIM model (&quot;closed-loop&quot;). As a result, this information is available to everyone involved in the construction process at all times. With a remote expert system, external experts can be connected to support the execution control and create holographic elements (3D elements, images, marking). These functions lead to an increase in the quality of the completed structures, as errors can be identified and worked on together. The people do not all have to be on construction site in the process. While initial positioning of the AR model works quite well on construction sites, reliable tracking of the AR glasses as well as the AR model in the construction site environment is another challenge. The tracking systems currently available cannot cope with the difficult conditions on construction sites and must therefore be adapted accordingly.\r\n\r\nMethods\r\nThe first step was an application and requirements analysis of augmented reality in the design, construction, and operation phases - with a focus on the construction phase. Then, with the support of experts, the project team determined the necessary requirements for tools, tracking, layouts, and workflows for HVAC acceptance control and for the remote expert system. The project team implemented these in the AR quality assurance tool in the next step. Construction experts (e.g., local construction supervisors, technical planners, students) tested all developments (including data exchange, tracking, remote expert system, layouts) under practical conditions in laboratories and on construction sites (including the Future Art Lab and the subway station &quot;An den alten Schanzen&quot;). In the process, the project team evaluated, among other things, the suitability for construction sites, the accuracy of the tracking system, the setup time of the AR model, the change in documentation, user satisfaction, a possible increase in the quality and energy efficiency of buildings on the construction site. The results were again incorporated into a further development of the AR tool software. Here, the project combined research expertise from construction operations, AR, BIM modeling, and international engineering experience.\r\n\r\nResults\r\nThe results of the application and requirement analysis show the wide range of possible uses of AR in the construction site context, with the experts seeing very high potential in construction inspection and defect recording. Additional interest was aroused by the possibility of displaying component information and collisions with components still to be installed, including maintenance space. The project team developed an optimized menu path for the user interface for entering defects in the HVAC system. The developed defect classification is based on the important IFC classes and fits ideally into the BIM model. The implemented closed-loop BIM exchange service allows data to be fed back into both the BIM model and a facility management database, thus supporting building operations. Investigations of the newly developed tracking methods in a real construction site environment showed significant improvements in the localization of the AR glasses and the AR model during long-distance movements. This enables the use in many construction site situations. The developed remote expert system for AR communication between construction site and remote experts could be extended by essential functions. This mainly concerns the interaction possibilities. An essential basic requirement for the use of the remote expert system is a stable and very good Internet connection. Unfortunately, this is still not the case in all areas of construction sites. Sometimes communication and interaction were interrupted in the construction site tests. For the satisfactory use of the AR remote expert system, there is still a need for development in the network technology.\r\nAugmented reality is a technology that can bring the benefits of using BIM to the construction site. Many processes can be simplified and optimized. The advanced AR system developed in this project for the acceptance of HVAC systems in building services is an important step in this direction. Further developments based on this system can drive the widespread use of AR in construction. In addition to the pollutant savings predicted by eliminating the need for trips to the construction site, other benefits in the use of AR became apparent during the COVID-19 pandemic. The remote expert system allows experts to be connected remotely, which means that fewer personnel are needed directly at the construction site during on-site inspections and travel to the site can be reduced.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schranz, C., Urban, H., Kaufmann, H., Schönauer, C., Rattenberger, J., O´Brien, P., Ozeraitis, L., &#38; Jaritz, P. (2021). <i>AR-AQ-Bau − Einsatz von Augmented Reality zur Abnahme und Qualitätssicherung auf Baustellen</i> (5/2022). http://hdl.handle.net/20.500.12708/40511</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schranz",
                    "position": 1,
                    "role": "Author",
                    "tid": "36976"
                },
                {
                    "first_name": "Harald",
                    "last_name": "Urban",
                    "position": 2,
                    "role": "Author",
                    "tid": "253092"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 4,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Jürgen",
                    "last_name": "Rattenberger",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Peter",
                    "last_name": "O´Brien",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Linas",
                    "last_name": "Ozeraitis",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Patrick",
                    "last_name": "Jaritz",
                    "position": 8,
                    "role": "Author",
                    "tid": "44430"
                }
            ],
            "foci": [],
            "projects": [
                "1577824"
            ]
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "4487",
            "handle": "20.500.12708/4503",
            "doi": "10.34726/hss.2019.68300",
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Analyse des Balztanzes des Golden-Collared Manakin von Videos",
            "keywords": [
                "Tracking in the wild",
                "behavior recognition"
            ],
            "abstract": "The golden-collared manakin (Manacus vitellinus) is a tropical bird species, in which the male performs an acrobatic displays to court mates. To be able to compare different courtship displays and better understand the courtship dance, biologists recorded the birds in the wild with high-speed cameras. To analyze the courtship dance the birds need to be first tracked, so that the behavior can be classified, and finally visualized. Manually labeling every frame in hours of video material is a time-consuming process. Automatic tracking and behavior recognition enables faster analysis of videos, which would save human annotators months of work. In this thesis, we present a thorough state-of-the-art review and highlight the challenges of the manakin videos. The manakin videos present several challenges for visual tracking and behavior recognition. The birds rapid and abrupt movement causes strong motion blur and is hard to predict. The birds appearance changes strongly. Additionally, background clutter visually resembles and occludes the bird. The ManakinTracker is a visual long-term tracker designed to handle the challenges of the manakin videos. The ManakinTracker finds potential bounding boxes with background subtraction, models the birds appearance with a convolutional neural network and learns a motion model. It is able to detect the bird moving out of the frame and re-detect it. Based on the trajectory obtained through the ManakinTracker, we identify the birds typical courtship behaviors: perching, jumping, beard-up posture, and wing-snap. The behavior is then visualized by plotting the trajectory and in a sequence plot. We compare our tracker to 11 state-of-the-art trackers in terms of robustness and accuracy and perform an analysis of tracking failures.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gostler, A. (2019). <i>Analyse des Balztanzes des Golden-Collared Manakin von Videos</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2019.68300</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "8995",
                    "name": "Gostler Anna - 2019 - Analyse des Balztanzes des Golden-Collared Manakin von...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3200014,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4503/2/Gostler%20Anna%20-%202019%20-%20Analyse%20des%20Balztanzes%20des%20Golden-Collared%20Manakin%20von...pdf"
                },
                {
                    "bsid": "75190",
                    "name": "Gostler Anna - 2019 - Analyse des Balztanzes des Golden-Collared Manakin von...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 180839,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4503/5/Gostler%20Anna%20-%202019%20-%20Analyse%20des%20Balztanzes%20des%20Golden-Collared%20Manakin%20von...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Anna",
                    "last_name": "Gostler",
                    "position": 1,
                    "role": "Author",
                    "tid": "249859"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "4493",
            "handle": "20.500.12708/4509",
            "doi": "10.34726/hss.2019.53448",
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Smart 3D geometry understanding within a dynamic large triangulated point cloud",
            "keywords": [
                "Virtual Reality",
                "Neural Networks",
                "Object Detection",
                "3D Surface Reconstruction",
                "SLAM"
            ],
            "abstract": "Recent developments of machine learning algorithms resulted in outstanding findings for many different fields of applied computer science. The superior object detection performance of convolutional neural networks leads to lots of different neural network types and architectures. This thesis explores the utilization of state of the art object detection networks to achieve real time semantic annotations within a reconstructed 3D scene. An existing reconstruction framework is extended to implement an universal interface for different neural network types. This allows for an easy exchange of the used neural network and enables fast integration of future developments. With object detection the geometric reconstruction is extended towards a semantic scene understanding. The automatic annotation and segmentation of scene objects can be used to assists the user with exploration tasks and enables interaction with scene objects. The existing framework allows the distant live exploration of a scanned environment in virtual reality. It is based on InfiniTAM and consists of three main modules. At server side an environment is scanned with a RGB-D camera to generate a reconstruction of the scene. This 3D representation is transmitted to the client side where it is triangulated to a mesh. Finally this mesh can be explored within virtual reality using the Unreal Engine. The RGB images of the camera stream are used as an input for a convolutional neural network. The object detection results, represented as 2D bounding boxes or segmentation masks, are projected onto the 3D surface reconstruction. Fundamental changes of the processing pipeline allow the use of fully convolutional segmentation networks with long processing times while keeping the live reconstruction and streaming capabilities of the framework. An extensive filtering pipeline and a novel voting algorithm optimize the segmentation of the scene objects. Finally annotated three-dimensional bounding boxes enclose detected scene objects in the reconstruction. Additionally generated colliders represent their coarse geometry. This enables efficient interaction with scene objects, increasing the immersion of the user. The SSD_Mobile_Net box detection network and the Mask-RCNN segmentation network are implemented to test the reconstruction framework against a ground truth. Each parameter of the filter pipeline is evaluated to optimize the performance of the developed framework. Numerical filters influence the overall detection rate, visual filters determine the spatial segmentation of scene objects. The fusion of 2D bounding boxes shows a better overall result than the projection of segmentation results. Guidelines provide advice for the integration of new neural networks.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Höller, B. (2019). <i>Smart 3D geometry understanding within a dynamic large triangulated point cloud</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2019.53448</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "9007",
                    "name": "Hoeller Benjamin - 2019 - Smart 3D geometry understanding within a dynamic large...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 6469221,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4509/2/Hoeller%20Benjamin%20-%202019%20-%20Smart%203D%20geometry%20understanding%20within%20a%20dynamic%20large...pdf"
                },
                {
                    "bsid": "75486",
                    "name": "Hoeller Benjamin - 2019 - Smart 3D geometry understanding within a dynamic large...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 291512,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4509/5/Hoeller%20Benjamin%20-%202019%20-%20Smart%203D%20geometry%20understanding%20within%20a%20dynamic%20large...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Benjamin",
                    "last_name": "Höller",
                    "position": 1,
                    "role": "Author",
                    "tid": "178076"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E104-03",
                "E104-04",
                "E193-03"
            ],
            "pid": "45148",
            "handle": "20.500.12708/41581",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Freeform Architecture and Discrete Differential Geometry",
            "keywords": [],
            "abstract": "Freeform structures play an important role within contemporary architecture. While there is a wealth of excellent tools for the digital design of free-form geometry, the actual fabrication on the architectural scale is a big challenge. Key issues in this context are free-form surfaces composed of panels which can be manufactured at reasonable cost, and the geometry and statics of the support structure. The present article is an extended abstract of a talk on the close relation between geometric computing for free-form architecture and discrete differential geometry. It addresses topics such as skins from planar, in particular quadrilateral panels, geometry and statics of supporting structures, structures in force equilibrium.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pottmann, H., &#38; Wallner, J. (2017). Freeform Architecture and Discrete Differential Geometry. In W. Kropatsch, N. Artner, &#38; I. Janusch (Eds.), <i>Discrete Geometry for Computer Imagery</i> (pp. 3–8). Springer. https://doi.org/10.1007/978-3-319-66272-5_1</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Helmut",
                    "last_name": "Pottmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "149417"
                },
                {
                    "first_name": "Johannes",
                    "last_name": "Wallner",
                    "position": 2,
                    "role": "Author",
                    "tid": "41741"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Editor",
                    "tid": "37618"
                },
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 3,
                    "role": "Editor",
                    "tid": "44201"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "4516",
            "handle": "20.500.12708/4532",
            "doi": "10.34726/hss.2014.24649",
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Genre-basierte automatische Komposition von MIDI-Daten",
            "keywords": [
                "Automated Composing",
                "Media Analysis"
            ],
            "abstract": "This master thesis dwells upon the creation of musical compositions which can be matched to specific genres. It is assumed that a set of characteristics and musical motifs can be used to obtain this goal. For this purpose different criteria of a group of MIDI data are analysed and monophonic and polyphonic motifs are extracted by the use of pattern recognition techniques. The obtained data and a genetic algorithm are used to create new compositions which are related to the original songs. The developed system uses a beat-based approach. In this thesis several musical and algorithmic concepts are introduced first. Second the analysis of the data and the concept for new compositions is presented. Finally the results received through interviews of a small group of people are shown. More than 50% of the used musical pieces were correctly assigned by the test persons. Also the quality of the compositions were positively ratet.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Adelbauer, T. (2014). <i>Genre-basierte automatische Komposition von MIDI-Daten</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2014.24649</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "9053",
                    "name": "Adelbauer Thomas - 2014 - Genre-basierte automatische Komposition von MIDI-Daten.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 807007,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4532/2/Adelbauer%20Thomas%20-%202014%20-%20Genre-basierte%20automatische%20Komposition%20von%20MIDI-Daten.pdf"
                },
                {
                    "bsid": "76931",
                    "name": "Adelbauer Thomas - 2014 - Genre-basierte automatische Komposition von MIDI-Daten.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 98443,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4532/5/Adelbauer%20Thomas%20-%202014%20-%20Genre-basierte%20automatische%20Komposition%20von%20MIDI-Daten.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Adelbauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "56739"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E186"
            ],
            "pid": "4576",
            "handle": "20.500.12708/4592",
            "doi": "10.34726/hss.2014.24198",
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Reeb graph based image representation for phenotyping of plants",
            "keywords": [
                "Reeb graph",
                "phenotyping",
                "shape description",
                "topological patterns"
            ],
            "abstract": "While genotypes are defined as the set of genes an organism holds, its phenotype is defined as the set of its observable characteristics. To determine the correlation of genotype and phenotype or how a phenotype is affected by environmental conditions, an evaluation on large datasets is needed. An automatic analysis of image data and extraction of characteristics allows for large scale evaluations. This thesis presents a comparison of two types of graph-based image representations: medial axis transformation and Reeb graphs and evaluates the feasibility of using this representations in image based plant phenotyping. A presegmented binary image of roots (of the plant Arabidopsis thaliana) is the basis for generating the well-known medial axis and the Reeb graphs. For phenotyping of plants their root structure is analyzed. The main characteristics used here are branching points, branch endings as well as the length and width of individual branches. These characteristics are captured by the presented graph representations. For the computation of the Reeb graphs two different Morse functions are used: height function and geodesic distance. As the roots are pictured as 2D image data, the projection of a 3D structure to a 2D space might result in an overlap of branches in the image. One major advantage, when analyzing roots based on Reeb graphs, is posed by the ability to immediately distinguish between branching points and overlaps in the root structure as the overlap introduces a cycle and thereby a certain type of node (saddle - merge) in the Reeb graph. This differentiation is not as easily possible by a medial axis representation or by an analysis solely based on contours. In order to use the advantages of different representations and the characteristics provided by them, a possibility to combine different graph  representations of one root image is needed. Therefore the equality of graphs is evaluated. This thesis shows that all three representations of a root are either isomorphic graphs or isomorphic subgraphs. For isomorphic graphs the characteristics, the nodes are attributed with, such as length or width, can be directly combined for matching nodes. For isomorphic subgraphs only the attributes of the matching subgraphs can be combined.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I. (2014). <i>Reeb graph based image representation for phenotyping of plants</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2014.24198</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "9173",
                    "name": "Janusch Ines - 2014 - Reeb graph based image representation for phenotyping of...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 5947414,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4592/2/Janusch%20Ines%20-%202014%20-%20Reeb%20graph%20based%20image%20representation%20for%20phenotyping%20of...pdf"
                },
                {
                    "bsid": "75198",
                    "name": "Janusch Ines - 2014 - Reeb graph based image representation for phenotyping of...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 103565,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4592/5/Janusch%20Ines%20-%202014%20-%20Reeb%20graph%20based%20image%20representation%20for%20phenotyping%20of...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "4788",
            "handle": "20.500.12708/4804",
            "doi": "10.34726/hss.2016.36058",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Open data REST-services",
            "keywords": [
                "REST",
                "OData",
                "Open Data"
            ],
            "abstract": "This thesis deals with a possible solution for the shortcomings that result from the common approach of offering Open Data, where the corresponding data sets are offered as downloadable files on a website. Instead, Open Data producers could offer its data via RESTful web services which conform with OData Version 4, an emerging international standard for REST services. This way, one generic client application could be built, which could enable querying and viewing the exposed data and its data model, without needing to download a file or discovering the data model on your own. By using REST services, also other shortcomings of the mentioned approach could be eliminated. In this thesis, prototypes of both an OData Version 4 compliant RESTful web service and a generic client application were implemented in order to show, that such a constellation is feasible. The generic client application has also been tested successfully against other OData Version 4 compliant REST services. Additionally, a questionnaire on OData has been conducted with domain experts. The domain experts confirmed, that OData supports/enables the building of generic client applications for accessing and querying data published via such interfaces.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kamenica, A. (2016). <i>Open data REST-services</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.36058</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "9597",
                    "name": "Kamenica Aleksandar - 2016 - Open data REST-services.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2059957,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4804/2/Kamenica%20Aleksandar%20-%202016%20-%20Open%20data%20REST-services.pdf"
                },
                {
                    "bsid": "81225",
                    "name": "Kamenica Aleksandar - 2016 - Open data REST-services.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 182909,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/4804/5/Kamenica%20Aleksandar%20-%202016%20-%20Open%20data%20REST-services.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Aleksandar",
                    "last_name": "Kamenica",
                    "position": 1,
                    "role": "Author",
                    "tid": "242741"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "5097",
            "handle": "20.500.12708/5113",
            "doi": "10.34726/hss.2017.45700",
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Applications of high dynamic range imaging : stereo matching and mesopic rendering",
            "keywords": [
                "High Dynamic Range Imaging",
                "Tone Mapping",
                "Stereo Matching",
                "Markov Random Field",
                "Graph Cuts",
                "Mesopic Vision",
                "Display Rendering"
            ],
            "abstract": "The High Dynamic Range Imaging (HDRI) pipeline provides us with greater level of detail in color and brightness pixel values by introducing methods from image/video acquisition, storage and compression, to display and quality evaluation. HDRI techniques are considered to offer massive potential for other computer vision and graphics applications to leverage the higher range of information which have not been the main purpose of the HDRI pipeline so far. In this work, we focus on two important computer vision problems and introduce new techniques and algorithms to take HDRI into account to solve them. Stereo Matching: State of the art stereo matching and 3D reconstruction approaches do not work as well in HDR scenes and low textured regions. The greater luminance information available in HDR content is taken into account to provide more informative disparity maps in tricky matching regions of stereo images.  Our combinational approaches close the gap between available HDR methods and stereo matching. We introduce an HDR stereo data set as well as a ground truth HDR stereo matching algorithm as a reference for our tone mapped stereo matching benchmarking. HDR stereo images were captured, calibrated and rectified. A combinational tone mapping approach is proposed for the most effective disparity map computation in HDR scenes. Our combinational tone mapped stereo matching method reduces the stereo matching errors by a factor of three. Mesopic Rendering: Human visual perception varies in different ambient conditions. Our color and contrast perception especially in darker environments is influenced by the combination of rods and cones photoreceptors. Available standard rendering methods do not take this important perceptual effect into account. HDR images cover the full luminance range of scenes from photopic and mesopic vision to schotopic vision.  In the second part of our research we focus more on color and contrast perception in mesopic vision based on HDRI concepts. As another usage of tone mapped images, effective content rendering in dark environments such as Virtual Reality (VR) head mounted displays, night time driving and watching TV in the dark is proposed. Furthermore, subjective evaluations were performed to suggest an effective tone reproduction along with a color retargeting method to compensate for visual detail loss in dark environments on dimmed displays. In the subjective studies our perceptual results were ranked as the most preferred solution in more than 95% of the cases.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Akhavan, T. (2017). <i>Applications of high dynamic range imaging : stereo matching and mesopic rendering</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2017.45700</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "10215",
                    "name": "Akhavan Tara - 2017 - Applications of high dynamic range imaging stereo matching...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 16675709,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5113/2/Akhavan%20Tara%20-%202017%20-%20Applications%20of%20high%20dynamic%20range%20imaging%20stereo%20matching...pdf"
                },
                {
                    "bsid": "74722",
                    "name": "Akhavan Tara - 2017 - Applications of high dynamic range imaging stereo matching...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 219631,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5113/5/Akhavan%20Tara%20-%202017%20-%20Applications%20of%20high%20dynamic%20range%20imaging%20stereo%20matching...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Tara",
                    "last_name": "Akhavan",
                    "position": 1,
                    "role": "Author",
                    "tid": "247963"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "5114",
            "handle": "20.500.12708/5130",
            "doi": "10.34726/hss.2015.24725",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Game-based health monitoring using tangible user interface objects",
            "keywords": [
                "tangible user interfaces",
                "health monitoring",
                "serious gaming",
                "tabletop interaction",
                "affordance",
                "children",
                "usability",
                "storytelling"
            ],
            "abstract": "Patients being monitored in hospitals often do not behave as in their daily life. Especially for children such examinations can lead to stress associated with the clinical visit and the exposure to a different environment. These influencing parameters can result in false readings and furthermore in incorrect diagnosis. This circumstance could be overcome with an interface that offers a stress-reducing distraction by recording medical data in an unperceived way. For that purpose we designed a tangible user interface which acquires health values during a game most suitable for children. To determine the requirements for such a tangible user interface one of the first steps is to identify the possible medical sensors that could be used in a tabletop game and which of them are able to record representative and comparable medical values. Furthermore these sensors and the required equipment for their correct usage needs to be adapted to fit the physical objects used as input and output devices for the designed interface. The combination of these developed tangibles in a game that also satisfies children brought us to our finally implemented interface called ``StoryCubes'': The tangibles - realized as wooden cubes - tell a story when held in a proper way, at the right time and for a certain duration. These restrictions are needed to enable the sensors to perform accurately and therefore one challenge during this master thesis lies in providing enough information to let the users know how to use them correctly. This so-called affordance is realized through feedback, appropriate constraints and the cube design itself. After completing the hardware and software, our game concept is evaluated with 20 participants. The participating children were asked about usability issues, for example if they are able to comprehend the  game logic and if they enjoy playing. The test with a number of adults on the other hand was mainly used for representative medical measurements and proof of concept. The results of this study are promising and show a high acceptance for this kind of user interfaces, but also give us some improvement suggestions for the future.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ternek, M. (2015). <i>Game-based health monitoring using tangible user interface objects</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.24725</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "10249",
                    "name": "Ternek Marianne - 2015 - Game-based health monitoring using tangible user...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 18438759,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5130/2/Ternek%20Marianne%20-%202015%20-%20Game-based%20health%20monitoring%20using%20tangible%20user...pdf"
                },
                {
                    "bsid": "74724",
                    "name": "Ternek Marianne - 2015 - Game-based health monitoring using tangible user...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 188690,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5130/5/Ternek%20Marianne%20-%202015%20-%20Game-based%20health%20monitoring%20using%20tangible%20user...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Marianne",
                    "last_name": "Ternek",
                    "position": 1,
                    "role": "Author",
                    "tid": "59780"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "40682"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "5115",
            "handle": "20.500.12708/5131",
            "doi": "10.34726/hss.2015.25684",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "A system for optical music recognition and audio synthesis",
            "keywords": [
                "OMR",
                "Music Retrieval"
            ],
            "abstract": "This paper addresses Optical Music Recognition (OMR), a way to convert music notation into a digital representation, and its acoustic rendition. On the basis of a software prototype designed for mobile devices, the necessary steps will be discussed and compared to related work. The focus is on the fields of image processing and pattern recognition as well as audio synthesis. Innovations and experiences that have emerged in the field of OMR have been selected and used throughout the development process of the prototype. There is a particular emphasis on projection-based methods, which have proven highly successful as early as in the 1980s. Of all the works that were carried out, the research of I. Fujinaga has to be named as the strongest influence of this thesis. Both its easy implementation and its efficiency are well suited for the task of OMR on mobile devices. Furthermore, similarities and differences between the techniques applied and other approaches in the field of OMR will be presented. Symbol recognition is carried out by means of simple algorithms of pattern recognition (template matching) while the results are translated into an audible representation via MIDI synthesis. While there exist several performant OMR systems, specially tuned for the use with personal computers and scanners, OMR on mobile devices is still in its infancy. With the assistance of well-known and proven methods in this very field, this thesis provides a playful contact with optical music recognition.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Wallner, M. (2015). <i>A system for optical music recognition and audio synthesis</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.25684</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "10251",
                    "name": "Wallner Matthias - 2015 - A system for optical music recognition and audio...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1411567,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5131/2/Wallner%20Matthias%20-%202015%20-%20A%20system%20for%20optical%20music%20recognition%20and%20audio...pdf"
                },
                {
                    "bsid": "81238",
                    "name": "Wallner Matthias - 2015 - A system for optical music recognition and audio...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 124764,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5131/5/Wallner%20Matthias%20-%202015%20-%20A%20system%20for%20optical%20music%20recognition%20and%20audio...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Matthias",
                    "last_name": "Wallner",
                    "position": 1,
                    "role": "Author",
                    "tid": "44295"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "5204",
            "handle": "20.500.12708/5220",
            "doi": "10.34726/hss.2015.25502",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Machine learning with dual process models",
            "keywords": [
                "Machine Learning",
                "Dual Process Models",
                "Similarity Measurement",
                "Thematic Thinking",
                "Taxonomic Thinking"
            ],
            "abstract": "Similarity measurement processes are a core part of most machine learning algorithms. Traditional approaches focus on either taxonomic or thematic thinking. Psychological research suggests that a combination of both is needed to model human-like similarity perception adequately. Such a combination is called a Similarity Dual Process Model (DPM). This thesis describes how to construct DPMs as a linear combination of existing measures of similarity and distance. We use generalization functions to convert distance into similarity. DPMs are similar to kernel functions. Thus, they can be integrated into any machine learning algorithm that uses kernel functions. To foster the use of DPMs, we provide kernel function implementations. Clearly, not all DPMs that can be formulated work equally well. Therefore, we test classification performance in a real-world task: the detection of pedestrians in images. We assume that DPMs are only viable if they are better classifiers than their constituting parts. In our experiments, we found DPM kernels that matched the performance of conventional kernels for our data set. Eventually, we provide a construction kit to build such kernels to encourage further experiments in other application domains of machine learning.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Unger, M. (2015). <i>Machine learning with dual process models</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.25502</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "10429",
                    "name": "Unger Martin - 2015 - Machine learning with dual process models.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4827365,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5220/2/Unger%20Martin%20-%202015%20-%20Machine%20learning%20with%20dual%20process%20models.pdf"
                },
                {
                    "bsid": "83249",
                    "name": "Unger Martin - 2015 - Machine learning with dual process models.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 106444,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5220/5/Unger%20Martin%20-%202015%20-%20Machine%20learning%20with%20dual%20process%20models.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Unger",
                    "position": 1,
                    "role": "Author",
                    "tid": "58080"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "5336",
            "handle": "20.500.12708/5352",
            "doi": "10.34726/hss.2019.53922",
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "VRNet -  a framework for multi-user virtual reality",
            "keywords": [
                "virtual reality",
                "multi-user",
                "networking",
                "vr",
                "unreal engine",
                "VRNet"
            ],
            "abstract": "Multi-user VR applications are challenging to develop and maintain. They are required to be highly responsive to not cause discomfort to users. Potential network issues, like delays and loss of connection, need to be taken into account. The goal of this thesis is to design and implement VRNet, a framework to provide developers with the necessary tools to aid networked VR application development. Game engines like Unreal Engine 4 (UE4) offer tools and a development environment to create complex multiplayer 3D applications. However, the underlying networking system was not designed to be used in local walkable VR scenarios. Our approach is to build the VRNet framework on top of the engine to enhance it with important functionalities. VRNet is implemented as an UE4 code plugin that extends core classes in C++. As a plugin it can easily be integrated and used in UE4 projects. It adds capabilities for networked user movement, interaction and representation in VR. Client-authoritative replication is implemented to ensure consistency between machines with optimal user experience, guarded from networking issues. Additionally, interpolation mechanisms are provided to smoothly simulate other clients. Furthermore, as a proof of concept, VRNet is integrated into a VR application that is developed in the scope of a research project by the CeMM Research Center for Molecular Medicine in cooperation with TU Wien. CeMM Holodeck is an UE4 application for visualization and interaction with biological data-networks. Another goal of this thesis is to extend the functionality of the application to allow multiple users to move around in a shared virtual environment and collaboratively interact with the data-network. The integration of VRNet was successful and showed that VRNet provides a robust and useful foundation for the creation of multi-user VR applications. Evaluation tests were conducted to investigate the performance efficiency of the VRNet framework, as well as the CeMM Holodeck. The results of the network traffic measurements show that the throughput is easily manageable for modern networks even with high network update rates. The results also indicate that resource usage is not noticeably affected by the integration of VRNet. We show that VRNet offers simplified and efficient development of multi-user VR applications that are performant and provide smooth user experience.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Wagner, M. L. (2019). <i>VRNet -  a framework for multi-user virtual reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2019.53922</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "10693",
                    "name": "Wagner Michael Leopold - 2019 - VRNet - a framework for multi-user virtual...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 7615384,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5352/2/Wagner%20Michael%20Leopold%20-%202019%20-%20VRNet%20-%20a%20framework%20for%20multi-user%20virtual...pdf"
                },
                {
                    "bsid": "79960",
                    "name": "Wagner Michael Leopold - 2019 - VRNet - a framework for multi-user virtual...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 284661,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5352/5/Wagner%20Michael%20Leopold%20-%202019%20-%20VRNet%20-%20a%20framework%20for%20multi-user%20virtual...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Michael Leopold",
                    "last_name": "Wagner",
                    "position": 1,
                    "role": "Author",
                    "tid": "65064"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "5350",
            "handle": "20.500.12708/5366",
            "doi": "10.34726/hss.2019.55455",
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automatisierte Prognose der Entwicklung von Kryptowährungspreisen",
            "keywords": [
                "Cryptocurrencies",
                "Deep Learning",
                "Recurrent Neural Networks",
                "LSTM"
            ],
            "abstract": "Since the introduction of Bitcoin, cryptocurrencies have become very attractive as an alternative digital payment method and a highly speculative investment. With the rise in computational power and the growth of available data, the artificial intelligence concept of deep neural networks had a surge of popularity over the last years as well. With the introduction of the long short-term memory (LSTM) architecture, neural networks became more efficient in understanding long-term dependencies in data such as time series. In this thesis, we combine these two topics, by using neural networks to make a prognosis of cryptocurrency prices. In particular, we test if LSTM based neural networks can produce profitable trading signals for the cryptocurrency Ethereum. We experiment with different preprocessing techniques and different targets, both for price regression and trading signal classification. We evaluate two LSTM based networks and one convolutional neural network (CNN) LSTM hybrid. As data for training we use historical Ethereum price data in one-minute intervals from August 2017 to December 2018. We measure the performance of the models via backtesting, where we simulate trading on historic data not used for training based on the models predictions. We analyze that performance and compare it with the buy and hold strategy. The simulation is carried out on bullish, bearish and stagnating time periods. In the evaluation, we find the best performing target and pinpoint two preprocessing combinations that are most suitable for this task. We conclude that the CNN LSTM hybrid is capable of profitably forecasting trading signals for Ethereum, outperforming the buy and hold strategy by roughly 30%, while the performance of the other two models was disappointing.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Aumayr, L. (2019). <i>Automatisierte Prognose der Entwicklung von Kryptowährungspreisen</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2019.55455</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "10721",
                    "name": "Aumayr Lukas - 2019 - Automatisierte Prognose der Entwicklung von...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1863996,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5366/2/Aumayr%20Lukas%20-%202019%20-%20Automatisierte%20Prognose%20der%20Entwicklung%20von...pdf"
                },
                {
                    "bsid": "74833",
                    "name": "Aumayr Lukas - 2019 - Automatisierte Prognose der Entwicklung von...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 212809,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5366/5/Aumayr%20Lukas%20-%202019%20-%20Automatisierte%20Prognose%20der%20Entwicklung%20von...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Lukas",
                    "last_name": "Aumayr",
                    "position": 1,
                    "role": "Author",
                    "tid": "273497"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "5423",
            "handle": "20.500.12708/5439",
            "doi": "10.34726/hss.2018.42481",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Fast CPU ray-triangle intersection method",
            "keywords": [
                "ray-tracing",
                "ray-traingle intersection",
                "geometry",
                "physically based rendering"
            ],
            "abstract": "Ray tracing is a technique capable of synthesizing highly realistic images. It accurately simulates the physical distribution of light in a scene and can be used for movie production, scientific visualization, in the automotive industry, or by artists or game developers. In this context, rays of light, as well as lines of sight, are represented by “rays” which are traced throughout the scene. Rays interact with objects in the scene (they penetrate them, are reflected, refracted, etc.) and ultimately define the pixel values of the generated image. Thus, one crucial functionality of a ray tracing application is calculating the intersection of rays with scene primitives. One of the most often used primitives is the triangle. It enables relatively simple intersection calculations, and complex objects can be tessellated into triangles. Thus, a ray tracers ray-triangle intersection routine is called a considerable number of times per scene. Ray tracing is generally seen as a rather slow method of image generation. Thus, several algorithms for fast ray-triangle intersection have emerged within the last years. In the scope of this thesis, a novel, fast ray-triangle intersection algorithm is implemented into the CPU-based ray tracing framework PBRT. The algorithm features early termination strategies and transforms the ray-plane intersection point into a 2D coordinate system. For this transformation, two different approaches are discussed. Different optimizations are explored to further improve the algorithms performance. The algorithm is evaluated against PBRTs default algorithm and against another state-of-the-art ray-triangle intersection algorithm in terms of efficiency. Realistic scenes with different ray-triangle hit-rates and different scene complexity are used for the tests. The results show that the new algorithm outperforms the default algorithm for every scene.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pichler, T. A. (2018). <i>Fast CPU ray-triangle intersection method</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.42481</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "10867",
                    "name": "Pichler Thomas Alois - 2018 - Fast CPU ray-triangle intersection method.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 6586036,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5439/2/Pichler%20Thomas%20Alois%20-%202018%20-%20Fast%20CPU%20ray-triangle%20intersection%20method.pdf"
                },
                {
                    "bsid": "78041",
                    "name": "Pichler Thomas Alois - 2018 - Fast CPU ray-triangle intersection method.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 188129,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5439/5/Pichler%20Thomas%20Alois%20-%202018%20-%20Fast%20CPU%20ray-triangle%20intersection%20method.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Thomas Alois",
                    "last_name": "Pichler",
                    "position": 1,
                    "role": "Author",
                    "tid": "243211"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E186"
            ],
            "pid": "5431",
            "handle": "20.500.12708/5447",
            "doi": "10.34726/hss.2018.50602",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automatic human-head and shoulder segmentation of frontal-view face images",
            "keywords": [
                "head and shoulder segmentation",
                "background removal"
            ],
            "abstract": "Object segmentation is one of the basic issues in image processing and computer vision. However, especially human-head and shoulder segmentation is a topic which was introduced only recently, gaining in importance for a wide area of computer vision applications, such as testing compliance for ID document issuing, improving images for facial recognition or even used in the upcoming e-governmental self services and commercial sector. In this thesis we address the problem of automatic human-head and shoulder segmentation of frontal-view face images from non-uniform complex backgrounds and propose an approach composed of different subtask. These subtasks can be viewed individually and consist of a novel face skin silhouette detection approach based on supervised classification learners, a study of two state-of-the-art superpixel algorithms in relation to the specified problem statement and a novel hair and shoulder segmentation approach. We discuss and evaluate our methods and present competitive results for each subtask.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Melán, R. (2018). <i>Automatic human-head and shoulder segmentation of frontal-view face images</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.50602</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "10883",
                    "name": "Melan Robin - 2018 - Automatic human-head and shoulder segmentation of...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 17870659,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5447/2/Melan%20Robin%20-%202018%20-%20Automatic%20human-head%20and%20shoulder%20segmentation%20of...pdf"
                },
                {
                    "bsid": "75855",
                    "name": "Melan Robin - 2018 - Automatic human-head and shoulder segmentation of...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 128638,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5447/5/Melan%20Robin%20-%202018%20-%20Automatic%20human-head%20and%20shoulder%20segmentation%20of...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Robin",
                    "last_name": "Melán",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54613",
            "handle": "20.500.12708/50955",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Automated Extraction of Profiles from 3D Models of Achaeological Fragments",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mara, H. (2003). Automated Extraction of Profiles from 3D Models of Achaeological Fragments. In <i>CIPA 2003: XIX Int. Symposium: New Perspectives to save Cultulral Heritage</i>. CIPA, Antalya, Austria. http://hdl.handle.net/20.500.12708/50955</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 1,
                    "role": "Author",
                    "tid": "51366"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54614",
            "handle": "20.500.12708/50956",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Development of a System for Computer aided diagnosis (CAD) in rheumatoid Arthritis (1): automated joint localization in handradiographs",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Peloschek, P. L., Langs, G., Bischof, H., Kainberger, F., Kropatsch, W., &#38; Imhof, H. (2003). Development of a System for Computer aided diagnosis (CAD) in rheumatoid Arthritis (1): automated joint localization in handradiographs. In <i>Proceedings of European Society of Musculoskeletal Radiology Meeting ESSR 2003</i> (p. 1). http://hdl.handle.net/20.500.12708/50956</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 2,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 3,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "F",
                    "last_name": "Kainberger",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 5,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "H",
                    "last_name": "Imhof",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54615",
            "handle": "20.500.12708/50957",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Development of a system for Computer aided diagnosis (CAD) in rheumatoid arthritis (2) automated accurate estimation of the bony contour of metacarpal bones",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Peloschek, P. L., Langs, G., Bischof, H., Kainberger, F., Kropatsch, W., &#38; Imhof, H. (2003). Development of a system for Computer aided diagnosis (CAD) in rheumatoid arthritis (2) automated accurate estimation of the bony contour of metacarpal bones. In <i>Proceedings of European Societa of Muscoskeletal Radiology Meeting ESSR 2003</i>. European Society of Muscoskeletal Radiology Meeting ESSR, Aarhus, Denmark, Austria. http://hdl.handle.net/20.500.12708/50957</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 2,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 3,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "F",
                    "last_name": "Kainberger",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 5,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "H",
                    "last_name": "Imhof",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "54623",
            "handle": "20.500.12708/50965",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Gigabit Ethernet-based Parallel Video Processing",
            "keywords": [],
            "abstract": "This paper describes solutions for parallel video processing based on LAN-connected PC-like workstations. We outline application scenarios for the processing of video with broadcast TV resolution and promising areas for future research. Additionally, we describe a prototype system implemented in our workgroup. This Network-of-Workstations is based on Gigabit Ethernet, free Java software and standard network protocols. Video data is streamed to and from processing hosts using IP multicast and the Real-time Transfer Protocol. Control mechanisms are based on network file sharing. In comparison to related approaches, our system makes use of high-performance network hardware and open source software. In consequence, our system is easier to implement, cheaper and more flexible than, for example, highly integrated commercial solutions.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2005). Gigabit Ethernet-based Parallel Video Processing. In <i>IEEE Multimedia Modelling Conference</i>. IEEE Multimedia Modelling Conference, Melbourne, Australien, Austria. http://hdl.handle.net/20.500.12708/50965</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54637",
            "handle": "20.500.12708/50979",
            "doi": null,
            "year": 2004,
            "issued": "2004",
            "issued_on": "2004-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Electronic Documentation Tool for Archaeology: an automated system for fragments of ceramics",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mara, H., &#38; Sablatnig, R. (2004). Electronic Documentation Tool for Archaeology: an automated system for fragments of ceramics. In <i>Proc. of EVA04: Electronic Imaging and the Visual Arts</i> (pp. 250–255). Pitagora Editrice Bologna. http://hdl.handle.net/20.500.12708/50979</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 1,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54640",
            "handle": "20.500.12708/50982",
            "doi": null,
            "year": 2004,
            "issued": "2004",
            "issued_on": "2004-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automated joint localization, estimation of the bone contour and consecutive detection of defects of the bone contour in metacarpal bones for computer aided diagnosis (CAD) in rheumatoid arthritis",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Peloschek, P. L., Langs, G., Bischof, H., Kropatsch, W., Imhof, H., &#38; Kainberger, F. (2004). Automated joint localization, estimation of the bone contour and consecutive detection of defects of the bone contour in metacarpal bones for computer aided diagnosis (CAD) in rheumatoid arthritis. In <i>European Congress of Radiology, ECR 2004</i>. European Congress of Radiology. http://hdl.handle.net/20.500.12708/50982</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 2,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 3,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "H",
                    "last_name": "Imhof",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "F",
                    "last_name": "Kainberger",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54641",
            "handle": "20.500.12708/50983",
            "doi": null,
            "year": 2004,
            "issued": "2004",
            "issued_on": "2004-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Concept of a fully automated web-based scoring system for desctructive changes by rheumatoid arthritis - RheumaCoachCAD",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Peloschek, P. L., Langs, G., Bischof, H., Kropatsch, W., &#38; Imhof, H. (2004). Concept of a fully automated web-based scoring system for desctructive changes by rheumatoid arthritis - RheumaCoachCAD. In <i>European Society of Muscoloskeletal Radiology Meeting ESSR 2004</i>. European Society of Muscoloskeletal Radiology. http://hdl.handle.net/20.500.12708/50983</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 2,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 3,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "H",
                    "last_name": "Imhof",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "54722",
            "handle": "20.500.12708/51064",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Discrimination and Retrieval of Animal Sounds",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mitrovic, D., Zeppelzauer, M., &#38; Breiteneder, C. (2006). Discrimination and Retrieval of Animal Sounds. In <i>Proceedings IEEE Multimedia Modeling Conference</i> (pp. 339–343). http://hdl.handle.net/20.500.12708/51064</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 1,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 3,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06",
                "E194-01"
            ],
            "pid": "54733",
            "handle": "20.500.12708/51075",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Content-based Querying Embedded in Multimedia Presentations",
            "keywords": [],
            "abstract": "This paper presents a solution for integrating content-based multimedia retrieval with semantics-based content selection techniques, content adaptation, composition and delivery. The integration of several approaches of these domains provides users of digital information systems with a powerful toolkit. We focus on the generation of multimedia presentations and templates that can be reused. In this context, we propose a novel approach where content-based querying is embedded into multimedia presentations. Queries are executed when presentations are consumed. Hence, the requested media data are retrieved ad hoc.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Divotkey, D., &#38; Eidenberger, H. (2005). Content-based Querying Embedded in Multimedia Presentations. In <i>Proceedings IEEE MMSP 2005</i>. IEEE Multimedia Signal Processing Workshop, Shanghai, Austria. http://hdl.handle.net/20.500.12708/51075</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Doris",
                    "last_name": "Divotkey",
                    "position": 1,
                    "role": "Author",
                    "tid": "93452"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 2,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54939",
            "handle": "20.500.12708/51281",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Hierarchical Image Partitioning using Combinatorial Maps",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Haxhimusa, Y., Kropatsch, W., &#38; Brun, L. (2005). Hierarchical Image Partitioning using Combinatorial Maps. In A. Hanbury &#38; H. Bischof (Eds.), <i>10th Computer Vision Winter Workshop - CVWW 2005</i> (pp. 43–52). Eigenverlag. http://hdl.handle.net/20.500.12708/51281</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Luc",
                    "last_name": "Brun",
                    "position": 4,
                    "role": "Author",
                    "tid": "227964"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Editor",
                    "tid": "48222"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Editor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54940",
            "handle": "20.500.12708/51282",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Inside and Outside within Combinatorial Pyramids",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brun, L., &#38; Kropatsch, W. (2005). Inside and Outside within Combinatorial Pyramids. In L. Brun &#38; M. vento (Eds.), <i>5th IAPR-TC15 Workshop on Graph-based Representation in Pattern Recognition</i> (pp. 122–131). Springer, LNCS. http://hdl.handle.net/20.500.12708/51282</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Luc",
                    "last_name": "Brun",
                    "position": 1,
                    "role": "Author",
                    "tid": "227964"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Luc",
                    "last_name": "Brun",
                    "position": 1,
                    "role": "Editor",
                    "tid": "227964"
                },
                {
                    "first_name": "Mario",
                    "last_name": "vento",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54941",
            "handle": "20.500.12708/51283",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Graph-Based Concept for Spatiotemporal Information in Cognitive Vision",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Haxhimusa, Y., &#38; Kropatsch, W. (2005). A Graph-Based Concept for Spatiotemporal Information in Cognitive Vision. In L. Brun &#38; M. vento (Eds.), <i>5th IAPR-TC15 Workshop on Graph-based Representation in Pattern Recognition</i> (pp. 223–232). Springer, LNCS. http://hdl.handle.net/20.500.12708/51283</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Luc",
                    "last_name": "Brun",
                    "position": 1,
                    "role": "Editor",
                    "tid": "227964"
                },
                {
                    "first_name": "Mario",
                    "last_name": "vento",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54942",
            "handle": "20.500.12708/51284",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Grouping of Non-connected Structures by an Irregular Graph Pyramid",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., &#38; Haxhimusa, Y. (2005). Grouping of Non-connected Structures by an Irregular Graph Pyramid. In J. Marques, N. P. de la Blanca, &#38; P. Pina (Eds.), <i>Pattern Recognition and Image Analysis: Second Iberian Conference, IbPRIA 2005</i> (pp. 107–114). Springer, LNCS. http://hdl.handle.net/20.500.12708/51284</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Jorge",
                    "last_name": "Marques",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Nicolas Perez",
                    "last_name": "de la Blanca",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Pina",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E376"
            ],
            "pid": "54943",
            "handle": "20.500.12708/51285",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Hierarchical Image Partitioning using Combinatorial Maps",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y., Ion, A., Kropatsch, W., &#38; Brun, L. (2005). Hierarchical Image Partitioning using Combinatorial Maps. In D. Chetverikov, L. Czuni, &#38; M. Vincze (Eds.), <i>Hierarchical Image Partitioning using Combinatorial Maps</i> (pp. 179–186). OCG Schriftenreihe, Österreichische Arbeitsgemeinschaft für Mustererkennung. http://hdl.handle.net/20.500.12708/51285</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Luc",
                    "last_name": "Brun",
                    "position": 4,
                    "role": "Author",
                    "tid": "227964"
                },
                {
                    "first_name": "Dimitry",
                    "last_name": "Chetverikov",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Laszlo",
                    "last_name": "Czuni",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Vincze",
                    "position": 3,
                    "role": "Editor",
                    "tid": "133231"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E376"
            ],
            "pid": "54944",
            "handle": "20.500.12708/51286",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Model-Based Occlusion Handling for Tracking in Crowded scenes",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Beleznai, C., Frühstück, B., Bischof, H., &#38; Kropatsch, W. (2005). Model-Based Occlusion Handling for Tracking in Crowded scenes. In D. Chetverikov, L. Czuni, &#38; M. Vincze (Eds.), <i>Joint Hungarian-Austrian Conference on Image Processing and Pattern Recognition</i> (pp. 227–234). OCG Schriftenreihe, Österreichische Arbeitsgemeinschaft für Mustererkennung. http://hdl.handle.net/20.500.12708/51286</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Csaba",
                    "last_name": "Beleznai",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Bernhard",
                    "last_name": "Frühstück",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 3,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Dimitry",
                    "last_name": "Chetverikov",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Laszlo",
                    "last_name": "Czuni",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Vincze",
                    "position": 3,
                    "role": "Editor",
                    "tid": "133231"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54945",
            "handle": "20.500.12708/51287",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluating Minimum Spanning Tree Based Segmentation Algorithms",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y., Ion, A., Kropatsch, W., &#38; Illetschko, T. (2005). Evaluating Minimum Spanning Tree Based Segmentation Algorithms. In A. Gagalowicz &#38; W. Philips (Eds.), <i>Computer Analysis of Images and Patterns: 11th International Conference, CAIP 2005</i> (pp. 579–586). Springer, LNCS. http://hdl.handle.net/20.500.12708/51287</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Illetschko",
                    "position": 4,
                    "role": "Author",
                    "tid": "97458"
                },
                {
                    "first_name": "Andre",
                    "last_name": "Gagalowicz",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Wilfried",
                    "last_name": "Philips",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54946",
            "handle": "20.500.12708/51288",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Texture Based Drawing Tool Classification in Infrared Reflectograms",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Lettner, M., &#38; Sablatnig, R. (2005). Texture Based Drawing Tool Classification in Infrared Reflectograms. In A. Hanbury &#38; H. Bischof (Eds.), <i>Proc. of the 10th Computer Vision Winter Workshop</i> (pp. 63–72). Eigenverlag. http://hdl.handle.net/20.500.12708/51288</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Lettner",
                    "position": 1,
                    "role": "Author",
                    "tid": "48706"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Author",
                    "tid": "133566"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Editor",
                    "tid": "48222"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Editor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54951",
            "handle": "20.500.12708/51293",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Clique of Active Appearance Models by Minimum",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langs, G., Peloschek, P. L., Donner, R., &#38; Bischof, H. (2005). A Clique of Active Appearance Models by Minimum. In <i>Proceedings of the British Machine Vision Conference</i> (pp. 859–868). British Machine Vision Association. http://hdl.handle.net/20.500.12708/51293</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 1,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 3,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 4,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01"
            ],
            "pid": "54953",
            "handle": "20.500.12708/51295",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Optimal Sub-Shape Models by Minimum Description",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langs, G., Peloschek, P. L., &#38; Bischof, H. (2005). Optimal Sub-Shape Models by Minimum Description. In <i>Proceedings of IEEE Converence on Computer Vision</i> (pp. 310–315). IEEE Computer Society Press. http://hdl.handle.net/20.500.12708/51295</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 1,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 3,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54954",
            "handle": "20.500.12708/51296",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "MDL-based Splitting of PCA Models",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langs, G., Peloschek, P. L., &#38; Bischof, H. (2005). MDL-based Splitting of PCA Models. In A. Hanbury &#38; H. Bischof (Eds.), <i>Proceedings of Computer Vision Winter Workshop CVWW</i> (pp. 13–22). Eigenverlag. http://hdl.handle.net/20.500.12708/51296</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 1,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 3,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Editor",
                    "tid": "48222"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Editor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54955",
            "handle": "20.500.12708/51297",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Segmentation and Surveying of Cutaneous Hemangiomas",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zambanini, S., Langs, G., &#38; Bischof, H. (2005). Segmentation and Surveying of Cutaneous Hemangiomas. In A. Hanbury &#38; H. Bischof (Eds.), <i>Proceedings of Computer Vision Winter Workshop CVWW</i> (pp. 103–112). Eigenverlag. http://hdl.handle.net/20.500.12708/51297</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Sebastian",
                    "last_name": "Zambanini",
                    "position": 1,
                    "role": "Author",
                    "tid": "53412"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 2,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 3,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Editor",
                    "tid": "48222"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Editor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54956",
            "handle": "20.500.12708/51298",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Robust Detection and Performance Evaluation of Individuals and Vehicles on an Airport's Apron",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Aguilera Antequera, J., Kampel, M., &#38; Blauensteiner, P. (2005). Robust Detection and Performance Evaluation of Individuals and Vehicles on an Airport’s Apron. In A. Hanbury &#38; H. Bischof (Eds.), <i>In Proc. of the Computer Vision Winter Workshop</i> (pp. 145–154). Eigenverlag. http://hdl.handle.net/20.500.12708/51298</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Josep",
                    "last_name": "Aguilera Antequera",
                    "position": 1,
                    "role": "Author",
                    "tid": "229250"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 2,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Philipp",
                    "last_name": "Blauensteiner",
                    "position": 3,
                    "role": "Author",
                    "tid": "53592"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Editor",
                    "tid": "48222"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Editor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03",
                "E376"
            ],
            "pid": "54957",
            "handle": "20.500.12708/51299",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "An Evaluation of Error Metrics for the Segmentation of Moving Objects on an Airport's Apron",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Wildenauer, H., Aguilera Antequera, J., &#38; Kampel, M. (2005). An Evaluation of Error Metrics for the Segmentation of Moving Objects on an Airport’s Apron. In D. Chetverikov, L. Czuni, &#38; M. Vincze (Eds.), <i>In Proc. of the Joint Hungarian-Austrian Conference on Image Processing and Pattern Recognition</i> (pp. 119–126). OCG Schriftenreihe, Österreichische Arbeitsgemeinschaft für Mustererkennung. http://hdl.handle.net/20.500.12708/51299</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Wildenauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "51087"
                },
                {
                    "first_name": "Josep",
                    "last_name": "Aguilera Antequera",
                    "position": 2,
                    "role": "Author",
                    "tid": "229250"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 3,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Dimitry",
                    "last_name": "Chetverikov",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Laszlo",
                    "last_name": "Czuni",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Vincze",
                    "position": 3,
                    "role": "Editor",
                    "tid": "133231"
                }
            ],
            "foci": [],
            "projects": [
                "1666"
            ]
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54958",
            "handle": "20.500.12708/51300",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluation of Motion Segmentation Quality for Aircraft Activity Surveillance",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Aguilera Antequera, J., Wildenauer, H., Kampel, M., Borg, M., Thirde, D., &#38; Ferryman, J. (2005). Evaluation of Motion Segmentation Quality for Aircraft Activity Surveillance. In R. Chellappa, J. Ferryman, &#38; T. Tan (Eds.), <i>In Proc. of the Second Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</i> (pp. 293–300). IEEE Computer Society. http://hdl.handle.net/20.500.12708/51300</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Josep",
                    "last_name": "Aguilera Antequera",
                    "position": 1,
                    "role": "Author",
                    "tid": "229250"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Wildenauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "51087"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 3,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Borg",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "David",
                    "last_name": "Thirde",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Rama",
                    "last_name": "Chellappa",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Tieniu",
                    "last_name": "Tan",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54959",
            "handle": "20.500.12708/51301",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Visual Surveillance for Aircraft Activity Monitoring",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Thirde, D., Borg, M., Valentin, V., Fusier, F., Aguilera Antequera, J., Ferryman, J., Bremond, F., Thonnat, M., &#38; Kampel, M. (2005). Visual Surveillance for Aircraft Activity Monitoring. In R. Chellappa, J. Ferryman, &#38; T. Tan (Eds.), <i>In Proc. of the Second Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</i> (pp. 255–262). IEEE Computer Society. http://hdl.handle.net/20.500.12708/51301</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "David",
                    "last_name": "Thirde",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Borg",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Valery",
                    "last_name": "Valentin",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Florent",
                    "last_name": "Fusier",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Josep",
                    "last_name": "Aguilera Antequera",
                    "position": 5,
                    "role": "Author",
                    "tid": "229250"
                },
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Francois",
                    "last_name": "Bremond",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Monique",
                    "last_name": "Thonnat",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 9,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Rama",
                    "last_name": "Chellappa",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Tieniu",
                    "last_name": "Tan",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54960",
            "handle": "20.500.12708/51302",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluation of Object Tracking for Aircraft Activity Surveillance",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Thirde, D., Borg, M., Aguilera Antequera, J., Ferryman, J., Baker, K., &#38; Kampel, M. (2005). Evaluation of Object Tracking for Aircraft Activity Surveillance. In R. Chellappa, J. Ferryman, &#38; T. Tan (Eds.), <i>In Proc. of the Second Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance, Beijing</i> (pp. 145–152). IEEE Computer Society. http://hdl.handle.net/20.500.12708/51302</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "David",
                    "last_name": "Thirde",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Borg",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Josep",
                    "last_name": "Aguilera Antequera",
                    "position": 3,
                    "role": "Author",
                    "tid": "229250"
                },
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Keith",
                    "last_name": "Baker",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 6,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Rama",
                    "last_name": "Chellappa",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Tieniu",
                    "last_name": "Tan",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54961",
            "handle": "20.500.12708/51303",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "On the Detection of Individuals and Vehicles on an Airport's Apron",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Aguilera Antequera, J., &#38; Kampel, M. (2005). On the Detection of Individuals and Vehicles on an Airport’s Apron. In <i>In Proc. of IEE International Conference on Visual Information Engineering</i> (pp. 107–114). IEE. http://hdl.handle.net/20.500.12708/51303</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Josep",
                    "last_name": "Aguilera Antequera",
                    "position": 1,
                    "role": "Author",
                    "tid": "229250"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 2,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54962",
            "handle": "20.500.12708/51304",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Distributed Multi-Camera Surveillance for Aircraft Servicing Operations",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Thirde, D., Borg, M., Ferryman, J., Aguilera Antequera, J., &#38; Kampel, M. (2005). Distributed Multi-Camera Surveillance for Aircraft Servicing Operations. In <i>International Symposium on Visual Computing 2005</i> (pp. 118–125). Springer, LNCS. http://hdl.handle.net/20.500.12708/51304</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "David",
                    "last_name": "Thirde",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Borg",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Josep",
                    "last_name": "Aguilera Antequera",
                    "position": 4,
                    "role": "Author",
                    "tid": "229250"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 5,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54963",
            "handle": "20.500.12708/51305",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Automated Scene Understanding for Airport Aprons",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ferryman, J., Borg, M., Thirde, D., Fusier, F., Valentin, V., Bremond, F., Thonnat, M., Aguilera Antequera, J., &#38; Kampel, M. (2005). Automated Scene Understanding for Airport Aprons. In <i>In Proc. of the Australian Joint Conference on Artificial Intelligence</i> (pp. 593–603). Springer, LNAI. http://hdl.handle.net/20.500.12708/51305</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Borg",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "David",
                    "last_name": "Thirde",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Florent",
                    "last_name": "Fusier",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Valery",
                    "last_name": "Valentin",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Francois",
                    "last_name": "Bremond",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Monique",
                    "last_name": "Thonnat",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Josep",
                    "last_name": "Aguilera Antequera",
                    "position": 8,
                    "role": "Author",
                    "tid": "229250"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 9,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54971",
            "handle": "20.500.12708/51313",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Illumination-invariant morphological texture classification",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A., Kandaswamy, U., &#38; Adjeroh, D. (2005). Illumination-invariant morphological texture classification. In C. Ronse, L. Najman, &#38; E. Decenciere (Eds.), <i>Proceedings of the International Symposium on Mathematical Morphology</i> (pp. 377–386). Springer. http://hdl.handle.net/20.500.12708/51313</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Umasankar",
                    "last_name": "Kandaswamy",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Donald",
                    "last_name": "Adjeroh",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Ronse",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Laurent",
                    "last_name": "Najman",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Etienne",
                    "last_name": "Decenciere",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54972",
            "handle": "20.500.12708/51314",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Supervised Texture Detection in Images",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Micusik, B., &#38; Hanbury, A. (2005). Supervised Texture Detection in Images. In A. Gagalowicz &#38; W. Philips (Eds.), <i>Computer Analysis of Images and Patterns (CAI</i> (pp. 441–448). Springer, LNCS. http://hdl.handle.net/20.500.12708/51314</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 1,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Andre",
                    "last_name": "Gagalowicz",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Wilfried",
                    "last_name": "Philips",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54973",
            "handle": "20.500.12708/51315",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Semi-automatic Segmentation of Textured Images",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Micusik, B., &#38; Hanbury, A. (2005). Semi-automatic Segmentation of Textured Images. In A. Hanbury &#38; H. Bischof (Eds.), <i>Proceedings of the 10th Computer Vision Winter Workshop (CVWW 2005)</i> (pp. 53–62). Eigenverlag. http://hdl.handle.net/20.500.12708/51315</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 1,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Editor",
                    "tid": "48222"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Editor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54974",
            "handle": "20.500.12708/51316",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Steerable Semi-automatic Segmentation of Textured Images",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Micusik, B., &#38; Hanbury, A. (2005). Steerable Semi-automatic Segmentation of Textured Images. In H. Kalviainen, J. Parkkinen, &#38; A. Kaarna (Eds.), <i>14th Scandinavian Conference on Image Analysis (SCIA)</i> (pp. 35–44). Springer, LNCS. http://hdl.handle.net/20.500.12708/51316</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 1,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Heikki",
                    "last_name": "Kalviainen",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Jussi",
                    "last_name": "Parkkinen",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Arto",
                    "last_name": "Kaarna",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E376"
            ],
            "pid": "54975",
            "handle": "20.500.12708/51317",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "An Application Analysis Approach For Noise Estimation in Panoramic X-Ray Images",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Göbel, P., &#38; Belbachir, A. N. (2005). An Application Analysis Approach For Noise Estimation in Panoramic X-Ray Images. In D. Chetverikov, L. Czuni, &#38; M. Vincze (Eds.), <i>Proc. of Joint Hungarian-Austrian Conference on Image Processing and Pattern Recognition</i> (pp. 203–210). OCG Schriftenreihe, Österreichische Arbeitsgemeinschaft für Mustererkennung. http://hdl.handle.net/20.500.12708/51317</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Göbel",
                    "position": 1,
                    "role": "Author",
                    "tid": "41359"
                },
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 2,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Dimitry",
                    "last_name": "Chetverikov",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Laszlo",
                    "last_name": "Czuni",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Vincze",
                    "position": 3,
                    "role": "Editor",
                    "tid": "133231"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54976",
            "handle": "20.500.12708/51318",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Sparse Image Representation Using Contourlets",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Belbachir, A. N., &#38; Göbel, P. (2005). A Sparse Image Representation Using Contourlets. In A. Hanbury &#38; H. Bischof (Eds.), <i>Proc. of 10th Computer Vision Winter Workshop</i> (pp. 165–174). Eigenverlag. http://hdl.handle.net/20.500.12708/51318</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 1,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Göbel",
                    "position": 2,
                    "role": "Author",
                    "tid": "41359"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Editor",
                    "tid": "48222"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Editor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54977",
            "handle": "20.500.12708/51319",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Color Image Compression: Early Vision and the Multiresolution Representations",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Belbachir, A. N., &#38; Göbel, P. (2005). Color Image Compression: Early Vision and the Multiresolution Representations. In W. Kropatsch, R. Sablatnig, &#38; A. Hanbury (Eds.), <i>Proc. of the 27th DAGM Symposium</i> (pp. 25–32). Springer, LNCS. http://hdl.handle.net/20.500.12708/51319</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 1,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Göbel",
                    "position": 2,
                    "role": "Author",
                    "tid": "41359"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Editor",
                    "tid": "133566"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Editor",
                    "tid": "48222"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54978",
            "handle": "20.500.12708/51320",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Blind Background Substraction in Dental Panoramic X-ray Images: An Application Approach",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Göbel, P., &#38; Belbachir, A. N. (2005). Blind Background Substraction in Dental Panoramic X-ray Images: An Application Approach. In W. Kropatsch, R. Sablatnig, &#38; A. Hanbury (Eds.), <i>Proc. of the 27th DAGM Symposium</i> (pp. 434–441). Springer, LNCS. http://hdl.handle.net/20.500.12708/51320</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Göbel",
                    "position": 1,
                    "role": "Author",
                    "tid": "41359"
                },
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 2,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Editor",
                    "tid": "133566"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Editor",
                    "tid": "48222"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54979",
            "handle": "20.500.12708/51321",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Background Removal in Dental Panoramic X-ray Images by the A-Trous Multiresolution Transform",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Göbel, P., &#38; Belbachir, A. N. (2005). Background Removal in Dental Panoramic X-ray Images by the A-Trous Multiresolution Transform. In <i>Proc. of the 17th IEEE European Conference on Circuit Theory and Design</i> (pp. 331–334). IEEE. http://hdl.handle.net/20.500.12708/51321</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Göbel",
                    "position": 1,
                    "role": "Author",
                    "tid": "41359"
                },
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 2,
                    "role": "Author",
                    "tid": "217349"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "5498",
            "handle": "20.500.12708/5514",
            "doi": "10.34726/hss.2017.36106",
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "An emergent system for multimedial document management",
            "keywords": [],
            "abstract": "The digital revolution changes our world in unprecedented manner and generates manifold challenges for humankind thereby. One of the major questions in the present time is concerned with the sustainable handling of (digital) data. Each individual produces and uses a multitude of data on a daily basis, which are assembled to meaningful information in reference to the particular context. A permanent storage of this information can be realised by means of multimedial documents. Altogether, these single activities constitute a natural cycle of collaboration, where humans capitalise on automation to create and retrieve information in relation to each other. The concrete use case to be realised within this diploma thesis includes the development of a software prototype for the acquisition and management of students- applications at the Vienna University of Technology. Several measures have to be taken into account to achieve this goal: the prototypical Student Records Manager allows to store information (documents and metadata) in student records on the one hand, and provides a user interface with three different views for the manipulation of information on the other hand. Students are enabled to upload documents and send their application in consequence. Employees of the Admission Office at the Vienna University of Technology are empowered to review these applications. Deans are set in a position to compose statements on the applications. In order to be able to meet these requirements, the Student Records Manager is built upon the open source electronic document management system Nuxeo Platform, which qualified as an adequate foundation in the course of a market sounding. Overall, the prototype is premised on a user-centred approach with selective automation support. Therefore, the main focus is directed towards an  optimised integration of the strengths and weaknesses of humans and machines, that should ensure the most efficient processing of university applications for all involved participants. The concluding evaluation of the software prototype acknowledges its suitability for the acquisition and management of applications at the Vienna University of Technology. As a next step, the Student Records Manager could possibly be implemented in the designated operation environment and tested accordingly.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sinnl, A. (2017). <i>An emergent system for multimedial document management</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2017.36106</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "11017",
                    "name": "Sinnl Alexander - 2017 - An emergent system for multimedial document management.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1862994,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5514/2/Sinnl%20Alexander%20-%202017%20-%20An%20emergent%20system%20for%20multimedial%20document%20management.pdf"
                },
                {
                    "bsid": "75616",
                    "name": "Sinnl Alexander - 2017 - An emergent system for multimedial document management.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 181831,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5514/5/Sinnl%20Alexander%20-%202017%20-%20An%20emergent%20system%20for%20multimedial%20document%20management.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Alexander",
                    "last_name": "Sinnl",
                    "position": 1,
                    "role": "Author",
                    "tid": "192602"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54980",
            "handle": "20.500.12708/51322",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "The Contourlet Transform for Image Compression",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Belbachir, A. N., &#38; Göbel, P. (2005). The Contourlet Transform for Image Compression. In P. Marthon (Ed.), <i>Proc. of 4th Conference on Physics in Signal and Image Processing</i> (pp. 251–256). SEE. http://hdl.handle.net/20.500.12708/51322</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 1,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Göbel",
                    "position": 2,
                    "role": "Author",
                    "tid": "41359"
                },
                {
                    "first_name": "Philippe",
                    "last_name": "Marthon",
                    "position": 1,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54981",
            "handle": "20.500.12708/51323",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Comparative Study of Artificial Neural Network Techniques for River Stage Forecasting",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dawson, C., See, L., Abrahart, R., Wilby, R., Shamseldin, A., Anctil, F., Belbachir, A. N., Bowden, G., Dandy, G., Lauzon, N., Maier, H., &#38; Mason, G. (2005). A Comparative Study of Artificial Neural Network Techniques for River Stage Forecasting. In <i>Proc. of the IEEE International Joint Conference on Neural Networks</i> (pp. 2666–2670). IEEE. http://hdl.handle.net/20.500.12708/51323</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Dawson",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Linda",
                    "last_name": "See",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Abrahart",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Rob",
                    "last_name": "Wilby",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Asaad",
                    "last_name": "Shamseldin",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Franois",
                    "last_name": "Anctil",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 7,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Gavin",
                    "last_name": "Bowden",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Graeme",
                    "last_name": "Dandy",
                    "position": 9,
                    "role": "Author"
                },
                {
                    "first_name": "Nicolas",
                    "last_name": "Lauzon",
                    "position": 10,
                    "role": "Author"
                },
                {
                    "first_name": "Holger",
                    "last_name": "Maier",
                    "position": 11,
                    "role": "Author"
                },
                {
                    "first_name": "Gary",
                    "last_name": "Mason",
                    "position": 12,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54982",
            "handle": "20.500.12708/51324",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "An automatic inspection system for the diagnosis of printed circuits based on neural networks",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Belbachir, A. N., Lera, M., &#38; Fanni, A. (2005). An automatic inspection system for the diagnosis of printed circuits based on neural networks. In <i>IEEE Industry Applications Conference 2005</i> (pp. 680–684). IEEE. http://hdl.handle.net/20.500.12708/51324</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 1,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Mario",
                    "last_name": "Lera",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Alessandra",
                    "last_name": "Fanni",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54983",
            "handle": "20.500.12708/51325",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Noise Estimation in Panoramic X-ray Images: An Application Analysis Approach",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Göbel, P., &#38; Belbachir, A. N. (2005). Noise Estimation in Panoramic X-ray Images: An Application Analysis Approach. In M. Najim &#38; E. Moulines (Eds.), <i>Proc. of 13th IEEE Workshop on Statistical Signal Processing</i>. IEEE. http://hdl.handle.net/20.500.12708/51325</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Göbel",
                    "position": 1,
                    "role": "Author",
                    "tid": "41359"
                },
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 2,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Mohamed",
                    "last_name": "Najim",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Eric",
                    "last_name": "Moulines",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54984",
            "handle": "20.500.12708/51326",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Combined Multiresolution Approach for Faint Source Extraction from Infrared Astronomical Raw Images Sequence",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Belbachir, A. N., &#38; Göbel, P. (2005). A Combined Multiresolution Approach for Faint Source Extraction from Infrared Astronomical Raw Images Sequence. In M. Najim &#38; E. Moulines (Eds.), <i>Proc. of 13th IEEE Workshop on Statistical Signal Processing</i>. IEEE. http://hdl.handle.net/20.500.12708/51326</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 1,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Göbel",
                    "position": 2,
                    "role": "Author",
                    "tid": "41359"
                },
                {
                    "first_name": "Mohamed",
                    "last_name": "Najim",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Eric",
                    "last_name": "Moulines",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E376"
            ],
            "pid": "54985",
            "handle": "20.500.12708/51327",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Method for Determining Geometrical Distortions of Off-the-shelf Wide-Angle Cameras",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zollner, H. (2005). A Method for Determining Geometrical Distortions of Off-the-shelf Wide-Angle Cameras. In D. Chetverikov, L. Czuni, &#38; M. Vincze (Eds.), <i>Proc. of Joint Hungarian-Austrian Conference on Image Processing and Pattern Recognition</i> (pp. 367–372). OCG Schriftenreihe, Österreichische Arbeitsgemeinschaft für Mustererkennung. http://hdl.handle.net/20.500.12708/51327</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Helmut",
                    "last_name": "Zollner",
                    "position": 1,
                    "role": "Author",
                    "tid": "50601"
                },
                {
                    "first_name": "Dimitry",
                    "last_name": "Chetverikov",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Laszlo",
                    "last_name": "Czuni",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Vincze",
                    "position": 3,
                    "role": "Editor",
                    "tid": "133231"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54986",
            "handle": "20.500.12708/51328",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Method for Determining Geometrical Distortion of Off-The-Shelf Wide-Angle Cameras",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zollner, H., &#38; Sablatnig, R. (2005). A Method for Determining Geometrical Distortion of Off-The-Shelf Wide-Angle Cameras. In W. Kropatsch, R. Sablatnig, &#38; A. Hanbury (Eds.), <i>Proc. of the 27th DAGM Symposium</i> (pp. 224–229). Springer, LNCS. http://hdl.handle.net/20.500.12708/51328</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Helmut",
                    "last_name": "Zollner",
                    "position": 1,
                    "role": "Author",
                    "tid": "50601"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Author",
                    "tid": "133566"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Editor",
                    "tid": "133566"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Editor",
                    "tid": "48222"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54987",
            "handle": "20.500.12708/51329",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "CCA-based Active Appearance Model Search",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Donner, R., Langs, G., Reiter, M., &#38; Bischof, H. (2005). CCA-based Active Appearance Model Search. In A. Hanbury &#38; H. Bischof (Eds.), <i>Computer Vision Winter Workshop 2005</i> (pp. 73–82). Eigenverlag. http://hdl.handle.net/20.500.12708/51329</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 1,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 2,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Reiter",
                    "position": 3,
                    "role": "Author",
                    "tid": "42378"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 4,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Editor",
                    "tid": "48222"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Editor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03",
                "E376"
            ],
            "pid": "54988",
            "handle": "20.500.12708/51330",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Fast Active Appearance Model Search based on CCA and its application to hand radiographs",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Donner, R., Langs, G., Reiter, A., &#38; Bischof, H. (2005). Fast Active Appearance Model Search based on CCA and its application to hand radiographs. In D. Chetverikov, L. Czuni, &#38; M. Vincze (Eds.), <i>Proceedings of the Joint Hungarian-Austrian Conference on Image Processing and Pattern Recognition}</i> (pp. 49–56). OCG Schriftenreihe, Österreichische Arbeitsgemeinschaft für Mustererkennung. http://hdl.handle.net/20.500.12708/51330</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 1,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 2,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Anton",
                    "last_name": "Reiter",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 4,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "Dimitry",
                    "last_name": "Chetverikov",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Laszlo",
                    "last_name": "Czuni",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Vincze",
                    "position": 3,
                    "role": "Editor",
                    "tid": "133231"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54989",
            "handle": "20.500.12708/51331",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Detection and Analysis of Lines on the Surface of Arachaeological Pottery",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kammerer, P., Mara, H., Kratzmueller,  bettina, &#38; Zolda, E. (2005). Detection and Analysis of Lines on the Surface of Arachaeological Pottery. In V. Cappellini &#38; J. Hemsley (Eds.), <i>Electronic Imaging and the Visual Arts</i> (pp. 154–159). Pitagora Editrice Bologna. http://hdl.handle.net/20.500.12708/51331</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Paul",
                    "last_name": "Kammerer",
                    "position": 1,
                    "role": "Author",
                    "tid": "49696"
                },
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 2,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "bettina",
                    "last_name": "Kratzmueller",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Ernestine",
                    "last_name": "Zolda",
                    "position": 4,
                    "role": "Author",
                    "tid": "229392"
                },
                {
                    "first_name": "Vito",
                    "last_name": "Cappellini",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "James",
                    "last_name": "Hemsley",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54990",
            "handle": "20.500.12708/51332",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Classification of Color Pigments in Hyperspectral Images",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Asinger, C., Kammerer, P., &#38; Zolda, E. (2005). Classification of Color Pigments in Hyperspectral Images. In A. Hanbury &#38; H. Bischof (Eds.), <i>Proc. of the 10th Computer Vision Winter Workshop</i> (pp. 205–214). Eigenverlag. http://hdl.handle.net/20.500.12708/51332</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Asinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "52882"
                },
                {
                    "first_name": "Paul",
                    "last_name": "Kammerer",
                    "position": 2,
                    "role": "Author",
                    "tid": "49696"
                },
                {
                    "first_name": "Ernestine",
                    "last_name": "Zolda",
                    "position": 3,
                    "role": "Author",
                    "tid": "229392"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Editor",
                    "tid": "48222"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 2,
                    "role": "Editor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54991",
            "handle": "20.500.12708/51333",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Investigation on Traditional and Modern Ceramic Documentation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kampel, M., Mara, H., &#38; Sablatnig, R. (2005). Investigation on Traditional and Modern Ceramic Documentation. In G. Vernazza &#38; G. Sicuranza (Eds.), <i>Intl. Conf. on Image Processing</i> (pp. 570–573). IEEE. http://hdl.handle.net/20.500.12708/51333</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 1,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 2,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 3,
                    "role": "Author",
                    "tid": "133566"
                },
                {
                    "first_name": "Gianni",
                    "last_name": "Vernazza",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Giovanni",
                    "last_name": "Sicuranza",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "54992",
            "handle": "20.500.12708/51334",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Automated 3D-Scanning and Analyis of Archaeological Objects",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mara, H. (2005). Automated 3D-Scanning and Analyis of Archaeological Objects. In <i>Proc. of the 24th Intl. Conference on Modelling, Identification and Control (MIC05)</i> (pp. 388–392). ACTA Press. http://hdl.handle.net/20.500.12708/51334</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 1,
                    "role": "Author",
                    "tid": "51366"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "54993",
            "handle": "20.500.12708/51335",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Robust 3D Reconstruction of Archaeological Pottery based on Concentric Circular Rills",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kampel, M., Sablatnig, R., &#38; Mara, H. (2005). Robust 3D Reconstruction of Archaeological Pottery based on Concentric Circular Rills. In <i>Proc. of the 6th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS05)</i>. SuviSoft. http://hdl.handle.net/20.500.12708/51335</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 1,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Author",
                    "tid": "133566"
                },
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 3,
                    "role": "Author",
                    "tid": "51366"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "55002",
            "handle": "20.500.12708/51344",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Panel: Virtual Reality and Spatial Ability",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., Rizzo, A., Kim, G., Darken, R. P., Astur, R., &#38; Tendick, F. (2005). Panel: Virtual Reality and Spatial Ability. In <i>VR 2005 Proceedings</i> (pp. 299–300). http://hdl.handle.net/20.500.12708/51344</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Albert",
                    "last_name": "Rizzo",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "G",
                    "last_name": "Kim",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "R. P",
                    "last_name": "Darken",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "R",
                    "last_name": "Astur",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "F",
                    "last_name": "Tendick",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01"
            ],
            "pid": "55004",
            "handle": "20.500.12708/51346",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluation of a method for digital definition of bone shapes",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Peloschek, P. L., Langs, G., Urschler, M., Sailer, J., Uffmann, M., Schlager, T., Kainberger, F., &#38; Bischof, H. (2005). Evaluation of a method for digital definition of bone shapes. In <i>European Congress of Radiology ECR 2005</i> (pp. 191–192). Springer. http://hdl.handle.net/20.500.12708/51346</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 2,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Urschler",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Johannes",
                    "last_name": "Sailer",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "M",
                    "last_name": "Uffmann",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Schlager",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "F",
                    "last_name": "Kainberger",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 8,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "55018",
            "handle": "20.500.12708/51360",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "The Potential of Augmented Reality in Dynamic Geometry Education",
            "keywords": [],
            "abstract": "This paper summarizes work done within the previous 6 years to integrate and implement Virtual and Augmented Reality (VR/AR) technologies in geometry education. A VR/AR application supporting dynamic 3D geometry is presented, intended for real use in high school and university geometry education. Related areas such as dynamic three-dimensional geometry, usability and user interface design, spatial abilities, pedagogy and low-cost VR systems influenced our work and the development of the educational geometry application Construct3D.\r\nAfter describing the design of Construct3D, the strengths of this learning environment for geometry education are investigated. Dynamic three-dimensional geometric content is depicted that fully benefits from the advantages provided by the application. \r\nIn order to adapt software and hardware to users´ needs, user interfaces were redesigned and in depth research was done on usability design. Very positive and useful feedback from teachers and students, who are excited by the possibilities, was collected in three evaluations with more than 100 students in over 500 teaching lessons. Results from these evaluations show that Construct3D is easy to use, requires little time to learn, encourages learners to explore geometry and can be used in a consistent way.\r\nVarious hardware setups have been studied that are suitable for educational purposes. An immersive setup that uses head mounted displays is most favored by teachers and students. It allows users to actually &quot;walk around&quot; geometric objects which are fixed in space.\r\nRegarding spatial ability research, a currently running project for training spatial abilities with more than 300 participants is outlined. A review of shortcomings of existing spatial ability tests concludes with ideas to conduct future testings directly in VR/AR. Finally the recent development of a low cost optical tracking system, that allows to build affordable, immersive VR systems is described.\r\nOur work enables teaching of three-dimensional dynamic geometry in an interactive, immersive learning environment, therefore offers new possibilities to modern geometry education. Despite the findings mentioned in this paper, many more research questions have emerged during the development of Construct3D. According to each of the related areas we indicate interesting topics that might require future work. \r\nBy summarizing our work, which aims to establish Augmented Reality in geometry education, and by providing insight into the problems and future challenges in each of the related areas, we want to highlight and advert the full potential of Augmented Reality in geometry education.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2006). The Potential of Augmented Reality in Dynamic Geometry Education. In <i>Proceedings of the 12th International Conference on Geometry and Graphics (ICGG) 2006</i>. 12th International Conference on Geometry and Graphics, Salvador, Brasilien, Non-EU. http://hdl.handle.net/20.500.12708/51360</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "55026",
            "handle": "20.500.12708/51368",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Kalman Filtering for Illumination-invariant Face Recognition",
            "keywords": [],
            "abstract": "We propose a novel algorithm for the identification of faces from image samples. The algorithm uses the Kalman filter to identify significant facial traits. Kalmanfaces are compact visual models that represent the invariant proportions of face classes. We employ the Kalmanfaces approach on the Physics-based Face Database (provided by the University of Oulu), a collection of face images that were recorded under varying illumination conditions. Kalman faces show robustness against luminance changes and outperform the classic Eigenfaces approach in terms of identification performance and algorithm speed. The paper discusses Kalmanfaces extraction, application, tunable parameters, experimental results and related work on Kalman filter application in face recognition.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2006). Kalman Filtering for Illumination-invariant Face Recognition. In <i>Proceedings IEEE ELMAR 2006</i>. IEEE ELMAR 2006, Zadar, Croatia, Non-EU. http://hdl.handle.net/20.500.12708/51368</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06",
                "E194-01"
            ],
            "pid": "55027",
            "handle": "20.500.12708/51369",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Analysis of the Data Quality of Audio Descriptions of Environmental Sounds",
            "keywords": [],
            "abstract": "In this paper we perform statistical data analysis of a broad set of state-of-the-art audio features and low-level MPEG-7 audio descriptors. The investigation comprises data analysis to reveal redundancies between state-of-the-art audio features and MPEG-7 audio descriptors. We introduce a novel measure to evaluate the information content of a descriptor in terms of variance. Statistical data analysis reveals the amount of variance contained in a feature. It enables identification of independent and redundant features. This approach assists in efficient selection of orthogonal features for content-based retrieval. We believe that a good feature should provide descriptions with high variance for the underlying data. Combinations of features should consist of decorrelated features in order to increase expressiveness of the descriptions. Although MPEG-7 is a popular and widely used standard for multimedia description, only few investigations do exist that address analysis of the data quality of low-level MPEG-7 descriptions.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mitrovic, D., Zeppelzauer, M., &#38; Eidenberger, H. (2006). Analysis of the Data Quality of Audio Descriptions of Environmental Sounds. In <i>Fourth Special Workshop Proceedings</i> (pp. 70–79). http://hdl.handle.net/20.500.12708/51369</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 1,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 3,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55148",
            "handle": "20.500.12708/51489",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "PAT: Profile Analysis Tool for the Documentation of Archaeological Finds",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Lettner, M., Mara, H., Müller, A., Sablatnig, R., Singer, M., &#38; Krenn, M. (2006). PAT: Profile Analysis Tool for the Documentation of Archaeological Finds. In <i>Digital Cultural Heritage - Essential for Tourism</i> (pp. 83–90). Oösterreichische Computergesellschaft. http://hdl.handle.net/20.500.12708/51489</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Lettner",
                    "position": 1,
                    "role": "Author",
                    "tid": "48706"
                },
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 2,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Müller",
                    "position": 3,
                    "role": "Author",
                    "tid": "52000"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 4,
                    "role": "Author",
                    "tid": "133566"
                },
                {
                    "first_name": "Marianne",
                    "last_name": "Singer",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Krenn",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Computer Engineering"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55169",
            "handle": "20.500.12708/51510",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Template patch driven image segmentation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Micusik, B., &#38; Hanbury, A. (2006). Template patch driven image segmentation. In <i>British Machine Vision Conference 2006, Volume Two</i> (pp. 819–829). http://hdl.handle.net/20.500.12708/51510</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 1,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55170",
            "handle": "20.500.12708/51511",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Automatic Image Segmentation by Positioning a Seed",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Micusik, B., &#38; Hanbury, A. (2006). Automatic Image Segmentation by Positioning a Seed. In <i>Computer Vision - ECCV2006</i> (pp. 468–480). Springer. http://hdl.handle.net/20.500.12708/51511</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 1,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55172",
            "handle": "20.500.12708/51513",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Computing Homology for Surfaces with Generalized Maps: Application to 3D Images",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Damiand, G., Peltier, S., &#38; Fuchs, L. (2006). Computing Homology for Surfaces with Generalized Maps: Application to 3D Images. In <i>Advances in Visual Computing</i> (pp. 1151–1160). Springer. http://hdl.handle.net/20.500.12708/51513</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Guillaume",
                    "last_name": "Damiand",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Samuel",
                    "last_name": "Peltier",
                    "position": 2,
                    "role": "Author",
                    "tid": "229413"
                },
                {
                    "first_name": "Laurent",
                    "last_name": "Fuchs",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55199",
            "handle": "20.500.12708/51540",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Fast pedestrian tracking based on spatial features and colour",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Seitner, F., &#38; Hanbury, A. (2006). Fast pedestrian tracking based on spatial features and colour. In <i>Proceedings of the 11th Computer Vision Winter Workshop</i> (pp. 105–110). http://hdl.handle.net/20.500.12708/51540</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Florian",
                    "last_name": "Seitner",
                    "position": 1,
                    "role": "Author",
                    "tid": "52948"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55200",
            "handle": "20.500.12708/51541",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Extraction of Attributes, Nature and Context of Images",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kuthan, S., &#38; Hanbury, A. (2006). Extraction of Attributes, Nature and Context of Images. In <i>Proceedings of the 11th Computer Vision Winter Workshop</i> (pp. 28–33). http://hdl.handle.net/20.500.12708/51541</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Stefan",
                    "last_name": "Kuthan",
                    "position": 1,
                    "role": "Author",
                    "tid": "89650"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55201",
            "handle": "20.500.12708/51542",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Texture segmentation through salient texture patches",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Szumilas, L., Micusik, B., &#38; Hanbury, A. (2006). Texture segmentation through salient texture patches. In <i>Proceedings of the 11th Computer Vision Winter Workshop</i> (pp. 111–116). http://hdl.handle.net/20.500.12708/51542</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Lech",
                    "last_name": "Szumilas",
                    "position": 1,
                    "role": "Author",
                    "tid": "38816"
                },
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 2,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55202",
            "handle": "20.500.12708/51543",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "On Colour Spaces for Change Detection and Shadow Suppression",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Blauensteiner, P., Wildenauer, H., &#38; Hanbury, A. (2006). On Colour Spaces for Change Detection and Shadow Suppression. In <i>Proceedings of the 11th Computer Vision Winter Workshop</i> (pp. 117–122). http://hdl.handle.net/20.500.12708/51543</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philipp",
                    "last_name": "Blauensteiner",
                    "position": 1,
                    "role": "Author",
                    "tid": "53592"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Wildenauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "51087"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55203",
            "handle": "20.500.12708/51544",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Waterfall Segmentation of Complex Scenes",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A., &#38; Marcotegui, B. (2006). Waterfall Segmentation of Complex Scenes. In <i>Computer Vision - ACCV 2006</i> (pp. 888–897). Springer. http://hdl.handle.net/20.500.12708/51544</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Beatriz",
                    "last_name": "Marcotegui",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55204",
            "handle": "20.500.12708/51545",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "How Humans Describe Short Videos",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Hausegger, H., Kropatsch, W., &#38; Haxhimusa, Y. (2006). How Humans Describe Short Videos. In <i>Proceedings of the Second International Cognitive Vision Workshop</i>. ICVW06, Second International Cognitive Vision Workshop, Graz, Austria. http://hdl.handle.net/20.500.12708/51545</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hubert",
                    "last_name": "Hausegger",
                    "position": 2,
                    "role": "Author",
                    "tid": "64539"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 4,
                    "role": "Author",
                    "tid": "64562"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55205",
            "handle": "20.500.12708/51546",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluating Hierarchical Graph-based Segmentation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y., Ion, A., &#38; Kropatsch, W. (2006). Evaluating Hierarchical Graph-based Segmentation. In <i>The 18th International Conference on Pattern Recognition</i> (pp. 195–198). IEEE Society. http://hdl.handle.net/20.500.12708/51546</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55206",
            "handle": "20.500.12708/51547",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "The Eccentricity Transform (of a digital shape)",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Ion, A., Haxhimusa, Y., &#38; Flanitzer, T. (2006). The Eccentricity Transform (of a digital shape). In <i>Discrete Geometry for Computer Imagery</i> (pp. 437–448). Springer Berlin-Heidelberg. http://hdl.handle.net/20.500.12708/51547</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Flanitzer",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55207",
            "handle": "20.500.12708/51548",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Distinguishing the 3 Primitive 3D-topological Configurations: Simplex, Hole, Tunnel",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Illetschko, T., Ion, A., Haxhimusa, Y., &#38; Kropatsch, W. (2006). Distinguishing the 3 Primitive 3D-topological Configurations: Simplex, Hole, Tunnel. In <i>Proceedings of the 11th Computer Vision Winter Workshop</i> (pp. 22–27). http://hdl.handle.net/20.500.12708/51548</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Illetschko",
                    "position": 1,
                    "role": "Author",
                    "tid": "97458"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55208",
            "handle": "20.500.12708/51549",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Considerations Regarding the Minimum Spanning Tree Pyramid Segmentation Method",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Kropatsch, W., &#38; Haxhimusa, Y. (2006). Considerations Regarding the Minimum Spanning Tree Pyramid Segmentation Method. In <i>Structural, Syntactic, and Statistical Pattern Recognition</i> (pp. 182–190). Springer. http://hdl.handle.net/20.500.12708/51549</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55209",
            "handle": "20.500.12708/51550",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Comparing Hierarchies of Segmentations: Humans, Normalized Cut, and Minimum Spanning Tree",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y., Ion, A., &#38; Kropatsch, W. (2006). Comparing Hierarchies of Segmentations: Humans, Normalized Cut, and Minimum Spanning Tree. In <i>Digital Imaging and Pattern Recognition</i> (pp. 95–103). OCG. http://hdl.handle.net/20.500.12708/51550</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55210",
            "handle": "20.500.12708/51551",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Collapsing 3D Combinatorial Maps",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Illetschko, T., Ion, A., Haxhimusa, Y., &#38; Kropatsch, W. (2006). Collapsing 3D Combinatorial Maps. In <i>Digital Imaging and Pattern Recognition</i> (pp. 85–93). OCG. http://hdl.handle.net/20.500.12708/51551</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Illetschko",
                    "position": 1,
                    "role": "Author",
                    "tid": "97458"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55211",
            "handle": "20.500.12708/51552",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Irregular Pyramid Segmentations with Stochastic Graph Decimation Strategies",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y., Ion, A., &#38; Kropatsch, W. (2006). Irregular Pyramid Segmentations with Stochastic Graph Decimation Strategies. In <i>Progress in Pattern Recognition, Image Analysis and Applications</i> (pp. 277–286). Springer. http://hdl.handle.net/20.500.12708/51552</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01"
            ],
            "pid": "55215",
            "handle": "20.500.12708/51556",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Automatic Detection of Erosions in Rheumatoid Arthritis Assessment",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langs, G., Peloschek, P. L., Bischof, H., &#38; Kainberger, F. (2006). Automatic Detection of Erosions in Rheumatoid Arthritis Assessment. In <i>Quantitative Automated Musculoskeletal Analysis</i> (pp. 33–40). http://hdl.handle.net/20.500.12708/51556</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 1,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 3,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "F",
                    "last_name": "Kainberger",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55217",
            "handle": "20.500.12708/51558",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Active Feature Models",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langs, G., Peloschek, P. L., Donner, R., Reiter, M., &#38; Bischof, H. (2006). Active Feature Models. In <i>The 18th International Conference on Pattern Recognition</i> (pp. 417–420). IEEE Society. http://hdl.handle.net/20.500.12708/51558</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 1,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 3,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Reiter",
                    "position": 4,
                    "role": "Author",
                    "tid": "42378"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 5,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55218",
            "handle": "20.500.12708/51559",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Annotation Propagation by MDL Based Correspondences",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langs, G., Peloschek, P. L., Donner, R., &#38; Bischof, H. (2006). Annotation Propagation by MDL Based Correspondences. In <i>11th Computer Vision Winter Workshop</i> (pp. 11–16). http://hdl.handle.net/20.500.12708/51559</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 1,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 3,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 4,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55228",
            "handle": "20.500.12708/51569",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Extension of Standard Active Contuor Models (Snakes) for Stroke Segmentation in Paintings",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vill, M., &#38; Kammerer, P. (2006). Extension of Standard Active Contuor Models (Snakes) for Stroke Segmentation in Paintings. In <i>Digital Cultural Heritage - Essential for Tourism</i> (pp. 11–18). OCG. http://hdl.handle.net/20.500.12708/51569</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maria",
                    "last_name": "Vill",
                    "position": 1,
                    "role": "Author",
                    "tid": "46819"
                },
                {
                    "first_name": "Paul",
                    "last_name": "Kammerer",
                    "position": 2,
                    "role": "Author",
                    "tid": "49696"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55242",
            "handle": "20.500.12708/51583",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Color Pair Clustering for Texture Detection",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Szumilas, L., &#38; Hanbury, A. (2006). Color Pair Clustering for Texture Detection. In <i>Advances in Visual Computing</i> (pp. 255–264). Springer. http://hdl.handle.net/20.500.12708/51583</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Lech",
                    "last_name": "Szumilas",
                    "position": 1,
                    "role": "Author",
                    "tid": "38816"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55243",
            "handle": "20.500.12708/51584",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Predicting Near Infrared Face Texture from Color Face Images using Canonical Correlation Analysis",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reiter, M., Donner, R., Langs, G., &#38; Bischof, H. (2006). Predicting Near Infrared Face Texture from Color Face Images using Canonical Correlation Analysis. In <i>Digital Imaging and Pattern Recognition</i> (pp. 161–167). OCG Schriftenreihe. http://hdl.handle.net/20.500.12708/51584</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Reiter",
                    "position": 1,
                    "role": "Author",
                    "tid": "42378"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 2,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 3,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 4,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55244",
            "handle": "20.500.12708/51585",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Estimation of Face Depth Maps from Color Textures using Canonical Correlation Analysis",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reiter, M., Donner, R., Langs, G., &#38; Bischof, H. (2006). Estimation of Face Depth Maps from Color Textures using Canonical Correlation Analysis. In <i>Proceedings of the Computer Vision Winter Workshop</i> (pp. 17–21). http://hdl.handle.net/20.500.12708/51585</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Reiter",
                    "position": 1,
                    "role": "Author",
                    "tid": "42378"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 2,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 3,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 4,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        }
    ],
    "publications2": [
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55245",
            "handle": "20.500.12708/51586",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "3D and Infrared Face Reconstruction from RGB data using Canonical Correlation Analysis",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reiter, M., Donner, R., Langs, G., &#38; Bischof, H. (2006). 3D and Infrared Face Reconstruction from RGB data using Canonical Correlation Analysis. In <i>The 18th International Conference on Pattern Recognition</i>. IAPR ICPR 2006, Hong Kong, Non-EU. IEEE Conmputer Society. http://hdl.handle.net/20.500.12708/51586</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Reiter",
                    "position": 1,
                    "role": "Author",
                    "tid": "42378"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 2,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 3,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 4,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55272",
            "handle": "20.500.12708/51613",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Determination of Ancient Manufacturing Techniques of Ceramics by 3D Shape Estimation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mara, H., &#38; Sablatnig, R. (2006). Determination of Ancient Manufacturing Techniques of Ceramics by 3D Shape Estimation. In <i>Interactive Technologies and Sociotechnical Systems</i> (pp. 349–357). Springer Berlin-Heidelberg. http://hdl.handle.net/20.500.12708/51613</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 1,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55273",
            "handle": "20.500.12708/51614",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Orientation of Fragments of Rotationally Symmetrical 3D-Shapes for Archaeological Documentation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mara, H., &#38; Sablatnig, R. (2006). Orientation of Fragments of Rotationally Symmetrical 3D-Shapes for Archaeological Documentation. In <i>Proc. of 3D Data Processing, Visualization, and Transmission (3DPVT’06)</i>. Proc. of 3D Data Processing, Visualization, and Transmission (3DPVT’06), Chapel Hill, NC, USA, Non-EU. http://hdl.handle.net/20.500.12708/51614</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 1,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55285",
            "handle": "20.500.12708/51626",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Motion Detection Using an Improved Colour model",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Wildenauer, H., Blauensteiner, P., Hanbury, A., &#38; Kampel, M. (2006). Motion Detection Using an Improved Colour model. In <i>Advances in Visual Computing</i> (pp. 607–616). Springer Berlin-Heidelberg. http://hdl.handle.net/20.500.12708/51626</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Wildenauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "51087"
                },
                {
                    "first_name": "Philipp",
                    "last_name": "Blauensteiner",
                    "position": 2,
                    "role": "Author",
                    "tid": "53592"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 4,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55295",
            "handle": "20.500.12708/51636",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Visual Surveillance for Airport Monitoring Applications",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Aguilera Antequera, J., Thirde, D., Kampel, M., Borg, M., Fernandez, G., &#38; Ferryman, J. (2006). Visual Surveillance for Airport Monitoring Applications. In <i>Proceedings of the 11th Computer Vision Winter Workshop</i> (pp. 5–10). http://hdl.handle.net/20.500.12708/51636</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Josep",
                    "last_name": "Aguilera Antequera",
                    "position": 1,
                    "role": "Author",
                    "tid": "229250"
                },
                {
                    "first_name": "David",
                    "last_name": "Thirde",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 3,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Borg",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Gustavo",
                    "last_name": "Fernandez",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55296",
            "handle": "20.500.12708/51637",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Multi-Camera Tracking for Airport Surveillance Applications",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Thirde, D., Ferryman, J., Aguilera Antequera, J., Kampel, M., &#38; Fernandez, G. (2006). Multi-Camera Tracking for Airport Surveillance Applications. In <i>Proceedings of the 11th Computer Vision Winter Workshop</i> (pp. 87–92). http://hdl.handle.net/20.500.12708/51637</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "David",
                    "last_name": "Thirde",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Josep",
                    "last_name": "Aguilera Antequera",
                    "position": 3,
                    "role": "Author",
                    "tid": "229250"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 4,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Gustavo",
                    "last_name": "Fernandez",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55298",
            "handle": "20.500.12708/51639",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automated Investigation of Archaeological Vessels",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kampel, M., Mara, H., &#38; Sablatnig, R. (2006). Automated Investigation of Archaeological Vessels. In <i>Proc. of EUSIPCO2006: 13th European Signal Processing Conference</i> (p. 5). http://hdl.handle.net/20.500.12708/51639</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 1,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 2,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 3,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55299",
            "handle": "20.500.12708/51640",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "3D Object Localisation and Evaluation from Video Streams",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kampel, M., Thirde, D., Fernandez, G., Borg, M., Wildenauer, H., Ferryman, J., &#38; Blauensteiner, P. (2006). 3D Object Localisation and Evaluation from Video Streams. In <i>Digital Imaging and Pattern Recognition</i> (pp. 113–122). http://hdl.handle.net/20.500.12708/51640</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 1,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "David",
                    "last_name": "Thirde",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Gustavo",
                    "last_name": "Fernandez",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Borg",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Wildenauer",
                    "position": 5,
                    "role": "Author",
                    "tid": "51087"
                },
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Philipp",
                    "last_name": "Blauensteiner",
                    "position": 7,
                    "role": "Author",
                    "tid": "53592"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55301",
            "handle": "20.500.12708/51642",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "People and Vehicle Tracking for Visual Surveillance",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Thirde, D., Borg, M., Valentin, V., Barthelemy, L., Aguilera Antequera, J., Fernandez, G., Ferryman, J., Bremond, F., Thonnat, M., &#38; Kampel, M. (2006). People and Vehicle Tracking for Visual Surveillance. In <i>Proc. of the Sixth IEEE International Workshop on Visual Surveillance</i> (pp. 169–176). http://hdl.handle.net/20.500.12708/51642</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "David",
                    "last_name": "Thirde",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Borg",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Valery",
                    "last_name": "Valentin",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Luc",
                    "last_name": "Barthelemy",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Josep",
                    "last_name": "Aguilera Antequera",
                    "position": 5,
                    "role": "Author",
                    "tid": "229250"
                },
                {
                    "first_name": "Gustavo",
                    "last_name": "Fernandez",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "James",
                    "last_name": "Ferryman",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Francois",
                    "last_name": "Bremond",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Monique",
                    "last_name": "Thonnat",
                    "position": 9,
                    "role": "Author"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 10,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55312",
            "handle": "20.500.12708/51653",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Motion and Shadow Detection with an Improved Colour Model",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Blauensteiner, P., Wildenauer, H., Hanbury, A., &#38; Kampel, M. (2006). Motion and Shadow Detection with an Improved Colour Model. In <i>First International Conference on Signal and Image Processing</i> (pp. 627–632). IEEE. http://hdl.handle.net/20.500.12708/51653</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philipp",
                    "last_name": "Blauensteiner",
                    "position": 1,
                    "role": "Author",
                    "tid": "53592"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Wildenauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "51087"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 4,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55478",
            "handle": "20.500.12708/51819",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Active Contour Segmentation of Painted Strokes without a priori Shape Knowledge",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vill, M., &#38; Kammerer, P. (2007). Active Contour Segmentation of Painted Strokes without a priori Shape Knowledge. In <i>Proceedings of the 12th Computer Vision Winter Workshop</i>. 12th CVWW07, St. Lambrecht, Austria. http://hdl.handle.net/20.500.12708/51819</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maria",
                    "last_name": "Vill",
                    "position": 1,
                    "role": "Author",
                    "tid": "46819"
                },
                {
                    "first_name": "Paul",
                    "last_name": "Kammerer",
                    "position": 2,
                    "role": "Author",
                    "tid": "49696"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55479",
            "handle": "20.500.12708/51820",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Stroke Segmentation with a Combination of Ribbon and Ziplock-Snakes",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vill, M., &#38; Kammerer, P. (2007). Stroke Segmentation with a Combination of Ribbon and Ziplock-Snakes. In <i>Performance Evaluation for Computer Vision 31st AAPR/OAGM Workshop 2007</i> (pp. 17–24). http://hdl.handle.net/20.500.12708/51820</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maria",
                    "last_name": "Vill",
                    "position": 1,
                    "role": "Author",
                    "tid": "46819"
                },
                {
                    "first_name": "Paul",
                    "last_name": "Kammerer",
                    "position": 2,
                    "role": "Author",
                    "tid": "49696"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55480",
            "handle": "20.500.12708/51821",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Image based recognition of coins -- An Overview of the COINS project",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaharieva, M., Kampel, M., &#38; Zambanini, S. (2007). Image based recognition of coins -- An Overview of the COINS project. In <i>Performance Evaluation for Computer Vision 31st AAPR/OAGM Workshop 2007</i> (pp. 57–64). http://hdl.handle.net/20.500.12708/51821</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 2,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Sebastian",
                    "last_name": "Zambanini",
                    "position": 3,
                    "role": "Author",
                    "tid": "53412"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55500",
            "handle": "20.500.12708/51841",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Colour Interest Points for Image Retrieval",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stöttinger, J., Sebe, N., Gevers, T., &#38; Hanbury, A. (2007). Colour Interest Points for Image Retrieval. In <i>Proceedings of the 12th Computer Vision Winter Workshop</i> (pp. 83–90). http://hdl.handle.net/20.500.12708/51841</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Nicu",
                    "last_name": "Sebe",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Theo",
                    "last_name": "Gevers",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 4,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06",
                "E194-01"
            ],
            "pid": "55558",
            "handle": "20.500.12708/51899",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Towards Integrated Authoring, Annotation, Retrieval, Adaptation, Personalization and Delivery of Multimedia Content",
            "keywords": [],
            "abstract": "We describe the CoCoMA task of the DELOS II European Network of Excellence on Digital Libraries. CoCoMA aims at the unification of the most important aspects of multimedia management and multimedia presentation, i.e., the integration of authoring, annotation and presentation design with on- demand content adaptation, ad hoc media retrieval (semantics-based and content-based), and personalized delivery and visualization of presentations. The paramount goal of the CoCoMA activity is to maximize the added value from task and data integration by the identification and exploitations of connection points and inherent workflow similarities. The paper provides a brief description of the involved research fields, suggests a architecture for integrated multimedia consumption and presentation, and discusses the most prominent connection points (e.g., the reuse of content-based metadata for content adaptation and personalization). Problems and solutions are discussed jointly and illustrated by the components of the application prototype developed for the DELOS project.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H., Boll, S., Christodoulakis, S., Divotkey, D., Leopold, K., Martin, A., Perego, A., Scherp, A., &#38; Tsinaraki, C. (2007). Towards Integrated Authoring, Annotation, Retrieval, Adaptation, Personalization and Delivery of Multimedia Content. In <i>Proceedings of DELOS Conference 2007</i>. DELOS Conference 2007, Pisa, Italy, EU. http://hdl.handle.net/20.500.12708/51899</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                },
                {
                    "first_name": "Susanne",
                    "last_name": "Boll",
                    "position": 2,
                    "role": "Author",
                    "tid": "126031"
                },
                {
                    "first_name": "Stavros",
                    "last_name": "Christodoulakis",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Doris",
                    "last_name": "Divotkey",
                    "position": 4,
                    "role": "Author",
                    "tid": "93452"
                },
                {
                    "first_name": "Klaus",
                    "last_name": "Leopold",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Alessandro",
                    "last_name": "Martin",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Andrea",
                    "last_name": "Perego",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Ansger",
                    "last_name": "Scherp",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Chrisa",
                    "last_name": "Tsinaraki",
                    "position": 9,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E194-01"
            ],
            "pid": "55559",
            "handle": "20.500.12708/51900",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Evaluation of content-based Features for User-Centred Image Retrieval in Small Media Collections",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H., &#38; Zaharieva, M. (2008). Evaluation of content-based Features for User-Centred Image Retrieval in Small Media Collections. In <i>Proceedings of SPIE IS&#38;T Electronic Imaging Conference 2008</i>. SPIE IS&#38;T Electronic Imaging Conference, San Jose, Kalifornien, Non-EU. SPIE Press. http://hdl.handle.net/20.500.12708/51900</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 2,
                    "role": "Author",
                    "tid": "39017"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "55580",
            "handle": "20.500.12708/51918",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Analysis of a Kalman Approach for a Pedestrian Positioning System in Indoor Environments",
            "keywords": [],
            "abstract": "In this work we present the design principles of a wearable positioning system for users in unprepared indoor environments. We describe the most suitable technology for our application and we model the dynamics of a walking user. The system uses low-cost inertial sensors and a location system based on ultrawideband (UWB). Data fusion is carried out with a Kalman filter.The user position is estimated from data provided by the UWB location system. To update the position and direction of the user we use a dead reckoning algorithm. The use of redundant sensors and the data fusion technique minimises the presence of shadow zones in the environment. We show the advantages of combining different sensors systems.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pulido Herrera, E., Quiros, R., &#38; Kaufmann, H. (2007). Analysis of a Kalman Approach for a Pedestrian Positioning System in Indoor Environments. In <i>Proceedings of the European Conference on Parallel and Distributed Computing</i> (pp. 931–941). http://hdl.handle.net/20.500.12708/51918</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Edith",
                    "last_name": "Pulido Herrera",
                    "position": 1,
                    "role": "Author",
                    "tid": "191422"
                },
                {
                    "first_name": "Ricardo",
                    "last_name": "Quiros",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "55582",
            "handle": "20.500.12708/51920",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Towards a Universal Implementation of 3D User Interaction Techniques",
            "keywords": [],
            "abstract": "This paper presents a versatile - write once, use everywhere - approach of standardizing the development of three-dimensional user interaction techniques. In order to achieve a platform and application independent implementation of 3D interaction techniques (ITs), we propose to implement the related techniques directly in the tracking middleware. Therefore a widely used tracking framework was extended by a Python binding to allow straight forward scripting of ITs. We cluster existing 3D ITs, into those which can be fully, partly or not implemented in the tracking middleware. A number of examples demonstrate how various interaction techniques can quickly and efficiently be implemented in the middleware and are therefore fully independent of the underlying application. We hint at how this approach can be used to decouple menu system control from the application with the final goal to help establishing standards for 3D interaction.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Csisinko, M., &#38; Kaufmann, H. (2007). Towards a Universal Implementation of 3D User Interaction Techniques. In <i>Mixed Reality User Interfaces: Specification, Authoring, Adaptation (MRUI’07)</i> (pp. 17–24). http://hdl.handle.net/20.500.12708/51920</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 1,
                    "role": "Author",
                    "tid": "47153"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-03"
            ],
            "pid": "55584",
            "handle": "20.500.12708/51922",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Face Recognition under Varying Illumination",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vucini, E., Gökmen, M., &#38; Gröller, E. (2007). Face Recognition under Varying Illumination. In V. Skala (Ed.), <i>Journal of WSCG 2007</i> (p. 8). http://hdl.handle.net/20.500.12708/51922</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Erald",
                    "last_name": "Vucini",
                    "position": 1,
                    "role": "Author",
                    "tid": "45707"
                },
                {
                    "first_name": "Muhittin",
                    "last_name": "Gökmen",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "position": 3,
                    "role": "Author",
                    "tid": "143572"
                },
                {
                    "first_name": "Vaclav",
                    "last_name": "Skala",
                    "position": 1,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "4043"
            ]
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "55595",
            "handle": "20.500.12708/51933",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Summary of Usability Evaluations of an Educational Augmented Reality Application",
            "keywords": [],
            "abstract": "We summarize three evaluations of an educational augmented reality application for geometry education, which have been conducted in 2000, 2003 and 2005 respectively. Repeated formative evaluations with more than 100 students guided the redesign of the application and its user interface throughout the years. We present and discuss the results regarding usability and simulator sickness providing guidelines on how to design augmented reality applications utilizing head-mounted displays.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., &#38; Dünser, A. (2007). Summary of Usability Evaluations of an Educational Augmented Reality Application. In R. Shumaker (Ed.), <i>Virtual Reality, HCI International Conference (HCII 2007), Volume 14, LNCS 4563</i> (pp. 660–669). Springer. http://hdl.handle.net/20.500.12708/51933</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Dünser",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Randall",
                    "last_name": "Shumaker",
                    "position": 1,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "55596",
            "handle": "20.500.12708/51934",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Assessment of Step Determination in a GPS/Compass/IMU System for Personal Positioning",
            "keywords": [],
            "abstract": "Nowadays, there have been great advances in the location technology, even though the user´s location indoor, outdoor is still a challenge. The personal positioning offers a very interesting field of research because the user walking has an unpredictable behaviour and it is difficult to assume predefined routes or to take into account other implemented location techniques for vehicles or robots. \r\n\r\nThe combination of GPS with sensors like accelerometers, gyroscopes or magnetometers is often used. The data fusion from these sensors is very important because we have to know the position and orientation constantly. \r\n\r\nIn this research we are interested in analyzing the system behaviour when the signal GPS is unavailable as when the signal is blocked or in indoor environments. The analysis will be carried out through the assessment of a Dead Reckoning algorithm to improve the position information. The system was tested both indoor and outdoor of the faculty building. The personal positioning system is made up of: a receiver GPS, an electronic compass, and an IMU. \r\n\r\nThe Dead Reckoning algorithm for pedestrians has two parameters: the travelled distance and the heading. The travelled distance is obtained by means of knowledge of the step length user. The pattern acceleration (forward and vertical) is analysed to determine when a user takes a step; once the step is detected the step length is calculated by a simply neuronal network. All that information is needed to obtain the relative position.\r\n\r\nThe implemented technique estimated was the Kalman filtering. According to it, the results of the position estimation can be improved if the filtering innovations are evaluated.\r\n\r\nWe present the bases of an evaluation mechanism to observe divergences and make the corrections to obtain better results.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pulido Herrera, E., Kaufmann, H., &#38; Quiros, R. (2007). Assessment of Step Determination in a GPS/Compass/IMU System for Personal Positioning. In <i>Proceedings of ION GNSS 2007</i> (pp. 1508–1515). The Institute of Navigation. http://hdl.handle.net/20.500.12708/51934</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Edith",
                    "last_name": "Pulido Herrera",
                    "position": 1,
                    "role": "Author",
                    "tid": "191422"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Ricardo",
                    "last_name": "Quiros",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55607",
            "handle": "20.500.12708/51945",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Multi-label image segmentation via max-sum solver",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Micusik, B., &#38; Pajdla, T. (2007). Multi-label image segmentation via max-sum solver. In <i>IEEE Conference on Computer Vision and Pattern Recognition, 2007</i>. IEEE Conference on Computer Vision and Pattern Recognition, 2007. CVPR ’07, Minneapolis, MN, USA, Non-EU. Omnipres. http://hdl.handle.net/20.500.12708/51945</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 1,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Tomas",
                    "last_name": "Pajdla",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55624",
            "handle": "20.500.12708/51962",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Computing Homology Group Generators of Images Using Irregular Graph Pyramids",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Peltier, S., Ion, A., Haxhimusa, Y., Kropatsch, W., &#38; Guillaume, D. (2007). Computing Homology Group Generators of Images Using Irregular Graph Pyramids. In <i>Proceedings of the 15th International Workshop on Graph-based Representation for Pattern Recognition</i> (pp. 283–294). Springer Lecture Notes in Computer Science. http://hdl.handle.net/20.500.12708/51962</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Samuel",
                    "last_name": "Peltier",
                    "position": 1,
                    "role": "Author",
                    "tid": "229413"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Damiand",
                    "last_name": "Guillaume",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55625",
            "handle": "20.500.12708/51963",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Approximating TSP Solution by MST based Graph Pyramid",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y., Kropatsch, W., Pizlo, Z., &#38; Ion, A. (2007). Approximating TSP Solution by MST based Graph Pyramid. In <i>Proceedings of the 15th International Workshop on Graph-based Representation for Pattern Recognition</i> (pp. 295–306). Springer Lecture Notes in Computer Science. http://hdl.handle.net/20.500.12708/51963</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Zygmunt",
                    "last_name": "Pizlo",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55626",
            "handle": "20.500.12708/51964",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Shape Matching Using the Geodesic Eccentricity Transform - A Study",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Peyre, G., Haxhimusa, Y., Peltier, S., Kropatsch, W., &#38; Cohen, L. (2007). Shape Matching Using the Geodesic Eccentricity Transform - A Study. In <i>The 31st annual workshop of the Austrian Association for Pattern Recognition (OAGM/AAPR)</i> (pp. 97–104). OCG. http://hdl.handle.net/20.500.12708/51964</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Gabriel",
                    "last_name": "Peyre",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Samuel",
                    "last_name": "Peltier",
                    "position": 4,
                    "role": "Author",
                    "tid": "229413"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 5,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Laurent",
                    "last_name": "Cohen",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55627",
            "handle": "20.500.12708/51965",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Controlling Geometry of Homology Generators",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Peltier, S., Ion, A., Haxhimusa, Y., &#38; Kropatsch, W. (2007). Controlling Geometry of Homology Generators. In <i>Proceedings of the 12th Computer Vision Winter Workshop</i>. Computer Vision Winter Workshop, St. Lambrecht, Austria. Verlag der Technischen Universität Graz. http://hdl.handle.net/20.500.12708/51965</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Samuel",
                    "last_name": "Peltier",
                    "position": 1,
                    "role": "Author",
                    "tid": "229413"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55631",
            "handle": "20.500.12708/51969",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Registration of Multispectral Manuscript Images as Prerequisite for Computer Aided Script Description",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Lettner, M., Diem, M., Sablatnig, R., Kammerer, P., &#38; Miklas, H. (2007). Registration of Multispectral Manuscript Images as Prerequisite for Computer Aided Script Description. In <i>Proceedings of the 12th Computer Vision Winter Workshop</i> (pp. 51–58). Verlag der Technischen Universität Graz. http://hdl.handle.net/20.500.12708/51969</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Lettner",
                    "position": 1,
                    "role": "Author",
                    "tid": "48706"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Diem",
                    "position": 2,
                    "role": "Author",
                    "tid": "48100"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 3,
                    "role": "Author",
                    "tid": "133566"
                },
                {
                    "first_name": "Paul",
                    "last_name": "Kammerer",
                    "position": 4,
                    "role": "Author",
                    "tid": "49696"
                },
                {
                    "first_name": "Heinz",
                    "last_name": "Miklas",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55636",
            "handle": "20.500.12708/51974",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Optical recognition of medieval coins",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kampel, M., &#38; Zaharieva, M. (2007). Optical recognition of medieval coins. In <i>CAA07: Layers of perception</i> (p. 7). http://hdl.handle.net/20.500.12708/51974</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 1,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 2,
                    "role": "Author",
                    "tid": "39017"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55637",
            "handle": "20.500.12708/51975",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Image based recognition of ancient coins",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaharieva, M., Kampel, M., &#38; Zambanini, S. (2007). Image based recognition of ancient coins. In W. Kropatsch, M. Kampel, &#38; A. Hanbury (Eds.), <i>Computer Analysis of Images and Patterns</i> (pp. 547–554). Springer Lecture Notes in Computer Science. http://hdl.handle.net/20.500.12708/51975</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 2,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Sebastian",
                    "last_name": "Zambanini",
                    "position": 3,
                    "role": "Author",
                    "tid": "53412"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 2,
                    "role": "Editor",
                    "tid": "49535"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Editor",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55638",
            "handle": "20.500.12708/51976",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "On ancient coin classification",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaharieva, M., Huber, R., Nölle, M., &#38; Kampel, M. (2007). On ancient coin classification. In <i>8th International Symposium on Virtual Reality, Archaeology and Cultural Heritage (VAST’07)</i> (pp. 55–62). http://hdl.handle.net/20.500.12708/51976</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Reinhold",
                    "last_name": "Huber",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Nölle",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 4,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55643",
            "handle": "20.500.12708/51981",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Do Colour Interest Points Improve Image Retrieval?",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stöttinger, J., Hanbury, A., Sebe, N., &#38; Gevers, T. (2007). Do Colour Interest Points Improve Image Retrieval? In <i>Proc. International Conference on Image Processing (ICIP)</i> (pp. 169–172). http://hdl.handle.net/20.500.12708/51981</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Nicu",
                    "last_name": "Sebe",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Theo",
                    "last_name": "Gevers",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55644",
            "handle": "20.500.12708/51982",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Colour Adjacency Histograms for Image Matching",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A., &#38; Marcotegui, B. (2007). Colour Adjacency Histograms for Image Matching. In <i>Proceedings of the Computer Analysis of Images and Patterns Conference (CAIP)</i>. 12th International Conference CAIP 2007, Wien, Austria. Springer Lecture Notes in Computer Science. http://hdl.handle.net/20.500.12708/51982</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Beatriz",
                    "last_name": "Marcotegui",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55645",
            "handle": "20.500.12708/51983",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Morphological Distinguished Regions",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A. (2007). Morphological Distinguished Regions. In <i>Proc. 12th Iberoamerican Congress on Pattern Recognition (CIARP)</i>. 12th Iboamerican Congress on Pattern Recognition, CIAPR2007, Vina del Mar-Valparaiso, Chile, Non-EU. Springer Lecture Notes in Computer Science. http://hdl.handle.net/20.500.12708/51983</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55646",
            "handle": "20.500.12708/51984",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Overview of the ImageCLEF 2006 Photographic Retrieval and Object Annotation Tasks",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Clough, P., Grubinger, M., Deselaers, T., Hanbury, A., &#38; Müller, H. (2006). Overview of the ImageCLEF 2006 Photographic Retrieval and Object Annotation Tasks. In <i>Proceedings of the CLEF 2006 Workshop</i> (pp. 579–594). Springer Lecture Notes in Computer Science. http://hdl.handle.net/20.500.12708/51984</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Paul",
                    "last_name": "Clough",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Grubinger",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Deselaers",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 4,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Henning",
                    "last_name": "Müller",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55647",
            "handle": "20.500.12708/51985",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Study of Vocabularies for Image Annotation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A. (2007). A Study of Vocabularies for Image Annotation. In <i>Proceedings of the second international conference on Semantics And digital Media Technologies (SAMT)</i> (pp. 284–287). Springer Lecture Notes in Computer Science. http://hdl.handle.net/20.500.12708/51985</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55652",
            "handle": "20.500.12708/51990",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Computing the Eccentricity Transform of a Polygonal Shape",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Ion, A., &#38; Peltier, S. (2007). Computing the Eccentricity Transform of a Polygonal Shape. In <i>12th Iberoamerican Congress on Pattern Recognition (CIARP 2007)</i> (pp. 291–300). Springer Lecture Notes in Computer Science. http://hdl.handle.net/20.500.12708/51990</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Samuel",
                    "last_name": "Peltier",
                    "position": 3,
                    "role": "Author",
                    "tid": "229413"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "55653",
            "handle": "20.500.12708/51991",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Decomposition for Efficient Eccentricity Transform of Convex Shapes",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Peltier, S., Haxhimusa, Y., &#38; Kropatsch, W. (2007). Decomposition for Efficient Eccentricity Transform of Convex Shapes. In <i>Computer Analysis of Images and Patterns</i> (pp. 653–661). Springer Lecture Notes in Computer Science. http://hdl.handle.net/20.500.12708/51991</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Samuel",
                    "last_name": "Peltier",
                    "position": 2,
                    "role": "Author",
                    "tid": "229413"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55678",
            "handle": "20.500.12708/52016",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Evaluation of 3D Shapes of Ceramics for Determination of Manufacturing Techniques",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mara, H., &#38; Sablatnig, R. (2007). Evaluation of 3D Shapes of Ceramics for Determination of Manufacturing Techniques. In <i>CAA07: Layers of Perception</i> (pp. 91–97). http://hdl.handle.net/20.500.12708/52016</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 1,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55696",
            "handle": "20.500.12708/52034",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Sparse MRF Appearance Models for Fast Anatomical Structure Localisation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Donner, R., Langs, G., Micusik, B., &#38; Bischof, H. (2007). Sparse MRF Appearance Models for Fast Anatomical Structure Localisation. In <i>Britisch Machine Vision Conference</i>. British Machine Vision Conference BMVC 2007, Warwick, EU. http://hdl.handle.net/20.500.12708/52034</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 1,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 2,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 3,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 4,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55697",
            "handle": "20.500.12708/52035",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Robust Autonomous Model Learning from 2D and 3D Data Sets",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langs, G., Donner, R., Peloschek, P. L., &#38; Bischof, H. (2007). Robust Autonomous Model Learning from 2D and 3D Data Sets. In <i>Medical Image Computing and Computer-Assisted Intervention - MICCAI 2007</i>. 10th International Conference MICCAI 2007, Brisbane, AQustralia, Non-EU. http://hdl.handle.net/20.500.12708/52035</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 1,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 2,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 4,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55698",
            "handle": "20.500.12708/52036",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Object Localization based on Markov Random Fields and Symmetry Interest Points",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Donner, R., Micusik, B., Langs, G., Szumilas, L., Peloschek, P. L., Friedrich, K., &#38; Bischof, H. (2007). Object Localization based on Markov Random Fields and Symmetry Interest Points. In <i>Medical Image Computing and Computer-Assisted Intervention - MICCAI 2007</i>. 10th International Conference MICCAI 2007, Brisbane, AQustralia, Non-EU. http://hdl.handle.net/20.500.12708/52036</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 1,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 2,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 3,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Lech",
                    "last_name": "Szumilas",
                    "position": 4,
                    "role": "Author",
                    "tid": "38816"
                },
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Klaus",
                    "last_name": "Friedrich",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 7,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55699",
            "handle": "20.500.12708/52037",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Motion Analysis of Endovascular Stent-Grafts by MDL Based Registration",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langs, G., Paragios, N., Donner, R., Desgranges, P., Rahmouni, A., &#38; Kobeiter, H. (2007). Motion Analysis of Endovascular Stent-Grafts by MDL Based Registration. In <i>Mathematical Methods in Biomedical Image Analysis</i> (p. 8). IEEE Computer Society. http://hdl.handle.net/20.500.12708/52037</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 1,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Nikos",
                    "last_name": "Paragios",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 3,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Pascal",
                    "last_name": "Desgranges",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Alain",
                    "last_name": "Rahmouni",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Hicham",
                    "last_name": "Kobeiter",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55705",
            "handle": "20.500.12708/52043",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Local Structure Detection with Orientation-invariant Radial Configuration",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Szumilas, L., Donner, R., Langs, G., &#38; Hanbury, A. (2007). Local Structure Detection with Orientation-invariant Radial Configuration. In <i>Computer Vision and Pattern Recognition</i> (p. 7). http://hdl.handle.net/20.500.12708/52043</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Lech",
                    "last_name": "Szumilas",
                    "position": 1,
                    "role": "Author",
                    "tid": "38816"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 2,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 3,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 4,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55707",
            "handle": "20.500.12708/52045",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Radial Edge Configuration for Semi-local Image Structure Description",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Szumilas, L., Wildenauer, H., Hanbury, A., &#38; Donner, R. (2007). Radial Edge Configuration for Semi-local Image Structure Description. In <i>Advances in Visual Computing</i> (pp. 633–643). Springer Lecture Notes in Computer Science. http://hdl.handle.net/20.500.12708/52045</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Lech",
                    "last_name": "Szumilas",
                    "position": 1,
                    "role": "Author",
                    "tid": "38816"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Wildenauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "51087"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 4,
                    "role": "Author",
                    "tid": "49701"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "55779",
            "handle": "20.500.12708/52117",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Affordable Infrared-Optical Pose Tracking for Virtual and Augmented Reality",
            "keywords": [],
            "abstract": "In this paper, we describe the hard- and software of a new low-cost infrared-optical pose-tracking system for room-sized virtual environments. The system consists of 4-8 shutter-synchronized 1394- cameras with an optical bandpass filter and infrared illuminator. All image-processing is done in software on an attached workstation. Preliminary results indicate low latency (20-40ms), minimal jitter (RMS less than 0.05mm/ 0.02°), submillimeter location resolution and an absolute accuracy of ±0.5cm. Currently, ten independent 6-DOF targets can be tracked in real-time with up to 60Hz.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pintaric, T., &#38; Kaufmann, H. (2007). Affordable Infrared-Optical Pose Tracking for Virtual and Augmented Reality. In G. Zachmann (Ed.), <i>IEEE VR Workshop on Trends and Issues in Tracking for Virtual Environments</i> (pp. 44–51). Shaker Verlag. http://hdl.handle.net/20.500.12708/52117</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Pintaric",
                    "position": 1,
                    "role": "Author",
                    "tid": "39909"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Gabriel",
                    "last_name": "Zachmann",
                    "position": 1,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "4151"
            ]
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "55821",
            "handle": "20.500.12708/52158",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "3D Acquisition of Archaeological Fragments and Web based 3D Data Storage",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kampel, M., Sablatnig, R., Mara, H., &#38; Lettner, M. (2006). 3D Acquisition of Archaeological Fragments and Web based 3D Data Storage. In <i>CAA06: Digital Discovery: Exploring New Frontiers in Human Heritage</i> (pp. 549–553). http://hdl.handle.net/20.500.12708/52158</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 1,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 2,
                    "role": "Author",
                    "tid": "133566"
                },
                {
                    "first_name": "Hubert",
                    "last_name": "Mara",
                    "position": 3,
                    "role": "Author",
                    "tid": "51366"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Lettner",
                    "position": 4,
                    "role": "Author",
                    "tid": "48706"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "55841",
            "handle": "20.500.12708/52178",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Analysis of Historical Artistic Documentaries",
            "keywords": [],
            "abstract": "The paper introduces a novel interdisciplinary project addressing the analysis of historical artistic films. The type of employed material has not been subject to automatic analyses, so far. It poses challenges in all areas of content-based analysis and retrieval due to its complex temporal structure and due to substantial degradations. We propose robust features and a method for shot cut detection for this material that outperforms established techniques.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zeppelzauer, M., Mitrovic, D., &#38; Breiteneder, C. (2008). Analysis of Historical Artistic Documentaries. In <i>2008 Ninth International Workshop on Image Analysis for Multimedia Interactive Services</i>. Ninth International Workshop on Image Analysis for Multimedia Interactive Services, Klagenfurt, Austria. https://doi.org/10.1109/wiamis.2008.11</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 2,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 3,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56020",
            "handle": "20.500.12708/52357",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Mapping A Coordinate System To A Non-Rigid Shape",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., &#38; Kropatsch, W. (2008). Mapping A Coordinate System To A Non-Rigid Shape. In <i>Challenge in the Biosciences: Image Analysis and Pattern Recognition Aspects</i> (pp. 169–178). OCG. http://hdl.handle.net/20.500.12708/52357</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56021",
            "handle": "20.500.12708/52358",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "3D Shape Matching by Geodesic Eccentricity",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Artner, N., Peyre, G., Lopez, S., Kropatsch, W., &#38; Cohen, L. (2008). 3D Shape Matching by Geodesic Eccentricity. In <i>Workshop on Search in 3D (in conjunction with CVPR 2008)</i>. Workshop on Search in 3D (in conjunction with CVPR 2008), Anchorge, Alaska, Non-EU. IEEE. http://hdl.handle.net/20.500.12708/52358</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Gabriel",
                    "last_name": "Peyre",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Salvador",
                    "last_name": "Lopez",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 5,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Laurent",
                    "last_name": "Cohen",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56110",
            "handle": "20.500.12708/52447",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Delineating Homology Generators in Graph Pyramids",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Iglesias-Ham, M., Ion, A., Kropatsch, W., &#38; Garcia, E. (2008). Delineating Homology Generators in Graph Pyramids. In <i>13th Iberoamerican Congress on Pattern Recognition (CIARP 2008)</i> (pp. 576–584). Springer. http://hdl.handle.net/20.500.12708/52447</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mabel",
                    "last_name": "Iglesias-Ham",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Edel",
                    "last_name": "Garcia",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56111",
            "handle": "20.500.12708/52448",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Coordinate System for Articulated 2D Shape Point Correspondences",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Haxhimusa, Y., Kropatsch, W., &#38; Lopez, S. (2008). A Coordinate System for Articulated 2D Shape Point Correspondences. In <i>The 19th International Conference on Pattern recognition</i>. The 19th International Conference on Image Processing (ICPR08), Tampa, Florida, Non-EU. IEEE Computer Society. http://hdl.handle.net/20.500.12708/52448</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Salvador",
                    "last_name": "Lopez",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56112",
            "handle": "20.500.12708/52449",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Euclidean Eccentricity Transform by Discrete Arc Paving",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Kropatsch, W., &#38; Andres, E. (2008). Euclidean Eccentricity Transform by Discrete Arc Paving. In <i>Proceedings of the 14th IAPR International Conference on Discrete Geometry for Computer Imagery (DGCI)</i> (pp. 213–224). Springer. http://hdl.handle.net/20.500.12708/52449</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Eric",
                    "last_name": "Andres",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56113",
            "handle": "20.500.12708/52450",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Video Object Segmentation Using Graphs",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Lopez, S., Artner, N., Ion, A., Kropatsch, W., &#38; Beleznai, C. (2008). Video Object Segmentation Using Graphs. In <i>13th Iberoamerican Congress on Pattern Recognition (CIARP 2008)</i> (pp. 733–740). Springer. http://hdl.handle.net/20.500.12708/52450</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Salvador",
                    "last_name": "Lopez",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Csaba",
                    "last_name": "Beleznai",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56133",
            "handle": "20.500.12708/52470",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Overview of the ImageCLEF 2007 Object Retrieval Task",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Deselaers, T., Hanbury, A., Viitaniemi, V., Benczur, A., Brendel, M., Daroczy, B., Escalante Balderas, H. J., Gevers, T., Hernandez Gracidas, C. A., Laaksonen, J., Li, M., Marın Castro, H. M., Ney, H., Rui, X., Sebe, N., &#38; Stöttinger, J. (2008). Overview of the ImageCLEF 2007 Object Retrieval Task. In <i>Advances in Multilingual and Multimodal Information Retrieval</i> (pp. 445–471). Springer. http://hdl.handle.net/20.500.12708/52470</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Deselaers",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Ville",
                    "last_name": "Viitaniemi",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Andras",
                    "last_name": "Benczur",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Matyas",
                    "last_name": "Brendel",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Balint",
                    "last_name": "Daroczy",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Hugo Jair",
                    "last_name": "Escalante Balderas",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Theo",
                    "last_name": "Gevers",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Carlos Arturo",
                    "last_name": "Hernandez Gracidas",
                    "position": 9,
                    "role": "Author"
                },
                {
                    "first_name": "Jorma",
                    "last_name": "Laaksonen",
                    "position": 10,
                    "role": "Author"
                },
                {
                    "first_name": "Mingjin",
                    "last_name": "Li",
                    "position": 11,
                    "role": "Author"
                },
                {
                    "first_name": "Heidy Marisol",
                    "last_name": "Marın Castro",
                    "position": 12,
                    "role": "Author"
                },
                {
                    "first_name": "Hermann",
                    "last_name": "Ney",
                    "position": 13,
                    "role": "Author"
                },
                {
                    "first_name": "Xiaoguang",
                    "last_name": "Rui",
                    "position": 14,
                    "role": "Author"
                },
                {
                    "first_name": "Nicu",
                    "last_name": "Sebe",
                    "position": 15,
                    "role": "Author"
                },
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 16,
                    "role": "Author",
                    "tid": "47904"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56134",
            "handle": "20.500.12708/52471",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Overview of the ImageCLEFphoto 2007 Photographic Retrieval Task",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Grubinger, M., Clough, P., Hanbury, A., &#38; Müller, H. (2008). Overview of the ImageCLEFphoto 2007 Photographic Retrieval Task. In <i>Advances in Multilingual and Multimodal Information Retrieval</i> (pp. 433–444). Springer. http://hdl.handle.net/20.500.12708/52471</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Grubinger",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Paul",
                    "last_name": "Clough",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 3,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Henning",
                    "last_name": "Müller",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56135",
            "handle": "20.500.12708/52472",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "How Do Superpixels Affect Image Segmentation?",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A. (2008). How Do Superpixels Affect Image Segmentation? In <i>Progress in Pattern Recognition, Image Anlysis and Applications</i> (pp. 178–186). Springer. http://hdl.handle.net/20.500.12708/52472</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "56143",
            "handle": "20.500.12708/52480",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Playmancer: A European Serious Gaming 3D Environment",
            "keywords": [],
            "abstract": "Serious games are about to enter the medical sector to give people\r\nwith behavioural or addictive disorders the ability to use them as part of health\r\npromotion and disease prevention. The PlayMancer framework will support\r\nphysical rehabilitations and psycho-education programs thru a modular multiplayer networked 3D game based on the Universally Accessible Games (UA games) guidelines.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kalapanidas, E., Wanatabe, H., Davarakis, C., Kaufmann, H., Fernandez Aranda, F., Lam, T., Ganchev, T., &#38; Konstantas, D. (2008). Playmancer: A European Serious Gaming 3D Environment. In <i>Proceedings of the 2nd International Workshop on e-health Services and Terchnologies - EHST 2008</i> (pp. 51–59). 3rd International Conference on Software and Data Technologies. http://hdl.handle.net/20.500.12708/52480</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Elias",
                    "last_name": "Kalapanidas",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hikari",
                    "last_name": "Wanatabe",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Costas",
                    "last_name": "Davarakis",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Fernando",
                    "last_name": "Fernandez Aranda",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Tony",
                    "last_name": "Lam",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Todor",
                    "last_name": "Ganchev",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Dimitri",
                    "last_name": "Konstantas",
                    "position": 8,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "56150",
            "handle": "20.500.12708/52487",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Simulating Educational Physical Experiments in Augmented Reality",
            "keywords": [],
            "abstract": "We present an augmented reality application for mechanics education.\r\nIt utilizes a recent physics engine developed for the PC\r\ngaming market to simulate physical experiments in the domain of\r\nmechanics in real time. Students are enabled to actively build own\r\nexperiments and study them in a three-dimensional virtual world.\r\nA variety of tools are provided to analyze forces, mass, paths and\r\nother properties of objects before, during and after experiments. Innovative\r\nteaching content is presented that exploits the strengths of\r\nour immersive virtual environment. PhysicsPlayground serves as\r\nan example of how current technologies can be combined to deliver\r\na new quality in physics education.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., &#38; Meyer, B. (2008). Simulating Educational Physical Experiments in Augmented Reality. In <i>Proceedings of ACM SIGGRAPH ASIA 2008 Educators Program</i> (p. 8). ACM Press. http://hdl.handle.net/20.500.12708/52487</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Bernd",
                    "last_name": "Meyer",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "56151",
            "handle": "20.500.12708/52488",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Design of a Virtual Reality Supported Test for Spatial Abilities",
            "keywords": [],
            "abstract": "This paper focuses on the development of a new spatial ability test in virtual reality\r\n(VR). This test measures the ability to visualize and mentally manipulate three-dimensional objects\r\ndirectly in 3D space, and should thus have a higher ecological validity than previous spatial ability\r\ntests. Items are viewed through head mounted displays and manipulated by means of a wireless pen\r\ninput device. As a dynamic tests consisting of a pretest, a training phase, and a posttest it does not\r\nonly measure a person´s current status but also his or her learning potential. Monitoring user\r\ninteractions in a VR environment allows to measure test performance in ways not possible with\r\ntraditional means. We describe design and development of the test and will present results of a\r\npre-study with 240 participants conducted in early 2008.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., Csisinko, M., Strasser, I., Strauß, S., &#38; Koller, I. (2008). Design of a Virtual Reality Supported Test for Spatial Abilities. In <i>Proceedings of the 13th International Conference on Geometry and Graphics (ICGG)</i> (pp. 122–123). International Society for Geometry and Graphics. http://hdl.handle.net/20.500.12708/52488</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 2,
                    "role": "Author",
                    "tid": "47153"
                },
                {
                    "first_name": "Irene",
                    "last_name": "Strasser",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Sabine",
                    "last_name": "Strauß",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Ingrid",
                    "last_name": "Koller",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "56152",
            "handle": "20.500.12708/52489",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Construct3D: An Augmented Reality Application for Geometry Education",
            "keywords": [],
            "abstract": "Construct3D is a three dimensional geometric construction tool specifically designed for mathematics and geometry education. We describe our efforts in developing a system for the improvement of spatial abilities and maximization of transfer of learning. In order to support various teacher-student interaction scenarios we implemented flexible methods for context and user dependent rendering of parts of the construction. Together with hybrid hardware setups they allow the use of Construct3D in today´s classrooms and provide a testbed for future evaluations. Means of application and integration in mathematics and geometry education at high school as well as university level are being discussed. Anecdotal evidence supports our claim that Construct3D is easy to learn, encourages experimentation with geometric constructions and improves spatial skills.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2008). Construct3D: An Augmented Reality Application for Geometry Education. In <i>Proceedings of the Tenth ACM International Conference on Multimedia</i> (pp. 656–657). Association for Computing Machinery. https://doi.org/10.1145/641007.641140</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "56153",
            "handle": "20.500.12708/52490",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Physics Playground: Physics Education in Virtual Reality",
            "keywords": [],
            "abstract": "This talk presents an immersive VR application for physics education. It utilizes a recent physics engine developed for the PC gaming market to simulate physical experiments correctly and accurately. Students are enabled to actively build own experiments and study them. A variety of tools are provided to analyze forces, mass, paths and other properties of objects before, during and after experiments. Innovative teaching content is presented that exploits the strengths of the 3D virtual environment. PhysicsPlayground serves as an example of how current technologies can be combined to deliver a new quality in physics education.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2008). Physics Playground: Physics Education in Virtual Reality. In <i>Virtual Reality Systems for Psychology</i>. Joint Seminar: Virtual Reality Systems for Psychology, Vienna, Austria, Austria. http://hdl.handle.net/20.500.12708/52490</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E191-03",
                "E193-03"
            ],
            "pid": "56167",
            "handle": "20.500.12708/52504",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Secure and customizable software applications in embedded networks",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Praus, F., Flanitzer, T., &#38; Kastner, W. (2008). Secure and customizable software applications in embedded networks. In <i>Proc. IEEE International Conference on Emerging Technologies and Factory Automation (ETFA 2008)</i> (pp. 1473–1480). http://hdl.handle.net/20.500.12708/52504</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Fritz",
                    "last_name": "Praus",
                    "position": 1,
                    "role": "Author",
                    "tid": "48165"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Flanitzer",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Wolfgang",
                    "last_name": "Kastner",
                    "position": 3,
                    "role": "Author",
                    "tid": "119612"
                }
            ],
            "foci": [
                "Computer Engineering"
            ],
            "projects": [
                "5829"
            ]
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "56178",
            "handle": "20.500.12708/52515",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "PlayMancer - A European Serious Gaming 3D Environment",
            "keywords": [],
            "abstract": "&quot;PlayMancer&quot; is a EC-funded research project aiming to implement a platform for universally accessible and serious games. Within this project, Vienna University of Technology has partnered with the Neurological Rehabilitation Center at Rosenhügel to develop Virtual Reality Games to assist in the physical rehabilitation of stroke patients. This presentation will give an overview of the current research efforts within PlayMancer. Preliminary results and game prototypes will be presented.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C. (2008). PlayMancer - A European Serious Gaming 3D Environment. In <i>Virtual Reality Systems for Psychology</i>. Joint Seminar: Virtual Reality Systems for Psychology, Vienna, Austria, Austria. http://hdl.handle.net/20.500.12708/52515</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56251",
            "handle": "20.500.12708/52588",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Numismatic Object Identification Using Fusion of Shape and Local Descriptors",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Huber-Mörk, R., Zaharieva, M., &#38; Czedik-Eysenberg, H. (2008). Numismatic Object Identification Using Fusion of Shape and Local Descriptors. In <i>Advances in Visual Computing</i> (pp. 368–379). Springer, LNCS. http://hdl.handle.net/20.500.12708/52588</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Reinhold",
                    "last_name": "Huber-Mörk",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 2,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "H.",
                    "last_name": "Czedik-Eysenberg",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56252",
            "handle": "20.500.12708/52589",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Recognizing Ancient Coins Based on Local Features",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kampel, M., &#38; Zaharieva, M. (2008). Recognizing Ancient Coins Based on Local Features. In <i>Advances in Visual Computing, Part I</i> (pp. 11–22). Springer, LNCS. http://hdl.handle.net/20.500.12708/52589</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 1,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 2,
                    "role": "Author",
                    "tid": "39017"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56253",
            "handle": "20.500.12708/52590",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Image-Based Classification Of Ancient Coins",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kampel, M., Vondrovec, K., Zaharieva, M., &#38; Zambanini, S. (2008). Image-Based Classification Of Ancient Coins. In <i>Proceedings of the 14th International Conference on Virtual Systems and Multimedia</i>. VSMM09, Limassol, Cyprus, EU. http://hdl.handle.net/20.500.12708/52590</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 1,
                    "role": "Author",
                    "tid": "49535"
                },
                {
                    "first_name": "Klaus",
                    "last_name": "Vondrovec",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 3,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Sebastian",
                    "last_name": "Zambanini",
                    "position": 4,
                    "role": "Author",
                    "tid": "53412"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56276",
            "handle": "20.500.12708/52613",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Improving Tracking Using Structure",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Lopez, S., Artner, N., Iglesias-Ham, M., Kropatsch, W., Clabian, M., &#38; Burger, W. (2008). Improving Tracking Using Structure. In <i>Computer Vision Workshop 2008</i> (pp. 69–76). http://hdl.handle.net/20.500.12708/52613</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Salvador",
                    "last_name": "Lopez",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Mabel",
                    "last_name": "Iglesias-Ham",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Clabian",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Willhelm",
                    "last_name": "Burger",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56277",
            "handle": "20.500.12708/52614",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Kernel-Based Tracking Using Spatial Structure",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., Lopez, S., Beleznai, C., &#38; Kropatsch, W. (2008). Kernel-Based Tracking Using Spatial Structure. In <i>Challenge in the Biosciences: Image Analysis and Pattern Recognition Aspects</i> (pp. 103–114). OCG. http://hdl.handle.net/20.500.12708/52614</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Salvador",
                    "last_name": "Lopez",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Csaba",
                    "last_name": "Beleznai",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56278",
            "handle": "20.500.12708/52615",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Tracking by Hierarchical Representation of Target Structure",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., Lopez, S., Beleznai, C., &#38; Kropatsch, W. (2008). Tracking by Hierarchical Representation of Target Structure. In <i>Structural, Syntactic, and Statistical Pattern Recognition</i> (pp. 441–450). Springer. http://hdl.handle.net/20.500.12708/52615</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Salvador",
                    "last_name": "Lopez",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Csaba",
                    "last_name": "Beleznai",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56291",
            "handle": "20.500.12708/52626",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluation of Gradient Vector Flow for Interest Point Detection",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stöttinger, J., Donner, R., Szumilas, L., &#38; Hanbury, A. (2008). Evaluation of Gradient Vector Flow for Interest Point Detection. In <i>Advances in Visual Computing, Part I</i> (pp. 338–348). Springer, LNCS. http://hdl.handle.net/20.500.12708/52626</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 2,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Lech",
                    "last_name": "Szumilas",
                    "position": 3,
                    "role": "Author",
                    "tid": "38816"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 4,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "56292",
            "handle": "20.500.12708/52627",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "The Sorcerer’s Apprentice: A serious game aiding rehabilitation in the context of Subacromial Impingement Syndrome",
            "keywords": [],
            "abstract": "Serious games can help to improve efficacy of motor rehabilitation especially in a home environment. We introduce &quot;The Sorcerer's Apprentice&quot;, a serious game improving strength and mobility of the shoulder area targeting support of supervised physiotherapy. It proposes a customizable environment for supplementary exercises in the context of rehabilitation for a one-sided Shoulder-Impingement-Syndrome. We introduce the medical background of the shoulder impingement syndrome, how the game aims to improve the health status of the patients through several options of exercises and how these exercises are embedded into the flow of game play. We will further explain how motivational factors are implemented and which additional factors were relevant in the design process. As the game makes use of motion tracking for input, we utilized Microsoft Kinect as a low-cost IO device suitable for a home-environment use case.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fikar, P., Schönauer, C., &#38; Kaufmann, H. (2013). The Sorcerer’s Apprentice: A serious game aiding rehabilitation in the context of Subacromial Impingement Syndrome. In <i>Proceedings of the ICTs for improving Patients Rehabilitation Research Techniques</i>. Pervasive Computing Technologies for Healthcare (PervasiveHealth), 2013 7th International Conference on, Venedig, EU. IEEE. https://doi.org/10.4108/icst.pervasivehealth.2013.252224</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Fikar",
                    "position": 1,
                    "role": "Author",
                    "tid": "40663"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56294",
            "handle": "20.500.12708/52629",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "On Segmentation Evaluation Metrics and Region Counts",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A., &#38; Stöttinger, J. (2008). On Segmentation Evaluation Metrics and Region Counts. In <i>Proceedings of the the 19th International Conference on Image Processing (ICPR09)</i>. The 19th International Conference on Image Processing (ICPR08), Tampa, Florida, Non-EU. IEEE Computer Society. http://hdl.handle.net/20.500.12708/52629</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 2,
                    "role": "Author",
                    "tid": "47904"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56302",
            "handle": "20.500.12708/52637",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Estimation Of Fit Confidence In Active Shape Model Search For The Reliable Measurement Of Knee Alignment",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Widhalm, P., Langs, G., Donner, R., Fakhrai, N., Peloschek, P. L., &#38; Sablatnig, R. (2008). Estimation Of Fit Confidence In Active Shape Model Search For The Reliable Measurement Of Knee Alignment. In <i>Challenge in the Biosciences: Image Analysis and Pattern Recognition Aspects</i>. 32nd Workshop of the Austrian Association for Pattern Recognition, Linz, Austria. OCG. http://hdl.handle.net/20.500.12708/52637</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Widhalm",
                    "position": 1,
                    "role": "Author",
                    "tid": "53565"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 2,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 3,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Negar",
                    "last_name": "Fakhrai",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Philipp L.",
                    "last_name": "Peloschek",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 6,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03",
                "E376"
            ],
            "pid": "56303",
            "handle": "20.500.12708/52638",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Using Generalized Sparse MRF Appearance Models for Anatomical Structure Localization",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Donner, R., Langs, G., Micusik, B., &#38; Bischof, H. (2008). Using Generalized Sparse MRF Appearance Models for Anatomical Structure Localization. In <i>20th International Conference of Society for Medical Innovations and Technology</i>. 20th International Conference of Society for Medical Innovations and Technology, Vienna, Austria, Austria. http://hdl.handle.net/20.500.12708/52638</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 1,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 2,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 3,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 4,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06",
                "E194-01"
            ],
            "pid": "56432",
            "handle": "20.500.12708/52767",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "On Feature Selection in Environmental Sound Recognition",
            "keywords": [],
            "abstract": "Given a broad set of content-based audio features, we employ principal component analysis for the composition of an optimal feature set for environmental sounds. We select features based on quantitative data analysis (factor analysis) and conduct retrieval experiments to evaluate the quality of the feature combinations. Retrieval results show that statistical data analysis gives useful hints for feature selection. The experiments show the importance of feature selection in environmental sound recognition.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mitrovic, D., Zeppelzauer, M., &#38; Eidenberger, H. (2009). On Feature Selection in Environmental Sound Recognition. In <i>Proceedings ELMAR-2009</i> (pp. 201–204). http://hdl.handle.net/20.500.12708/52767</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 1,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 3,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56567",
            "handle": "20.500.12708/52901",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Tracking Using a Hierarchical Structural Representation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., &#38; Kropatsch, W. (2009). Tracking Using a Hierarchical Structural Representation. In <i>33rd Workshop of the Austrian Association of Pattern Recognition (AAPR/OAGM)</i> (pp. 249–260). OCG. http://hdl.handle.net/20.500.12708/52901</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "56570",
            "handle": "20.500.12708/52904",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Finding the Missing Piece: Content-Based Video Comparison",
            "keywords": [],
            "abstract": "The contribution of this paper consists of a framework for video comparison that allows for the analysis of different movie versions. Furthermore, a second contribution is \r\nan evaluation of state-of-the-art, local feature-based approaches for content-based video retrieval in a real world scenario. Eventually, the experimental results show the outstanding performance of a simple, edge-based descriptor within the presented framework.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaharieva, M., Zeppelzauer, M., Mitrovic, D., &#38; Breiteneder, C. (2009). Finding the Missing Piece: Content-Based Video Comparison. In <i>2009 11th IEEE International Symposium on Multimedia</i>. 11th IEEE International Symposium on Multimedia (ISM 2009), San Diego, USA, Non-EU. IEEE Computer Society. https://doi.org/10.1109/ism.2009.32</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 3,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56572",
            "handle": "20.500.12708/52906",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Acquisition of 3D Coin Models and Their Potential in Numismatic Research",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zambanini, S., Schlapke, M., Müller, A., &#38; Kampel, M. (2009). Acquisition of 3D Coin Models and Their Potential in Numismatic Research. In <i>Konferenzband EVA 2009 Berlin</i>. EVA Berlin 2009, Berlin, EU. http://hdl.handle.net/20.500.12708/52906</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Sebastian",
                    "last_name": "Zambanini",
                    "position": 1,
                    "role": "Author",
                    "tid": "53412"
                },
                {
                    "first_name": "M.",
                    "last_name": "Schlapke",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Müller",
                    "position": 3,
                    "role": "Author",
                    "tid": "52000"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 4,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56577",
            "handle": "20.500.12708/52911",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Automatic Region Template Generation for Shape Particle Filtering based Image Segmentation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fischer, L., Donner, R., Kainberger, F., &#38; Langs, G. (2009). Automatic Region Template Generation for Shape Particle Filtering based Image Segmentation. In <i>Proceedings</i> (pp. 289–300). http://hdl.handle.net/20.500.12708/52911</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Lukas",
                    "last_name": "Fischer",
                    "position": 1,
                    "role": "Author",
                    "tid": "46508"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 2,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "F",
                    "last_name": "Kainberger",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 4,
                    "role": "Author",
                    "tid": "69204"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56582",
            "handle": "20.500.12708/52916",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Rigid Part Decomposition in a Graph Pyramid",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., Ion, A., &#38; Kropatsch, W. (2009). Rigid Part Decomposition in a Graph Pyramid. In <i>14th Iberoamerican Congress on Pattern Recognition (CIARP 2009)</i> (pp. 758–765). http://hdl.handle.net/20.500.12708/52916</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56583",
            "handle": "20.500.12708/52917",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Object Classification by Topology of Convex Deficiencies",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Iglesias_Ham, M., Kropatsch, W., González-Díaz, R., &#38; Ion, A. (2009). Object Classification by Topology of Convex Deficiencies. In <i>Computational Topology in Image Context, CTIC 2009</i> (pp. 75–81). http://hdl.handle.net/20.500.12708/52917</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mabel",
                    "last_name": "Iglesias_Ham",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "R.",
                    "last_name": "González-Díaz",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56584",
            "handle": "20.500.12708/52918",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "An Improved Coordinate System for Point Correspondences of 2D Articulated Shapes",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Haxhimusa, Y., &#38; Kropatsch, W. (2009). An Improved Coordinate System for Point Correspondences of 2D Articulated Shapes. In <i>15th IAPR International Conference on Discrete Geometry for Computer Imagery (DGCI)</i> (pp. 92–103). http://hdl.handle.net/20.500.12708/52918</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56585",
            "handle": "20.500.12708/52919",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Coarse-to-Fine Tracking of Articulated Objects Using a Hierarchical Spring System",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., Ion, A., &#38; Kropatsch, W. (2009). Coarse-to-Fine Tracking of Articulated Objects Using a Hierarchical Spring System. In <i>The 13th International Conference on Computer Analysis of Images and Patterns (CAIP)</i> (pp. 1011–1018). http://hdl.handle.net/20.500.12708/52919</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56586",
            "handle": "20.500.12708/52920",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Irregular Graph Pyramids and Representative Cocycles of Cohomology Generators",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">González-Díaz, R., Ion, A., Iglesias-Ham, M., &#38; Kropatsch, W. (2009). Irregular Graph Pyramids and Representative Cocycles of Cohomology Generators. In <i>Proceedings of the 7th IAPR-TC15 International Workshop on Graph-based Representations in Pattern Recognition</i> (pp. 263–272). Springer. http://hdl.handle.net/20.500.12708/52920</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "R.",
                    "last_name": "González-Díaz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Mabel",
                    "last_name": "Iglesias-Ham",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56587",
            "handle": "20.500.12708/52921",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Tracking Objects Beyond Rigid Motion",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., Ion, A., &#38; Kropatsch, W. (2009). Tracking Objects Beyond Rigid Motion. In <i>Proceedings of the 7th IAPR-TC15 International Workshop on Graph-based Representations in Pattern Recognition</i> (pp. 82–91). Springer. http://hdl.handle.net/20.500.12708/52921</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56588",
            "handle": "20.500.12708/52922",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Tracking Articulated Objects Using Structure",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., Ion, A., &#38; Kropatsch, W. (2009). Tracking Articulated Objects Using Structure. In <i>Proceedings of the Computer Vision Winter Workshop 2009</i> (pp. 51–58). http://hdl.handle.net/20.500.12708/52922</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56589",
            "handle": "20.500.12708/52923",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Combining an Optical Flow Feature Detector with Graph-Based Segmentation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stubenschrott, M., Kropatsch, W., &#38; Haxhimusa, Y. (2009). Combining an Optical Flow Feature Detector with Graph-Based Segmentation. In <i>Proceedings of the Computer Vision Winter Workshop 2009</i> (pp. 91–98). http://hdl.handle.net/20.500.12708/52923</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Stubenschrott",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56591",
            "handle": "20.500.12708/52925",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Comparison of Adaptive Approaches for Dominant Points Detection",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Antunez, E., Bandera, A., &#38; Marfill, R. (2009). Comparison of Adaptive Approaches for Dominant Points Detection. In <i>Proceedings of the Computer Vision Winter Workshop 2009</i> (pp. 13–20). http://hdl.handle.net/20.500.12708/52925</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Esther",
                    "last_name": "Antunez",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Antonio",
                    "last_name": "Bandera",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Rebeca",
                    "last_name": "Marfill",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56592",
            "handle": "20.500.12708/52926",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "An Adaptive Approach for Affine-Invariant 2D Shape Description",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Bandera, A., Antunez, E., &#38; Marfill, R. (2009). An Adaptive Approach for Affine-Invariant 2D Shape Description. In <i>4th Iberian Conference Pattern Recognition and Image Analysis; IbPRIA 2009</i> (pp. 417–424). http://hdl.handle.net/20.500.12708/52926</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Antonio",
                    "last_name": "Bandera",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Esther",
                    "last_name": "Antunez",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Rebeca",
                    "last_name": "Marfill",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56595",
            "handle": "20.500.12708/52929",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Dual Graph Pyramid Approach to Grid-Based and Topological Maps Integration for Mobile Robotics",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Perez-Lorenzo, J., Vazquez-Martin, R., Antunez, E., &#38; Bandera, A. (2009). A Dual Graph Pyramid Approach to Grid-Based and Topological Maps Integration for Mobile Robotics. In <i>Bio-Inspired Systems: Computational and Ambient Intelligence</i> (pp. 781–788). Springer. http://hdl.handle.net/20.500.12708/52929</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "J.",
                    "last_name": "Perez-Lorenzo",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Riccardo",
                    "last_name": "Vazquez-Martin",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Esther",
                    "last_name": "Antunez",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Antonio",
                    "last_name": "Bandera",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56596",
            "handle": "20.500.12708/52930",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Adaptive Dominant Point Detector for Visual Landmark Description",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Antunez, E., Marfill, R., Bandera, A., Vazquez-Martin, R., &#38; Kropatsch, W. (2009). Adaptive Dominant Point Detector for Visual Landmark Description. In <i>Visual Learning</i> (pp. 189–200). OCG. http://hdl.handle.net/20.500.12708/52930</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Esther",
                    "last_name": "Antunez",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Rebeca",
                    "last_name": "Marfill",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Antonio",
                    "last_name": "Bandera",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Riccardo",
                    "last_name": "Vazquez-Martin",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 5,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56597",
            "handle": "20.500.12708/52931",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Homological Tree-Based Strategies for Image Analysis",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Real, P., Molina Abril, H., &#38; Kropatsch, W. (2009). Homological Tree-Based Strategies for Image Analysis. In <i>The 13th International Conference on Computer Analysis of Images and Patterns (CAIP)</i> (pp. 326–333). Springer. http://hdl.handle.net/20.500.12708/52931</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Pedro",
                    "last_name": "Real",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Helena",
                    "last_name": "Molina Abril",
                    "position": 2,
                    "role": "Author",
                    "tid": "55000"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56598",
            "handle": "20.500.12708/52932",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Homological Computation Using Spanning Trees",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Molina Abril, H., &#38; Real, P. (2009). Homological Computation Using Spanning Trees. In <i>Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</i> (pp. 272–278). Springer. http://hdl.handle.net/20.500.12708/52932</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Helena",
                    "last_name": "Molina Abril",
                    "position": 1,
                    "role": "Author",
                    "tid": "55000"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Real",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56599",
            "handle": "20.500.12708/52933",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Computing Homology Generators relations in 3D Digital Images",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Berciano, A., Molina Abril, H., Jimeney, M., &#38; Real, P. (2009). Computing Homology Generators relations in 3D Digital Images. In <i>Computational Topology in Image Context, CTIC 2009</i> (pp. 11–17). http://hdl.handle.net/20.500.12708/52933</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ainhoa",
                    "last_name": "Berciano",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Helena",
                    "last_name": "Molina Abril",
                    "position": 2,
                    "role": "Author",
                    "tid": "55000"
                },
                {
                    "first_name": "Maria",
                    "last_name": "Jimeney",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Real",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56600",
            "handle": "20.500.12708/52934",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Open Issues and Chances for Topological Pyramids",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., &#38; Molina Abril, H. (2009). Open Issues and Chances for Topological Pyramids. In <i>cvww</i> (pp. 121–125). http://hdl.handle.net/20.500.12708/52934</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Helena",
                    "last_name": "Molina Abril",
                    "position": 2,
                    "role": "Author",
                    "tid": "55000"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56601",
            "handle": "20.500.12708/52935",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Decomposing Cavities in Digital Volumes into Products of Cycles",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Berciano, A., Molina Abril, H., Pacheco, A., Pilarczyk, P., &#38; Real, P. (2009). Decomposing Cavities in Digital Volumes into Products of Cycles. In <i>15th IAPR International Conference on Discrete Geometry for Computer Imagery (DGCI)</i> (pp. 263–274). Springer. http://hdl.handle.net/20.500.12708/52935</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ainhoa",
                    "last_name": "Berciano",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Helena",
                    "last_name": "Molina Abril",
                    "position": 2,
                    "role": "Author",
                    "tid": "55000"
                },
                {
                    "first_name": "Ana",
                    "last_name": "Pacheco",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Pawel",
                    "last_name": "Pilarczyk",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Real",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56602",
            "handle": "20.500.12708/52936",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Cell AT-Models for Digital Volumes",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Real, P., &#38; Molina Abril, H. (2009). Cell AT-Models for Digital Volumes. In <i>7th IAPR-TC-15 International Workshop on Graph-Based Representations in Pattern Recognition GBR 2009</i> (pp. 314–323). Springer. http://hdl.handle.net/20.500.12708/52936</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Pedro",
                    "last_name": "Real",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Helena",
                    "last_name": "Molina Abril",
                    "position": 2,
                    "role": "Author",
                    "tid": "55000"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56609",
            "handle": "20.500.12708/52943",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Lonely but Attractive: Sparse Color Salient Points for Object Retrieval and Categorization",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stöttinger, J., Hanbury, A., Gevers, T., &#38; Sebe, N. (2009). Lonely but Attractive: Sparse Color Salient Points for Object Retrieval and Categorization. In <i>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), Workshop on Feature Detectors and Descriptors: The State Of The Art and Beyond</i> (pp. 1–8). http://hdl.handle.net/20.500.12708/52943</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Theo",
                    "last_name": "Gevers",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Nicu",
                    "last_name": "Sebe",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56610",
            "handle": "20.500.12708/52944",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Translating Journalists' Requirements into Features for Image Search",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stöttinger, J., Banova, J., Ponitz, T., Sebe, N., &#38; Hanbury, A. (2009). Translating Journalists’ Requirements into Features for Image Search. In <i>2009 15th International Conference on Virtual Systems and Multimedia</i>. 15th International Conference on Virtual Systems and Multimedia ({VSMM} 2009), Wien, Austria. IEEE. https://doi.org/10.1109/vsmm.2009.28</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Jana",
                    "last_name": "Banova",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Ponitz",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Nicu",
                    "last_name": "Sebe",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 5,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56611",
            "handle": "20.500.12708/52945",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Skin Paths for Contextual Flagging Adult Videos",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stöttinger, J., Hanbury, A., Liensberger, C., &#38; Khan, R. (2009). Skin Paths for Contextual Flagging Adult Videos. In <i>Advances in Visual Computing</i> (pp. 303–314). Springer. http://hdl.handle.net/20.500.12708/52945</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Liensberger",
                    "position": 3,
                    "role": "Author",
                    "tid": "41662"
                },
                {
                    "first_name": "Rehanullah",
                    "last_name": "Khan",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56670",
            "handle": "20.500.12708/53004",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Weakly Supervised Group-Wise Model Learning Based on Discrete Optimization",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Donner, R., Wildenauer, H., Bischof, H., &#38; Langs, G. (2009). Weakly Supervised Group-Wise Model Learning Based on Discrete Optimization. In <i>Medical Image Computing and Computer-Assisted Intervention -MICCAI2009</i> (pp. 860–868). Springer. http://hdl.handle.net/20.500.12708/53004</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 1,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Wildenauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "51087"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 3,
                    "role": "Author",
                    "tid": "126596"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 4,
                    "role": "Author",
                    "tid": "69204"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "56671",
            "handle": "20.500.12708/53005",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "matVTK - 3D visualization for Matlab",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Birngruber, E., Donner, R., &#38; Langs, G. (2009). matVTK - 3D visualization for Matlab. In <i>Proceedings of the MICCAI 2009 Workshop on systems and architectures for CAI</i> (p. 8). http://hdl.handle.net/20.500.12708/53005</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Erich",
                    "last_name": "Birngruber",
                    "position": 1,
                    "role": "Author",
                    "tid": "40478"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 2,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 3,
                    "role": "Author",
                    "tid": "69204"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "56679",
            "handle": "20.500.12708/53013",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Novel Trajectory Clustering Approach for Motion Segmentation",
            "keywords": [],
            "abstract": "We propose a novel clustering scheme for spatio-temporal\r\nsegmentation of sparse motion fields obtained from feature tracking. The\r\napproach allows for the segmentation of meaningful motion components\r\nin a scene, such as short- and long-term motion of single objects, groups\r\nof objects and camera motion. The method has been developed within a\r\nproject on the analysis of low-quality archive films. We qualitatively and\r\nquantitatively evaluate the performance and the robustness of the ap-\r\nproach. Results show, that our method successfully segments the motion\r\ncomponents even in particularly noisy sequences.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zeppelzauer, M., Zaharieva, M., Mitrovic, D., &#38; Breiteneder, C. (2010). A Novel Trajectory Clustering Approach for Motion Segmentation. In <i>Advances in Multimedia Modeling</i> (pp. 433–443). Springer. http://hdl.handle.net/20.500.12708/53013</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 2,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 3,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "56682",
            "handle": "20.500.12708/53016",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Camera Take Reconstruction",
            "keywords": [],
            "abstract": "In this paper we focus on a novel issue in the field of video retrieval stemming from film analysis, namely the investigation of film montage patterns. For this purpose it is first necessary to reconstruct the original film sequences, i.e. the camera takes. For the decision whether or not two shots occurring anywhere in a film stem from the same take we use edge histograms and local feature tracking. Evaluation results on experimental film material (where montage patterns are of great importance) show a very good performance of the algorithm proposed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaharieva, M., Zeppelzauer, M., Breiteneder, C., &#38; Mitrovic, D. (2010). Camera Take Reconstruction. In S. Boll, Q. Tian, L. Zhang, Z. Zhang, &#38; Y.-P. P. Chen (Eds.), <i>Advances in Multimedia Modeling</i> (pp. 379–388). Springer. https://doi.org/10.1007/978-3-642-11301-7_39</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 3,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 4,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Susanne",
                    "last_name": "Boll",
                    "position": 1,
                    "role": "Editor",
                    "tid": "126031"
                },
                {
                    "first_name": "Qi",
                    "last_name": "Tian",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Lei",
                    "last_name": "Zhang",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Zili",
                    "last_name": "Zhang",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Yi-Ping Phoebe",
                    "last_name": "Chen",
                    "position": 5,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56718",
            "handle": "20.500.12708/53052",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "When Pyramids Learned Walking",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. (2009). When Pyramids Learned Walking. In <i>14th Iberoamerican Congress on Pattern Recognition (CIARP 2009)</i> (pp. 397–414). http://hdl.handle.net/20.500.12708/53052</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "56834",
            "handle": "20.500.12708/53167",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "VITAL - The Virtual Environment Interaction Technique Abstraction Layer",
            "keywords": [],
            "abstract": "Traditionally 3D interaction techniques (3DITs) are implemented in\r\nVR applications in a proprietary way on specific target platforms.\r\nMixing 3DIT specific code with application code neither allows for\r\nreusability in other applications nor for exchanging 3DITs in a comfortable\r\nand flexible way. We propose an additional system software\r\nlayer called Virtual Environment Interaction Technique Abstraction\r\nLayer (VITAL) targeted on platform and application independent\r\n(portable) 3DIT implementation. We describe the underlying\r\nconcepts and provide details on how to integrate VITAL in VR\r\nframeworks. Furthermore, development mechanisms targeted on\r\nportability and general-purpose interfacing techniques with other\r\nsystem components are outlined and demonstrated in examples.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Csisinko, M., &#38; Kaufmann, H. (2010). VITAL - The Virtual Environment Interaction Technique Abstraction Layer. In <i>Proceedings of the IEEE Virtual Reality 2010 Workshop: Software Engineering and Architectures for Realtime Interactive Systems</i> (pp. 77–86). Shaker Verlag. http://hdl.handle.net/20.500.12708/53167</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 1,
                    "role": "Author",
                    "tid": "47153"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56925",
            "handle": "20.500.12708/53258",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Efficient computation of persistent homology for cubical data",
            "keywords": [],
            "abstract": "In this paper we present an efficient framework for computation of persistent\r\nhomology of cubical data in arbitrary dimensions. An existing algorithm using\r\nsimplicial complexes is adapted to the setting of cubical complexes. The proposed\r\napproach enables efficient application of persistent homology in domains where the\r\ndata is naturally given in a cubical form. By avoiding triangulation of the data, we\r\nsignificantly reduce the size of the complex. We also present a data-structure designed\r\nto compactly store and quickly manipulate cubical complexes. By means\r\nof numerical experiments, we show high speed and memory efficiency of our approach.\r\nWe compare our framework to other available implementations, showing its\r\nsuperiority. Finally, we report performance on selected 3D and 4D data-sets.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Wagner, H., Chen, C., &#38; Vucini, E. (2011). Efficient computation of persistent homology for cubical data. In <i>Proceedings of the 4th Workshop on Topology-based Methods in Data Analysis and Visualization (TopoInVis 2011)</i> (pp. 1–14). http://hdl.handle.net/20.500.12708/53258</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hubert",
                    "last_name": "Wagner",
                    "position": 1,
                    "role": "Author",
                    "tid": "217144"
                },
                {
                    "first_name": "Chao",
                    "last_name": "Chen",
                    "position": 2,
                    "role": "Author",
                    "tid": "179282"
                },
                {
                    "first_name": "Erald",
                    "last_name": "Vucini",
                    "position": 3,
                    "role": "Author",
                    "tid": "45707"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "56926",
            "handle": "20.500.12708/53259",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Image Segmentation by Figure-Ground Composition into Maximal Cliques",
            "keywords": [],
            "abstract": "We propose a mid-level statistical model for image segmentation\r\nthat composes multiple figure-ground hypotheses\r\n(FG) obtained by applying constraints at different locations\r\nand scales, into larger interpretations (tilings) of\r\nthe entire image. Inference is cast as optimization over\r\nsets of maximal cliques sampled from a graph connecting\r\nall non-overlapping figure-ground segment hypotheses. Potential\r\nfunctions over cliques combine unary, Gestalt-based\r\nfigure qualities, and pairwise compatibilities among spatially\r\nneighboring segments, constrained by T-junctions and\r\nthe boundary interface statistics of real scenes. Learning\r\nthe model parameters is based on maximum likelihood, alternating\r\nbetween sampling image tilings and optimizing\r\ntheir potential function parameters. State of the art results\r\nare reported on the Berkeley and Stanford segmentation\r\ndatasets, as well as VOC2009, where a 28% improvement\r\nwas achieved.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Carreira, J., &#38; Sminchisescu, C. (2011). Image Segmentation by Figure-Ground Composition into Maximal Cliques. In <i>13th IEEE International Conference on Computer Vision</i> (pp. 2110–2117). IEEE. http://hdl.handle.net/20.500.12708/53259</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Joao",
                    "last_name": "Carreira",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Cristian",
                    "last_name": "Sminchisescu",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E226-02"
            ],
            "pid": "56927",
            "handle": "20.500.12708/53260",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Enforcing topological constraints in random field image segmentation",
            "keywords": [],
            "abstract": "We introduce TopoCut: a new way to integrate knowledge \r\nabout topological properties (TPs) into random ï¬ eld\r\nimage segmentation model. Instead of including TPs as\r\nadditional constraints during minimization of the energy\r\nfunction, we devise an efï¬ cient algorithm for modifying\r\nthe unary potentials such that the resulting segmentation\r\nis guaranteed with the desired properties. Our method is\r\nmore ï¬'exible in the sense that it handles more topology\r\nconstraints than previous methods, which were only able\r\nto enforce pairwise or global connectivity. In particular,\r\nour method is very fast, making it for the ï¬ rst time possible\r\nto enforce global topological properties in practical image\r\nsegmentation tasks.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Chen, C., Freedman, D., &#38; Lampert, C. (2011). Enforcing topological constraints in random field image segmentation. In <i>IEEE Computer Vision and Pattern Recognition (CVPR 2011)</i> (pp. 2089–2096). http://hdl.handle.net/20.500.12708/53260</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Chao",
                    "last_name": "Chen",
                    "position": 1,
                    "role": "Author",
                    "tid": "179282"
                },
                {
                    "first_name": "Daniel",
                    "last_name": "Freedman",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Christoph",
                    "last_name": "Lampert",
                    "position": 3,
                    "role": "Author",
                    "tid": "124640"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "56931",
            "handle": "20.500.12708/53264",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Study Of Gradual Transition Detection in Historic Film Material",
            "keywords": [],
            "abstract": "The detection of gradual transitions focuses on two types of\r\napproaches: unified approaches, i.e. one detector for all gradual\r\ntransition types, and approaches that use specialized detectors for\r\neach gradual transition type. We present an overview on existing\r\nmethods and extend an existing unified approach for the detection\r\nof gradual transitions in historic material. In an experimental\r\nstudy we evaluate our approach on complex and low quality\r\nhistoric material as well as on contemporary material from the\r\nTRECVid evaluation. Additionally we investigate different\r\nfeatures, feature combinations and fusion strategies. We observe\r\nthat the historic material requires the use of texture features in\r\ncontrast to the contemporary material that in most of the cases\r\nrequires the use of colour and luminance features.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Seidl, M., Zeppelzauer, M., &#38; Breiteneder, C. (2010). A Study Of Gradual Transition Detection in Historic Film Material. In <i>ACM Multimedia 2010 Workshop - Electronic Heritage and Digital Art Preservation (eHeritage)</i> (p. 13). http://hdl.handle.net/20.500.12708/53264</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Seidl",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 3,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "56944",
            "handle": "20.500.12708/53277",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Scene Segmentation in Artistic Archive Documentaries",
            "keywords": [],
            "abstract": "Scene segmentation is a crucial task in the structural analysis of film. State-of-the-art scene segmentation algorithms usually target\r\nfiction films (e.g. Hollywood films). Documentaries (especially artistic archive documentaries) follow different montage rules than fiction films and consequently require specialized approaches for scene segmentation. We propose a scene segmentation algorithm targeted at artistic archive documentaries. We evaluate the performance of our techniquewith archive documentaries and contemporary movies and obtain satisfactory results in both domains.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mitrovic, D., Hartlieb, S., Zeppelzauer, M., &#38; Breiteneder, C. (2010). Scene Segmentation in Artistic Archive Documentaries. In <i>HCI in Work and Learning, Life and Leisure</i> (pp. 400–410). Springer-Verlag Berlin Heidelberg,. http://hdl.handle.net/20.500.12708/53277</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 1,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Stefan",
                    "last_name": "Hartlieb",
                    "position": 2,
                    "role": "Author",
                    "tid": "51711"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 3,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "56998",
            "handle": "20.500.12708/53331",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Adaptive Methods of Kalman Filtering for Personal Positioning Systems",
            "keywords": [],
            "abstract": "Kalman filtering is very efficient for data fusion, in which the definition of the process and measurement noises (i.e. the matrices Q and R, respectively) greatly influences the filter performance. In recent years several studies reported that adjustments of Q and R can be helpful to reduce the errors of the estimations.\r\n\r\nIn this paper, various methods for making adjustments to the matrices Q and R are introduced for the particular case of Personal Positioning Systems (PPS). The aim is to observe the improvements achieved in an extended Kalman filter when adaptive methods are applied, in other words to observe their influence on the user's path obtained. These adjustments are considered to be needed because environmental conditions in such systems are often not fixed.\r\n\r\nThe methods to be analyzed are: (1) the weighted Kalman filter; (2) scaling matrix Q (3) adjustments of Q and R based on sequence-innovation; and (4) a combination of the method (2) and (3), i.e. Q is estimated by applying a scale factor and adjustments to R are realized in accordance with the method (3).\r\n\r\nGiven that the filter may diverge, we use the 2? test to evaluate validity of the estimations, which is based on the analysis of the innovations. Individual components of the innovation vector are evaluated in order to correct or eliminate wrong information for data fusion.\r\n\r\nThe PPS is based on the Dead Reckoning (DR) algorithm. The errors of the DR parameters are estimated with an Extended Kalman Filter (EKF), which combines the measurements of a GPS and an inertial measurement unit (IMU). The results show that each method allows us to obtain consistent Kalman filtering and they help to obtain better user´s trajectories, but additional techniques and/or technologies should be used.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pulido Herrera, E., &#38; Kaufmann, H. (2010). Adaptive Methods of Kalman Filtering for Personal Positioning Systems. In <i>Proceedings of 23rd International Technical Meeting of the Satellite Division of The Institute of Navigation</i> (p. 6). http://hdl.handle.net/20.500.12708/53331</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Edith",
                    "last_name": "Pulido Herrera",
                    "position": 1,
                    "role": "Author",
                    "tid": "191422"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57019",
            "handle": "20.500.12708/53351",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Combining Regular Decimation and Dual Graph Contraction for Hierarchical Image Segmentation",
            "keywords": [],
            "abstract": "When a two-dimensional (2D) TSP is presented on a computer screen, human\r\nsubjects can produce near-optimal tours in linear time. In this study we tested human performance on a real and virtual\r\noor, as well as in a 3D virtual space. Human performance on the real floor is as good as that on a computer screen.\r\nPerformance on a virtual floor is very similar, while that in a 3D space is slightly, but systematically worse. We modeled these results by a graph pyramid algorithm. The same algorithm can account for the results with 2D and 3D problems, which suggests that deterioration of performance in the 3D space can be attributed to geometrical relations between hierarchical clustering in a 3D space\r\nand coarse-to-",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Torres Garcia, F., Marfill, R., Haxhimusa, Y., &#38; Bandera, A. (2010). Combining Regular Decimation and Dual Graph Contraction for Hierarchical Image Segmentation. In R. González-Díaz &#38; P. Real (Eds.), <i>3rd International Workshop on Computational Topology in Image Context</i> (pp. 97–104). ACM. http://hdl.handle.net/20.500.12708/53351</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Fuensanta",
                    "last_name": "Torres Garcia",
                    "position": 1,
                    "role": "Author",
                    "tid": "235851"
                },
                {
                    "first_name": "Rebeca",
                    "last_name": "Marfill",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Antonio",
                    "last_name": "Bandera",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "R.",
                    "last_name": "González-Díaz",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Real",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57020",
            "handle": "20.500.12708/53352",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Next Generation Typeface Representations: Revisiting Parametric Fonts",
            "keywords": [],
            "abstract": "A number of methods for evaluating table structure recognition systems have been proposed in the literature, which have been used successfully for automatic and manual optimization of their respective algorithms. Unfortunately, the lack of standard, ground-truthed datasets coupled with the ambiguous nature of how humans interpret tabular data has made it difficult to compare the obtained results between different systems developed by different research groups.\r\n\r\nWith reference to these approaches, we describe our experiences in comparing our algorithm for table detection and structure recognition to another recently published system using a freely available dataset of 75 PDF documents. Based on examples from this dataset, we define several classes of errors and propose how they can be treated consistently to eliminate ambiguities and ensure the repeatability of the results and their comparability between different systems from different research groups.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hassan, T., Hu, C., &#38; Hersch, R. D. (2010). Next Generation Typeface Representations: Revisiting Parametric Fonts. In <i>Proceedings of the 10th ACM symposium on Document engineering - DocEng ’10</i> (pp. 181–184). Association for Computing Machinery (ACM). https://doi.org/10.1145/1860559.1860596</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Tamir",
                    "last_name": "Hassan",
                    "position": 1,
                    "role": "Author",
                    "tid": "44748"
                },
                {
                    "first_name": "Changyuan",
                    "last_name": "Hu",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Roger D.",
                    "last_name": "Hersch",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57064",
            "handle": "20.500.12708/53396",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Representing the Surface of Objects by Combinatorial Pyramids",
            "keywords": [],
            "abstract": "Abstract This paper introduces a new method to rep-\r\nresent the surface of objects using two dimensional com-\r\nbinatorial maps. The classical de nition of two dimen-\r\nsional combinatorial maps is extended here by adding\r\na \\back face&quot; that corresponds to the non{visible part\r\nof the object. As a rst step, every object in the scene\r\nis extracted in one image pyramid, where levels are re-\r\nlated by means of coordinates. Finally, an algorithm\r\nto complete the description of the surface is presented.\r\nSuch representation is translation, rotation and scale\r\ninvariant. It will allow to update information about\r\nmovements and parts of the object that become visible,\r\nreducing the complexity and the computational time.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Antunez, E., Molina Abril, H., &#38; Kropatsch, W. (2010). Representing the Surface of Objects by Combinatorial Pyramids. In <i>Proceedings of the 15th Computer Vision Winter Workshop</i> (pp. 85–90). Libor {\\v S}pa{\\v c}ek and Vojt{\\v e}ch Franc. http://hdl.handle.net/20.500.12708/53396</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Esther",
                    "last_name": "Antunez",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Helena",
                    "last_name": "Molina Abril",
                    "position": 2,
                    "role": "Author",
                    "tid": "55000"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57065",
            "handle": "20.500.12708/53397",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Multi-View Integration for a Rotating 3D Object",
            "keywords": [],
            "abstract": "Abstract The aim of this paper is to propose a novel object\r\nrepresentation for a 3D rigid object in tracking applications.\r\nA 3D volumetric object is bounded by a closed surface (2D\r\nmanifold) on which visible features are perceived (e.g. color,\r\ntexture). In our representation, we collect the topological\r\nstructures from the visible surface of the target object. As the\r\nobject rotates, new visible parts of surface would reveal new\r\ntopological structures. This collection of small 3D surface\r\npatches is reconstructed by processing topological completion.\r\nAnd the object representation is incrementally updated\r\nfrom observing the target object in a video sequence.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mateos, L., &#38; Kropatsch, W. (2010). Multi-View Integration for a Rotating 3D Object. In <i>Proceedings of the 15th Computer Vision Winter Workshop</i> (pp. 72–76). Libor {\\v S}pa{\\v c}ek and Vojt{\\v e}ch Franc. http://hdl.handle.net/20.500.12708/53397</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Luis",
                    "last_name": "Mateos",
                    "position": 1,
                    "role": "Author",
                    "tid": "178947"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-03"
            ],
            "pid": "57066",
            "handle": "20.500.12708/53398",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Irregular {L}aplacian {G}raph {P}yramid",
            "keywords": [],
            "abstract": "Abstract This paper presents a novel image representation,\r\nwhich incorporates the principles of Laplacian Pyramid\r\ninto the irregular graph pyramid. The drawback of the\r\nregular Laplacian Pyramid is their lack to keep the topological\r\nstructure of the image, due to the contraction process in\r\nbuilding the Gaussian Pyramid. Irregular graph pyramid is\r\nable to hierarchically represent the topological structure of\r\nan image with multiresolution, where each level is a graph\r\ndescribing the image with various resolutions by contracting\r\nthe graph from the level below. We build irregular Laplacian\r\ngraph pyramid by storing the difference of levels in irregular\r\ngraph pyramid. Experiments and results are presented in the\r\npaper to show the characteristic of the irregular Laplacian\r\ngraph pyramid and some immediate advantages in computer\r\nvision applications.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Shao, D., &#38; Kropatsch, W. (2010). Irregular {L}aplacian {G}raph {P}yramid. In <i>Proceedings of the 15th Computer Vision Winter Workshop</i> (pp. 66–71). Libor {\\v S}pa{\\v c}ek and Vojt{\\v e}ch Franc. http://hdl.handle.net/20.500.12708/53398</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Dan",
                    "last_name": "Shao",
                    "position": 1,
                    "role": "Author",
                    "tid": "70910"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57067",
            "handle": "20.500.12708/53399",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Algorithm to Compute a Minimal Length Basis of Representative Cocycles of Cohomology Generators",
            "keywords": [],
            "abstract": "Abstract An algorithm to compute a minimal length basis of representative cocycles of\r\ncohomology generators for 2D images is proposed. We based the computations on combinatorial\r\npyramids foreseeing its future extension to 3D objects. In our research we are looking\r\nfor a more refined topological description of deformable 2D and 3D shapes, than they are\r\nthe often used Betti numbers. We define contractions on the object edges toward the inner\r\nof the object until the boundaries touch each other, building an irregular pyramid with\r\nthis purpose. We show the possible use of the algorithm seeking the minimal cocycles that\r\nconnect the convex deficiencies on a human silhouette. We used minimality in the number\r\nof cocycle edges in the basis, which is a robust description to rotations and noise.\r\nKeyworks cohomology; combinatorial pyramids; representative cocycles of cohomology\r\ngenerators.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Iglesias-Ham, M., Garcia, E., Kropatsch, W., &#38; González-Díaz, R. (2010). Algorithm to Compute a Minimal Length Basis of Representative Cocycles of Cohomology Generators. In <i>3rd Workshop on Computational Topology in Image Context (CTIC)</i> (pp. 121–127). Roc\\’io Gonz\\’alez D\\’iaz and Pedro Real Jurado. http://hdl.handle.net/20.500.12708/53399</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mabel",
                    "last_name": "Iglesias-Ham",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Edel",
                    "last_name": "Garcia",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "R.",
                    "last_name": "González-Díaz",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57147",
            "handle": "20.500.12708/53478",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Affective image classification using features inspired by psychology and art theory",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Machajdik, J., &#38; Hanbury, A. (2010). Affective image classification using features inspired by psychology and art theory. In <i>Proceedings of the international conference on Multimedia - MM ’10</i>. ACM Multimedia 2010 International Conference, Florenz, Italien, EU. Association for Computing Machinery (ACM). https://doi.org/10.1145/1873951.1873965</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Jana",
                    "last_name": "Machajdik",
                    "position": 1,
                    "role": "Author",
                    "tid": "43465"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03",
                "E194"
            ],
            "pid": "57149",
            "handle": "20.500.12708/53480",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Efficient and Distinct Large Scale Bags of Words",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pönitz, T., Stöttinger, J., Donner, R., &#38; Hanbury, A. (2010). Efficient and Distinct Large Scale Bags of Words. In P. Blauensteiner, M. Lettner, &#38; J. Stöttinger (Eds.), <i>Computer Vision in a Global Society - 34th Annual Workshop of the Austrian Association for Pattern Recognition (AAPR) and the WG Visual Computing of the Austrian Computer Society</i> (pp. 139–146). Österreichische Computer Gesellschaft. http://hdl.handle.net/20.500.12708/53480</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Pönitz",
                    "position": 1,
                    "role": "Author",
                    "tid": "39225"
                },
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 2,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Rene",
                    "last_name": "Donner",
                    "position": 3,
                    "role": "Author",
                    "tid": "49701"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 4,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Philipp",
                    "last_name": "Blauensteiner",
                    "position": 1,
                    "role": "Editor",
                    "tid": "53592"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Lettner",
                    "position": 2,
                    "role": "Editor",
                    "tid": "48706"
                },
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 3,
                    "role": "Editor",
                    "tid": "47904"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57150",
            "handle": "20.500.12708/53481",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "FeEval A Dataset for Evaluation of Spatio-temporal Local Features",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stöttinger, J., Zambanini, S., Khan, R., &#38; Hanbury, A. (2010). FeEval A Dataset for Evaluation of Spatio-temporal Local Features. In M. Cetin, K. Boyer, &#38; S.-W. Lee (Eds.), <i>2010 20th International Conference on Pattern Recognition</i>. IEEE. https://doi.org/10.1109/icpr.2010.128</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Sebastian",
                    "last_name": "Zambanini",
                    "position": 2,
                    "role": "Author",
                    "tid": "53412"
                },
                {
                    "first_name": "Rehanullah",
                    "last_name": "Khan",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 4,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Müjdat",
                    "last_name": "Cetin",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Kim",
                    "last_name": "Boyer",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Seong-Whan",
                    "last_name": "Lee",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57152",
            "handle": "20.500.12708/53483",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Behavior and properties of spatio-temporal local features under visual transformations",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stöttinger, J., Goras, B. T., Sebe, N., &#38; Hanbury, A. (2010). Behavior and properties of spatio-temporal local features under visual transformations. In <i>Proceedings of the international conference on Multimedia - MM ’10</i>. ACM Multimedia 2010 International Conference, Florenz, Italien, EU. Association for Computing Machinery (ACM). https://doi.org/10.1145/1873951.1874174</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Bogdan Tudor",
                    "last_name": "Goras",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Nicu",
                    "last_name": "Sebe",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 4,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03",
                "E194"
            ],
            "pid": "57153",
            "handle": "20.500.12708/53484",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Systematic Evaluation of Spatio-Temporal Features on Comparative Video Challenges",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stöttinger, J., Goras, B. T., Pönitz, T., Sebe, N., Hanbury, A., &#38; Gevers, T. (2011). Systematic Evaluation of Spatio-Temporal Features on Comparative Video Challenges. In R. Koch &#38; F. Huang (Eds.), <i>Computer Vision – ACCV 2010 Workshops</i> (pp. 349–358). Lecture Notes in Computer Science. https://doi.org/10.1007/978-3-642-22822-3_35</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Bogdan Tudor",
                    "last_name": "Goras",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Pönitz",
                    "position": 3,
                    "role": "Author",
                    "tid": "39225"
                },
                {
                    "first_name": "Nicu",
                    "last_name": "Sebe",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 5,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Theo",
                    "last_name": "Gevers",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Reinhard",
                    "last_name": "Koch",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Fay",
                    "last_name": "Huang",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57157",
            "handle": "20.500.12708/53488",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Co-occurrence Bag of Words for Object Recognition",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Bhatti, N., &#38; Hanbury, A. (2010). Co-occurrence Bag of Words for Object Recognition. In L. Spaček &#38; V. Franc (Eds.), <i>Proceedings of the Computer Vision Winter Workshop 2010</i> (pp. 21–28). Czech Society for Cybernetics and Informatics. http://hdl.handle.net/20.500.12708/53488</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Naeem",
                    "last_name": "Bhatti",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Libor",
                    "last_name": "Spaček",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Vojtech",
                    "last_name": "Franc",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57158",
            "handle": "20.500.12708/53489",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Weighted Skin Color Segmentation and Detection Using Graph Cuts",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Khan, R., Hanbury, A., &#38; Stöttinger, J. (2010). Weighted Skin Color Segmentation and Detection Using Graph Cuts. In L. Spaček &#38; V. Franc (Eds.), <i>Proceedings of the Computer Vision Winter Workshop 2010</i> (pp. 107–114). Czech Society for Cybernetics and Informatics. http://hdl.handle.net/20.500.12708/53489</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rehanullah",
                    "last_name": "Khan",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 3,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Libor",
                    "last_name": "Spaček",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Vojtech",
                    "last_name": "Franc",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57169",
            "handle": "20.500.12708/53500",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Skin detection: A random forest approach",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Khan, R., Hanbury, A., &#38; Stoettinger, J. (2010). Skin detection: A random forest approach. In <i>2010 IEEE International Conference on Image Processing</i>. International Conference on Image Processing 2010, Hong Kong, Non-EU. IEEE. https://doi.org/10.1109/icip.2010.5651638</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rehanullah",
                    "last_name": "Khan",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Julian",
                    "last_name": "Stoettinger",
                    "position": 3,
                    "role": "Author",
                    "tid": "47904"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57170",
            "handle": "20.500.12708/53501",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Augmentation of Skin Segmentation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Khan, R., Hanbury, A., &#38; Stöttinger, J. (2010). Augmentation of Skin Segmentation. In H. Arabnia, L. Deligiannidis, G. Schaefer, &#38; A. Solo (Eds.), <i>Proceedings of the 2010 International Conference on Image Processing, Computer Vision, &#38; Pattern Recognition, IPCV 2010, July 12-15, 2010, Las Vegas, Nevada, USA, 2 Volumes</i> (pp. 473–479). CSREA Press. http://hdl.handle.net/20.500.12708/53501</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rehanullah",
                    "last_name": "Khan",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 3,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "Hamid",
                    "last_name": "Arabnia",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Leonidas",
                    "last_name": "Deligiannidis",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Gerald",
                    "last_name": "Schaefer",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Ashu",
                    "last_name": "Solo",
                    "position": 4,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57171",
            "handle": "20.500.12708/53502",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Universal Seed Skin Segmentation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Khan, R., Hanbury, A., &#38; Stöttinger, J. (2010). Universal Seed Skin Segmentation. In G. Bebis, R. Boyle, B. Parvin, D. Koracin, R. Chung, R. Hammound, M. Hussain, T. Kar-Han, R. Crawfis, D. Thalmann, D. Kao, &#38; L. Avila (Eds.), <i>Advances in Visual Computing</i> (pp. 75–84). Lecture Notes in Computer Science. https://doi.org/10.1007/978-3-642-17274-8_8</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Rehanullah",
                    "last_name": "Khan",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                },
                {
                    "first_name": "Julian",
                    "last_name": "Stöttinger",
                    "position": 3,
                    "role": "Author",
                    "tid": "47904"
                },
                {
                    "first_name": "George",
                    "last_name": "Bebis",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Richard",
                    "last_name": "Boyle",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Bahram",
                    "last_name": "Parvin",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Darko",
                    "last_name": "Koracin",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Ronald",
                    "last_name": "Chung",
                    "position": 5,
                    "role": "Editor"
                },
                {
                    "first_name": "Riad",
                    "last_name": "Hammound",
                    "position": 6,
                    "role": "Editor"
                },
                {
                    "first_name": "Muhammad",
                    "last_name": "Hussain",
                    "position": 7,
                    "role": "Editor"
                },
                {
                    "first_name": "Tan",
                    "last_name": "Kar-Han",
                    "position": 8,
                    "role": "Editor"
                },
                {
                    "first_name": "Roger",
                    "last_name": "Crawfis",
                    "position": 9,
                    "role": "Editor"
                },
                {
                    "first_name": "Daniel",
                    "last_name": "Thalmann",
                    "position": 10,
                    "role": "Editor"
                },
                {
                    "first_name": "David",
                    "last_name": "Kao",
                    "position": 11,
                    "role": "Editor"
                },
                {
                    "first_name": "Lisa",
                    "last_name": "Avila",
                    "position": 12,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "57296",
            "handle": "20.500.12708/53626",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Full body interaction for serious games in motor rehabilitation",
            "keywords": [],
            "abstract": "Serious games and especially their use in healthcare applications are an active and rapidly growing area of research. A key aspect of games in rehabilitation is 3D input. In this paper we present our implementation of a full body motion capture (MoCap) system, which, together with a biosignal acquisition device, has been integrated in a game engine. Furthermore, a workflow has been established that enables the use of acquired skeletal data for serious games in a medical environment. Finally, a serious game has been implemented, targeting rehabilitation of patients with chronic pain of the lower back and neck, a group that has previously been neglected by serious games. The focus of this work is on the full body MoCap system and its integration with biosignal devices and the game engine. A short overview of the application and prelimiary results are provided.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., Pintaric, T., &#38; Kaufmann, H. (2011). Full body interaction for serious games in motor rehabilitation. In <i>Proceedings of the 2nd Augmented Human International Conference on - AH ’11</i>. ACM, Austria. ACM. https://doi.org/10.1145/1959826.1959830</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Pintaric",
                    "position": 2,
                    "role": "Author",
                    "tid": "39909"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "57299",
            "handle": "20.500.12708/53629",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Cutting the Cord: Wireless Mixed Reality Displays",
            "keywords": [],
            "abstract": "In this paper we present wireless technologies that can be used to transmit uncompressed stereoscopic video signals to wireless displays in real time. We introduce two output devices, a stereoscopic head mounted display (HMD) and a TFT display module. Both of them have been adapted to act as receivers in order to display wirelessly streamed Mixed Reality (MR) content. By way of example two educational MR applications are presented which were used for demonstration and testing purposes. A number of teaching scenarios are described where teachers and students greatly benefit from the use of wireless displays. We briefly summarize the results of our observations while developing and evaluating these displays.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Csisinko, M., &#38; Kaufmann, H. (2011). Cutting the Cord: Wireless Mixed Reality Displays. In <i>Proceedings of the Virtual Reality International Conference (VRIC 2011)</i> (p. 10). http://hdl.handle.net/20.500.12708/53629</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 1,
                    "role": "Author",
                    "tid": "47153"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57300",
            "handle": "20.500.12708/53630",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Probabilistic Joint Image Segmentation and Labeling",
            "keywords": [],
            "abstract": "We present a joint image segmentation and labeling model (JSL) which, given a bag of &#64257;gure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as &#64257;rst sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag [1], followed by sampling labels for those segments, conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on Maximum Likelihood with a novel Incremental Saddle Point\r\nestimation procedure. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect con&#64257;gurations that a not-yet-competent model rates probable during learning. We show that the proposed methodology matches the current state of the art in the Stanford dataset [2], as well as in VOC2010, where 41.7% accuracy on the test set is achieved.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Carreira, J., &#38; Sminchisescu, C. (2011). Probabilistic Joint Image Segmentation and Labeling. In <i>Advances in Neural Information Processing Systems 24</i> (pp. 1–9). Advances in Neural Information Processing Systems 24 , edited by J. Shawe-Taylor and R.S. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger (2011). http://hdl.handle.net/20.500.12708/53630</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Joao",
                    "last_name": "Carreira",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Cristian",
                    "last_name": "Sminchisescu",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "57301",
            "handle": "20.500.12708/53631",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Retrieval of Visual Composition in Film",
            "keywords": [],
            "abstract": "The spatial arrangement of visual elements of an image, i.e. the visual composition, is a research subject in the domain of visual arts which include painting, film, etc. Film experts face the problem of retrieval of visual compositions in film on a daily basis. Although, visual composition is a crucial element to consider in content-based video retrieval, little scientific effort has been invested into this problem so far. Actually, it is unclear if content-based retrieval of visual compositions is feasible. We present a user study conducted to investigate the feasibility of content-based retrieval of visual compositions as they are understood by film experts. For that reason, we create a data set derived from real world material and let the film experts evaluate the retrieval performance.\r\nThe user study investigates the applicability of state-of-the-art visual features and shows differences in evaluations by film experts (test group) and computer scientists (reference group).",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mitrovic, D., Zeppelzauer, M., Zaharieva, M., &#38; Breiteneder, C. (2011). Retrieval of Visual Composition in Film. In <i>Proceedings 12th International Workshop on Image Analysis for Multimedia Interactive Services</i> (p. 4). http://hdl.handle.net/20.500.12708/53631</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 1,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 3,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 4,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "57367",
            "handle": "20.500.12708/53697",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Cross-Modal Analysis of Audio-Visual Film Montage",
            "keywords": [],
            "abstract": "A stylistic device frequently employed by filmmakers\r\nis the synchronous montage (composition) of audio and visual elements.\r\nSynchronous montage helps to increase tension and tempo\r\nin a scene and highlights important events in the story. Sequences\r\nwith synchronous montage usually contain rich semantics which\r\nis relevant for understanding a movie. This property is currently\r\nnot exploited in automated indexing, annotation, and summarization\r\nof movies. We propose a cross-modal approach that\r\nextracts sequences from a movie with synchronous audio-visual\r\nmontage. Experiments confirm that the extracted sequences have\r\nhigh semantic relevance. Consequently, they represent a useful\r\nbasis for different high-level movie abstraction tasks such as\r\nautomated movie annotation and movie summarization.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zeppelzauer, M., Mitrovic, D., &#38; Breiteneder, C. (2011). Cross-Modal Analysis of Audio-Visual Film Montage. In <i>International Conference on Computer Communication and Netwirks (ICCCN)</i> (p. 6). IEEE eXpress Conference Publishing. http://hdl.handle.net/20.500.12708/53697</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrovic",
                    "position": 2,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 3,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57389",
            "handle": "20.500.12708/53719",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "An output-sensitive algorithm for persistent homology",
            "keywords": [],
            "abstract": "In this paper, we present the first output-sensitive algorithm\r\nto compute the persistence diagram of a filtered simplicial\r\ncomplex. For any &#1048576; > 0, it returns only those homology\r\nclasses with persistence at least &#1048576;. Instead of the classical\r\nreduction via column operations, our algorithm performs\r\nrank computations on submatrices of the boundary matrix.\r\nFor an arbitrary constant &#948; &#8712; (0, 1), the running time is\r\nO(C(1&#8722;&#948;)&#1048576;R(n) log n), where C(1&#8722;&#948;)&#1048576; is the number of homology\r\nclasses with persistence at least (1&#8722;&#948;)&#1048576;, n is the total\r\nnumber of simplices, and R(n) is the complexity of computing\r\nthe rank of an n×n matrix with O(n) nonzero entries.\r\nDepending on the choice of the rank algorithm, this yields a\r\ndeterministic O(C(1&#8722;&#948;)&#1048576;n2.376) algorithm, a O(C(1&#8722;&#948;)&#1048576;n2.28)\r\nLas-Vegas algorithm, or a O(C(1&#8722;&#948;)&#1048576;n2+&#491;) Monte-Carlo algorithm\r\nfor an arbitrary &#491; > 0.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Chen, C., &#38; Kerber, M. (2011). An output-sensitive algorithm for persistent homology. In <i>27th Annual Symposium on Computational Geometry (SoCG 2011)</i> (pp. 1–9). http://hdl.handle.net/20.500.12708/53719</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Chao",
                    "last_name": "Chen",
                    "position": 1,
                    "role": "Author",
                    "tid": "179282"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Kerber",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-03"
            ],
            "pid": "57419",
            "handle": "20.500.12708/53749",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Enhancing Visualization with Real-Time Frequency-based Transfer Functions",
            "keywords": [],
            "abstract": "Transfer functions have a crucial role in the understanding and visualization of 3D data. While research has scrutinized the possible uses of one and multi-dimensional transfer functions in the spatial domain, to our knowledge, no attempt has been done to explore transfer functions in the frequency domain. In this work we propose transfer functions for the purpose of frequency analysis and visualization of 3D data. Frequency-based transfer functions offer the possibility to discriminate signals, composed from different frequencies, to analyze problems related to signal processing, and to help understanding the link between the modulation of specific frequencies and their impact on the spatial domain. We demonstrate the strength of frequency-based transfer functions by applying them to medical CT, ultrasound and MRI data, physics data as well as synthetic seismic data. The interactive design of complex filters for feature enhancement can be a useful addition to conventional classification techniques.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vucini, E., Patel, D., &#38; Gröller, E. (2011). Enhancing Visualization with Real-Time Frequency-based Transfer Functions. In <i>Proceedings of IS&#38;T/SPIE Conference on Visualization and Data Analysis</i> (pp. 1–12). http://hdl.handle.net/20.500.12708/53749</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Erald",
                    "last_name": "Vucini",
                    "position": 1,
                    "role": "Author",
                    "tid": "45707"
                },
                {
                    "first_name": "Daniel",
                    "last_name": "Patel",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "position": 3,
                    "role": "Author",
                    "tid": "143572"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57515",
            "handle": "20.500.12708/53843",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Spatio-Temporal Extraction of Articulated Models in a Graph Pyramid Graph-Based Representations in Pattern Recognition",
            "keywords": [],
            "abstract": "This paper presents a method to create a model of an articulated object using the planar motion in an initialization video. The model consists of rigid parts connected by points of articulation. The rigid parts are described by the positions of salient feature-points tracked throughout the video. Following a filtering step that identifies points that belong to different objects, rigid parts are found by a grouping process in a graph pyramid. Valid articulation points are selected by verifying multiple hypotheses for each pair of parts.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., Ion, A., &#38; Kropatsch, W. (2011). Spatio-Temporal Extraction of Articulated Models in a Graph Pyramid Graph-Based Representations in Pattern Recognition. In <i>GbRPR’11 Proceedings of the 8th international conference on Graph-based representations in pattern recognition</i> (pp. 215–224). Lecture Notes in Computer Science, Vol. 6658, Springer/Xiaoyi Jiang, Miquel Ferrer, Andrea Torsello. http://hdl.handle.net/20.500.12708/53843</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57522",
            "handle": "20.500.12708/53850",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Robustness and modularity of 2-dimensional size functions",
            "keywords": [],
            "abstract": "This paper deals with the concepts of 2-dimensional size function and 2-dimensional matching distance. These are two ingredients of (2-dimensional) Size Theory, a geometrical/topological approach to shape analysis and comparison. 2-dimensional size functions are shape descriptors providing a signature of the shapes under study, while the\r\n2-dimensional distance is the tool to compare them. The aim of the present paper is to validate, through some experiments on 3D-models, a computational framework recently introduced to deal with 2-dimensional Size Theory. We will show that the cited framework is modular and robust with respect to noise, non-rigid and non-metric-preserving shape transformations. The proposed framework allows us to improve the ability of 2-dimensional size functions in discriminating between shapes.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Biasotti, S., Cerri, A., &#38; Giorgi, D. (2011). Robustness and modularity of 2-dimensional size functions. In <i>CAIP 2011 - 14th International Conference on Computer Analysis of Images and Patterns</i> (pp. 34–41). Proc. CAIP 2011 - 14th International Conference on Computer Analysis of Images and Patterns, LNCS 6854, Ainhoa Berciano et al. Eds./Springer. http://hdl.handle.net/20.500.12708/53850</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Silvia",
                    "last_name": "Biasotti",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Andrea",
                    "last_name": "Cerri",
                    "position": 2,
                    "role": "Author",
                    "tid": "233419"
                },
                {
                    "first_name": "Daniela",
                    "last_name": "Giorgi",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57526",
            "handle": "20.500.12708/53854",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Global Method for Reducing Multidimensional Size Graphs",
            "keywords": [],
            "abstract": "This paper introduces the concept of discrete multidimensional size function, a mathematical tool studying the so-called size graphs. These graphs constitutes an ingredient of Size Theory, a geometrical/topological approach to shape analysis and comparison. A global method for reducing size graphs is presented, together with a theorem stating that size graphs reduced in such a way preserve all the information in terms of multidimensional size functions. This approach can lead to simplify the effective computation of discrete multidimensional size\r\nfunctions, as shown by examples.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Cerri, A., Frosini, P., Kropatsch, W., &#38; Landi, C. (2011). A Global Method for Reducing Multidimensional Size Graphs. In <i>Proc. GbRPR 2011 - Graph-based Representation in Pattern Recognition, LNCS 6658, X. Jiang, M. Ferrer and A. Torsello Eds. (2011), 1-11</i> (pp. 1–11). Springer. http://hdl.handle.net/20.500.12708/53854</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Andrea",
                    "last_name": "Cerri",
                    "position": 1,
                    "role": "Author",
                    "tid": "233419"
                },
                {
                    "first_name": "Patrizio",
                    "last_name": "Frosini",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Claudia",
                    "last_name": "Landi",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57534",
            "handle": "20.500.12708/53862",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Exploiting Spatial Consistency for Object Classification and Pose Estimation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hödlmoser, M., Micusik, B., &#38; Kampel, M. (2011). Exploiting Spatial Consistency for Object Classification and Pose Estimation. In <i>Proceedings of IEEE International Conference on Image Processing (ICIP’11)</i> (pp. 1009–1012). IEEE. http://hdl.handle.net/20.500.12708/53862</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Hödlmoser",
                    "position": 1,
                    "role": "Author",
                    "tid": "65240"
                },
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 2,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 3,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57535",
            "handle": "20.500.12708/53863",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Camera Auto-Calibration Using Pedestrians and Zebra-Crossings",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hödlmoser, M., Micusik, B., &#38; Kampel, M. (2011). Camera Auto-Calibration Using Pedestrians and Zebra-Crossings. In <i>2011 IEEE International Conference on Computer Vision Workshops</i> (pp. 1697–1704). IEEE. http://hdl.handle.net/20.500.12708/53863</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Hödlmoser",
                    "position": 1,
                    "role": "Author",
                    "tid": "65240"
                },
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 2,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 3,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57541",
            "handle": "20.500.12708/53869",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Architectural Style Classification of Building Facade Windows",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Shalunts, G., Haxhimusa, Y., &#38; Sablatnig, R. (2011). Architectural Style Classification of Building Facade Windows. In <i>Advances in Visual Computing</i> (pp. 280–289). Lecture Notes on Computer Science. https://doi.org/10.1007/978-3-642-24031-7_28</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Gayane",
                    "last_name": "Shalunts",
                    "position": 1,
                    "role": "Author",
                    "tid": "193583"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 3,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57556",
            "handle": "20.500.12708/53884",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "3D Image Segmentation Using the Bounded Irregular Pyramid",
            "keywords": [],
            "abstract": "This paper presents a novel pyramid approach for fast segmentation\r\nof 3D images. A pyramid is a hierarchy of successively reduced\r\ngraphs whose efficiency is strongly influenced by the data structure that\r\ncodes the information within the pyramid and the decimation process\r\nused to build a graph from the graph below. Depending on these two\r\nfeatures, pyramids have been classified as regular and irregular ones.\r\nThe proposed approach extends the idea of the Bounded Irregular Pyramid\r\n(BIP) [5] to 3D images. Thus, the 3D-BIP is a mixture of both types\r\nof pyramids whose goal is to combine their advantages: the low computational\r\ncost of regular pyramids with the consistent and useful results\r\nprovided by the irregular ones. Specifically, its data structure combines a\r\nregular decimation process with an union-find strategy to build the successive\r\n3D levels of the structure. Experimental results show that this\r\napproach is able to provide a low-level segmentation of 3D images at a\r\nlow computational cost.\r\n1",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Torres Garcia, F., Marfill, R., &#38; Bandera, A. (2009). 3D Image Segmentation Using the Bounded Irregular Pyramid. In <i>Pattern Recognition The Journal of the Pattern Recognition Society</i> (pp. 979–986). http://hdl.handle.net/20.500.12708/53884</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Fuensanta",
                    "last_name": "Torres Garcia",
                    "position": 1,
                    "role": "Author",
                    "tid": "235851"
                },
                {
                    "first_name": "Rebeca",
                    "last_name": "Marfill",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Antonio",
                    "last_name": "Bandera",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57559",
            "handle": "20.500.12708/53887",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Hierarchical Interactive Image Segmentation using Irregular Pyramids",
            "keywords": [],
            "abstract": "In this paper we describe modifications of irregular image\r\nsegmentation pyramids based on user-interaction. We first build a hierarchy\r\nof segmentations by the minimum spanning tree based method,\r\nthen regions from different (granularity) levels are combined to a final\r\n(better) segmentation with user-specified operations guiding the segmentation\r\nprocess. Based on these operations the users can produce a final\r\nimage segmentation that best suits their applications. This work can be\r\nused for applications where we need accuracy in image segmentation, in\r\nannotating images or creating ground truth among others.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gerstmayer, M., Haxhimusa, Y., &#38; Kropatsch, W. (2011). Hierarchical Interactive Image Segmentation using Irregular Pyramids. In <i>Lecture Notes in Computer Science 6658</i> (pp. 245–254). Springer-Verlag Berlin Heidelberg 2011. http://hdl.handle.net/20.500.12708/53887</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Gerstmayer",
                    "position": 1,
                    "role": "Author",
                    "tid": "42581"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57561",
            "handle": "20.500.12708/53889",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Diffusion runs low on persistence fast",
            "keywords": [],
            "abstract": "Interpreting an image as a function on a compact subset of \r\nthe Euclidean plane, we get its scale-space by diffusion, \r\nspreading the image over the entire plane. This generates \r\na 1-parameter family of functions alternatively deï¬ ned\r\nas convolutions with a progressively wider Gaussian kernel. \r\nWe prove that the corresponding 1-parameter family of\r\npersistence diagrams have norms that go rapidly to zero as\r\ntime goes to inï¬ nity. This result rationalizes experimental\r\nobservations about scale-space. We hope this will lead to\r\ntargeted improvements of related computer vision methods.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Chen, C., &#38; Edelsbrunner, H. (2011). Diffusion runs low on persistence fast. In <i>13th IEEE International Conference on Computer Vision</i> (pp. 1–8). http://hdl.handle.net/20.500.12708/53889</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Chao",
                    "last_name": "Chen",
                    "position": 1,
                    "role": "Author",
                    "tid": "179282"
                },
                {
                    "first_name": "Herbert",
                    "last_name": "Edelsbrunner",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57567",
            "handle": "20.500.12708/53895",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Incremental-Decremental Algorithm for Computing AT-Models and Persistent Homology",
            "keywords": [],
            "abstract": "In this paper, we establish a correspondence between the incremental\r\nalgorithm for computing AT-models [8,9] and the one for computing\r\npersistent homology [6,14,15]. We also present a decremental algorithm\r\nfor computing AT-models that allows to extend the persistence\r\ncomputation to a wider setting. Finally, we show how to combine incremental\r\nand decremental techniques for persistent homology computation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">González-Díaz, R., Ion, A., Jose Jimenez, M., &#38; Poyates, R. (2011). Incremental-Decremental Algorithm for Computing AT-Models and Persistent Homology. In <i>CAIP 2011, Part I, LNCS 6854</i> (pp. 286–293). Proc. CAIP 2011 - 14th International Conference on Computer Analysis of Images and Patterns, LNCS 6854, Ainhoa Berciano et al. Eds./Springer. http://hdl.handle.net/20.500.12708/53895</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "R.",
                    "last_name": "González-Díaz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Maria",
                    "last_name": "Jose Jimenez",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Regina",
                    "last_name": "Poyates",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57574",
            "handle": "20.500.12708/53902",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Persistent homology computation with a twist",
            "keywords": [],
            "abstract": "The persistence diagram of a filtered simplicial complex\r\nis usually computed by reducing the boundary\r\nmatrix of the complex. We introduce a simple optimization\r\ntechnique: by processing the simplices of\r\nthe complex in decreasing dimension, we can &quot;kill&quot;\r\ncolumns (i.e., set them to zero) without reducing\r\nthem. This technique completely avoids reduction on\r\nroughly half of the columns. We demonstrate that\r\nthis idea significantly improves the running time of\r\nthe reduction algorithm in practice. We also give an\r\noutput-sensitive complexity analysis for the new algorithm\r\nwhich yields to sub-cubic asymptotic bounds\r\nunder certain assumptions.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Chen, C., &#38; Kerber, M. (2011). Persistent homology computation with a twist. In <i>27th European Workshop on Computational Geometry (EuroCG 2011)</i> (pp. 1–4). http://hdl.handle.net/20.500.12708/53902</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Chao",
                    "last_name": "Chen",
                    "position": 1,
                    "role": "Author",
                    "tid": "179282"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Kerber",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57577",
            "handle": "20.500.12708/53905",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Semantic Segmentation of Microscopic Images Using a Morphological Hierarchy",
            "keywords": [],
            "abstract": "The objective of semantic segmentation in microscopic images is to extract the cellular, nuclear or tissue components. This problem is challenging due to the large variations of these components features (size, shape, orientation or texture). In this paper we present an automatic technique to robustly identify the epithelial nuclei (crypt) against interstitial nuclei in microscopic images taken from colon tissues. The relationship between the histological structures (epithelial layer, lumen and stroma) and the ring like shape of the crypt are considered. The crypt inner boundary is detected using a closing morphological hierarchy and its associated binary hierarchy. The outer border is determined by the epithelial nuclei, overlapped by the maximal isoline of the inner boundary. The evaluation of the proposed method is made by computing the percentage of the mis-segmented nuclei against epithelial nuclei per crypt.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Smochina, C., Manta, V., &#38; Kropatsch, W. (2011). Semantic Segmentation of Microscopic Images Using a Morphological Hierarchy. In <i>The 14th International Conference on Computer Analysis of Images and Patterns (CAIP), volume 6854 of Lecture Notes in Computer Science</i> (pp. 102–109). Proc. CAIP 2011 - 14th International Conference on Computer Analysis of Images and Patterns, LNCS 6854, Ainhoa Berciano et al. Eds./Springer. http://hdl.handle.net/20.500.12708/53905</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Cristian",
                    "last_name": "Smochina",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Vasile",
                    "last_name": "Manta",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57578",
            "handle": "20.500.12708/53906",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluation of Facial Reconstructive Surgery on Patients with Facial Palsy using Optical Strain",
            "keywords": [],
            "abstract": "We explore marker-less tracking methods for the purpose of\r\nevaluating the efficacy of facial re-constructive surgery on patients with\r\nfacial palsies. After experimenting with several optical flow methods, we\r\nchoose an approach that results in less than 2 pixels in tracking error for\r\n15 markers tracked on the face. A novel method is presented that utilizes\r\nthe non-rigid deformation observed on facial skin tissue to visualize the\r\nseverity of facial paralysis. Results are given on a dataset that contains\r\nthree videos of an individual recorded using a standard definition camera\r\nboth before and after undergoing facial reconstructive surgery over a\r\nperiod of three years.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Shreve, M., Jain, N., Goldgof, D., Sarkar, S., &#38; Kropatsch, W. (2011). Evaluation of Facial Reconstructive Surgery on Patients with Facial Palsy using Optical Strain. In <i>The 14th International Conference on Computer Analysis of Images and Patterns (CAIP), volume 6854 of Lecture Notes in Computer Science</i> (pp. 512–519). Proc. CAIP 2011 - 14th International Conference on Computer Analysis of Images and Patterns, LNCS 6854, Ainhoa Berciano et al. Eds./Springer. http://hdl.handle.net/20.500.12708/53906</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Matthew",
                    "last_name": "Shreve",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Neeha",
                    "last_name": "Jain",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Dimitri",
                    "last_name": "Goldgof",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Sudeep",
                    "last_name": "Sarkar",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 5,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57580",
            "handle": "20.500.12708/53908",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Characterizing Obstacle- Avoiding Paths using Cohomology Theory",
            "keywords": [],
            "abstract": "In this paper, we investigate the problem of analyzing the\r\nshape of obstacle-avoiding paths in a space. Given a d-dimensional space\r\nwith holes, representing obstacles, we ask if certain paths are equivalent,\r\ninformally if one path can be continuously deformed into another,\r\nwithin this space. Algebraic topology is used to distinguish between topologically\r\ndifferent paths. A compact yet complete signature of a path is\r\nconstructed, based on cohomology theory. Possible applications include\r\nassisted living, residential, security and environmental monitoring. Numerical\r\nresults will be presented in the final version of this paper.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dlotko, P., Kropatsch, W., &#38; Wagner, H. (2011). Characterizing Obstacle- Avoiding Paths using Cohomology Theory. In <i>Lecture Notes in Computer Science</i> (pp. 310–317). Proc. CAIP 2011 - 14th International Conference on Computer Analysis of Images and Patterns, LNCS 6854, Ainhoa Berciano et al. Eds./Springer. http://hdl.handle.net/20.500.12708/53908</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Pawel",
                    "last_name": "Dlotko",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Hubert",
                    "last_name": "Wagner",
                    "position": 3,
                    "role": "Author",
                    "tid": "217144"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57583",
            "handle": "20.500.12708/53911",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Sampling Step Importance in Hierarchical Semantic Segmentation of Microscopic Images",
            "keywords": [],
            "abstract": "The objective of semantic segmentation in\r\nmicroscopic images is to extract the cellular, nuclear or tissue\r\ncomponents. This problem is challenging due to the large\r\nvariations of these components features (size, shape,\r\norientation or texture). In this paper we improve the\r\ntechnique presented in [17] used to identify the epithelial\r\nnuclei (crypt) against interstitial nuclei in microscopic images\r\ntaken from colon tissues. In the proposed enhanced approach,\r\nthe crypt inner boundary is detected using the closing\r\nmorphological pyramid instead of morphological hierarchy.\r\nThe outer crypt border is determined by the epithelial nuclei,\r\noverlapped by the maximal isoline of the inner boundary. The\r\nuse of sampling in building the pyramid offers computational\r\nefficiency, reduces the amount of used memory, increase the\r\nrobustness and preserve the quality results. An analysis of the\r\ntwo approaches is performed considering the number of pixels\r\nprocessed to create each level. Also the relation between the\r\nlevels of the hierarchical structures is established.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Smochina, C., Manta, V., &#38; Kropatsch, W. (2011). Sampling Step Importance in Hierarchical Semantic Segmentation of Microscopic Images. In <i>15th International Conference on System Theory, Control and Computing 2011</i> (pp. 1–6). http://hdl.handle.net/20.500.12708/53911</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Cristian",
                    "last_name": "Smochina",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Vasile",
                    "last_name": "Manta",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57585",
            "handle": "20.500.12708/53913",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Epithelial Area Detection in Cytokeratin Microscopic Images Using MSER Segmentation in Anisotropic Pyramid",
            "keywords": [],
            "abstract": "The objective of semantic segmentation in microscopic images is to extract the cellular, nuclear or tissue components. This problem is challenging due to the large variations of features of these components (size, shape, orientation or texture). In this paper we present an automatic technique to robustly delimit the epithelial area (crypts) in microscopic images taken from colon tissues sections marked with cytokeratin-8. The epithelial area is highlighted using the anisotropic diffusion pyramid and segmented using MSER+. The crypts separation and lumen detection is performed by imposing topological constraints about the epithelial layer distribution within the tissue and the round-like shape of the crypt. The evaluation of the proposed method is made by comparing the results with ground-truth segmentations.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Smochina, C., Rogojanu, R., Manta, V., &#38; Kropatsch, W. (2011). Epithelial Area Detection in Cytokeratin Microscopic Images Using MSER Segmentation in Anisotropic Pyramid. In <i>Lecture Notes in Bioinformatics, The 6th IAPR International Conference on Pattern Recognition in Bioinformatics, volume 7036</i> (pp. 318–329). Lecture Notes in Bioinformatics. http://hdl.handle.net/20.500.12708/53913</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Cristian",
                    "last_name": "Smochina",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Radu",
                    "last_name": "Rogojanu",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Vasile",
                    "last_name": "Manta",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57587",
            "handle": "20.500.12708/53915",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Describing When and Where in Vision",
            "keywords": [],
            "abstract": "Different from the what and where pathways in the organization\r\nof the visual system, we address representations that describe\r\ndynamic visual events in a unified way.\r\nRepresentations are an essential tool for any kind of process that operates\r\non data, as they provide a language to describe, store and retrieve\r\nthat data. They define the possible properties and aspects that\r\nare stored, and govern the levels of abstraction at which the respective\r\nproperties are described. In the case of visual computing (computer vision,\r\nimage processing), a representation is used to describe information\r\nobtained from visual input (e.g. an image or image sequence and the\r\nobjects it may contain) as well as related prior knowledge (experience).\r\nThe ultimate goal, to make applications of visual computing be part\r\nof our daily life, requires that vision systems operate reliably, nearly\r\nanytime and anywhere. Therefore, the research community aims to solve\r\nincreasingly more complex scenarios. Vision both in humans and computers\r\nis a dynamic process, thus variations (change) always appear in\r\nthe spatial and the temporal dimensions. Nowadays significant research\r\nefforts are undertaken to represent variable shape and appearance, however,\r\njoint representation and processing of spatial and temporal domains\r\nis not a well-investigated topic yet. Visual computing tasks are\r\nmostly solved by a two-stage approach of frame-based processing and\r\nsubsequent temporal processing. Unfortunately, this approach reaches\r\nits limits in scenes with high complexity or difficult tasks e.g. action\r\nrecognition. Therefore, we focus our research on representations which\r\njointly describe information in space and time and allow to process data\r\nof space-time volumes (several consecutive frames).\r\nIn this keynote we relate our own experience and motivations, to the\r\ncurrent state of the art of representations of shape, of appearance, of\r\nstructure, and of motion. Challenges for such representations are in applications\r\nlike multiple object tracking, tracking non-rigid objects and\r\nhuman action recognition.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W., Ion, A., &#38; Artner, N. (2011). Describing When and Where in Vision. In <i>The 16th International Congress on Pattern Recognition, CIARP 2011, volume 7042 of Lecture Notes in Computer Science</i> (pp. 25–26). Lecture Notes on Computer Science. http://hdl.handle.net/20.500.12708/53915</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 3,
                    "role": "Author",
                    "tid": "37618"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "57670",
            "handle": "20.500.12708/53995",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Chronic Pain Rehabilitation with a Serious Game using Multimodal Input",
            "keywords": [],
            "abstract": "Rehabilitation for chronic pain follows a\r\nmultidisciplinary approach, which despite the effort, often\r\nlacks the long term success and patients often fail to translate\r\nthe skills learned in therapy to every day life. Serious games\r\nare hypothesized to support patients to self manage their\r\ncomplaints and keep training their physical functions by\r\nthemselves, especially, when the game is controlled by the\r\npatient´s own body performance. In this paper we present the\r\nimplementation of a system providing multimodal input,\r\nincluding our own full body motion capture system, a low cost\r\nmotion capture system (Microsoft Kinect) and biosignal\r\nacquisition devices to a game engine. In addition, a workflow\r\nhas been established, that enables the use of the acquired\r\nmultimodal data for serious games in a medical environment.\r\nFinally, a serious game has been implemented, targeting\r\nrehabilitation of patients with chronic pain of the lower back\r\nand neck. The focus of this work is on the multimodal input\r\nand how it is used in a game to support rehabilitation of\r\nchronic pain patients. A brief comparison of a marker-based\r\nfull body MoCap system and Microsoft´s Kinect is included.\r\nPreliminary results of tests currently underway are provided.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., Pintaric, T., Kaufmann, H., Jansen-Kosterink, S., &#38; Vollenbroek-Hutten, M. (2011). Chronic Pain Rehabilitation with a Serious Game using Multimodal Input. In <i>Proceedings of Virtual Rehabilitation 2011</i> (p. 8). http://hdl.handle.net/20.500.12708/53995</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Pintaric",
                    "position": 2,
                    "role": "Author",
                    "tid": "39909"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Stephanie",
                    "last_name": "Jansen-Kosterink",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Miriam",
                    "last_name": "Vollenbroek-Hutten",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "57671",
            "handle": "20.500.12708/53996",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Wide Area Motion Tracking Using Consumer Hardware",
            "keywords": [],
            "abstract": "In this paper, we present a wide area tracking system based on consumer hardware and available motion capture modules and middleware. We are using multiple depth cameras for human pose tracking in order to increase the captured space. Commercially available cameras can capture human movements in a non-intrusive way, while associated software-modules produce pose information of a simplified skeleton model. We calibrate the cameras relatively to each other to seamlessly combine their tracking data. Our design allows an arbitrary number of sensors to be integrated and used in parallel over a local area network. This enables us to capture human movements in a large arbitrarily shaped area. In addition we can improve motion capture data in regions, where the field of view of multiple cameras overlaps, by mutually completing partly occluded poses. In various examples we demonstrate, how human pose data is being merged in order to cover a wide area and how this data can easily be used for character animation in a virtual environment.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., &#38; Kaufmann, H. (2011). Wide Area Motion Tracking Using Consumer Hardware. In <i>Proceedings of Workshop on Whole Body Interaction in Games and Entertainment</i> (p. 6). http://hdl.handle.net/20.500.12708/53996</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "57672",
            "handle": "20.500.12708/53997",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Full Body Motion Capture - A Flexible Marker Based Solution",
            "keywords": [],
            "abstract": "Full body motion capture (MoCap) plays an increasingly im-portant role in many fields from entertainment to medicine. How-ever, accurate motion capture systems have not been used in some application areas due to reasons like cost, inflexibility and complexity of operation. We developed a complete marker based optical MoCap system, which targets these issues. In this paper we describe how a flexible calibration method, robust skeleton tracking and interfaces to third party software have been devel-oped. To demonstrate robustness, reliability and accuracy, our system has been successfully employed to provide input to a serious game targeting rehabilitation of patients with chronic pain of the lower back and neck, a group that has previously been neglected by serious games. A brief overview of the application and preliminary results of a medical evaluation with chronic pain patients is provided.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., Pintaric, T., &#38; Kaufmann, H. (2012). Full Body Motion Capture - A Flexible Marker Based Solution. In <i>Proceedings of Workshop on Accessibility Engineering with user models, simulation and VR</i> (p. 8). http://hdl.handle.net/20.500.12708/53997</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Pintaric",
                    "position": 2,
                    "role": "Author",
                    "tid": "39909"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06"
            ],
            "pid": "57876",
            "handle": "20.500.12708/54195",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "How to Select and Customize Object Recognition Approaches for an Application?",
            "keywords": [],
            "abstract": "Recently, object recognition has been successfully implemented in a couple of multimedia content annotation and retrieval applications. The employed recognition approaches are carefully selected and adapted to the specific needs of their tasks. In this work, we propose a framework to automate the simultaneous selection and customization of the entire recognition process. This framework only requires an annotated set of sample images or videos and precisely specified task requirements to select an appropriate setup among thousands of possibilities. We use an efficient recognition infrastructure and iterative analysis strategies to make this approach practicable for real-world applications. A case study for face recognition from a single image per person demonstrates the capabilities of this holistic approach.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sorschag, R. (2012). How to Select and Customize Object Recognition Approaches for an Application? In K. Schoeffmann, B. Marialdo, A. G. Hauptmann, C.-W. Ngo, Y. Andreopoulos, &#38; C. Breiteneder (Eds.), <i>Advances in Multimedia Modeling</i> (pp. 452–462). Lecture Notes in Computer Science / Springer. https://doi.org/10.1007/978-3-642-27355-1_42</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Robert",
                    "last_name": "Sorschag",
                    "position": 1,
                    "role": "Author",
                    "tid": "69044"
                },
                {
                    "first_name": "Klaus",
                    "last_name": "Schoeffmann",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Bernard",
                    "last_name": "Marialdo",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Alexander G.",
                    "last_name": "Hauptmann",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Chong-Wah",
                    "last_name": "Ngo",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Yiannis",
                    "last_name": "Andreopoulos",
                    "position": 5,
                    "role": "Editor"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 6,
                    "role": "Editor",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "57927",
            "handle": "20.500.12708/54246",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Multimodal motion guidance",
            "keywords": [],
            "abstract": "The ability to guide human motion through automatically\r\ngenerated feedback has significant potential for applications\r\nin areas, such as motor learning, human-computer interaction, telepresence, and augmented reality.\r\nThis paper focuses on the design and development of such\r\nsystems from a human cognition and perception perspective. We analyze the dimensions of the design space for motion guidance systems, spanned by technologies and human information processing, and identify opportunities for new feedback techniques.\r\nWe present a novel motion guidance system, that was implemented based on these insights to enable feedback for position, direction and continuous velocities. It uses motion capture to track a user in space and guides using visual, vibrotactile and pneumatic actuation. Our system also introduces motion retargeting through time warping, motion dynamics and prediction, to allow more flexibility and adapt-ability to user performance",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., Fukushi, K., Olwal, A., Kaufmann, H., &#38; Raskar, R. (2012). Multimodal motion guidance. In <i>Proceedings of the 14th ACM international conference on Multimodal interaction - ICMI ’12</i>. ICMI 2012, 14th ACM International Conference on Multimodal Interaction, Santa Monica, CA, USA, Non-EU. ACM. https://doi.org/10.1145/2388676.2388706</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Kenichiro",
                    "last_name": "Fukushi",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Alex",
                    "last_name": "Olwal",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Ramesh",
                    "last_name": "Raskar",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "57928",
            "handle": "20.500.12708/54247",
            "doi": null,
            "year": 2000,
            "issued": "2000",
            "issued_on": "2000-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Construct3D: A Virtual Reality Application for Mathematics and Geometry Education",
            "keywords": [],
            "abstract": "Construct3D is a three dimensional geometric construction tool based on the collaborative augmented reality system &quot;Studierstube&quot;. Our setup uses a stereoscopic head mounted display (HMD) and the Personal Interaction Panel(PIP) - a two-handed 3D interaction tool that simplifies 3D model interaction. Means of application in mathematics and geometry education at high school as well as university level are being discussed. A pilot study summarizes the strengths and possible extensions of our system. Anecdotal evidence supports our claim that the use of Construct3D is easy to learn and encourages experimentation with geometric constructions.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., Schmalstieg, D., &#38; Wagner, M. (2000). Construct3D: A Virtual Reality Application for Mathematics and Geometry Education. In <i>Education and Information Technologies</i> (pp. 263–276). Kluwer Academic Publishers. https://doi.org/10.1023/a:1012049406877</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Dieter",
                    "last_name": "Schmalstieg",
                    "position": 2,
                    "role": "Author",
                    "tid": "110794"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Wagner",
                    "position": 3,
                    "role": "Author",
                    "tid": "65064"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "57930",
            "handle": "20.500.12708/54249",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Physically-Based Depth of Field in Augmented Reality",
            "keywords": [],
            "abstract": "We present a novel method for rendering and compositing video in augmented reality. We focus on calculating\r\nthe physically correct result of the depth of field caused by a lens with finite sized aperture. In order to correctly\r\nsimulate light transport, ray-tracing is used and in a single pass combined with differential rendering to compose\r\nthe final augmented video. The image is fully rendered on GPUs, therefore an augmented video can be produced at\r\ninteractive frame rates in high quality. Our method runs on the fly, no video postprocessing is needed. In addition\r\nwe evaluated the user experiences with our rendering system with the hypothesis that a depth of field effect in\r\naugmented reality increases the realistic look of composited video. Results with 30 users show that 90% perceive\r\nvideos with a depth of field considerably more realistic.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., &#38; Kaufmann, H. (2012). Physically-Based Depth of Field in Augmented Reality. In C. Andujar &#38; E. Puppo (Eds.), <i>Eurographics 2012 - Short Papers</i> (pp. 89–92). Eurographics Association. https://doi.org/10.2312/conf/EG2012/short/089-092</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Carlos",
                    "last_name": "Andujar",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Enrico",
                    "last_name": "Puppo",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "57931",
            "handle": "20.500.12708/54250",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Event-driven body motion analysis for real-time gesture recognition",
            "keywords": [],
            "abstract": "This paper presents an evaluation of spatiotemporal\r\ndata generated by a dynamic stereo vision sensor in a\r\nhighdimensional space (3D volume and time) for motion analysis\r\nand gesture recognition. In contrast to traditional frame-based\r\n(synchronous) stereo cameras, dynamic stereo vision sensors\r\nasynchronously generates events upon scene dynamics. Motion\r\nactivities are intrinsically (on-chip) segmented by the sensor, such\r\nthat activity, gesture recognition and tracking can be intuitively\r\nand efficiently performed. In this work, we investigated the\r\napplicability of this sensor for gesture recognition. We developed\r\na machine lerning method based on the Hidden Markow Model\r\nfor training and automated classifications of gestures using the\r\nevent data generated by the sensor. By training eight different\r\nactivities (dance figures) with 15 persons we build up a library\r\nof 580 recorded activites. An average recognition rate of 97%\r\nhas been reached.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kohn, B., Belbachir, A. N., Hahn, T., &#38; Kaufmann, H. (2012). Event-driven body motion analysis for real-time gesture recognition. In <i>2012 IEEE International Symposium on Circuits and Systems</i>. IEEE Computer Society. https://doi.org/10.1109/iscas.2012.6272132</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Bernhard",
                    "last_name": "Kohn",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 2,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Hahn",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "57932",
            "handle": "20.500.12708/54251",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "High-Quality Reflections, Refractions, and Caustics in Augmented Reality and their Contribution to Visual Coherence",
            "keywords": [],
            "abstract": "In this paper we present a novel high-quality rendering system for\r\nAugmented Reality (AR). We study ray-tracing based rendering\r\ntechniques in AR with the goal of achieving real-time performance\r\nand improving visual quality as well as visual coherence between\r\nreal and virtual objects in a final composited image. A number of\r\nrealistic and physically correct rendering effects are demonstrated,\r\nthat have not been presented in real-time AR environments before.\r\nExamples are high-quality specular effects such as caustics, refraction,\r\nreflection, together with a depth of field effect and antialiasing.\r\n\r\nWe present a new GPU implementation of photon mapping and\r\nits application for the calculation of caustics in environments where\r\nreal and virtual objects are combined. The composited image is\r\nproduced on-the-fly without the need of any preprocessing step. A\r\nmain contribution of our work is the achievement of interactive rendering\r\nspeed for high-quality ray-tracing algorithms in AR setups.\r\n\r\nFinally we performed an evaluation to study how users perceive\r\nvisual quality and visual coherence with different realistic rendering\r\neffects. The results of our user study show that in 40.1% cases users\r\nmistakenly judged virtual objects as real ones. Moreover we show\r\nthat high-quality rendering positively affects the perceived visual\r\ncoherence.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., &#38; Kaufmann, H. (2012). High-Quality Reflections, Refractions, and Caustics in Augmented Reality and their Contribution to Visual Coherence. In <i>Proceedings of International Symposium on Mixed and Augmented Reality (ISMAR)</i> (pp. 99–108). IEEE Computer Society. http://hdl.handle.net/20.500.12708/54251</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "57933",
            "handle": "20.500.12708/54252",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "AR Record & Replay: Situated Compositing of Video Content in Mobile Augmented Reality",
            "keywords": [],
            "abstract": "In this paper we present a novel approach to record and replay video content composited in-situ with a live view of the real environment. Our real-time technique works on mobile phones, and uses an panorama-based tracker to create visually seamless and spatially registered overlay of video content. We apply a temporal foreground-background segmentation of video footage and show how the segmented information can be precisely registered in real-time in the camera view of a mobile phone. We describe the user interface and the video post effects implemented in our prototype as well as our approach with a skateboard training application. Our technique can also be used with online video material and supports the creation of augmented situated documentaries.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langlotz, T., Zingerle, M., Grasset, R., Kaufmann, H., &#38; Reitmayr, G. (2012). AR Record &#38; Replay: Situated Compositing of Video Content in Mobile Augmented Reality. In <i>Proceedings of the 24th Australian Computer-Human Interaction Conference on - OzCHI ’12</i> (pp. 318–326). ACM Press. https://doi.org/10.1145/2414536.2414588</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Tobias",
                    "last_name": "Langlotz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Mathäus",
                    "last_name": "Zingerle",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Raphael",
                    "last_name": "Grasset",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Gerhard",
                    "last_name": "Reitmayr",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "57934",
            "handle": "20.500.12708/54253",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Applications of Virtual and Augmented Reality in Healthcare",
            "keywords": [],
            "abstract": "Augmented Reality (AR) combines the real and the virtual and is interactive in real-time.\r\nIn addition, virtual and real objects are registered in 3D space to pinpoint their positions\r\nfor precise overlays. AR has been in the news a lot in recent years, mainly because of AR\r\nApps on smartphones. Augmented Reality has a lot more to offer though. Hardware setups\r\nare versatile and range from mobile devices to immersive lab installations. Just as versatile\r\nare the application areas ranging from industrial uses (e.g., automotive, manufacturing),\r\ntraining and education, modelling (architectural planning), design, visualization (e.g., scientific,\r\nmedical, and information), entertainment and more recently, the widening spectrum\r\nof possibilities in the medical domain, rehabilitation and therapy. An example of the latter\r\nis an EU FP7 ICT project on virtual rehabilitation - PLAYMANCER. It focused on\r\ndeveloping serious games for cognitive behavioural therapy - specifically for patients with\r\neating disorders and pathological gambling, and on serious games for the rehabilitation of\r\nchronic back pain patients. Rehabilitation for chronic pain follows a multidisciplinary approach,\r\nwhich despite the effort, often lacks long term success. Patients fail to translate skills\r\nlearned in therapy to everyday life. In order to encourage continuous training and ensure\r\nimpact at a wider scale when it comes to \"Active Ageing\", technology can and should be\r\nused to motivate people to exercise at home.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2012). Applications of Virtual and Augmented Reality in Healthcare. In K. Wac, D. Hausheer, M. Fiedler, &#38; P. Bonato (Eds.), <i>Dagstuhl Seminar 12231 “Future Internet for eHealth”</i> (pp. 12–13). Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, Germany. https://doi.org/10.4230/DagRep.2.6.1</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Katarzyna",
                    "last_name": "Wac",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "David",
                    "last_name": "Hausheer",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Fiedler",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Paolo",
                    "last_name": "Bonato",
                    "position": 4,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06"
            ],
            "pid": "57940",
            "handle": "20.500.12708/54259",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Detection and Classification of Petroglyphs in Gigapixel Images",
            "keywords": [],
            "abstract": "With the advances of digital photography, the number of high quality images of rock panels containing petroglyphs grows steadily. Different time-consuming manual methods to determine and document the exact shapes and spatial locations of petroglyphs on a panel have been carried out over decades. We aim at automated methods to a) segment rock images with petroglyphs, b) classify the petroglyphs and c) retrieve similar petroglyphs from different archives. In this short paper, we present an approach for the unsolved problem of rock art image segmentation. A first evaluation demonstrates promising results.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Seidl, M., &#38; Breiteneder, C. (2011). Detection and Classification of Petroglyphs in Gigapixel Images. In F. Niccolucci, M. Dellepiane, S. Pena Serna, H. Rushmeier, &#38; L. Van Gool (Eds.), <i>VAST 11</i> (pp. 45–48). Eurographics Association. https://doi.org/10.2312/PE/VAST/VAST11S/045-048</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Seidl",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 2,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Franco",
                    "last_name": "Niccolucci",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Matteo",
                    "last_name": "Dellepiane",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Sebastian",
                    "last_name": "Pena Serna",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Holly",
                    "last_name": "Rushmeier",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Luc",
                    "last_name": "Van Gool",
                    "position": 5,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06"
            ],
            "pid": "57943",
            "handle": "20.500.12708/54262",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Recurring Element Detection in Movies",
            "keywords": [],
            "abstract": "Recurring elements in movies contribute significantly to the development of narration, themes, or even mood. The detection of such elements is impeded by the large variance of their visual appearance and usually relies on the experience and attentiveness of the viewer. In this paper, we present a new approach for the automated detection of recurring elements in movies such as motifs and main characters. Performed experiments show the reliability of the algorithm and its potential for automated high-level film analysis.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaharieva, M., &#38; Breiteneder, C. (2012). Recurring Element Detection in Movies. In <i>Lecture Notes in Computer Science</i> (pp. 222–232). Springer. https://doi.org/10.1007/978-3-642-27355-1_22</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 2,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57953",
            "handle": "20.500.12708/54272",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Document Understanding of Graphical Content in Natively Digital PDF Documents",
            "keywords": [],
            "abstract": "This paper presents an object-based method for analysing the content drawn by graphical operators in natively digital PDF documents. We propose that graphical content in a document can be classified either as structural or non-structural and present an output model for our analysis result. Heuristic techniques are used to group the instructions into regions and determine their logical role in the document's structure. Experimental results demonstrate the effectiveness of the algorithm.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gabdulkhakova, A., &#38; Hassan, T. (2012). Document Understanding of Graphical Content in Natively Digital PDF Documents. In <i>DocEng’12 Proceedings of the 2012 ACM symposium on Document engineering</i> (pp. 137–140). http://hdl.handle.net/20.500.12708/54272</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Aysylu",
                    "last_name": "Gabdulkhakova",
                    "position": 1,
                    "role": "Author",
                    "tid": "235653"
                },
                {
                    "first_name": "Tamir",
                    "last_name": "Hassan",
                    "position": 2,
                    "role": "Author",
                    "tid": "44748"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57963",
            "handle": "20.500.12708/54281",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Initializing 3D Model-based Tracking of Dice",
            "keywords": [],
            "abstract": "The initialization process of a 3D model-based tracking\r\nmethod with an extremely fast checking time is presented.\r\nOne initialized, the algorithm will execute a 3D model-based\r\ntracking with pose determination. The tracking method is a\r\ntwo-steps approach, a prediction-correction method. We are\r\nstudying the possibility of finding the minimum group of interest\r\npoints and a set of rules which form all the possible\r\nmovements of a 3D tracked object. Thereby, the computation\r\ntime-memory for the tracking task could be reduced. We consider\r\nrigid objects which can be approximately modeled by a\r\nconvex polyhedral shape. Our approach works in monocular\r\nvideos and where the position of the camera is fixed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Torres Garcia, F., Kropatsch, W., &#38; Artner, N. (2012). Initializing 3D Model-based Tracking of Dice. In <i>2012 19th International Conference on Systems, Signals and Image Processing (IWSSIP)</i> (pp. 338–341). http://hdl.handle.net/20.500.12708/54281</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Fuensanta",
                    "last_name": "Torres Garcia",
                    "position": 1,
                    "role": "Author",
                    "tid": "235851"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 3,
                    "role": "Author",
                    "tid": "37618"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57964",
            "handle": "20.500.12708/54282",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Top-Down Tracking and Estimating 3D Pose of a Die",
            "keywords": [],
            "abstract": "Real-time 3D pose estimation from monocular image sequences\r\nis a challenging research topic. Although current methods are\r\nable to recover 3D pose, they are severely challenged by the computational\r\ncost. To address this problem, we propose a tracking and 3D pose\r\nestimation method supported by three main pillars: a pyramidal structure,\r\nan aspect graph and the checkpoints. Once initialized the systems\r\nperforms a top-down tracking. At a high level it detects the position of\r\nthe object and segments its time-space trajectory. This stage increases\r\nthe stability and the robustness for the tracking process. Our main objective\r\nis the 3D pose estimation, the pose is estimated only in relevant\r\nevents of the segmented trajectory, which reduces the computational effort\r\nrequired. In order to obtain the 3D pose estimation in the complete\r\ntrajectory, an interpolation method, based on the aspect graph describing\r\nthe structure of the object´s surface, can be used to roughly estimate the\r\nposes between two relevant events. This early version of the method has\r\nbeen developed to work with a specific type of polyhedron with strong edges, \r\ntexture and differentiated faces, a die.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Torres, F., &#38; Kropatsch, W. (2012). Top-Down Tracking and Estimating 3D Pose of a Die. In G. Gimel’farb, E. Hancock, A. Imiya, A. Kuijper, M. Kudo, S. Omachi, T. Windeatt, &#38; K. Yamada (Eds.), <i>Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</i> (pp. 492–500). Springer 2012. https://doi.org/10.1007/978-3-642-34166-3_54</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Fuensanta",
                    "last_name": "Torres",
                    "position": 1,
                    "role": "Author",
                    "tid": "235851"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Georgy",
                    "last_name": "Gimel’farb",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Edwin",
                    "last_name": "Hancock",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Atsushi",
                    "last_name": "Imiya",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Arjan",
                    "last_name": "Kuijper",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Mineichi",
                    "last_name": "Kudo",
                    "position": 5,
                    "role": "Editor"
                },
                {
                    "first_name": "Shinichiro",
                    "last_name": "Omachi",
                    "position": 6,
                    "role": "Editor"
                },
                {
                    "first_name": "Terry",
                    "last_name": "Windeatt",
                    "position": 7,
                    "role": "Editor"
                },
                {
                    "first_name": "Keiji",
                    "last_name": "Yamada",
                    "position": 8,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57967",
            "handle": "20.500.12708/54285",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Semi-automatic Tracking of Markers in Facial Palsy",
            "keywords": [],
            "abstract": "We introduce a semi-automatic tracking method that\r\ncan be utilized for the analysis of facial markers in the\r\nmedical condition of facial palsy. Tracking of markers\r\nwill help medical physicians in evaluating this medical\r\ncondition quantitatively. We use particle filtering to\r\ntrack markers towards measuring distances needed to\r\nevaluate the degree of facial palsy. We show that by employing\r\ntracking methods, the analysis time is reduced\r\nwithout losing the high accuracy of the results.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Limbeck, P., Kropatsch, W., &#38; Haxhimusa, Y. (2012). Semi-automatic Tracking of Markers in Facial Palsy. In <i>21st International Conference on Pattern Recognition</i> (pp. 69–72). http://hdl.handle.net/20.500.12708/54285</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philip",
                    "last_name": "Limbeck",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 3,
                    "role": "Author",
                    "tid": "64562"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57989",
            "handle": "20.500.12708/54307",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Interactive Labeling of Image Segmentation Hierarchies",
            "keywords": [],
            "abstract": "We study the task of interactive semantic labeling of a given segmentation hierarchy and\r\npresent a framework consisting of two parts: an automatic component, based on a Conditional\r\nRandom Field whose dependencies are de ned by the inclusion tree of the segmentation\r\nhierarchy, and a feedback-loop provided by a human user. Experiments on two data sets\r\nshow higher classi cation rates for the proposed framework than a baseline, that classi es\r\nall segments independently.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y. (2012). Interactive Labeling of Image Segmentation Hierarchies. In <i>Statistische Woche</i> (pp. 71–72). http://hdl.handle.net/20.500.12708/54307</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57990",
            "handle": "20.500.12708/54308",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Segmentation of Building Facade Domes",
            "keywords": [],
            "abstract": "Domes are architectural structural elements typical for ecclesiastical and secular grand buildings, like churches, mosques, palaces, capitols and city halls. The current paper targets the problem of segmentation of domes within the framework of architectural style classification of building facades. We perform segmentation of building facade domes by combining bilateral symmetry detection, graph-based segmentation approaches and image analysis and processing technics into a single method. Our algorithm achieves good segmentation results on buildings belonging to variety of architectural styles, such as Renaissanse, Neo-Renaissance, Baroque, Neo-Baroque, Neoclassical and Islamic.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Shalunts, G., Haxhimusa, Y., &#38; Sablatnig, R. (2012). Segmentation of Building Facade Domes. In <i>Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</i> (pp. 324–331). Springer. https://doi.org/10.1007/978-3-642-33275-3_40</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Gayane",
                    "last_name": "Shalunts",
                    "position": 1,
                    "role": "Author",
                    "tid": "193583"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 3,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "57991",
            "handle": "20.500.12708/54309",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Interactive Labeling of Image Segmentation Hierarchies",
            "keywords": [],
            "abstract": "We study the task of interactive semantic labeling of a segmentation hierarchy. \r\nTo this end we propose a framework interleaving two components: an automatic labeling step, \r\nbased on a Conditional Random Field whose dependencies are defined by the inclusion tree of \r\nthe segmentation hierarchy, and an interaction step that integrates incremental input from a human user. \r\nEvaluated on two distinct datasets, the proposed interactive approach efficiently integrates human interventions \r\nand illustrates the advantages of structured prediction in an interactive framework",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zankl, G., Haxhimusa, Y., &#38; Ion, A. (2012). Interactive Labeling of Image Segmentation Hierarchies. In <i>Lecture Notes in Computer Science</i> (pp. 11–20). Springer. https://doi.org/10.1007/978-3-642-32717-9_2</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Zankl",
                    "position": 1,
                    "role": "Author",
                    "tid": "42713"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "57992",
            "handle": "20.500.12708/54310",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Architectural Style Classification of Domes",
            "keywords": [],
            "abstract": "Domes are architectural structural elements characteristic\r\nfor ecclesiastical and secular monumental buildings, like churches, basilicas,\r\nmosques, capitols and city halls. In the scope of building facade\r\narchitectural style classification the current paper addresses the problem\r\nof architectural style classification of facade domes. Building facade\r\nclassification by architectural styles is achieved by classification and voting\r\nof separate architectural elements, like domes, windows, towers, etc.\r\nTypical forms of the structural elements bear the signature of each architectural\r\nstyle. Our approach classifies domes of three architectural styles\r\n- Renaissance, Russian and Islamic. We present a three-step approach,\r\nwhich in the first step analyzes the height and width of the dome for the\r\nidentification of Islamic saucer domes, in the second step detects golden\r\ncolor in YCbCr color space to determine Russian golden onion domes\r\nand in the third step performs classification based on dome shapes, using\r\nclustering and learning of local features. Thus we combine three features\r\n- the relation of dome width and height, color and shape, in a single\r\nmethodology to achieve high classification rate.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Shalunts, G., Haxhimusa, Y., &#38; Sablatnig, R. (2012). Architectural Style Classification of Domes. In <i>Advances in Visual Computing</i> (pp. 420–429). Springer 2012. https://doi.org/10.1007/978-3-642-33191-6_41</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Gayane",
                    "last_name": "Shalunts",
                    "position": 1,
                    "role": "Author",
                    "tid": "193583"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 3,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "58021",
            "handle": "20.500.12708/54339",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Maschinelle Verarbeitung von Digitalisaten - Stand der Technik und Forschungsrichtungen",
            "keywords": [],
            "abstract": "Einleitung\r\n-\r\nScan-Anforderungen\r\n&#61607;\r\nDas OCR-Verfahren\r\n-\r\nBinarisierung und Layoutanalyse\r\n-\r\nZeichenerkennung\r\n-\r\nProblemfälle\r\n&#61607;\r\nForschungsthemen\r\n-\r\nLogische und semantische Strukturerkennung\r\n-\r\nTypografie",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hassan, T. (2012). Maschinelle Verarbeitung von Digitalisaten - Stand der Technik und Forschungsrichtungen. In <i>Schnittstelle: Von der Digitalisierungsidee zur digitalen Bibliothek Wege für Museen, Bibliotheken und Archive in die Europeana</i>. Schnittstelle: Von der Digitalisierungsidee zur digitalen Bibliothek Wege für Museen, Bibliotheken und Archive in die Europeana, 1080 Wien, Don Juan Archiv, Austria. http://hdl.handle.net/20.500.12708/54339</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Tamir",
                    "last_name": "Hassan",
                    "position": 1,
                    "role": "Author",
                    "tid": "44748"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "58023",
            "handle": "20.500.12708/54341",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A methodology for evaluating algorithms for table understanding in PDF documents",
            "keywords": [],
            "abstract": "This paper presents a methodology for the evaluation of\r\ntable understanding algorithms for PDF documents. The\r\nevaluation takes into account three major tasks: table detection,\r\ntable structure recognition and functional analysis.\r\nWe provide a general and \r\nexible output model for\r\neach task along with corresponding evaluation metrics and\r\nmethods. We also present a methodology for collecting\r\nand ground-truthing PDF documents based on consensusreaching\r\nprinciples and provide a publicly available groundtruthed\r\ndataset.\r\nCategories and Subject Descriptors: I.7.5\r\n[Document and Text Processing]: Document Capture|\r\ndocument analysis; H.3.4 [Information Storage and Re-\r\ntrieval]: Systems and Software|performance evaluation\r\nKeywords: Table processing, metrics, ground-truth\r\ndataset, performance evaluation, document analysis, document\r\nunderstanding",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Göbel, M., Hassan, T., Oro, E., &#38; Orsi, G. (2012). A methodology for evaluating algorithms for table understanding in PDF documents. In <i>Proceedings of the 2012 ACM symposium on Document engineering - DocEng ’12</i>. DocEng 2012 ACM Symposium on Document Engineering, Pairs, France, EU. ACM New York, NY, USA ©2012. https://doi.org/10.1145/2361354.2361365</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Max",
                    "last_name": "Göbel",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Tamir",
                    "last_name": "Hassan",
                    "position": 2,
                    "role": "Author",
                    "tid": "44748"
                },
                {
                    "first_name": "Ermelinda",
                    "last_name": "Oro",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Giorgio",
                    "last_name": "Orsi",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58054",
            "handle": "20.500.12708/54372",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Design eines Serious Games für die Rehabilitation von chronischen Rückenschmerzen",
            "keywords": [],
            "abstract": "Die Rehabilitation von Patienten mit chronischen Rückenschmerzen verfolgt einen multidisziplinären Ansatz, welcher trotz des betriebenen Aufwandes oft den langfristigen Erfolg vermissen lässt. Serious Games sollen an dieser Stelle helfen die Patienten zu motivieren und den langfristigen Erfolg der Therapie zu verbessern. Spiele in der Rehabilitation im Allgemeinen und in der Anwendung für die Therapie von chronischen Rückenschmerzen im Speziellen haben jedoch spezifische Anforderungen an das Gamedesign. Wir haben diese Anforderungen in einem iterativen Designprozess in ein Serious Game umgesetzt. Spezielle Features sind Konfigurierbarkeit und ein Belohnungssystem, das mangelnden Therapiefortschritt nicht zusätzlich bestraft. Für das Spiel verwenden wir dabei innovative Interaktionstechnologien, welche die Möglichkeit bieten exakte Bewegungsdaten sowohl zur Interaktion mit dem Spiel, als auch in einer medizinischen Analyse zu verwenden. Eine Studie mit acht Patienten zeigt gute Benutzbarkeit und dass das Serious Game auch Spaß macht. Außerdem konnten die meisten Patienten ihren physischen Zustand in Bezug auf die trainierten Parameter in der Therapie verbessern.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., Kaufmann, H., Jansen-Kosterink, S., &#38; Vollenbroek-Hutten, M. (2012). Design eines Serious Games für die Rehabilitation von chronischen Rückenschmerzen. In <i>Future and Reality of Gaming Vienna Games Conference \\ FROG 2012 ///Game Over. Was nun? Vom Nutzen und Nachteil des digitalen Spiels für das Leben</i> (pp. 31–46). Bundesministerium für Wirtschaft, Familie und Jugend, Abt. II/5. http://hdl.handle.net/20.500.12708/54372</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Stephanie",
                    "last_name": "Jansen-Kosterink",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Miriam",
                    "last_name": "Vollenbroek-Hutten",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "58065",
            "handle": "20.500.12708/54383",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Classification and Pose Estimation of Vehicles in Videos by 3D Modeling within Discrete-Continuous Optimization",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hödlmoser, M., Micusik, B., Liu, M. Y., Pollefeys, M., &#38; Kampel, M. (2012). Classification and Pose Estimation of Vehicles in Videos by 3D Modeling within Discrete-Continuous Optimization. In <i>2012 Second International Conference on 3D Imaging, Modeling, Processing, Visualization &#38;amp; Transmission</i>. International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), Zürich, Schweiz, Non-EU. IEEE. https://doi.org/10.1109/3dimpvt.2012.23</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Hödlmoser",
                    "position": 1,
                    "role": "Author",
                    "tid": "65240"
                },
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 2,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Ming Yiu",
                    "last_name": "Liu",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Marc",
                    "last_name": "Pollefeys",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 5,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "58074",
            "handle": "20.500.12708/54392",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Classification of Gothic and Baroque architectural elements",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Shalunts, G., Haxhimusa, Y., &#38; Sablatnig, R. (2012). Classification of Gothic and Baroque architectural elements. In <i>Proceedings of the 19th International Conference on Systems, Signals and Image Processing</i> (pp. 330–333). http://hdl.handle.net/20.500.12708/54392</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Gayane",
                    "last_name": "Shalunts",
                    "position": 1,
                    "role": "Author",
                    "tid": "193583"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 3,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E194-04"
            ],
            "pid": "58123",
            "handle": "20.500.12708/54441",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Robust camera self-calibration from monocular images of Manhattan worlds",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Wildenauer, H., &#38; Hanbury, A. (2012). Robust camera self-calibration from monocular images of Manhattan worlds. In <i>2012 IEEE Conference on Computer Vision and Pattern Recognition</i>. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Providence, RI, USA, Non-EU. https://doi.org/10.1109/cvpr.2012.6248008</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "H.",
                    "last_name": "Wildenauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "51087"
                },
                {
                    "first_name": "A.",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "58132",
            "handle": "20.500.12708/54450",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A concept for shape representation with linked local coordinate systems",
            "keywords": [],
            "abstract": "This paper discusses a concept for the repre-\r\nsentation of n-dimensional shapes by means of a\r\nmodel, based on linked local coordinate systems.\r\nThrough application of the medial axis transform\r\n(MAT) and decomposition of the resulting medial\r\naxis (MA), articulated, as well as non-rigid abstract\r\nn-dimensional bodies can be described by defining\r\ncorresponding local coordinate systems for each ele-\r\nment. This should allow a distinct and invariant rep-\r\nresentation of every point of the shape, which can be\r\nused for complex composite transformations of the\r\nobject in the context of robotic manipulation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaindl, M., &#38; Kropatsch, W. (2016). A concept for shape representation with linked local coordinate systems. In L. Cehovin, R. Mandeljc, &#38; V. Struc (Eds.), <i>Proceedings of the 21st Computer Vision Winter Workshop</i> (p. 9). Slovenian Pattern Recognition Society. http://hdl.handle.net/20.500.12708/54450</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Manuela",
                    "last_name": "Kaindl",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Luka",
                    "last_name": "Cehovin",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Rok",
                    "last_name": "Mandeljc",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Vitomir",
                    "last_name": "Struc",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E389-02"
            ],
            "pid": "58181",
            "handle": "20.500.12708/54499",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Top-down 3D Tracking and Pose Estimation of a Die Using Check-points",
            "keywords": [],
            "abstract": "Real-time 3D pose estimation from monocular image\r\nsequences is a challenging research topic. Although\r\ncurrent methods are able to recover 3D pose,\r\nthey require a high computational cost to process\r\nhigh-resolution images in a video sequence at high\r\nframe-rates. To address that problem, we introduce\r\nthe new concept of check-points. They are the minimum\r\nnumber of points needed to detect a 3D object\r\nmotion. Our method tracks the 2D projections of the\r\ncheck-points over a 2D maximum pyramid. To handle\r\nlarge displacements of the object, our approach\r\nevaluates the projection of the check-points at highest\r\nlevels of the pyramid. Moreover, it refines the\r\npose localization by utilizing the check-points at lowest\r\nlevels of the hierarchy. We show that just checking\r\na few cells per frame, our method estimates the\r\n3D pose of the tracked object. This early version\r\nof the method works with a specific type of object a\r\n3D cube, with six well distinguished faces and which\r\nsalient features in all the faces are dots, a die.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Torres Garcia, F., &#38; Kropatsch, W. (2013). Top-down 3D Tracking and Pose Estimation of a Die Using Check-points. In W. Kropatsch, F. Torres Garcia, &#38; G. Ramachandran (Eds.), <i>Proceedings of the 18th Computer Vision Winter Workshop</i> (pp. 102–109). Prip 186/3. http://hdl.handle.net/20.500.12708/54499</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Fuensanta",
                    "last_name": "Torres Garcia",
                    "position": 1,
                    "role": "Author",
                    "tid": "235851"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Fuensanta",
                    "last_name": "Torres Garcia",
                    "position": 2,
                    "role": "Editor",
                    "tid": "235851"
                },
                {
                    "first_name": "Geetha",
                    "last_name": "Ramachandran",
                    "position": 3,
                    "role": "Editor",
                    "tid": "195689"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06"
            ],
            "pid": "58251",
            "handle": "20.500.12708/54569",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automated Petroglyph Image Segmentation with Interactive Classifier Fusion",
            "keywords": [],
            "abstract": "The number of high quality images of rock panels containing petroglyphs grows steadily. Different time-consuming manual methods to determine and document the exact shapes and spatial locations of petrogyphs on a panel have been carried out over decades. The first step for classification and retrieval of petroglyphs is the segmentation of the images. In this paper, we present and evaluate an automated approach to segment petroglyph images.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Seidl, M., &#38; Breiteneder, C. (2012). Automated Petroglyph Image Segmentation with Interactive Classifier Fusion. In <i>Proceedings of the Eighth Indian Conference on Computer Vision, Graphics and Image Processing - ICVGIP ’12</i>. Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP), Chennai, India, Non-EU. https://doi.org/10.1145/2425333.2425399</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Seidl",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 2,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "58273",
            "handle": "20.500.12708/54588",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "3DTouch and HOMER-S",
            "keywords": [],
            "abstract": "Existing interaction techniques for mobile AR often use the multi-touch capabilities of the device´s display for object selection and manipulation. To provide full 3D manipulation by touch in an integral way, existing approaches use complex multi finger and hand gestures. However, they are difficult or impossible to use in one-handed handheld AR scenarios and their usage requires prior knowledge. Furthermore, a handheld´s touch screen offers only two dimensions for interaction and limits manipulation to physical screen size. To overcome these problems, we present two novel intuitive six degree-of-freedom (6DOF) manipulation techniques, 3DTouch and HOMER-S. While 3DTouch uses only simple touch gestures and decomposes the degrees of freedom, Homer-S provides full 6DOF and is decoupled from screen input to overcome physical limitations. In a comprehensive user study, we explore performance, usability and accuracy of both techniques. Therefore, we compare 3DTouch with HOMER-S in four different scenarios with varying transformation requirements. Our results reveal both techniques to be intuitive to translate and rotate objects. HOMER-S lacks accuracy compared to 3DTouch but achieves significant performance increases in terms of speed for transformations addressing all 6DOF.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Venditti, B., Kaufmann, H., &#38; Mossel, A. (2013). 3DTouch and HOMER-S. In <i>Proceedings of the Virtual Reality International Conference: Laval Virtual</i>. Laval Virtual - 15th International Conference on Virtual Reality and Converging Technologies, Laval, France, EU. ACM. https://doi.org/10.1145/2466816.2466829</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Benjamin",
                    "last_name": "Venditti",
                    "position": 1,
                    "role": "Author",
                    "tid": "182097"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 3,
                    "role": "Author",
                    "tid": "58429"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "58274",
            "handle": "20.500.12708/54589",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "DrillSample",
            "keywords": [],
            "abstract": "One of the primary tasks in a dense mobile augmented reality (AR) environment is to ensure precise selection of an object, even if it is occluded or highly similar to surrounding virtual scene objects. Existing interaction techniques for mobile AR usually use the multi-touch capabilities of the device for object selection. However, single touch input is imprecise, but existing two handed selection techniques to increase selection accuracy do not apply for one-handed handheld AR environments. To address the requirements of accurate selection in a one-handed dense handheld AR environment, we present the novel selection technique DrillSample. It requires only single touch input for selection and preserves the full original spatial context of the selected objects. This allows disambiguating and selection of strongly occluded objects or of objects with high similarity in visual appearance. In a comprehensive user study, we compare two existing selection techniques with DrillSample to explore performance, usability and accuracy. The results of the study indicate that DrillSampe achieves significant performance increases in terms of speed and accuracy. Since existing selection techniques are designed for virtual environments (VEs), we furthermore provide a first approach towards a foundation for exploring 3D selection techniques in dense handheld AR.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., Venditti, B., &#38; Kaufmann, H. (2013). DrillSample. In <i>Proceedings of the Virtual Reality International Conference: Laval Virtual</i>. Laval Virtual - 15th International Conference on Virtual Reality and Converging Technologies, Laval, France, EU. ACM. https://doi.org/10.1145/2466816.2466827</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Benjamin",
                    "last_name": "Venditti",
                    "position": 2,
                    "role": "Author",
                    "tid": "182097"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "58275",
            "handle": "20.500.12708/54590",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "New Content-Based Features for the Distinction of Violent Videos and Martial Arts",
            "keywords": [],
            "abstract": "Real violence is unwanted content in video portals as it is\r\nforensically relevant in video surveillance systems. Naturally,\r\nboth domains have to deal with mass data which makes the\r\ndetection of violence by hand an impossible task. We introduce\r\none component of a system for automated violence detection\r\nfrom video content: the differentiation of real violence\r\nand martial arts videos. In particular, we introduce two new\r\nfeature transformations for jitter detection and local interest\r\npoint detection with Gestalt laws. Descriptions are classified\r\nin a two-step machine learning process. The experimental\r\nresults are highly encouraging: the novel features perform\r\nexceptionally well and the classification process practically\r\nacceptable recall and precision.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hörhan, M., &#38; Eidenberger, H. (2013). New Content-Based Features for the Distinction of Violent Videos and Martial Arts. In <i>Proceedings of the International Conference on Acoustics, Speech, and Signal Processing</i>. IEEE, Austria. IEEE Press. http://hdl.handle.net/20.500.12708/54590</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Hörhan",
                    "position": 1,
                    "role": "Author",
                    "tid": "40036"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 2,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58277",
            "handle": "20.500.12708/54592",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "3D building reconstruction and thermal mapping in fire brigade operations",
            "keywords": [],
            "abstract": "Fire fighting remains a dangerous profession despite many recent technological and organizational measures. Sensors and technical systems can augment the performance of fire fighters to increase safety and efficiency during operation. An important aspect in that context is the awareness of location, structure and thermal properties of the environment.\r\nAbstract This paper focuses on the design and development of a mobile system, which can reconstruct a 3d model of a building's interior structure in real-time and fuses the visualization with the image of a thermal camera. In addition the position and viewing direction of the fire fighter within the model is determined and a thermal map can be generated from the gathered data, which helps an operational commander to guide his men during a mission. \r\nFirst tests with our system in different situations showed good results, being able to reconstruct different larger scenes and create thermal maps thereof.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., Vonach, E., Gerstweiler, G., &#38; Kaufmann, H. (2013). 3D building reconstruction and thermal mapping in fire brigade operations. In <i>Proceedings of the 4th Augmented Human International Conference on - AH ’13</i>. 4th Augmented Human International Conference, Stuttgart, EU. ACM. https://doi.org/10.1145/2459236.2459271</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 2,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 3,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58282",
            "handle": "20.500.12708/54597",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Flexible Spaces: Dynamic Layout Generation for Infinite Walking in Virtual Environments",
            "keywords": [],
            "abstract": "Redirected walking techniques enable natural locomotion through\r\nimmersive virtual environments (VEs) that are larger than the real\r\nworld workspace. Most existing techniques rely upon\r\nmanipulating the mapping between physical and virtual motions\r\nwhile the layout of the environment remains constant. However, if\r\nthe primary focus of the experience is on the virtual world´s\r\ncontent, rather than on its spatial layout, then the goal of\r\nredirected walking can be achieved through an entirely different\r\nstrategy. In this paper, we introduce flexible spaces - a novel\r\nredirection technique that enables infinite real walking in virtual\r\nenvironments that do not require replication of real world layouts.\r\nFlexible spaces overcome the limitations and generalize the use of\r\noverlapping (impossible) spaces and change blindness by\r\nemploying procedural layout generation. Our approach allows VE\r\ndesigners to focus on the content of the virtual world independent\r\nof the implementation details imposed by real walking, thereby\r\nmaking spatial manipulation techniques more practical for use in a\r\nvariety of application domains.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vasylevska, K., Kaufmann, H., Suma, E. A., &#38; Bolas, M. (2013). Flexible Spaces: Dynamic Layout Generation for Infinite Walking in Virtual Environments. In <i>Flexible Spaces: Dynamic Layout Generation for Infinite Walking in Virtual Environments</i> (pp. 1–4). http://hdl.handle.net/20.500.12708/54597</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Evan A.",
                    "last_name": "Suma",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Bolas",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58283",
            "handle": "20.500.12708/54598",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "3D building reconstruction and thermal mapping in fire brigade operations",
            "keywords": [],
            "abstract": "Fire fighting remains a dangerous profession despite many recent technological and organizational measures. Sensors and technical systems can augment the performance of fire fighters and increase safety and efficiency during operation. An important aspect in that context is the awareness of location, structure and thermal properties of the environment. This work focuses on the design and development of a mobile system, which can reconstruct a 3d model of a building's interior structure in real-time and fuses the visualization with the image of a thermal camera. In addition, the position and viewing direction of the fire fighter within the model is determined and a thermal map can be generated from the gathered data. This helps an operational commander to provide accurate instructions to his men during a mission. First tests with our system in different situations showed good results, being able to reconstruct different larger scenes and create thermal maps thereof.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., Vonach, E., Gerstweiler, G., &#38; Kaufmann, H. (2013). 3D building reconstruction and thermal mapping in fire brigade operations. In <i>2013 IEEE Virtual Reality (VR)</i>. IEEE, Austria. IEEE. https://doi.org/10.1109/vr.2013.6549445</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 2,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 3,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58284",
            "handle": "20.500.12708/54599",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Parallel Tracking and Mapping in Hofburg Festsaal",
            "keywords": [],
            "abstract": "Precise localization for mobile Augmented Reality in large indoor environments without specific tracking infrastructure is challenging. This is especially true for rooms with changing properties, like lighting, seating and carpeting. With these constraints a map for a vision based tracking approach has to be continuously updated. The Parallel Tracking and Mapping (PTAM) algorithm is capable of generating and extending a map while tracking the camera pose in an unknown environment. However, it has originally been designed for small workspace environments and has therefore certain limitations. We have extended and modified the original implementation in order to ensure efficient and robust map generation and tracking in large rooms. Furthermore, we have tested a mobile setup with the system in Festsaal in Vienna´s Hofburg, which is close to thousand square meters in size. The user´s position and path was tracked while the environment was augmented with virtual objects and the system was successfully tested for robustness and occlusions.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gerstweiler, G., Kaufmann, H., Schönauer, C., &#38; Vonach, E. (2013). Parallel Tracking and Mapping in Hofburg Festsaal. In <i>IEEE Virtual Reality</i> (p. 2). IEEE. http://hdl.handle.net/20.500.12708/54599</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 1,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 3,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 4,
                    "role": "Author",
                    "tid": "40682"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "58333",
            "handle": "20.500.12708/54648",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "PS Move API: A Cross-Platform 6DoF Tracking Framework",
            "keywords": [],
            "abstract": "With the introduction of 6DoF motion controllers for game consoles,\r\nlow cost hardware for 3D interaction became widely available.\r\nHowever, no fully-featured software solution for 6DoF tracking\r\nexists that takes advantage of the PlayStation (PS) Move Motion\r\nController without additional hardware.\r\nWe designed, developed and evaluated a library - the PS Move\r\nAPI - that enables developers to use the PS Move Motion Controller\r\nas 6DoF input device in combination with a camera. Initially we\r\nsolved hardware related problems such as pairing and communication\r\nover USB and Bluetooth. In this paper we describe how we\r\nperform visual tracking and sensor fusion, combining visual and inertial\r\ndata. Performance results show that multiple controllers can\r\nbe tracked simultaneously in real time.\r\nDevelopers using the library can choose between a low-level C\r\nAPI or higher-level abstractions in Python, Java, C# or the Processing\r\nframework. The library is open source, has been developed\r\nand tested on Windows, Mac OS X and Linux, and is released\r\nunder a Simplified BSD License. It also runs on mobile\r\nLinux distributions such as MeeGo 1.2 Harmattan and Android.\r\nThe full source code is available on the PS Move API website at\r\nhttp://thp.io/2010/psmove/.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Perl, T., Venditti, B., &#38; Kaufmann, H. (2013). PS Move API: A Cross-Platform 6DoF Tracking Framework. In <i>Proceedings of the Workshop on Off-The-Shelf Virtual Reality</i> (p. 8). http://hdl.handle.net/20.500.12708/54648</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Perl",
                    "position": 1,
                    "role": "Author",
                    "tid": "60724"
                },
                {
                    "first_name": "Benjamin",
                    "last_name": "Venditti",
                    "position": 2,
                    "role": "Author",
                    "tid": "182097"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58356",
            "handle": "20.500.12708/54671",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Differential Progressive Path Tracing for High-Quality Previsualization and Relighting in Augmented Reality",
            "keywords": [],
            "abstract": "In this paper we present a novel method for real-time high\r\nquality previsualization and cinematic relighting. The physically based Path Tracing algorithm is used within an Augmented Reality setup to\r\npreview high-quality light transport. A novel differential version of progressive path tracing is proposed, which calculates two global light transport solutions that are required for differential rendering. A real-time pre-visualization framework is presented, which renders the solution with a\r\nlow number of samples during interaction and allows for progressive quality improvement. If a user requests the high-quality solution of a certain\r\nview, the tracking is stopped and the algorithm progressively converges\r\nto an accurate solution. The problem of rendering complex light paths\r\nis solved by using photon mapping. Specular global illumination effects\r\nlike caustics can easily be rendered. Our framework utilizes the massive\r\nparallel power of modern GPUs to achieve fast rendering with complex\r\nglobal illumination, a depth of field effect, and antialiasing.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., &#38; Kaufmann, H. (2013). Differential Progressive Path Tracing for High-Quality Previsualization and Relighting in Augmented Reality. In G. Bebis (Ed.), <i>ISVC 2013, Part II, LNCS 8034</i> (pp. 328–338). Springer. http://hdl.handle.net/20.500.12708/54671</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "George",
                    "last_name": "Bebis",
                    "position": 1,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58373",
            "handle": "20.500.12708/54688",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Autonomous Flight using a Smartphone as On-Board Processing Unit in GPS-Denied Environments",
            "keywords": [],
            "abstract": "In this paper, we present a low-weight and low-cost Unmanned\r\nAerial Vehicle (UAV) for autonomous flight and navigation in GPS-denied environments using an off the shelf\r\nsmartphone as its core on-board processing unit. Thereby,\r\nour approach is independent from additional ground\r\nhardware and the UAV core unit can be easily replaced\r\nwith more powerful hardware that simpli es setup updates\r\nas well as maintenance. The UAV is able to map, locate\r\nand navigate in an unknown indoor environment fusing vision\r\nbased tracking with inertial and attitude measurements.\r\nWe choose an algorithmic approach for mapping and localization\r\nthat does not require GPS coverage of the target\r\narea, therefore autonomous indoor navigation is made possible.\r\nWe demonstrate the UAVs capabilities of mapping,\r\nlocalization and navigation in an unknown 2D marker environment.\r\nOur promising results enable future research on 3D self-localization and dense mapping using mobile hardware\r\nas the only on-board processing unit.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Leichtfried, M., Kaltenriner, C., Mossel, A., &#38; Kaufmann, H. (2013). Autonomous Flight using a Smartphone as On-Board Processing Unit in GPS-Denied Environments. In <i>Proceedings of International Conference on Advances in Mobile Computing &#38;amp; Multimedia - MoMM ’13</i>. 11th International Conference on Advances in Mobile Computing &#38; Multimedia (MoMM), Vienna, Austria, Austria. ACM New York, NY, USA. https://doi.org/10.1145/2536853.2536898</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Leichtfried",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Christoph",
                    "last_name": "Kaltenriner",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 3,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58383",
            "handle": "20.500.12708/54698",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Wide area optical user tracking in unconstrained indoor environments",
            "keywords": [],
            "abstract": "In this paper, we present a robust infrared optical 3D position tracking for wide area indoor environments up to 30m. The system\r\nconsists of two shutter-synchronized cameras that track multiple\r\ntargets, which are equipped with infrared light emitting diodes.\r\nOur system is able to learn targets as well as to perform extrinsic\r\ncalibration and 3D position tracking in unconstrained environments,\r\nwhich exhibit occlusions and static as well as locomotive\r\ninterfering infrared lights. Tracking targets can directly be used\r\nfor calibration which minimizes the amount of necessary hardware.\r\nWith the presented approach, limitations of state-of-the-art tracking\r\nsystems in terms of volume coverage, sensitivity during training\r\nand calibration, setup complexity and hardware costs can be\r\nminimized. Preliminary results indicate interactive tracking with\r\nminimal jitter < 0.0675mm and 3D point accuracy of < 9.22mm\r\nthroughout the entire tracking volume up to 30m.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., &#38; Kaufmann, H. (2013). Wide area optical user tracking in unconstrained indoor environments. In <i>2013 23rd International Conference on Artificial Reality and Telexistence (ICAT)</i>. The 23rd International Conference on Artificial Reality and Telexistence (ICAT2013), Tokyo, Japan, Non-EU. IEEE. https://doi.org/10.1109/icat.2013.6728915</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58393",
            "handle": "20.500.12708/54708",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Differential Irradiance Caching for Fast High-Quality Light Transport Between Virtual and Real Worlds",
            "keywords": [],
            "abstract": "Fast and realistic synthesis of real videos with computer generated content has been a challenging problem in computer graphics. It involves computationally expensive light transport calculations. We present a novel and efficient algorithm for diffuse light transport calculation between virtual and real worlds called Differential Irradiance Caching. Our algorithm produces a high-quality result while preserving interactivity and allowing dynamic geometry, materials, lighting, and camera movement. The problem of expensive differential irradiance evaluation is solved by exploiting the spatial coherence in indirect illumination using irradiance caching. We enable multiple bounces of global illumination by using Monte Carlo integration in GPU ray-tracing to evaluate differential irradiance at irradiance cache records in one pass. The combination of ray-tracing and rasterization is used in an extended irradiance cache splatting algorithm to provide a fast GPU-based solution of indirect illumination. Limited information stored in the irradiance splat buffer causes errors for pixels on edges in case of depth of field rendering. We propose a solution to this problem using a reprojection technique to access the irradiance splat buffer. A novel cache miss detection technique is introduced which allows for a linear irradiance cache data structure. We demonstrate the integration of differential irradiance caching into a rendering framework for Mixed Reality applications capable of simulating complex global illumination effects.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., &#38; Kaufmann, H. (2013). Differential Irradiance Caching for Fast High-Quality Light Transport Between Virtual and Real Worlds. In <i>Proceedings of International Symposium on Mixed and Augmented Reality (ISMAR)</i> (pp. 133–141). IEEE Computer Society. http://hdl.handle.net/20.500.12708/54708</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E120-05",
                "E193-02"
            ],
            "pid": "58399",
            "handle": "20.500.12708/54714",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Echtzeitvermessung mit Infrarottrackingkameras - Untersuchung einer neuen Messtechnik für untertage",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Chmelina, K., Mossel, A., &#38; Kaufmann, H. (2014). Echtzeitvermessung mit Infrarottrackingkameras - Untersuchung einer neuen Messtechnik für untertage. In <i>Ingenieurvermessung 14: Beiträge zum 17. Internationalen Ingenieursvermessungskurs</i> (pp. 3–14). Herbert Wichmann-Verlag. http://hdl.handle.net/20.500.12708/54714</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Klaus",
                    "last_name": "Chmelina",
                    "position": 1,
                    "role": "Author",
                    "tid": "127455"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 2,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58419",
            "handle": "20.500.12708/54733",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "15 Years of VR/AR in Education - A Personal Summary and Outlook",
            "keywords": [],
            "abstract": "15 Years of VR/AR in Education - A Personal Summary and Outlook",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2013). 15 Years of VR/AR in Education - A Personal Summary and Outlook. In <i>VARE 2013 Proceedings - Abstract Book</i> (p. 19). http://hdl.handle.net/20.500.12708/54733</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E389-02"
            ],
            "pid": "58426",
            "handle": "20.500.12708/54740",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Detection of Brain Tumors Based on Automatic Symmetry Analysis",
            "keywords": [],
            "abstract": "This article focuses on the detection of a\r\nbrain tumor location in magnetic resonance images.\r\nThe aim of this work is not the precise segmentation\r\nof the tumor and its parts but only the detection of\r\nits approximate location. It will be used in future\r\nwork for more accurate segmentation. For this rea-\r\nson, it also does not deal with detecting of the im-\r\nages containing the tumor. The algorithm expects a\r\n2D T2-weighted magnetic resonance image of brain\r\ncontaining a tumor. The detection is based on lo-\r\ncating the area that breaks the left-right symmetry of\r\nthe brain. The created algorithm was tested on 73\r\nimages containing tumor, tumor with edema or only\r\nedema. These pathological structures had various\r\nsizes and shapes and were located in various parts\r\nof the brain.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dvorak, P., &#38; Kropatsch, W. (2013). Detection of Brain Tumors Based on Automatic Symmetry Analysis. In W. Kropatsch, F. Torres Garcia, &#38; G. Ramachandran (Eds.), <i>Proceedings of the 18th Computer Vision Winter Workshop 2013</i> (pp. 24–31). PRIP. http://hdl.handle.net/20.500.12708/54740</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Pavel",
                    "last_name": "Dvorak",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Fuensanta",
                    "last_name": "Torres Garcia",
                    "position": 2,
                    "role": "Editor",
                    "tid": "235851"
                },
                {
                    "first_name": "Geetha",
                    "last_name": "Ramachandran",
                    "position": 3,
                    "role": "Editor",
                    "tid": "195689"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E389-02"
            ],
            "pid": "58427",
            "handle": "20.500.12708/54741",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Logical Layout Recovery: approach for graphic-based features.",
            "keywords": [],
            "abstract": "In contrast to the existing approaches\r\nfor document analysis and understanding this paper\r\nrepresents a system that considers a logical role\r\nfor graphic content in predominantly textual, born\r\ndigital PDF documents. This work was inspired by\r\nthe idea of using structural graphic objects in order\r\nto clarify the logical layout even of complex mostly\r\ngraphic documents. Based on visual cognition,\r\ngeometric features and spatial relations, the\r\nproposed statistical method distinguishes illustrative\r\ngraphic objects from structural graphic objects. We\r\nperformed evaluation on two document domains\r\n- newspapers and technical manuals - and found\r\nthe results to be reliable. We propose using\r\nlogical information about the graphic content to be\r\na new step towards domain-independent document\r\nunderstanding systems",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gabdulkhakova, A., Hassan, T., &#38; Kropatsch, W. (2013). Logical Layout Recovery: approach for graphic-based features. In W. Kropatsch, F. Torres Garcia, &#38; G. Ramachandran (Eds.), <i>Proceedings of the 18th Computer Vision Winter Workshop 2013</i> (pp. 47–54). Prip 186/3. http://hdl.handle.net/20.500.12708/54741</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Aysylu",
                    "last_name": "Gabdulkhakova",
                    "position": 1,
                    "role": "Author",
                    "tid": "235653"
                },
                {
                    "first_name": "Tamir",
                    "last_name": "Hassan",
                    "position": 2,
                    "role": "Author",
                    "tid": "44748"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Fuensanta",
                    "last_name": "Torres Garcia",
                    "position": 2,
                    "role": "Editor",
                    "tid": "235851"
                },
                {
                    "first_name": "Geetha",
                    "last_name": "Ramachandran",
                    "position": 3,
                    "role": "Editor",
                    "tid": "195689"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E389-02"
            ],
            "pid": "58428",
            "handle": "20.500.12708/54742",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Localizing and Segmenting Objects with 3D Objectness.",
            "keywords": [],
            "abstract": "This paper presents a novel method to lo-\r\ncalize and segment objects on close-range table-top\r\nscenarios sensed with a depth sensor. The method is\r\nbased on a novel\r\nobjectness\r\nmeasure that evaluates\r\nhow likely a 3D region in space (defined by an ori-\r\nented bounding box) could contain an object. Within\r\na parametrized volume of interest placed above the\r\ntable plane, a set of 3D bounding boxes is generated\r\nthat exhaustively covers the parameter space. Effi-\r\nciently evaluating - thanks to integral volumes and\r\nparallel computing - the 3D objectness at each sam-\r\npled bounding box allows efficiently defining a set\r\nof regions in space with high probability of contain-\r\ning an object. Bounding boxes characterized by high\r\nobjectness are then processed by means of a global\r\noptimization stage aimed at discarding inconsistent\r\nobject hypotheses with respect to the scene. We eval-\r\nuate the effectiveness of the method for the task of\r\nscene segmentation",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Aldoma, A., Tombari, F., Kropatsch, W., &#38; Vincze, M. (2013). Localizing and Segmenting Objects with 3D Objectness. In W. Kropatsch, F. Torres Garcia, &#38; G. Ramachandran (Eds.), <i>Proceedings of the 18th Computer Vision Winter Workshop 2013</i> (pp. 86–93). Prip 186/3. http://hdl.handle.net/20.500.12708/54742</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Aitor",
                    "last_name": "Aldoma",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Federico",
                    "last_name": "Tombari",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Vincze",
                    "position": 4,
                    "role": "Author",
                    "tid": "133231"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Editor",
                    "tid": "38472"
                },
                {
                    "first_name": "Fuensanta",
                    "last_name": "Torres Garcia",
                    "position": 2,
                    "role": "Editor",
                    "tid": "235851"
                },
                {
                    "first_name": "Geetha",
                    "last_name": "Ramachandran",
                    "position": 3,
                    "role": "Editor",
                    "tid": "195689"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "58446",
            "handle": "20.500.12708/54760",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Estimation of Distribution Algorithm for the Max-Cut Problem",
            "keywords": [],
            "abstract": "Abstract.\r\nIn this paper, we investigate the\r\nMax-Cut\r\nproblem and pro-\r\npose a probabilistic heuristic to address its classic and weighted version.\r\nOur approach is based on the Estimation of Distribution Algorithm\r\n(EDA) that creates a population of individuals capable of evolving at\r\neach generation towards the global solution. We have applied the\r\nMax-\r\nCut\r\nproblem for image segmentation and de ned the edges' weights as\r\na modi ed function of the L2 norm between the RGB values of nodes.\r\nThe main goal of this paper is to introduce a heuristic for\r\nMax-Cut\r\nand\r\nadditionally to investigate how it can be applied in the segmentation context.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">de Sousa, S., Haxhimusa, Y., &#38; Kropatsch, W. (2013). Estimation of Distribution Algorithm for the Max-Cut Problem. In <i>Lecture Notes in Computer Science</i>. GbR 2013, 9th IAPR - TC-15 Workshop, Wien, Austria. Lecture Notes in Computer Science, Volume 7877, Springer-Verlag. https://doi.org/10.1007/978-3-642-38221-5</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Samuel",
                    "last_name": "de Sousa",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "58447",
            "handle": "20.500.12708/54761",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "On the Evaluation of Graph Centrality for Shape Matching",
            "keywords": [],
            "abstract": "Graph centrality has been extensively applied in Social Net-\r\nwork Analysis to model the interaction of actors and the information ow\r\ninside a graph. In this paper, we investigate the usage of graph centrali-\r\nties in the Shape Matching task. We create a graph-based representation\r\nof a shape and describe this graph by using di erent centrality measures.\r\nWe build a Naive Bayes classi er whose input feature vector consists of\r\nthe measurements obtained by the centralities and evaluate the di erent\r\nperformances for each centrality",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">de Sousa, S., Artner, N., &#38; Kropatsch, W. (2013). On the Evaluation of Graph Centrality for Shape Matching. In <i>Graph-based Representations in Pattern Recognition, 9th IAPR - TC-15 Workshop</i> (pp. 204–213). Lecture Notes in Computer Science, Volume 7877, Springer-Verlag. http://hdl.handle.net/20.500.12708/54761</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Samuel",
                    "last_name": "de Sousa",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "58448",
            "handle": "20.500.12708/54762",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automated Segmentation of Brain Tumor Edema in FLAIR MRI Using Symmetry and Thresholding",
            "keywords": [],
            "abstract": "Nowadays, the brain tumor detection and segmentation in MR images is a de-\r\nveloping issue. There are many research teams producing di®erent and interesting methods and\r\nalgorithms for this particular task of medical image processing. Many of them are semi-automatic,\r\nbut the aim of current research, and of this work, is to ¯nd a fully automatic method.\r\nThis paper focuses on the automatic edema segmentation in FLAIR images. This type of contrast\r\nimages was selected because of the visibility and manifestation of edema in this image type. Since\r\nin axial plane of healthy brain, the approximate left-right symmetry exists, it is used as the prior\r\nknowledge for searching the approximate edema location. It is assumed that the edema is not\r\nlocated symmetrically in both hemispheres, which is met in most cases. For the detection, the\r\nmulti-resolution approach is used. Since the edemas manifest as a hyperintense area in FLAIR\r\nimages, it is extracted using thresholding. For the automatic determination of the threshold, the\r\nOtsu's algorithm is used. This work does not deal with the tumor presence detection. One of our\r\nprevious work focuses on this topic. The main reason for the edema segmentation is for the tumor\r\nclassi¯cation. This will be carried out by applying the resulting mask of the proposed method to\r\nperfusion MR images. Since perfusion images are of very low contrast, the pathological area, it\r\nmeans the tumor and a potential edema around it, has to be detected and segmented in another\r\ntype of MR images.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dvorak, P., Bartusek, K., &#38; Kropatsch, W. (2013). Automated Segmentation of Brain Tumor Edema in FLAIR MRI Using Symmetry and Thresholding. In <i>Proceedings of PIERS 2013</i> (pp. 936–939). http://hdl.handle.net/20.500.12708/54762</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Pavel",
                    "last_name": "Dvorak",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Karel",
                    "last_name": "Bartusek",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "58449",
            "handle": "20.500.12708/54763",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automatic Detection of Brain Tumors in MR Images",
            "keywords": [],
            "abstract": "This paper deals with automatic brain tumor de-\r\ntection in magnetic resonance images. The goal is to determine\r\nwhether the MR image of a brain contains a tumor. The proposed\r\nmethod works with T2-weighted magnetic resonance images,\r\nwhere the head is vertically aligned. The detection is based\r\non checking the left-right symmetry of the brain, which is the\r\nassumption for healthy brain. The algorithm was tested by five-\r\nfold cross-validation technique on 72 images of brain containing\r\ntumors and 131 images of healthy brain. The proposed method\r\nreaches the true positive rate of 91.16% and the true negative\r\nrate of 94.68%.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dvorak, P., Kropatsch, W., &#38; Bartusek, K. (2013). Automatic Detection of Brain Tumors in MR Images. In <i>Proceedings of the 36th International Conference on Telecommunications and Signal Processing (TSP)</i> (pp. 577–580). http://hdl.handle.net/20.500.12708/54763</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Pavel",
                    "last_name": "Dvorak",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Karel",
                    "last_name": "Bartusek",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "58450",
            "handle": "20.500.12708/54764",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Probabilistic Joint Image Segmentation and Labeling by Figure-Ground Composition",
            "keywords": [],
            "abstract": "We propose a layered statistical model for image\r\nsegmentation and labeling obtained by combining independently\r\nextracted, possibly overlapping sets of figure-ground\r\n(FG) segmentations. The process of constructing consistent\r\nimage segmentations, called tilings, is cast as optimization\r\nover sets of maximal cliques sampled from a graph connecting\r\nall non-overlapping figure-ground segment hypotheses.\r\nPotential functions over cliques combine unary, Gestaltbased\r\nfigure qualities, and pairwise compatibilities among\r\nspatially neighboring segments, constrained by T-junctions\r\nand the boundary interface statistics of real scenes. Building\r\non the segmentation layer, we further derive a joint image\r\nsegmentation and labeling model (JSL) which, given a bag\r\nof FGs, constructs a joint probability distribution over both\r\nthe compatible image interpretations (tilings) composed from\r\nthose segments, and over their labeling into categories. The\r\nprocess of drawing samples from the joint distribution can\r\nbe interpreted as first sampling tilings, followed by sampling\r\nlabelings conditioned on the choice of a particular tiling.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A., Carreira, J., &#38; Sminchisescu, C. (2013). Probabilistic Joint Image Segmentation and Labeling by Figure-Ground Composition. In <i>International Journal of Computer Vision</i> (pp. 40–57). Springer Science+Business Media New York 2013. https://doi.org/10.1007/s11263-013-0663-7</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "João",
                    "last_name": "Carreira",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Cristian",
                    "last_name": "Sminchisescu",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "58583",
            "handle": "20.500.12708/54896",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Automated social event detection in large photo collections",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaharieva, M., Zeppelzauer, M., &#38; Breiteneder, C. (2013). Automated social event detection in large photo collections. In <i>Proceedings of the 3rd ACM conference on International conference on multimedia retrieval - ICMR ’13</i>. ACM, Austria. ACM. https://doi.org/10.1145/2461466.2461495</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 3,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "220239"
            ]
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "58584",
            "handle": "20.500.12708/54897",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Acoustic detection of elephant presence in noisy environments",
            "keywords": [],
            "abstract": "The automated acoustic detection of elephants is an important factor in alleviating the human-elephant conflict in Asia and Africa. In this paper, we present a method for the automated detection of elephant presence and evaluate it on a large dataset of wildlife recordings. We introduce a novel technique for signal enhancement to improve the robustness of the detector in noisy situations. Experiments show that the proposed detector outperforms existing methods and that signal enhancement strongly improves the robustness to noise sources from the environment. The proposed method is a first step towards an automated detection system for elephant presence.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zeppelzauer, M., Stöger, A. S., &#38; Breiteneder, C. (2013). Acoustic detection of elephant presence in noisy environments. In <i>Proceedings of the 2nd ACM international workshop on Multimedia analysis for ecological data - MAED ’13</i>. ACM, Austria. ACM. https://doi.org/10.1145/2509896.2509900</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Angela S.",
                    "last_name": "Stöger",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 3,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "58621",
            "handle": "20.500.12708/54934",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Surface Layout Estimation Using Multiple Segmentation Methods and 3D Reasoning",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hödlmoser, M., &#38; Micusik, B. (2013). Surface Layout Estimation Using Multiple Segmentation Methods and 3D Reasoning. In <i>Pattern Recognition and Image Analysis</i> (pp. 41–49). Lecture Notes on Computer Science. https://doi.org/10.1007/978-3-642-38628-2_5</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Hödlmoser",
                    "position": 1,
                    "role": "Author",
                    "tid": "65240"
                },
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 2,
                    "role": "Author",
                    "tid": "229635"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "58623",
            "handle": "20.500.12708/54936",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Model-Based Vehicle Pose Estimation and Tracking in Videos Using Random Forests",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hödlmoser, M., Micusik, B., Pollefeys, M., Liu, M. Y., &#38; Kampel, M. (2013). Model-Based Vehicle Pose Estimation and Tracking in Videos Using Random Forests. In <i>2013 International Conference on 3D Vision</i>. International Conference on 3D Vision (3DV), Seattle, USA, Non-EU. IEEE. https://doi.org/10.1109/3dv.2013.63</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Hödlmoser",
                    "position": 1,
                    "role": "Author",
                    "tid": "65240"
                },
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 2,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Marc",
                    "last_name": "Pollefeys",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Ming Yiu",
                    "last_name": "Liu",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 5,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "58624",
            "handle": "20.500.12708/54937",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Sparse Point Cloud Densification by Combining Multiple Segmentation Methods",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hödlmoser, M., Micusik, B., &#38; Kampel, M. (2013). Sparse Point Cloud Densification by Combining Multiple Segmentation Methods. In <i>2013 International Conference on 3D Vision</i>. International Conference on 3D Vision (3DV), Seattle, USA, Non-EU. IEEE. https://doi.org/10.1109/3dv.2013.64</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Hödlmoser",
                    "position": 1,
                    "role": "Author",
                    "tid": "65240"
                },
                {
                    "first_name": "Branislav",
                    "last_name": "Micusik",
                    "position": 2,
                    "role": "Author",
                    "tid": "229635"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Kampel",
                    "position": 3,
                    "role": "Author",
                    "tid": "49535"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E192-01",
                "E193-03"
            ],
            "pid": "58716",
            "handle": "20.500.12708/55028",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A mixed integer model for the stamina-aware sightseeing tour problem",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hu, B., Ölz, W., &#38; Raidl, G. (2013). A mixed integer model for the stamina-aware sightseeing tour problem. In <i>Extended Abstracts of the 14th International Conference on Computer Aided Systems Theory</i> (pp. 200–202). http://hdl.handle.net/20.500.12708/55028</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Bin",
                    "last_name": "Hu",
                    "position": 1,
                    "role": "Author",
                    "tid": "53106"
                },
                {
                    "first_name": "Werner",
                    "last_name": "Ölz",
                    "position": 2,
                    "role": "Author",
                    "tid": "49190"
                },
                {
                    "first_name": "Günther",
                    "last_name": "Raidl",
                    "position": 3,
                    "role": "Author",
                    "tid": "45040"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "58775",
            "handle": "20.500.12708/55083",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluating RGB+D Hand Posture Detection Methods for Mobile 3D Interaction",
            "keywords": [],
            "abstract": "In mobile applications it is crucial to provide intuitive means for 2D and 3D interaction. A large number of techniques exist to support a natural user interface (NUI) by detecting the user´s hand posture in RGB+D (depth) data. Depending on a given interaction scenario, each technique hast its advantages and disadvantages. To evaluate the performance of the various techniques on a mobile device, we conducted a systematic study by comparing the accuracy of five common posture recognition approaches with varying illumination and background. To be able to perform this study, we developed a powerful hard- and software framework that is capable of processing and fusing RGB and depth data directly on a handheld device. Overall results reveal best recognition rate of posture detection for combined RGB+D data at the expense of update rate. Finally, to support users in choosing the appropriate technique for their specific mobile interaction task, we derived guidelines based on our study.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fritz, D., Mossel, A., &#38; Kaufmann, H. (2014). Evaluating RGB+D Hand Posture Detection Methods for Mobile 3D Interaction. In <i>Proceedings of the 16th International Conference of Virtual Technologies (VRIC’14)</i>. 16th International Conference and Exibition on Virtual Technologies, Laval, France, EU. ACM. http://hdl.handle.net/20.500.12708/55083</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Daniel",
                    "last_name": "Fritz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 2,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E120-05",
                "E193-02"
            ],
            "pid": "58789",
            "handle": "20.500.12708/55094",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Real-Time Machine Guidance with Tracking Cameras",
            "keywords": [],
            "abstract": "Mining and tunneling processes are more and more automatized, mechanized and speeded up. Machines, such as roadheaders, jumbos, dredgers, loaders etc. contribute to significant cost reductions and the increase of safety and efficiency of underground works. For an\r\nefficient control of these machines the continuous and precise determination of their absolute 3D position and orientation in the underground space is mandatory. To further increase the current degree of automation of mining and tunneling machinery, a\r\nnew survey technology is currently researched in the European FP7 research project I2Mine (2011-2015). The technology is based on infrared tracking cameras that measure to infrared LEDs (light-emitting diodes) mounted on machines and tools. The contribution\r\ndescribes the technology, the developed first prototype system and the tests that have been carried out.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Chmelina, K., Lammer, E., Mossel, A., &#38; Kaufmann, H. (2014). Real-Time Machine Guidance with Tracking Cameras. In <i>Proceeding of the 6th International Symposium High Performance Mining</i> (pp. 679–687). Druckservice Zillekens. http://hdl.handle.net/20.500.12708/55094</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Klaus",
                    "last_name": "Chmelina",
                    "position": 1,
                    "role": "Author",
                    "tid": "127455"
                },
                {
                    "first_name": "Egmont",
                    "last_name": "Lammer",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 3,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58795",
            "handle": "20.500.12708/55099",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "The Effects of Direct and Global Illumination on Presence in Augmented Reality",
            "keywords": [],
            "abstract": "In this paper we present an experiment, which was designed and conducted with the goal to study the effect of lighting on the sense of presence in Augmented Reality. We compared presence ratings between global illumination rendering and direct illumination and asked study participants to judge which of the shown objects are virtual and which ones are real. Thirty people participated in a within-group experiment. A set of questionnaires was used to measure the sense of presence, perception of realism and the rate of interpretation of virtual objects as real ones with both global and direct illumination conditions. The results of our experiment show that global illumination rendering increases the sense of presence in comparison to direct illumination and that there is a correlation between perception of realism and feeling of presence in augmented reality. We discuss the major differences between real and virtual objects as observed by the users and the categories of important features for visual realism in Augmented Reality.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., Dünser, A., Billinghurst, M., Schönauer, C., &#38; Kaufmann, H. (2014). The Effects of Direct and Global Illumination on Presence in Augmented Reality. In <i>Challenging Presence - Proceedings of 15th International Conference on Presence (ISPR 2014)</i> (pp. 223–230). Facultas Verlags- und Buchhandels AG. http://hdl.handle.net/20.500.12708/55099</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Dünser",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Billinghurst",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 4,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 5,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58872",
            "handle": "20.500.12708/55149",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Influence of Vertical Navigation Metaphors on Presence",
            "keywords": [],
            "abstract": "A sense of spatial presence as a feeling of &quot;being there&quot; is an important part of the virtual experience. Navigation is a fundamental task in virtual environments and the specific navigation methods that are employed influence a user´s sense of presence. In this paper we investigate how different metaphors for vertical navigation impact spatial presence. We introduce a new elevator metaphor for vertical navigation in virtual environments which includes a multimodal simulation. In a user study our approach is compared to the existing flying and teleportation metaphors with respect to spatial presence, comfort, real world awareness and other parameters. The results show that our elevator simulation increases the sense of presence and is more natural and realistic for vertical navigation in multilevel virtual environments than previous methods.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vasylevska, K., &#38; Kaufmann, H. (2014). Influence of Vertical Navigation Metaphors on Presence. In <i>In Challenging Presence - Proceedings of 15th International Conference on Presence (ISPR 2014)</i> (pp. 205–212). http://hdl.handle.net/20.500.12708/55149</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "58917",
            "handle": "20.500.12708/55192",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Graph-based Similarity of Petroglyphs",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Seidl, M., Wieser, E., Zeppelzauer, M., Pinz, A., &#38; Breiteneder, C. (2014). Graph-based Similarity of Petroglyphs. In <i>VISART “Where Computer Vision Meets Art”</i> (p. 9). Springer. http://hdl.handle.net/20.500.12708/55192</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Seidl",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Ewald",
                    "last_name": "Wieser",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 3,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Axel",
                    "last_name": "Pinz",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 5,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "58919",
            "handle": "20.500.12708/55194",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Smooth and iteratively restore: A simple and fast edge-preserving smoothing model",
            "keywords": [],
            "abstract": "In image processing, it can be a useful pre-processing step to smooth away small\r\nstructures, such as noise or unimportant details, while retaining the overall structure of the image\r\nby keeping edges, which separate objects, sharp. Typically this edge-preserving smoothing process\r\nis achieved using edge-aware filters. However such filters may preserve unwanted small structures as\r\nwell if they contain edges. In this work we present a novel framework for edge-preserving smoothing\r\nwhich separates the process into two different steps: First the image is smoothed using a blurring\r\nfilter and in the second step the important edges are restored using a guided edge-aware filter. The\r\npresented method proves to deliver very good results, compared to state-of-the-art edge-preserving\r\nsmoothing filters, especially at removing unwanted small structures. Furthermore it is very versatile\r\nand can easily be adapted to different fields of applications while at the same time being very fast\r\nto compute and therefore well-suited for real time applications.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kniefacz, P., &#38; Kropatsch, W. (2015). Smooth and iteratively restore: A simple and fast edge-preserving smoothing model. In A. Uhl &#38; R. Kwitt (Eds.), <i>Proceedings of the ÖAGM Workshop 2015, Salzburg, Austria, May 2015</i> (p. 9). http://hdl.handle.net/20.500.12708/55194</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Philip",
                    "last_name": "Kniefacz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Uhl",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Roland",
                    "last_name": "Kwitt",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "58920",
            "handle": "20.500.12708/55195",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "ACTO",
            "keywords": [],
            "abstract": "We introduce a customizable, reusable actuated tangible user interface object: ACTO. Its modular design allows quick adaptations for different scenarios and setups on tabletops, making otherwise integral parts like the actuation mechanism or the physical configuration interchangeable. Drawing on the resources of well-established maker communities makes prototyping especially quick and easy. This allows the exploration of new concepts without the\r\nneed to redesign the whole system, which qualifies it as an ideal research and education platform for tangible user interfaces. We present a detailed description of the hardware and software architecture of our system. Several implemented example configurations and application scenarios demonstrate the capabilities of the platform.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vonach, E., Gerstweiler, G., &#38; Kaufmann, H. (2014). ACTO. In <i>Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces - ITS ’14</i>. ACM International Conference on Interactive Tabletops and Surfaces (ITS 2014), Dresden, Germany, EU. ACM. https://doi.org/10.1145/2669485.2669522</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 2,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "58931",
            "handle": "20.500.12708/55206",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "FT-RANSAC: Towards robust multi-modal homography estimation",
            "keywords": [],
            "abstract": "As the golden standard in robust estimation, the\r\nclassic RANSAC approach has undergone extensive research that\r\ncontributed to further enhancements in run-time performance,\r\nrobustness, and multi-structure support to name a few. Yet, the\r\naccelerating growth of multi-modal co-registered datasets\r\nrequires a new adaptation of the RANSAC algorithm. In this\r\npaper, we propose a multi-modal fault-tolerant extension to\r\nRANSAC, termed FT-RANSAC, with a model-independent\r\ntolerance to degenerate configurations. Besides building on stateof-\r\nthe-art RANSAC variants, such as PROSAC, our approach\r\nintroduces a Hough inspired dimensionality reduction and\r\nconsistency voting processes, to enable robust estimation in the\r\npresence of non-homogenous multi-modal correspondence sets.\r\nThrough experimental evaluation using homography estimation of\r\nRGB-D data, we demonstrate that our approach outperforms the\r\nclassic single-modality RANSAC in robustness and tolerance to\r\ndegenerate configurations. Finally, the proposed approach lends\r\nitself to parallel multi-core implementations, and could be adapted\r\nto specialized RANSAC extensions found in the literature.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Barclay, A., &#38; Kaufmann, H. (2014). FT-RANSAC: Towards robust multi-modal homography estimation. In <i>2014 8th IAPR Workshop on Pattern Reconition in Remote Sensing</i>. 8th IAPR Workshop on Pattern Recognition in Remote Sensing (PRRS), Stockholm, Non-EU. IEEE. https://doi.org/10.1109/prrs.2014.6914290</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adam",
                    "last_name": "Barclay",
                    "position": 1,
                    "role": "Author",
                    "tid": "54614"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "58998",
            "handle": "20.500.12708/55273",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Reeb graph based examination of root development",
            "keywords": [],
            "abstract": "This paper presents an approach to analyze\r\nplant root development by means of topological image anal-\r\nysis. For phenotyping of plants their root development, the\r\narchitecture of their root systems and thereby root charac-\r\nteristics such as branches and branch endings are analyzed.\r\nIn order to simplify the examination of root characteristics\r\nand enable an efficient comparison of roots, a representa-\r\ntion of imaged root data by Reeb graphs is introduced. Reeb\r\ngraphs capture the topology of the represented structure -\r\nin this case the locations of branches and branch endings\r\nof the roots - and form a skeletal representation of the un-\r\nderlying image data in this way. As the roots are pictured\r\nas 2D image data, the projection of a 3D structure to a 2D\r\nspace might result in an overlap of branches in the image.\r\nOne major advantage when analyzing roots based on Reeb\r\ngraphs is posed by the ability to immediately distinguish be-\r\ntween branching points and overlaps in the root structure.\r\nThis is not as easily possible by an analysis solely based on\r\ncontours.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., Kropatsch, W., &#38; Busch, W. (2014). Reeb graph based examination of root development. In Z. Kúkelová &#38; J. Heller (Eds.), <i>Proceedings of the 19th Computer Vision Winter Workshop 2014</i> (pp. 43–50). Proceedings of the 19th Computer Vision Winter Workshop 2014. http://hdl.handle.net/20.500.12708/55273</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Wolfgang",
                    "last_name": "Busch",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Zusana",
                    "last_name": "Kúkelová",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Jan",
                    "last_name": "Heller",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "58999",
            "handle": "20.500.12708/55274",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Canonical Encoding of the Combinatorial Pyramid",
            "keywords": [],
            "abstract": "This paper presents a novel framework to en-\r\ncode a combinatorial pyramid. A combinatorial pyramid\r\nis a hierarchy of successively reduced combinatorial maps.\r\nImportant properties of the combinatorial pyramids such as\r\ntopology preservation, the process global and local features\r\nwithin the same data structure, etc. made them useful for\r\nimage processing and pattern recognition tasks. Their ad-\r\nvantages have been widely proved in the literature. Never-\r\ntheless, the main disadvantage of this approach is the high\r\nrate of memory requirement. A combinatorial map of an im-\r\nage maybe stored in an array of size approximately equal to\r\nfour times the number of pixels of the image. Furthermore,\r\nevery level of the combinatorial pyramid stores a different\r\ncombinatorial map. In respond to this problem a canoni-\r\ncal encoding of the combinatorial pyramid is provided. It\r\nconsists of a single array where its elements are ordered\r\nwith respect to the construction history of the pyramid. In\r\nthis manner the memory consumptions are equal to the size\r\nof the initial combinatorial map and do not depend on the\r\nnumber of pyramid´s levels. In addition, this canonical en-\r\ncoding allows the whole reconstruction of the pyramid in\r\nboth directions: from the base to the top level and from the\r\ntop to the base level, without additional information.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Torres Garcia, F., &#38; Kropatsch, W. (2014). Canonical Encoding of the Combinatorial Pyramid. In Z. Kúkelová &#38; J. Heller (Eds.), <i>Proceedings of the 19th Computer Vision Winter Workshop 2014</i> (pp. 118–125). Proceedings of the 19th Computer Vision Winter Workshop 2014. http://hdl.handle.net/20.500.12708/55274</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Fuensanta",
                    "last_name": "Torres Garcia",
                    "position": 1,
                    "role": "Author",
                    "tid": "235851"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Zusana",
                    "last_name": "Kúkelová",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Jan",
                    "last_name": "Heller",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59000",
            "handle": "20.500.12708/55275",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Impact of topology-related attributes from local binary patterns on texture classification",
            "keywords": [],
            "abstract": "A general texture description model is proposed, using topology\r\nrelated attributes calculated from Local Binary Patterns (LBP). The\r\nproposed framework extends and generalises existing LBP-based descriptors\r\nlike LBP-rotation invariant uniform patterns (LBPriu2), and Local\r\nBinary Count (LBC). Like them, it allows contrast and rotation invariant\r\nimage description using more compact descriptors than classic LBP.\r\nHowever, its expressiveness, and then its discrimination capability, is\r\nhigher, since it includes additional information, including the number of\r\nconnected components. The impact of the di erent attributes on texture\r\nclassi cation performance is assessed through a systematic comparative\r\nevaluation, performed on three texture datasets. The results validate the\r\ninterest of the proposed approach, by showing that some combinations\r\nof attributes outperform state-of-the-art LBP-based texture descriptors.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Nguyen, T. P., Manzanera, A., &#38; Kropatsch, W. (2014). Impact of topology-related attributes from local binary patterns on texture classification. In A. Hadid, J.-L. Dugelay, &#38; S. Z. Li (Eds.), <i>Proceedings of the 2nd Intl. Workshop on Computer Vision With Local Binary Pattern Variants (ECCV’14)</i> (p. 14). Proceedings of the 2nd Intl. Workshop on Computer Vision With Local Binary Pattern Variants (ECCV’14). http://hdl.handle.net/20.500.12708/55275</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thanh Phuong",
                    "last_name": "Nguyen",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Antoine",
                    "last_name": "Manzanera",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Abdenour",
                    "last_name": "Hadid",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Jean-Luc",
                    "last_name": "Dugelay",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Stan Z.",
                    "last_name": "Li",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59002",
            "handle": "20.500.12708/55277",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Representing roots on the basis of reeb graphs in plant phenotyping",
            "keywords": [],
            "abstract": "This paper presents a new representation for root images\r\nbased on Reeb graphs. The representation proposed captures lengths and\r\ndistances in root structures as well as locations of branches, numbers of\r\nlateral roots and the locations of the root tips. An analysis of root images\r\nusing Reeb graphs is presented and results are compared to ground truth\r\nmeasurements. This paper shows, that the Reeb graph based approach\r\nnot only captures the characteristics needed for phenotyping of plants,\r\nbut it also provides a solution to the problem of overlapping roots in\r\nthe images. Using a Reeb graph based representation, such overlaps can\r\nbe directly detected without further analysis, during the computation of\r\nthe graph.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., Kropatsch, W., Busch, W., &#38; Ristova, D. (2014). Representing roots on the basis of reeb graphs in plant phenotyping. In S. A. Tsaftaris &#38; H. Scharr (Eds.), <i>Proceedings of the ECCV-Workshop on Computer Vision Problems in Plant Phenotyping (CVPPP)</i> (p. 14). Proceedings of the ECCV-Workshop on Computer Vision Problems in Plant Phenotyping (CVPPP). http://hdl.handle.net/20.500.12708/55277</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Wolfgang",
                    "last_name": "Busch",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Daniela",
                    "last_name": "Ristova",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Sotirios A.",
                    "last_name": "Tsaftaris",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Hanno",
                    "last_name": "Scharr",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59003",
            "handle": "20.500.12708/55278",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Image-based phenotyping of the mature arabidopsis shoot system",
            "keywords": [],
            "abstract": "The image-based phenotyping of mature plants faces several\r\nchallenges from the image acquisition to the determination of quanti-\r\ntative characteristics describing their appearance. In this work a frame-\r\nwork to extract geometrical and topological traits of 2D images of mature\r\nArabidopsis thaliana\r\nis proposed. The phenotyping pipeline recovers the\r\nrealistic branching architecture of dried and attened plants in two steps.\r\nIn the rst step, a tracing approach is used for the extraction of centerline\r\nsegments of the plant. In the second step, a hierarchical reconstruction\r\nis done to group the segments according to continuity principles. This\r\npaper covers an overview of the relevant processing steps along the pro-\r\nposed pipeline and provides an insight into the image acquisition as well\r\nas into the most relevant results from the evaluation process.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Augustin, M., Haxhimusa, Y., Busch, W., &#38; Kropatsch, W. (2014). Image-based phenotyping of the mature arabidopsis shoot system. In S. A. Tsaftaris &#38; H. Scharr (Eds.), <i>Proceedings of the ECCV-Workshop on Computer Vision Problems in Plant Phenotyping (CVPPP)</i> (p. 16). Proceedings of the ECCV-Workshop on Computer Vision Problems in Plant Phenotyping (CVPPP). http://hdl.handle.net/20.500.12708/55278</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Marco",
                    "last_name": "Augustin",
                    "position": 1,
                    "role": "Author",
                    "tid": "59438"
                },
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 2,
                    "role": "Author",
                    "tid": "64562"
                },
                {
                    "first_name": "Wolfgang",
                    "last_name": "Busch",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Sotirios A.",
                    "last_name": "Tsaftaris",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Hanno",
                    "last_name": "Scharr",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59004",
            "handle": "20.500.12708/55279",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Topological image analysis and (normalised) representations for plant phenotyping",
            "keywords": [],
            "abstract": "This paper discusses the use of topological image\r\nanalysis to derive characteristics needed in plant phenotyping.\r\nDue to certain features of root systems (deformation over time,\r\noverlaps of branches in a 2D image of the root system) a topolog-\r\nical analysis is needed to correctly derive these characteristics.\r\nThe advantages of such a topological analysis are highlighted\r\nin this paper and root phenotyping is presented as a new\r\napplication for computational topology. Characteristics used in\r\nplant phenotyping that can be derived from root images using\r\nmethods of topological image analysis are further presented. A\r\nReeb graph based representation of root images is shown as\r\nan example for such a topological analysis. Based on a graph\r\nrepresentation a new, normalised representation of root images\r\nis introduced.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., Kropatsch, W., &#38; Busch, W. (2014). Topological image analysis and (normalised) representations for plant phenotyping. In D. M. Onchis &#38; P. Real Jurado (Eds.), <i>Proceedings of the 5th International Workshop on Computational Topology in Image Context, CTIC2014</i> (p. 8). Proceedings of the 5th International Workshop on Computational Topology in Image Context 2014. http://hdl.handle.net/20.500.12708/55279</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Wolfgang",
                    "last_name": "Busch",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Darian M.",
                    "last_name": "Onchis",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Real Jurado",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59005",
            "handle": "20.500.12708/55280",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Characterizing configurations of critical points through lbp.",
            "keywords": [],
            "abstract": "In this abstract we extend ideas and results submit-\r\nted to [3] in which a new codification of Local Binary Patterns\r\n(LBP) is given using combinatorial maps and a method for\r\nobtaining a representative LBP image is developed based on\r\nmerging regions and Minimum Contrast Algorithm. The LBP\r\ncode characterizes the topological category (max, min, slope,\r\nsaddle) of the 2D gray level landscape around the center region.\r\nWe extend the result studying how to merge non-singular slopes\r\nwith one of its neighbors and how to extend the results to non-\r\nwell formed images/maps. Some ideas related to robust LBP and\r\nisolines are also given in last section.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">González-Díaz, R., Kropatsch, W., Cerman, M., &#38; Lamar, J. (2014). Characterizing configurations of critical points through lbp. In D. M. Onchis &#38; P. Real Jurado (Eds.), <i>Proceedings of the 5th International Workshop on Computational Topology in Image Context 2014</i> (p. 4). Proceedings of the 5th International Workshop on Computational Topology in Image Context 2014. http://hdl.handle.net/20.500.12708/55280</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "R.",
                    "last_name": "González-Díaz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Cerman",
                    "position": 3,
                    "role": "Author",
                    "tid": "43978"
                },
                {
                    "first_name": "Javier",
                    "last_name": "Lamar",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Darian M.",
                    "last_name": "Onchis",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Pedro",
                    "last_name": "Real Jurado",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59184",
            "handle": "20.500.12708/55406",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Data Graph Formulation as the Minimum-Weight Maximum-Entropy Problem",
            "keywords": [],
            "abstract": "Consider a point-set coming from an object which was sampled using a digital sensor (depth range, camera, etc). We are interested in finding a graph that would represent that point-set according to some properties. Such a representation would allow us to match two objects (graphs) by exploiting topological properties instead of solely relying on geometrical properties. The Delaunay triangulation is a common out off-the-shelf strategy to triangulate a point-set and it is used by many researchers as the standard way to create the so called data-graph and despite its positive properties, there are also some drawbacks. We are interested in generating a graph with the following properties: the graph is (i) as unique as possible, (ii) and as discriminative as possible regarding the degree distribution. We pose a combinatorial optimization problem (Min-Weight Max-Entropy Problem) to build such a graph by minimizing the total weight cost of the edges and at the same time maximizing the entropy of the degree distribution. Our optimization approach is based on Dynamic Programming (DP) and yields a polynomial time algorithm.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">de Sousa, S., &#38; Kropatsch, W. G. (2015). Data Graph Formulation as the Minimum-Weight Maximum-Entropy Problem. In <i>Graph-Based Representations in Pattern Recognition</i> (pp. 13–22). Springer International Publishing. https://doi.org/10.1007/978-3-319-18224-7_2</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Samuel",
                    "last_name": "de Sousa",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59214",
            "handle": "20.500.12708/55436",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Shape Normalizing and Tracking Dancing Worms",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sansone, C., Pucher, D., Artner, N., Kropatsch, W., Saggese, A., &#38; Vento, M. (2016). Shape Normalizing and Tracking Dancing Worms. In A. Robles-Kelly, M. Loog, B. Biggio, F. Escolano, &#38; R. Wilson (Eds.), <i>Structural, Syntactic, and Statistical Pattern Recognition</i> (pp. 390–400). Structural, Syntactic, and Statistical Pattern Recognition, Springer International. https://doi.org/10.1007/978-3-319-49055-7_35</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Carmine",
                    "last_name": "Sansone",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Daniel",
                    "last_name": "Pucher",
                    "position": 2,
                    "role": "Author",
                    "tid": "255108"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 3,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Alessia",
                    "last_name": "Saggese",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "M.",
                    "last_name": "Vento",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Antonio",
                    "last_name": "Robles-Kelly",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Marco",
                    "last_name": "Loog",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Battista",
                    "last_name": "Biggio",
                    "position": 3,
                    "role": "Editor"
                },
                {
                    "first_name": "Francisco",
                    "last_name": "Escolano",
                    "position": 4,
                    "role": "Editor"
                },
                {
                    "first_name": "Richard",
                    "last_name": "Wilson",
                    "position": 5,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "59218",
            "handle": "20.500.12708/55440",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Similarity Assessment as a Dual Process Model of Counting and Measuring",
            "keywords": [],
            "abstract": "Based on recent findings from the field of human similarity perception, we propose a dual process model\r\n(DPM) of taxonomic and thematic similarity assessment which can be utilised in machine learning applications.\r\nTaxonomic reasoning is related to predicate based measures (counting) whereas thematic reasoning is\r\nmostly associated with metric distances (measuring). We suggest a procedure that combines both processes\r\ninto a single similarity kernel. For each feature dimension of the observational data, an optimal measure is\r\nselected by a Greedy algorithm: A set of possible measures is tested, and the one that leads to improved classification\r\nperformance of the whole model is denoted. These measures are combined into a single SVM kernel\r\nby means of generalisation (converting distances into similarities) and quantisation (applying predicate based\r\nmeasures to interval scale data). We then demonstrate how to apply our model to a classification problem\r\nof MPEG-7 features from a test set of images. Evaluation shows that the performance of the DPM kernel is\r\nsuperior to those of the standard SVM kernels. This supports our theory that the DPM comes closer to human\r\nsimilarity judgment than any singular measure, and it motivates our suggestion to employ the DPM not only\r\nin image retrieval but also in related tasks.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H., &#38; Klauninger, B. (2016). Similarity Assessment as a Dual Process Model of Counting and Measuring. In <i>Proceedings of the 5th International Conference on Pattern Recognition Applications and Methods</i>. 5th International Conference on Pattern Recognition Applications and Methods, Rom, EU. https://doi.org/10.5220/0005655801410147</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                },
                {
                    "first_name": "Bert",
                    "last_name": "Klauninger",
                    "position": 2,
                    "role": "Author",
                    "tid": "93396"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E260-01"
            ],
            "pid": "59276",
            "handle": "20.500.12708/55498",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Line voronoi diagrams using elliptical distances.",
            "keywords": [],
            "abstract": "The paper introduces an Elliptical Line Voronoi diagram. In contrast to the classical approaches, it represents the line segment by its end points, and computes the distance from point to line segment using the Confocal Ellipse-based Distance. The proposed representation offers specific mathematical properties, prioritizes the sites of the greater length and corners with the obtuse angles without using an additional weighting scheme. The above characteristics are suitable for the practical applications such as skeletonization and shape smoothing.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gabdulkhakova, A., Langer, M., Langer, B., &#38; Kropatsch, W. (2018). Line voronoi diagrams using elliptical distances. In <i>Lecture Notes in Computer Science</i>. Springer Nature Switzerland AG 2021. https://doi.org/10.1007/978-3-319-97785-0</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Aysylu",
                    "last_name": "Gabdulkhakova",
                    "position": 1,
                    "role": "Author",
                    "tid": "235653"
                },
                {
                    "first_name": "Maximilian",
                    "last_name": "Langer",
                    "position": 2,
                    "role": "Author",
                    "tid": "254918"
                },
                {
                    "first_name": "Bernhard",
                    "last_name": "Langer",
                    "position": 3,
                    "role": "Author",
                    "tid": "108189"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59336",
            "handle": "20.500.12708/55558",
            "doi": null,
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "The Difficulties of Detecting Deformable Objects Using Deep Neural Networks",
            "keywords": [],
            "abstract": "Object detectors based on deep neuralnetworks have revolutionized the way we look forobjects in an image, outperforming traditional im-age processing techniques. These detectors are of-ten trained on huge datasets of labelled images andare used to detect objects of different classes. We ex-plore how they perform at detecting custom objectsand show how shape and deformability of an objectaffect the detection performance. We propose an au-tomated method for synthesizing the training imagesand target the real-time scenario using YOLOv3 asthe baseline for object detection. We show that rigidobjects have a high chance of being detected withan AP (average precision) of 87.38%. Slightly de-formable objects like scissors and headphones showa drop in detection performance with precision aver-aging at 49.54%. Highly deformable objects like achain or earphones show an even further drop in APto 26.58%.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Djukic, N., Kropatsch, W., &#38; Vincze, M. (2020). The Difficulties of Detecting Deformable Objects Using Deep Neural Networks. In <i>Proceedings of the Joint Austrian Computer Vision and Robotics Workshop 2020</i> (p. 6). https://doi.org/10.3217/978-3-85125-752-6-30</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nikola",
                    "last_name": "Djukic",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Vincze",
                    "position": 3,
                    "role": "Author",
                    "tid": "133231"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "59522",
            "handle": "20.500.12708/55744",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Gestalt Interest Points for Image Description in Weight-Invariant Face Recognition",
            "keywords": [],
            "abstract": "In this work, we propose two improvements of the Gestalt Interest Points (GIP) algorithm for the recognition\r\nof faces of people that have underwent signi cant weight change. The basic assumption is that some interest\r\npoints contribute more to the description of such objects than others. We assume that we can eliminate certain\r\ninterest points to make the whole method more e cient while retaining our classi cation results. To nd out\r\nwhich gestalt interest points can be eliminated, we did experiments concerning contrast and orientation of face\r\nfeatures. Furthermore, we investigated the robustness of GIP against image rotation. The experiments show\r\nthat our method is rotational invariant and { in this practically relevant forensic domain { outperforms the\r\nstate-of-the-art methods such as SIFT, SURF, ORB and FREAK.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hörhan, M., &#38; Eidenberger, H. (2014). Gestalt Interest Points for Image Description in Weight-Invariant Face Recognition. In <i>SPIE Visual Communications Proceedings</i>. SPIE Visual Communications and Image Processing Conference, Paris, FR, EU. SPIE. http://hdl.handle.net/20.500.12708/55744</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Hörhan",
                    "position": 1,
                    "role": "Author",
                    "tid": "40036"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 2,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "59585",
            "handle": "20.500.12708/55805",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Development of Tests to Evaluate the Sensory Abilities of Children with Autism Spectrum Disorder using Touch and Force Sensors",
            "keywords": [],
            "abstract": "An emerging line of research that attempts to reveal underlying mechanisms of Autism Spectrum Disorder (ASD) studies differences in sensory processing in individuals with ASD. In this paper, we introduce new methods to measure proprioceptive functions of children with ASD. The instruments use a low-cost Arduino board and shield to acquire data from force and touch sensors. Data are transferred to mobile devices and analyzed with cross-platform application development tools. The instruments were pilot tested with typically developing children to test for functionality and usability of the instruments. They will be used in a larger study with children with ASD.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Riederer, M., Schönauer, C., Kaufmann, H., Söchting, E., &#38; Lamm, C. (2014). Development of Tests to Evaluate the Sensory Abilities of Children with Autism Spectrum Disorder using Touch and Force Sensors. In <i>Proceedings of 4th International Conference on Wireless Mobile Communication and Healthcare</i> (p. 4). IEEE. http://hdl.handle.net/20.500.12708/55805</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Riederer",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Elisabeth",
                    "last_name": "Söchting",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Claus",
                    "last_name": "Lamm",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "59618",
            "handle": "20.500.12708/55838",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Towards Segmentation of Human Teeth Contours in Dental Radiographs Using Active Shape Models",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sprinzl, M., Kropatsch, W., Langs, G., &#38; Sablatnig, R. (2015). Towards Segmentation of Human Teeth Contours in Dental Radiographs Using Active Shape Models. In P. Wolhart (Ed.), <i>Proceedings of the 20th Computer Vision Winter Workshop</i> (pp. 11–20). Verlag der Technischen Universität Graz. http://hdl.handle.net/20.500.12708/55838</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Sprinzl",
                    "position": 1,
                    "role": "Author",
                    "tid": "45618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 4,
                    "role": "Author",
                    "tid": "133566"
                },
                {
                    "first_name": "Paul",
                    "last_name": "Wolhart",
                    "position": 1,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "5963",
            "handle": "20.500.12708/5978",
            "doi": "10.34726/hss.2016.29064",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Streaming und Exploration von sich dynamisch ändernden Oberflächenrekonstruktionen in Immersive Virtual Reality",
            "keywords": [
                "Virtual Reality",
                "3D Interaction",
                "Oclulus Rift",
                "Virtualizer",
                "Interactive Scene Exploration"
            ],
            "abstract": "Low cost commodity depth cameras like the Microsoft Kinect allow to sense the structure of the environment. With the aid of dense surface reconstruction methods, a detailed 3D model can be computed in real-time from the acquired camera data. Autonomous robots can apply this techniques in order to build a map of the scene while they are exploring it. This allows the robot to locate itself and to navigate in unknown environments. Besides that, the reconstructed model can be interesting for different parties, who want to explore these environments as well. Especially, distant or dangerous areas can be scanned by robots while remote observers are able to safely get an overview of the scene. In order to support remote exploration while the scene is still scanned, the reconstructed information has to be streamed incrementally over wireless network. Since currently no solution exists with this feature, the existing reconstruction framework InfiniTAM is extended to support the transmission of a large scale, dynamically changing model. The visualization and exploration of the model is performed with the aid of Unreal Engine 4, a state-of-the-art 3d engine. For this purpose, a triangular mesh representation is favored, while dense reconstruction methods mostly operate on a volumetric representation. In current approaches, the mesh is extracted in a post-processing step, which is not applicable when the scene should be explored while being scanned and updated. The used reconstruction framework is therefore adapted to maintain an up-to-date mesh in real-time. The reconstructed mesh finally is explored in a virtual reality setup using a head-mounted display and an omnidirectional treadmill. The usage of virtual reality hardware enhances the ease of use and makes it possible to navigate in a natural way. The developed system is  evaluated in terms of memory requirements and data rates as well as within a user study, that analyzes the effect of the incremental streaming and the virtual reality exploration on spatial knowledge acquisition.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kröter, M. (2016). <i>Streaming und Exploration von sich dynamisch ändernden Oberflächenrekonstruktionen in Immersive Virtual Reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.29064</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "11946",
                    "name": "Kroeter Manuel - 2016 - Streaming und Exploration von sich dynamisch aendernden...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 17292193,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5978/2/Kroeter%20Manuel%20-%202016%20-%20Streaming%20und%20Exploration%20von%20sich%20dynamisch%20aendernden...pdf"
                },
                {
                    "bsid": "84916",
                    "name": "Kroeter Manuel - 2016 - Streaming und Exploration von sich dynamisch aendernden...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 237766,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/5978/5/Kroeter%20Manuel%20-%202016%20-%20Streaming%20und%20Exploration%20von%20sich%20dynamisch%20aendernden...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Manuel",
                    "last_name": "Kröter",
                    "position": 1,
                    "role": "Author",
                    "tid": "180261"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "59822",
            "handle": "20.500.12708/56041",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Touch, Movement and Vibration: User Perception of Vibrotactile Feedback for Touch and Mid-Air Gestures",
            "keywords": [],
            "abstract": "Designing appropriate feedback for gesture interfaces is an important aspect of user experience and performance. We conduct the first investigation of users´ perceptions of vibrotactile stimuli during touch and mid-air gesture input for smart devices. Furthermore, we explore perception of feedback that is decoupled from the smart device and delivered outside its operating range by an accessory wearable, i.e., feedback delivered at arm-level. Results show user perception of vibrotactile stimuli up to 80% accurate, which we use to recommend guidelines for practitioners to design new vibrotactile feedback techniques for smart devices.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., Mossel, A., Zaiți, I.-A., &#38; Vatavu, R.-D. (2015). Touch, Movement and Vibration: User Perception of Vibrotactile Feedback for Touch and Mid-Air Gestures. In <i>Human-Computer Interaction – INTERACT 2015</i> (pp. 165–172). INTERACT 2015. https://doi.org/10.1007/978-3-319-22723-8_14</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 2,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Ionuț-Alexandru",
                    "last_name": "Zaiți",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Radu-Daniel",
                    "last_name": "Vatavu",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "59823",
            "handle": "20.500.12708/56042",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Interactive HDR Environment Map Capturing on Mobile Devices",
            "keywords": [],
            "abstract": "Real world illumination, captured by digitizing devices, is beneficial to solve many problems in computer graphics. Therefore, practical methods for capturing this illumination are of high interest. In this paper, we present a novel method for capturing environmental illumination by a mobile device. Our method is highly practical as it requires only a consumer mobile phone and the result can be instantly used for rendering or material estimation. We capture the real light in high dynamic range (HDR) to preserve its high contrast. Our method utilizes the moving camera of a mobile phone in auto-exposure mode to reconstruct HDR values. The projection of the image to the spherical environment map is based on the orientation of the mobile device. Both HDR reconstruction and projection run on the mobile GPU to enable interactivity. Moreover, an additional image alignment step is performed. Our results show that the presented method faithfully captures the real environment and that the rendering with our reconstructed environment maps achieves high quality, comparable to reality.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P. (2015). Interactive HDR Environment Map Capturing on Mobile Devices. In <i>EUROGRAPHICS 2015 Short papers</i> (pp. 29–32). Eurographics Association. http://hdl.handle.net/20.500.12708/56042</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "59834",
            "handle": "20.500.12708/56053",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Towards An Immersive Virtual Reality Training System For CBRN Disaster Preparedness",
            "keywords": [],
            "abstract": "Over the past decade, training in virtual reality for military and disaster preparedness has been increasingly recognized as an important adjunct to traditional modalities of real-life drills. However, there are only a few existing solutions that provide immersive virtual reality training that implies improved learning through an increased amount of presence. In this paper, we present a thorough analysis of the state of the art of virtual reality training systems and outline the requirements of two peer stakeholders for disaster relief with an explicit focus on CBRN disaster preparedness. We compare both analyses to specify if - and to which extent - existing virtual reality training solutions meet the stakeholder requirements. Based on the comparison, we present an outlook on existing and upcoming virtual reality components that have the potential to fulfil the stakeholders' requirements of a flexible multi-user immersive virtual reality training system.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., Peer, A., Göllner, J., &#38; Kaufmann, H. (2015). Towards An Immersive Virtual Reality Training System For CBRN Disaster Preparedness. In <i>The 12th International Multidisciplinary Modeling &#38; Simulation Multiconference</i>. DIME University of Genoa, DIMEH University of Calabria. http://hdl.handle.net/20.500.12708/56053</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Peer",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Johannes",
                    "last_name": "Göllner",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "59835",
            "handle": "20.500.12708/56054",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Requirements Analysis On A Virtual Reality Training System For CBRN Crisis Preparedness",
            "keywords": [],
            "abstract": "Over the past decade, training in virtual reality for military and disaster preparedness has been increasingly recognized as an important adjunct to traditional modalities of real-life drills. However, there are only a few existing solutions that provide immersive virtual reality training that implies improved learning through an increased amount of presence. In this paper, we present a thorough analysis of the state of the art of virtual reality training systems and outline the requirements of two peer stakeholders for disaster relief with an explicit focus on CBRN disaster preparedness. We discuss both analyses to draw conclusions if - and to which extent - existing virtual reality training solutions meet the stakeholder requirements. As a conclusion, we outline future research that must be conducted to fulfil the stakeholders´ requirements of a flexible multi-user immersive virtual reality training system.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., Peer, A., Göllner, J., &#38; Kaufmann, H. (2015). Requirements Analysis On A Virtual Reality Training System For CBRN Crisis Preparedness. In <i>Proceedings of the 59th Annual Meeting of the ISSS</i>. International Society for the Systems Sciences, Berlin, EU. Inderscience Publisher. http://hdl.handle.net/20.500.12708/56054</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Peer",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Johannes",
                    "last_name": "Göllner",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E194-01"
            ],
            "pid": "59892",
            "handle": "20.500.12708/56105",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Indoor skydiving in immersive virtual reality with embedded storytelling",
            "keywords": [],
            "abstract": "We describe the Virtual Jump Simulator, which allows subjects to\r\nperform an indoor parachute jump in a virtual environment. The\r\nnecessity to jump physically off a platform combined with immersive\r\nvirtual reality and tactile feedback creates an experience with\r\na high amount of presence, as the evaluation of the prototype confirms.\r\nThe system consists of a steel cube, a mechanical absorber\r\nsystem with stacked eccentric wheels and counterweights that allows\r\nsubjects in the weight range from 35 to 150kg to jump without\r\nthe need for individual calibration, a virtual reality setup with\r\nhigh-quality 3D content and tactile stimuli. In the immersive virtual\r\njump experience, we embed a story using rich multimedia content,\r\nsuch as images and sound. We iteratively tested the entire system\r\nwith users of different backgrounds. Thereby, we gathered user\r\nfeedback from the very beginning to create a novel virtual reality\r\nsystem that allows for actual physical jumping and flying with free\r\nbody movement.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H., &#38; Mossel, A. (2015). Indoor skydiving in immersive virtual reality with embedded storytelling. In <i>Proceedings of the 21st ACM Symposium on Virtual Reality Software and Technology</i>. ACM Symposium on Virtual Reality Software and Technology (VRST), Beijing, China, Non-EU. Proceedings of the 21st ACM Symposium on Virtual Reality Software and Technology. https://doi.org/10.1145/2821592.2821612</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 2,
                    "role": "Author",
                    "tid": "58429"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "59934",
            "handle": "20.500.12708/56147",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Mobile Multiview Diffuse Texture Extraction",
            "keywords": [],
            "abstract": "This paper presents a novel method for diffuse texture extraction from a set of multiview images. We address the problem of specularities removal by pixel value minimization across multiple automatically aligned input images. Our method is based on the fact that the presence of specular reflection only increases the captured pixel value. Moreover, we propose an algorithm for estimation of material region in the image by optimization on the GPU. Previous methods for diffuse component separation from multiple images require a complex hardware setup. In contrast to that, our method is highly usable because only a mobile phone is needed to reconstruct diffuse texture in an environment with arbitrary lighting. Moreover, our method is fully automatic and besides capturing of images from multiple viewpoints it does not require any user intervention. Many fields can benefit from our method, particularly material reconstruction, image processing, and digital content creation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., &#38; Kaufmann, H. (2015). Mobile Multiview Diffuse Texture Extraction. In <i>Smart Tools and Applications in Computer Graphics</i> (pp. 113–120). Eurographics Association. http://hdl.handle.net/20.500.12708/56147</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "59941",
            "handle": "20.500.12708/56154",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Towards Segmentation of Human Teeth Contours in Dental Radiographs using Active Shape Models",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sprinzl, M., Kropatsch, W., Sablatnig, R., &#38; Langs, G. (2015). Towards Segmentation of Human Teeth Contours in Dental Radiographs using Active Shape Models. In <i>Symposium on Statistical Shape Modelling and Applications - Porceedings</i> (p. 10). http://hdl.handle.net/20.500.12708/56154</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Sprinzl",
                    "position": 1,
                    "role": "Author",
                    "tid": "45618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 3,
                    "role": "Author",
                    "tid": "133566"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "59954",
            "handle": "20.500.12708/56167",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "High-Quality Consistent Illumination in Mobile Augmented Reality by Radiance Convolution on the GPU",
            "keywords": [],
            "abstract": "Consistent illumination of virtual and real objects in augmented reality (AR) is essential to achieve visual coherence. This paper presents a practical method for rendering with consistent illumination in AR in two steps. In the first step, a user scans the surrounding environment by rotational motion of the mobile device and the real illumination is captured. We capture the real light in high dynamic range (HDR) to preserve its high contrast. In the second step, the captured environment map is used to precalculate a set of reflection maps on the mobile GPU which are then used for real-time rendering with consistent illumination. Our method achieves high quality of the reflection maps because the convolution of the environment map by the BRDF is calculated accurately per each pixel of the output map. Moreover, we utilize multiple render targets to calculate reflection maps for multiple materials simultaneously. The presented method for consistent illumination in AR is beneficial for increasing visual coherence between virtual and real objects. Additionally, it is highly practical for mobile AR as it uses only a commodity mobile device.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., Unterguggenberger, J., &#38; Kaufmann, H. (2015). High-Quality Consistent Illumination in Mobile Augmented Reality by Radiance Convolution on the GPU. In G. Bebis (Ed.), <i>ISVC 2015, Part I, LNCS 9474</i> (pp. 574–585). Springer. http://hdl.handle.net/20.500.12708/56167</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Johannes",
                    "last_name": "Unterguggenberger",
                    "position": 2,
                    "role": "Author",
                    "tid": "42532"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "George",
                    "last_name": "Bebis",
                    "position": 1,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59958",
            "handle": "20.500.12708/56171",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Novel concepts for recognition and representation of structure in spatio-temporal classes of images",
            "keywords": [],
            "abstract": "This paper discusses open problems and\r\nfuture research regarding the recognition and rep-\r\nresentation of structures in sequences of either 2D\r\nimages or 3D data. All presented concepts aim at\r\nimproving the recognition of structure in data (espe-\r\ncially by decreasing the influence of noise) and at\r\nextending the representational power of known de-\r\nscriptors (within the scope of this paper graphs and\r\nskeletons). For the recognition of structure critical\r\npoints of a shape may be computed. We present an\r\napproach to derive such critical points based on a\r\ncombination of skeletons and local features along a\r\nskeleton. We further consider classes of data (for\r\nexample a temporal sequence of images of an ob-\r\nject), instead of a single data sample only. This so\r\ncalled co-analysis reduces the sensitivity of analysis\r\nto noise in the data. Moreover, a representative for a\r\nwhole class can be provided. Temporal sequences\r\nmay not only be used as a class of data in a co-\r\nanalysis process - focusing on the temporal aspect\r\nand changes of the data over time an analysis of these\r\nchanges is needed. For this purpose we explore the\r\npossibility to analyse a shape over time and to derive\r\na spatio-temporal representation. To extend the rep-\r\nresentational power of skeletons we further present\r\nan extension to skeletons using model fitting.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., &#38; Kropatsch, W. (2015). Novel concepts for recognition and representation of structure in spatio-temporal classes of images. In P. Wohlhart &#38; V. Lepetit (Eds.), <i>Proceedings of the 20th Computer Vision Winter Workshop Seggau, Austria</i> (pp. 49–56). TU Graz. http://hdl.handle.net/20.500.12708/56171</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Paul",
                    "last_name": "Wohlhart",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Vincent",
                    "last_name": "Lepetit",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "59961",
            "handle": "20.500.12708/56174",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Influence of Path Complexity on Spatial Overlap Perception in Virtual Environments",
            "keywords": [],
            "abstract": "Real walking in large virtual indoor environments within a limited real world workspace requires effective spatial compression methods. These methods should be unnoticed by the user. Scene manipulation that creates overlapping spaces has been suggested in recent work. However, there is little research focusing on users´ perception of over-lapping spaces depending on the layout of the environment. In this paper we investigate how the complexity of the path influences the perception of the overlapping spaces it connects. We compare three spatial virtual layouts with paths that differ in complexity (length and number of turns). Our results suggest that an increase of the path´s length is less efficient in decreasing overlap detection than a combination of length and additional turns. Furthermore, combination of paths that differ in complexity influences the distance perception within overlapping spaces.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vasylevska, K., &#38; Kaufmann, H. (2015). Influence of Path Complexity on Spatial Overlap Perception in Virtual Environments. In M. Imura, P. Figueroa, &#38; B. Mohler (Eds.), <i>Proceedings of International Conference on Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments</i> (pp. 1–8). The Eurographics Association. https://doi.org/10.2312/egve.20151324</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Masataka",
                    "last_name": "Imura",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Pablo",
                    "last_name": "Figueroa",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Betty",
                    "last_name": "Mohler",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59964",
            "handle": "20.500.12708/56177",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "The minimum spanning tree of maximum entropy",
            "keywords": [],
            "abstract": "In computer vision, we have the problem of creating graphs\r\nout of unstructured point-sets, i.e. the data graph. A common approach\r\nfor this problem consists of building a triangulation which might not al-\r\nways lead to the best solution. Small changes in the location of the points\r\nmight generate graphs with unstable configurations and the topology of\r\nthe graph could change significantly. After building the data-graph, one\r\ncould apply Graph Matching techniques to register the original point-sets.\r\nIn this paper, we propose a data graph technique based on the Minimum\r\nSpanning Tree of Maximum Entropty (MSTME). We aim at a data graph\r\nconstruction which could be more stable than the Delaunay triangula-\r\ntion with respect to small variations in the neighborhood of points. Our\r\ntechnique aims at creating data graphs which could help the point-set reg-\r\nistration process. We propose an algorithm with a single free parameter\r\nthat weighs the importance between the total weight cost and the entropy\r\nof the current spanning tree. We compare our algorithm on a number of\r\ndifferent databases with the Delaunay triangulation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">de Sousa, S., &#38; Kropatsch, W. (2015). The minimum spanning tree of maximum entropy. In <i>Proceedings of the ÖAGM Workshop 2015, Salzburg, Austria, May 2015</i> (p. 7). http://hdl.handle.net/20.500.12708/56177</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Samuel",
                    "last_name": "de Sousa",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59965",
            "handle": "20.500.12708/56178",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "LBP and Irregular Graph Pyramids",
            "keywords": [],
            "abstract": "In this paper, a new codification of Local Binary Patterns\r\n(LBP) is given using graph pyramids. The LBP code characterizes the\r\ntopological category (local max, min, slope, saddle) of the gray level\r\nlandscape around the center region. Given a 2D grayscale image\r\nI\r\n,our\r\ngoal is to obtain a simplified image which can be seen as &quot;minimal&quot;\r\nrepresentation in terms of topological characterization of\r\nI\r\n.Forthis,a\r\nmethod is developed based on merging regions and Minimum Contrast\r\nAlgorithm.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Cerman, M., Gonzalez-Diaz, R., &#38; Kropatsch, W. (2015). LBP and Irregular Graph Pyramids. In N. Petkov &#38; G. Azzopardi (Eds.), <i>Computer Analysis of Images and Patterns</i> (pp. 687–699). Lecture Notes in Computer Science. https://doi.org/10.1007/978-3-319-23117-4_59</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Cerman",
                    "position": 1,
                    "role": "Author",
                    "tid": "43978"
                },
                {
                    "first_name": "Rocio",
                    "last_name": "Gonzalez-Diaz",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Nicolai",
                    "last_name": "Petkov",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "George",
                    "last_name": "Azzopardi",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "59966",
            "handle": "20.500.12708/56179",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Reeb Graphs Through Local Binary Patterns",
            "keywords": [],
            "abstract": "This paper presents an approach to derive critical points of a shape, the basis of a Reeb graph, using a combination of a medial axis skeleton and features along this skeleton. A Reeb graph captures the topology of a shape. The nodes in the graph represent critical points (positions of change in the topology), while edges represent topological persistence. We present an approach to compute such critical points using Local Binary Patterns. For one pixel the Local Binary Pattern feature vector is derived comparing this pixel to its neighbouring pixels in an environment of a certain radius. We start with an initial segmentation and a medial axis representation. Along this axis critical points are computed using Local Binary Patterns with the radius, defining the neighbouring pixels, set a bit larger than the radius according to the medial axis transformation. Critical points obtained in this way form the node set in a Reeb graph, edges are given through the connectivity of the skeleton. This approach aims at improving the representation of flawed segmented data. In the same way segmentation artefacts, as for example single pixels representing noise, may be corrected based on this analysis.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., &#38; Kropatsch, W. G. (2015). Reeb Graphs Through Local Binary Patterns. In <i>Graph-Based Representations in Pattern Recognition</i> (pp. 54–63). Springer International Publishing. https://doi.org/10.1007/978-3-319-18224-7_6</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E389-02"
            ],
            "pid": "59975",
            "handle": "20.500.12708/56188",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A combined distance measure for 2D shape matching",
            "keywords": [],
            "abstract": "We present a method for 2D shape matching using\r\na combination of distance functions and discrete curvature. The\r\neccentricity transform computes the longest geodesic distance\r\nacross the object. This transform is invariant to translation\r\nand rotation. The maximal eccentricity points yield diameters\r\nacross the image. We compute the Euclidean distances from the\r\nboundary to the diameter to characterize the curvature of the\r\nshape. Our shape descriptor is comprised of the best matches\r\nretrieved from the normalized histogram of the eccentricities, the\r\nHausdorff distance between the set of distances to the diameter\r\nand a measure of the number of points lying on either side of the\r\ndiameter along with the peak values. We evaluate this descriptor\r\non 2D image databases consisting of rigid and articulated shapes\r\nby ranking the number of matches. In almost all cases, the shapes\r\nare matched with at least one shape from the same class.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ramachandran, G., &#38; Kropatsch, W. (2015). A combined distance measure for 2D shape matching. In <i>International Conference on Computer Vision and Image Analysis Applications</i>. International Conference on Computer Vision and Image Analysis, ICCVIA2015, Tunesien, Suisse, Non-EU. IEEE: Computer Vision and Image Analysis Applications (ICCVIA), 2015 International Conference on. https://doi.org/10.1109/iccvia.2015.7351875</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Geetha",
                    "last_name": "Ramachandran",
                    "position": 1,
                    "role": "Author",
                    "tid": "195689"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "60066",
            "handle": "20.500.12708/56279",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Mutual Proximity Awareness in Immersive Multi-User Virtual Environments with Real Walking",
            "keywords": [],
            "abstract": "Are users aware of each other in an immersive multi-user virtual environment if they cannot see or hear each\r\nother? We present a study on users´ awareness of other users who share the same physical space. The goal of our\r\nresearch is to investigate proximity awareness when walking in multi-user virtual environments. The high degree\r\nof immersion in our virtual environment is achieved through the use of a head-mounted display and real walking in\r\na large tracked space. In our experiment, pairs of participants are required to walk on pre-defined paths towards\r\nor side by side to each other and point at their test partners if they feel their presence. Our results show that 1/3 of\r\nthe participants who had a-priori knowledge about the possible proximity of their test partners could notice their\r\ntest partners during the experiment at a distance shorter than 1m. The test subjects who did not have any a-priori\r\nknowledge proved to be not aware of other users.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Podkosova, I., &#38; Kaufmann, H. (2015). Mutual Proximity Awareness in Immersive Multi-User Virtual Environments with Real Walking. In M. Imura, P. Figueroa, &#38; B. Mohler (Eds.), <i>Proceedings of International Conference on Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments</i> (pp. 109–116). Eurographics Association. https://doi.org/10.2312/egve.20151317</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Masataka",
                    "last_name": "Imura",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Pablo",
                    "last_name": "Figueroa",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Betty",
                    "last_name": "Mohler",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60086",
            "handle": "20.500.12708/56299",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Ellipse-Fitting Approaches Based On Medial Representation",
            "keywords": [],
            "abstract": "This paper presents two approaches related to the area of computer vision, where shape modelling (representation) is an essential part of every system. Specifically, the problem of ellipse fitting is approached. The developed methods deal with elongated round-ended shapes, and represent them with ellipses based on medial representation. The evaluation shows that the proposed approaches outperform the state-of-the-art techniques on the examined dataset.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gabdulkhakova, A., &#38; Kropatsch, W. (2015). Ellipse-Fitting Approaches Based On Medial Representation. In <i>Proceedings of 3rd International Conference on Intelligent Technologies for Information Processing and Management (ITIPM´2015)</i> (pp. 168–172). http://hdl.handle.net/20.500.12708/56299</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Aysylu",
                    "last_name": "Gabdulkhakova",
                    "position": 1,
                    "role": "Author",
                    "tid": "235653"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06"
            ],
            "pid": "60190",
            "handle": "20.500.12708/56403",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Interactive Segmentation of Rock-Art in High-Resolution 3D Reconstructions",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zeppelzauer, M., Poier, G., Seidl, M., Reinbacher, C., Breiteneder, C., &#38; Bischof, H. (2016). Interactive Segmentation of Rock-Art in High-Resolution 3D Reconstructions. In <i>Proceedings of the International Digital Heritage Conference</i> (p. 8). http://hdl.handle.net/20.500.12708/56403</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Poier",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Seidl",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Reinbacher",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 5,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 6,
                    "role": "Author",
                    "tid": "126596"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06"
            ],
            "pid": "60229",
            "handle": "20.500.12708/56442",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Group Feature Selection for Audio-Based Video Genre Classification",
            "keywords": [],
            "abstract": "The performance of video genre classification approaches strongly depends on the selected feature set. Feature selection requires for expert knowledge and is commonly driven by the underlying data, investigated video genres, and previous experience in related application scenarios. An alteration of the genres of interest results in reconsideration of the employed features by an expert. In this work, we introduce an unsupervised method for the selection of features that efficiently represent the underlying data. Performed experiments in the context of audio-based video genre classification demonstrate the outstanding performance of the proposed approach and its robustness across different video datasets and genres.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sageder, G., Zaharieva, M., &#38; Breiteneder, C. (2016). Group Feature Selection for Audio-Based Video Genre Classification. In <i>MultiMedia Modeling</i> (pp. 29–41). https://doi.org/10.1007/978-3-319-27671-7_3</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Gerhard",
                    "last_name": "Sageder",
                    "position": 1,
                    "role": "Author",
                    "tid": "64498"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 2,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 3,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "220239"
            ]
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "60259",
            "handle": "20.500.12708/56472",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Digital Vibrons: Understanding Users' Perceptions of Interacting with Invisible, Zero-Weight Matter",
            "keywords": [],
            "abstract": "We investigate in this work users´ perceptions of interacting\r\nwith invisible, zero-weight digital matter for smart mobile scenarios.\r\nTo this end, we introduce the concept of a digital vibron\r\nas vibrational manifestation of a digital object located outside\r\nits container device. We exemplify gesture-based interactions\r\nand show how thinking interactions in terms of digital vibrons\r\nleads to new experiences in the mixed physical-digital space.\r\nWe present results of a user study showing high scores on\r\nusers´ perceived experience. We also discuss the results of an\r\nelicitation study for collecting users´ preferences for vibration\r\npatterns to inform design of vibrotactile feedback for digital\r\nvibrons. We hope that our results on this first exploration of\r\ndigital vibrons will inspire researchers and practitioners to\r\nfurther develop our concept and design localized vibrotactile\r\nfeedback for digital objects outside their smart devices toward\r\nnew interactive experiences in the physical-digital space.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vatavu, R.-D., Mossel, A., &#38; Schönauer, C. (2016). Digital Vibrons: Understanding Users’ Perceptions of Interacting with Invisible, Zero-Weight Matter. In <i>Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services</i> (pp. 217–226). ACM. https://doi.org/10.1145/2935334.2935364</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Radu-Daniel",
                    "last_name": "Vatavu",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 2,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 3,
                    "role": "Author",
                    "tid": "54835"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-05"
            ],
            "pid": "60275",
            "handle": "20.500.12708/56488",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "A Hybrid Sound Model for 3D Audio Games with Real Walking",
            "keywords": [],
            "abstract": "Spatialized audio is the only output that players receive in audio games. In order to provide a realistic view of the environment, it has to be of superior quality in terms of immersion and realism. Complex sound models can be used to generate realistic sound effects, including reflections and reverb. An implementation of a hybrid sound model based on the ODEON approach is introduced and adapted for real-time sound calculations. This model is evaluated and compared to a baseline model usually used in audio games in a user study in a virtual reality environment. The results show that the implemented hybrid model allows players to adjust to the game faster and provides them more support in avoiding virtual obstacles in simple room geometries than the baseline model.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Podkosova, I., Urbanek, M., &#38; Kaufmann, H. (2016). A Hybrid Sound Model for 3D Audio Games with Real Walking. In <i>Proceedings of the 29th International Conference on Computer Animation and Social Agents</i>. ACM. https://doi.org/10.1145/2915926.2915948</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Michael",
                    "last_name": "Urbanek",
                    "position": 2,
                    "role": "Author",
                    "tid": "277244"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "60276",
            "handle": "20.500.12708/56489",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "ImmersiveDeck: A large-scale wireless VR system for multiple users",
            "keywords": [],
            "abstract": "We present preliminary results of work on a low-cost multi-user\r\nimmersive Virtual Reality system that enables collaborative experiences\r\nin large virtual environments. In the proposed setup at least\r\nthree users can walk and interact freely and untethered in a 200 m2\r\narea. The required equipment is worn on the body and rendering is\r\nperformed locally on each user to minimize latency. Inside-out optical\r\nhead tracking is coupled with a low-cost motion capture suit to\r\ntrack the full body and the head. Movements of users, 3D interactions\r\nand the positions of selected real world objects are distributed\r\nover a wireless network in a server-client architecture. As a result,\r\nusers see the effect of their interactions with objects and other\r\nusers in real time. We describe the architecture of our implemented\r\nproof-of-concept system.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Podkosova, I., Vasylevska, K., Schönauer, C., Vonach, E., Fikar, P., Broneder, E., &#38; Kaufmann, H. (2016). ImmersiveDeck: A large-scale wireless VR system for multiple users. In <i>Proceesings of the 9th Workshop on Software Engineering and Architectures for Realtime Interactive Systems SEARIS 2016</i>. IEEE Xplore Digital Library. http://hdl.handle.net/20.500.12708/56489</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 2,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 3,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 4,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Fikar",
                    "position": 5,
                    "role": "Author",
                    "tid": "40663"
                },
                {
                    "first_name": "Elisabeth",
                    "last_name": "Broneder",
                    "position": 6,
                    "role": "Author",
                    "tid": "57709"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 7,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-02"
            ],
            "pid": "60278",
            "handle": "20.500.12708/56491",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Design of a Health Monitoring Toy for Children",
            "keywords": [],
            "abstract": "Especially for young children measuring their physiological\r\nparameters to assess their health can be stressful, even when\r\nconducted at home by their parents. Therefore we present a\r\nconcept that can relieve some of the anxiety correlated with\r\nan examination and implemented it in a test setup we call\r\n&quot;MediCubes&quot; to investigate how this approach is received.\r\nIn this system cube shaped tangible objects are fitted with\r\nnoninvasive sensors measuring pulse, temperature, blood\r\noxygen saturation and lung capacity while interacting with\r\nthem. Incorporation in a storytelling game allows guiding a\r\nchild through a series of unperceived physiological\r\nmeasurements as an enjoyable experience. The acquired\r\ndata is stored on a smartphone and can be reviewed by\r\nparents or doctors.\r\nIn this paper the design process and the developed hardand\r\nsoftware are presented. Furthermore we report on a\r\nusability study with 8 children and 12 adults indicating high\r\nacceptance and enjoyment of the system. These results as\r\nwell as our &quot;lessons learned&quot; could have implications on\r\nthe future development of home health monitoring toys.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vonach, E., Ternek, M., Gerstweiler, G., &#38; Kaufmann, H. (2016). Design of a Health Monitoring Toy for Children. In <i>Proceedings of the The 15th International Conference on Interaction Design and Children</i>. International Conference for Interaction Design and Children (IDC), Tampere, Finland, EU. https://doi.org/10.1145/2930674.2930694</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Marianne",
                    "last_name": "Ternek",
                    "position": 2,
                    "role": "Author",
                    "tid": "59780"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 3,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-02",
                "E193-06"
            ],
            "pid": "60308",
            "handle": "20.500.12708/56521",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Simulation of Robust PCA for Supervised Audio Outlier Detection",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brodinova, S., Ortner, T., Filzmoser, P., Zaharieva, M., &#38; Breiteneder, C. (2015). Simulation of Robust PCA for Supervised Audio Outlier Detection. In <i>Eighth International Workshop on Simulation: Book of Abstracts</i>. International Workshop on Simulation, Vienna, Austria. http://hdl.handle.net/20.500.12708/56521</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 1,
                    "role": "Author",
                    "tid": "281480"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 2,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 3,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 4,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 5,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "220239"
            ]
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-02",
                "E193-06"
            ],
            "pid": "60312",
            "handle": "20.500.12708/56525",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Evaluation of robust PCA for supervised audio outlier detection",
            "keywords": [],
            "abstract": "Outliers often reveal crucial information about the underlying data such as the presence of unusual observations that require for in-depth analysis. The detection of outliers is especially challenging in real-world application scenarios dealing with high-dimensional and flat data bearing different subpopulations of potentially varying data distributions. In the context of high-dimensional data, PCA-based methods are commonly applied to reduce dimensionality and to reveal outliers. Thus, a thorough empirical evaluation of various PCA-based methods for the detection of outliers in a challenging audio data set is provided. The various experimental data settings are motivated by the requirements of real-world scenarios, such as varying number of outliers, available training data, and data characteristics in terms of potential subpopulations.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brodinova, S., Ortner, T., Filzmoser, P., Zaharieva, M., &#38; Breiteneder, C. (2016). Evaluation of robust PCA for supervised audio outlier detection. In <i>Proceeding of 22nd International Conference on Computational Statistics (COMPSTAT)</i> (p. 12). http://hdl.handle.net/20.500.12708/56525</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 1,
                    "role": "Author",
                    "tid": "281480"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 2,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 3,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 4,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 5,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "220239"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60384",
            "handle": "20.500.12708/56596",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "2d tracking of platynereis dumerilii worms during spawning",
            "keywords": [],
            "abstract": "Platynereis dumerilii are marine worms\r\nthat reproduce by external fertilisation and exhibit\r\nparticular swimming behaviours during spawning.\r\nIn this paper we propose a novel worm tracking\r\napproach that enables the 2D tracking and feature\r\nextraction during the spawning process of these\r\nworms.\r\nThe gathered data will be used in the\r\nfuture to characterise and compare male and female\r\nspawning behaviours.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pucher, D., Artner, N., &#38; Kropatsch, W. (2016). 2d tracking of platynereis dumerilii worms during spawning. In L. Cehovin, R. Mandeljc, &#38; V. Struc (Eds.), <i>Proceedings of the 21st Computer Vision Winter Workshop</i> (p. 9). Slovenian Pattern Recognition Society. http://hdl.handle.net/20.500.12708/56596</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Daniel",
                    "last_name": "Pucher",
                    "position": 1,
                    "role": "Author",
                    "tid": "255108"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Luka",
                    "last_name": "Cehovin",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Rok",
                    "last_name": "Mandeljc",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Vitomir",
                    "last_name": "Struc",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60385",
            "handle": "20.500.12708/56597",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A Novel Concept for Smart Camera Image Stitching",
            "keywords": [],
            "abstract": "As panoramic images are widely used\r\nin many applications, efficient image stitching meth-\r\nods that provide visually pleasant image mosaics are\r\nneeded. In this paper we discuss a novel concept for\r\nsmart camera image stitching based on graph pyra-\r\nmids. For a multi-camera system, the images have\r\nto be aligned accordingly to create an image mosaic.\r\nInstead of calculating the corresponding transforma-\r\ntions centrally, we aim at enabling each camera to in-\r\ndividually calculate the transformation of the image\r\nit takes. Graph pyramids used for image segmenta-\r\ntion provide information about the segmentation pro-\r\ncess. We analyze how this information can be used to\r\ncalculate the transformations for image alignment.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Banaeyan, M., Huber, H., Kropatsch, W., &#38; Barth, R. (2016). A Novel Concept for Smart Camera Image Stitching. In L. Cehovin, R. Mandeljc, &#38; V. Struc (Eds.), <i>Proceedings of the 21st Computer Vision Winter Workshop</i> (p. 9). Slovenian Pattern Recognition Society. http://hdl.handle.net/20.500.12708/56597</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Majid",
                    "last_name": "Banaeyan",
                    "position": 1,
                    "role": "Author",
                    "tid": "286018"
                },
                {
                    "first_name": "Hanna",
                    "last_name": "Huber",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Raphael",
                    "last_name": "Barth",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Luka",
                    "last_name": "Cehovin",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Rok",
                    "last_name": "Mandeljc",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Vitomir",
                    "last_name": "Struc",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60387",
            "handle": "20.500.12708/56599",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Shape Classification according to LBP Persistence of Critical Points",
            "keywords": [],
            "abstract": "This paper introduces a shape descriptor based on a com-\r\nbination of topological image analysis and texture information. Critical\r\npoints of a shape´s skeleton are determined first. The shape is described\r\naccording to persistence of the local topology at these critical points over\r\na range of scales. The local topology over scale-space is derived using the\r\nlocal binary pattern texture operator with varying radii. To visualise\r\nthe descriptor, a new type of persistence graph is defined which cap-\r\ntures the evolution, respectively persistence, of the local topology. The\r\npresented shape descriptor may be used in shape classification or the\r\ngrouping of shapes into equivalence classes. Classification experiments\r\nwere conducted for a binary image dataset and the promising results are\r\npresented. Because of the use of persistence, the influence of noise or\r\nirregular shape boundaries (e.g. due to segmentation artefacts) on the\r\nresult of such a classification or grouping is bounded.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., &#38; Kropatsch, W. (2016). Shape Classification according to LBP Persistence of Critical Points. In <i>Proceedings of the 21st Computer Vision Winter Workshop</i> (p. 12). Slovenian Pattern Recognition Society. http://hdl.handle.net/20.500.12708/56599</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60390",
            "handle": "20.500.12708/56602",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Shape Classification According to LBP Persistence of Critical Points",
            "keywords": [],
            "abstract": "This paper introduces a shape descriptor based on a com-\r\nbination of topological image analysis and texture information. Critical\r\npoints of a shape´s skeleton are determined first. The shape is described\r\naccording to persistence of the local topology at these critical points over\r\na range of scales. The local topology over scale-space is derived using the\r\nlocal binary pattern texture operator with varying radii. To visualise\r\nthe descriptor, a new type of persistence graph is defined which cap-\r\ntures the evolution, respectively persistence, of the local topology. The\r\npresented shape descriptor may be used in shape classification or the\r\ngrouping of shapes into equivalence classes. Classification experiments\r\nwere conducted for a binary image dataset and the promising results are\r\npresented. Because of the use of persistence, the influence of noise or\r\nirregular shape boundaries (e.g. due to segmentation artefacts) on the\r\nresult of such a classification or grouping is bounded.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., &#38; Kropatsch, W. (2016). Shape Classification According to LBP Persistence of Critical Points. In N. Normand, J. Guédon, &#38; F. Autrusseau (Eds.), <i>Discrete Geometry for Computer Imagery</i> (pp. 166–177). Lecture Notes in Computer Science - Springer, Berlin Heidelberg. https://doi.org/10.1007/978-3-319-32360-2_13</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Nicolas",
                    "last_name": "Normand",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Jeanpierre",
                    "last_name": "Guédon",
                    "position": 2,
                    "role": "Editor"
                },
                {
                    "first_name": "Florent",
                    "last_name": "Autrusseau",
                    "position": 3,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60392",
            "handle": "20.500.12708/56604",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Persistence Based on LBP Scale Space",
            "keywords": [],
            "abstract": "This paper discusses the connection between the texture operator LBP\r\n(local binary pattern) and an application of LBPs to persistent homology. A shape\r\nrepresentation - the LBP scale space - is defined as a filtration based on the varia-\r\ntion of an LBP parameter. A relation between the LBP scale space and a variation\r\nof thresholds used in the segmentation of a graylevel image is discussed. Using\r\nthe LBP scale space a characterization of (parts of) shapes is demonstrated based\r\non simple shape primitives, the observations may also be generalized for smooth\r\ncurves. The LBP scale space is augmented by associating it with polar coordi-\r\nnates (with the origin located at the LBP center). In this way a procedure of shape\r\nreconstruction based on the LBP scale space is defined and its reconstruction ac-\r\ncuracy is demonstrated in an experiment. Furthermore, this augmented LBP scale\r\nspace representation is invariant to translation and rotation of the shape.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., &#38; Kropatsch, W. G. (2016). Persistence Based on LBP Scale Space. In J.-L. Mari &#38; A. Bac (Eds.), <i>Computational Topology in Image Context</i> (pp. 240–252). Lecture Notes in Computer Science, Springer-Verlag New York, Inc. New York, NY, USA ©2016. https://doi.org/10.1007/978-3-319-39441-1_22</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Jean-Luc",
                    "last_name": "Mari",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Alexandra",
                    "last_name": "Bac",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60393",
            "handle": "20.500.12708/56605",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Noise Robustness of Irregular Graph Pyramids using LBP and Combinatorial Maps",
            "keywords": [],
            "abstract": "In this paper, we briefly introduce the SCIS algorithm - a hierarchical image segmentation approach\r\nbased on LBP pyramids - and evaluate its robustness to uniform, Gaussian, and Poisson distributed\r\nadditive chromatic noise. Moreover, we study the influence of image properties such as the amount\r\nof details and SNR on the segmentation performance. Our evaluation shows that SCIS is robust to\r\nGaussian and Poisson noise for our testing environment.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Körner, C., Janusch, I., &#38; Kropatsch, W. (2016). Noise Robustness of Irregular Graph Pyramids using LBP and Combinatorial Maps. In K. Niel &#38; W. Burger (Eds.), <i>Vision Meets Robotics, Proceedings of the ÖAGM Workshop 2016</i> (pp. 101–108). http://hdl.handle.net/20.500.12708/56605</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christoph",
                    "last_name": "Körner",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 2,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Kurt",
                    "last_name": "Niel",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Willhelm",
                    "last_name": "Burger",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E362"
            ],
            "pid": "60397",
            "handle": "20.500.12708/56609",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "A real-time algorithm for train position monitoring using optical time-domain reflectometry",
            "keywords": [],
            "abstract": "We propose an algorithm which uses an optical time-domain reflectometer (OTDR) for real-time tracking of trains. OTDR sensing, often also termed distributed acoustical sensing (DAS), measures the Rayleigh backscattering of a light pulse along an optical fiber. The resulting signal provides information on local acoustic pressure at linearly spaced positions along the fiber. While different approaches for train tracking with DAS are described in the literature, the results have been evaluated only for short time recordings with few train crossings. In this paper we provide details on the tracking performance of a novel algorithm that finds and tracks trains over 15km. Furthermore, this is the first contribution that uses ground truth data to assess the performance of the method. For the evaluation two one hour recordings are used.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Papp, A., Wiesmeyr, C., Litzenberger, M., Garn, H., &#38; Kropatsch, W. (2016). A real-time algorithm for train position monitoring using optical time-domain reflectometry. In <i>2016 IEEE International Conference on Intelligent Rail Transportation (ICIRT)</i>. 2016 IEEE International Conference on Intelligent Rail Transportation, Birmingham, UK, EU. IEEE. https://doi.org/10.1109/icirt.2016.7588715</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adam",
                    "last_name": "Papp",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Christoph",
                    "last_name": "Wiesmeyr",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Litzenberger",
                    "position": 3,
                    "role": "Author",
                    "tid": "111056"
                },
                {
                    "first_name": "Heinrich",
                    "last_name": "Garn",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 5,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "60421",
            "handle": "20.500.12708/56633",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "An Efficient DCT template-based Object Detection Method using Phase Correlation",
            "keywords": [],
            "abstract": "In this work, we propose an efficient algorithm,\r\nwhich utilizes the combination of discrete cosine transform (DCT)\r\nand phase correlation (PC) for fast object detection. To test the\r\nalgorithm´s classification performance and computational\r\ncomplexity we developed a prototype and conducted several\r\nexperiments with a public available car dataset. Furthermore, we\r\ncompared our experimental results to a state-of-the-art object\r\ndetection method. The proposed method uses the energy\r\ncompaction property of DCT and requires less number of\r\ncoefficients than fast Fourier transformation (FFT)-based\r\ntechniques to compute PC. The computational complexity and\r\nmemory requirements are significantly reduced using this method.\r\nAccording to our results, the proposed algorithm outperforms the\r\nbaseline method with respect to training time and classification\r\naccuracy.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H., &#38; Hörhan, M. (2016). An Efficient DCT template-based Object Detection Method using Phase Correlation. In <i>Asilomar Conference on Signals, Systems, and Computers Proceedings MA8b3-4</i>. Asilomar Conference on Signals, Systems, and Computers, Pacific Grove, Non-EU. http://hdl.handle.net/20.500.12708/56633</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                },
                {
                    "first_name": "Markus",
                    "last_name": "Hörhan",
                    "position": 2,
                    "role": "Author",
                    "tid": "40036"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "60424",
            "handle": "20.500.12708/56636",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Machine Learning with Dual Process Models",
            "keywords": [],
            "abstract": "Similarity measurement processes are a core part of most machine learning algorithms. Traditional approaches\r\nfocus on either taxonomic or thematic thinking. Psychological research suggests that a combination of both\r\nis needed to model human-like similarity perception adequately. Such a combination is called a Similarity\r\nDual Process Model (DPM). This paper describes how to construct DPMs as a linear combination of existing\r\nmeasures of similarity and distance. We use generalisation functions to convert distance into similarity. DPMs\r\nare similar to kernel functions. Thus, they can be integrated into any machine learning algorithm that uses\r\nkernel functions.Clearly, not all DPMs that can be formulated work equally well. Therefore we test classification\r\nperformance in a real-world task: the detection of pedestrians in images. We assume that DPMs are only\r\nviable if they yield better classifiers than their constituting parts. In our experiments, we found DPM kernels\r\nthat matched the performance of conventional ones for our data set. Eventually, we provide a construction kit\r\nto build such kernels to encourage further experiments in other application domains of machine learning.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H., Klauninger, B., &#38; Unger, M. (2016). Machine Learning with Dual Process Models. In <i>Proceedings of the 5th International Conference on Pattern Recognition Applications and Methods</i>. 5th International Conference on Pattern Recognition Applications and Methods, Rom, EU. https://doi.org/10.5220/0005655901480153</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                },
                {
                    "first_name": "Bert",
                    "last_name": "Klauninger",
                    "position": 2,
                    "role": "Author",
                    "tid": "93396"
                },
                {
                    "first_name": "Martin",
                    "last_name": "Unger",
                    "position": 3,
                    "role": "Author",
                    "tid": "58080"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "60704",
            "handle": "20.500.12708/56915",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "VROnSite: Towards immersive training of first responder squad leaders in untethered virtual reality",
            "keywords": [],
            "abstract": "We present the VROnSite platform that enables immersive training\r\nof first responder on-site squad leaders. Our training platform is\r\nfully immersive, entirely untethered to ease use and provides two\r\nmeans of navigation - abstract and natural walking - to simulate\r\nstress and exhaustion, two important factors for decision making.\r\nWith the platform´s capabilities, we close a gap in prior art for first\r\nresponder training. Our research is closely interlocked with stakeholders\r\nfrom fire brigades and paramedics to gather early feedback\r\nin an iterative design process. In this paper, we present our first\r\nresearch results, which are the system´s design rationale, the single\r\nuser training prototype and results from a preliminary user study.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., Froeschl, M., Schoenauer, C., Peer, A., Goellner, J., &#38; Kaufmann, H. (2017). VROnSite: Towards immersive training of first responder squad leaders in untethered virtual reality. In <i>2017 IEEE Virtual Reality (VR)</i>. IEEE Virtual Reality, Alexandria, Virginia, USA, Non-EU. https://doi.org/10.1109/vr.2017.7892324</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Mario",
                    "last_name": "Froeschl",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schoenauer",
                    "position": 3,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Peer",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Johannes",
                    "last_name": "Goellner",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 6,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "60715",
            "handle": "20.500.12708/56926",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "VRRobot: Robot actuated props in an infinite virtual environment",
            "keywords": [],
            "abstract": "We present the design and development of a fully immersive virtual reality (VR) system that can provide prop-based haptic feedback in an infinite virtual environment. It is conceived as a research tool for studying topics related to haptics in VR and based on off-the-shelf components. A robotic arm moves physical props, dynamically matching pose and location of an object in the virtual world. When the user reaches for the virtual object, his or her hands also encounter it in the real physical space. The interaction is not limited to specific body parts and does not rely on an external structure like an exoskeleton. In combination with a locomotion platform for close-to-natural walking, this allows unrestricted haptic interaction in a natural way in virtual environments of unlimited size.\r\nWe describe the concept, the hardware and software architecture in detail. We establish safety design guidelines for human-robot interaction in VR. Our technical evaluation shows good response times and accuracy. We report on a user study conducted with 34 participants indicating promising results, and discuss the potential of our system.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vonach, E., Gatterer, C., &#38; Kaufmann, H. (2017). VRRobot: Robot actuated props in an infinite virtual environment. In <i>2017 IEEE Virtual Reality (VR)</i>. IEEE Virtual Reality 2017, Los Angeles, CA, USA, Non-EU. IEEE. https://doi.org/10.1109/vr.2017.7892233</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Clemens",
                    "last_name": "Gatterer",
                    "position": 2,
                    "role": "Author",
                    "tid": "56871"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-06"
            ],
            "pid": "60804",
            "handle": "20.500.12708/57014",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Robust and sparse clustering for high-dimensional data",
            "keywords": [],
            "abstract": "We introduce a robust and sparse clustering procedure for high-dimensional data. The robustness aspect is addressed by a weighting function incorporated in the k-means procedure, consequently leading to an automatic weight assignment for each observation. The sparsity aspect is given by a lasso-type penalty on weighted between-cluster sum of squares. We additionally propose a framework for determining the optimal number of both clusters and variables that contribute to a cluster separation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brodinova, S., Filzmoser, P., Ortner, T., Zaharieva, M., &#38; Breiteneder, C. (2017). Robust and sparse clustering for high-dimensional data. In <i>CLADAG 2017 Book of Short Papers</i>. Conference of the CLAssification and Data Analysis Group (CLADAG) of the Italian Statistical Society (SIS), Milan, Italy, EU. http://hdl.handle.net/20.500.12708/57014</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 1,
                    "role": "Author",
                    "tid": "281480"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 2,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 3,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 4,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 5,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "220239"
            ]
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "60814",
            "handle": "20.500.12708/57024",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Automated Interior Design Using a Genetic Algorithm",
            "keywords": [],
            "abstract": "In this paper, we present a system that automatically populates indoor virtual scenes with furniture objects and optimizes their positions and orientations with respect to aesthetic, ergonomic and functional rules called interior design guidelines. These guidelines are represented as mathematical expressions which form the cost function. Our system optimizes the set of multiple interior designs by minimizing the cost function using a genetic algorithm. Moreover, we extend the optimization to transdimensional space by enabling automatic selection of furniture objects. Finally, we optimize the assignment of materials to the furniture objects to achieve a unified design and harmonious color distribution. We investigate the capability of our system to generate sensible and livable interior designs in a perceptual study.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., &#38; Kaufmann, H. (2017). Automated Interior Design Using a Genetic Algorithm. In <i>Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology</i> (pp. 1–10). IEEE Computer Society. http://hdl.handle.net/20.500.12708/57024</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60869",
            "handle": "20.500.12708/57078",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Detecting Ellipses in Elongated Shapes Using the Thickness Profile",
            "keywords": [],
            "abstract": "This paper presents a method that detects elliptical parts of a given elongated shape. For this purpose, first, the shape is represented by its skeleton. In case of branches, the skeleton is partitioned into a set of lines/curves. Second, the ellipse parameters are estimated using the thickness profile along each line/curve, and the properties of its first and second derivatives. The proposed method requires no prior information about the model, number of ellipses and their parameter values. The detected ellipses are then used in our second proposed approach for ellipse-based shape description. It can be applied for analysing motion and deformation of biological objects like roots, worms, and diatoms.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gabdulkhakova, A., &#38; Kropatsch, W. G. (2016). Detecting Ellipses in Elongated Shapes Using the Thickness Profile. In <i>Lecture Notes in Computer Science</i> (pp. 412–423). Springer Nature Switzerland AG 2021. https://doi.org/10.1007/978-3-319-49055-7_37</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Aysylu",
                    "last_name": "Gabdulkhakova",
                    "position": 1,
                    "role": "Author",
                    "tid": "235653"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60875",
            "handle": "20.500.12708/57084",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "LBP Scale Space Origins for Shape Classification",
            "keywords": [],
            "abstract": "The LBP scale space serves as a shape\r\nrepresentation, which thus allows not only for shape\r\nclassification but also for an (approximate) recon-\r\nstruction of the original shape. In this paper possible\r\nLBP scale space origins within a shape are evalu-\r\nated. The influence of the LBP scale space center\r\non the reconstruction quality is studied by comput-\r\ning the LBP scale space for every position inside a\r\nshape and by comparing the according reconstruc-\r\ntion errors. For shape classification, the LBP scale\r\nspace is further evaluated on the MPEG-7 dataset.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., &#38; Kropatsch, W. (2017). LBP Scale Space Origins for Shape Classification. In <i>Proceedings of the 22nd Computer Vision Winter Workshop 2017</i> (p. 9). http://hdl.handle.net/20.500.12708/57084</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60877",
            "handle": "20.500.12708/57086",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "From trajectories to behaviors: an algorithm to track and describe dancing birds",
            "keywords": [],
            "abstract": "The movement of the males in some\r\nspecies of birds during the courtship determines the\r\nprobability to be chosen by the female for mating.\r\nIn order to analyze the behaviors of the birds, bi-\r\nologists nowadays perform a manual annotation of\r\nvideos displaying the courtship phase. This a tedious\r\nand time consuming task. Thus, there is a strong in-\r\nterest in the development of algorithms able to au-\r\ntomatically process these videos in order to analyze\r\nthe behaviors. In this paper, we propose a novel ap-\r\nproach able to track the movement of the males of a\r\nparticular species of birds, namely the Golden Col-\r\nlared Manakin. Furthermore, we describe their mat-\r\ning dance by means of synthetic parameters, which\r\nare useful for the biologist. Both the tracking and\r\nthe parameters used for describing the dance could\r\nbe easily re-adapted to similar types of birds. The\r\nproposed approach has been tested on a set of videos\r\nfrom the biologists and the obtained results confirm\r\nthe effectiveness of the proposed approach.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Oliva, L., Saggese, A., Artner, N., Kropatsch, W., &#38; vento, M. (2017). From trajectories to behaviors: an algorithm to track and describe dancing birds. In <i>Proceedings of the 22nd Computer Vision Winter Workshop 2017</i> (p. 9). http://hdl.handle.net/20.500.12708/57086</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Leonardo",
                    "last_name": "Oliva",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Alessia",
                    "last_name": "Saggese",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 3,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Mario",
                    "last_name": "vento",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60878",
            "handle": "20.500.12708/57087",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Analysis and Calibration of a Mirror Setup producing Mirror-reflected, Multi-view Videos",
            "keywords": [],
            "abstract": "In this paper mirror-reflected, multi-view\r\nvideo sequences produced by a mirror setup are\r\nanalysed. The mirror setup is composed of two mirror\r\nplanes and a camera, which records the scene.\r\nThe relation between the angle between the two mirror\r\nplanes and the number of mirror-reflected views\r\nis analysed mathematically and the results are presented\r\nin this paper. Furthermore a calibration approach\r\nusing a cylindrical checkerboard pattern is\r\nintroduced. Using the cylindrical checkerboard pattern\r\nthe relation between the central view and the\r\nmirror-reflected views can be provided and the orientation\r\nand position of the mirror planes can be specified.\r\nThe knowledge about the orientation and position\r\nof the mirror planes can be used to estimate the\r\n3D position of marker points in the central and the\r\nreflected views.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mucciolo, G., Koneczny, B., Saggese, A., Artner, N., Kropatsch, W., &#38; vento, M. (2017). Analysis and Calibration of a Mirror Setup producing Mirror-reflected, Multi-view Videos. In <i>Proceedings of the 22nd Computer Vision Winter Workshop 2017</i> (p. 9). http://hdl.handle.net/20.500.12708/57087</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Gianluigi",
                    "last_name": "Mucciolo",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Barbara",
                    "last_name": "Koneczny",
                    "position": 2,
                    "role": "Author",
                    "tid": "255050"
                },
                {
                    "first_name": "Alessia",
                    "last_name": "Saggese",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 4,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 5,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Mario",
                    "last_name": "vento",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "60879",
            "handle": "20.500.12708/57088",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Euclidean and Geodesic Distance Profiles",
            "keywords": [],
            "abstract": "This paper presents a boundary-based, topological shape de-\r\nscriptor: the distance profile. It is inspired by the LBP (= local binary\r\npattern) scale space - a topological shape descriptor computed by a fil-\r\ntration with concentric circles around a reference point. For rigid objects,\r\nthe distance profile is computed by the Euclidean distance of each bound-\r\nary pixel to a reference point. A geodesic distance profile is proposed for\r\narticulated or deformable shapes: the distance is measured by a combina-\r\ntion of the Euclidean distance of each boundary pixel to the nearest pixel\r\nof the shape´s medial axis and the geodesic distance along the shape´s\r\nmedial axis to the reference point. In contrast to the LBP scale space, it\r\nis invariant to deformations and articulations and the persistence of the\r\nextrema in the profiles allows pruning of spurious branches (i.e. robust-\r\nness against noise on the boundary). The distance profiles are applicable\r\nto any shape, but the geodesic distance profile is especially well-suited\r\nfor articulated or deformable objects (e.g.applications in biology).",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., Artner, N., &#38; Kropatsch, W. (2017). Euclidean and Geodesic Distance Profiles. In <i>International Conference on Discrete Geometry for Computer Imagery DGCI 2017: Discrete Geometry for Computer Imagery</i> (pp. 307–318). Springer International Publishing. http://hdl.handle.net/20.500.12708/57088</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "61030",
            "handle": "20.500.12708/57239",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Towards efficient spatial compression in self-overlapping virtual environments",
            "keywords": [],
            "abstract": "Space available for any virtual reality experience is often strictly limited and abridges the virtual world to a size of a room. To extend the amount of virtual space accessible by walking within the same real workspace the methods of spatial compression were proposed. Scene manipulation with a controlled spatial overlap has been shown to be an efficient method. However, in order to apply space compression effectively for a dynamic, scalable and robust 3D user interface, it is important to study how the human perceives different layouts with overlapping spaces. In this paper, we explore the influence of the properties of the layout used on human spatial perception in a physically impossible spatial arrangement. Our first reported study focuses on the following parameters of the path within a simple self-overlapping layout: number of turns, relative door positions, sequences of counter- and clockwise turns, symmetry and asymmetry of the path used. In addition, in the second study we explore the effect of path smoothing by substituting the right-angled corridors by smooth curves. Our studies show that usage of the smooth curved corridors is more beneficial for spatial compression than the conventional right-angled approach.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vasylevska, K., &#38; Kaufmann, H. (2017). Towards efficient spatial compression in self-overlapping virtual environments. In <i>2017 IEEE Symposium on 3D User Interfaces (3DUI)</i>. IEEE Symposium on 3D User Interfaces (3DUI), Orlando, FL (USA), Non-EU. IEEE. https://doi.org/10.1109/3dui.2017.7893312</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "61031",
            "handle": "20.500.12708/57240",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Teaching Virtual Reality with HTC Vive and Leap Motion",
            "keywords": [],
            "abstract": "Creating high quality virtual reality (VR) experience takes time\r\nand requires extensive practice. Although, multiple virtual and\r\naugmented reality courses existed for years all over the world, high\r\ncosts of the equipment were always in the way of building up the\r\nup to date knowledge. Rapid development of the technology caused\r\nthe new VR boom and exposed a serious lack of the experienced\r\nVR/AR developers.\r\nIn this paper, we present our introductory VR courses for master\r\nstudents. The aim of the courses is to provide introduction to\r\nthe basics of VR and AR and supporting technology. We move the\r\nteaching focus from the specifics of a particular VR system to the\r\nskill build-up and development of the VR user experience. For that\r\nwe provide the access to the up to date consumer hardware, such as\r\nHTC Vive and Leap Motion.We discuss the structure of the courses\r\nand methodology, and provide the teaching materials. Moreover,\r\nwe discuss in details our updated practical course that unites the\r\ndevelopment of a low-cost desktop and a high-quality immersive\r\nVR applications using only off-the-shelf consumer equipment. Furthermore,\r\nwe discuss the overall course evaluation by the students\r\nand further opportunities for their professional growth, as well as\r\nconsecutive changes that will be made next.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vasylevska, K., Podkosova, I., &#38; Kaufmann, H. (2017). Teaching Virtual Reality with HTC Vive and Leap Motion. In <i>Proceedings of SIGGRAPH Asia Symposium on Education</i> (pp. 1–8). ACM Siggraph. http://hdl.handle.net/20.500.12708/57240</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 2,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "61033",
            "handle": "20.500.12708/57242",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Preventing Imminent Collisions between Co-Located Users in HMD-Based VR in Non-Shared Scenarios",
            "keywords": [],
            "abstract": "This paper presents two experiments set in a\r\nmulti-user HMD-based VR system where users\r\nnavigate by real walking in a large real and vir-\r\ntual area. We investigate a case that could be\r\nused in a multi-user VR game or a training ap-\r\nplication: several users are walking in the same\r\nphysical space without seeing each other in the\r\nvirtual environment. Such a scenario involves\r\nthe risk of collisions between users. In the first\r\nexperiment, we investigate the strategy of stop-\r\nping a walking user in a dangerous situation. In\r\nparticular, we compare the effectiveness and the\r\nperceived difficulty of two visual and two audi-\r\ntory stopping signals. The results of this com-\r\nparison show that the tested visual and auditory\r\nsignals are equally effective in stopping users.\r\nWith both visual and auditory signals, partici-\r\npants prefer the signal to contain a &quot;stop&quot; com-\r\nmand. In the second experiment, avatars are\r\ndisplayed at users´ positions if the distance be-\r\ntween users is dangerously small. The method\r\nis tested with four avatars of various degrees of\r\nanthropomorphism and in two different appli-\r\ncation scenarios. Our results suggest that the\r\ntype of scenario influences users´ preference of\r\na notification avatar. It is sufficient to display\r\nan area occupied by other users in scenarios\r\nwith specific goals and interactive content. If\r\nusers are exploring a virtual world without hav-\r\ning any other goal, they prefer to see human-\r\nlike avatars as a possible collision notification.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Podkosova, I., &#38; Kaufmann, H. (2017). Preventing Imminent Collisions between Co-Located Users in HMD-Based VR in Non-Shared Scenarios. In <i>Proceedings of the 30 th International Conference on Computer Animation and Social Agents</i> (pp. 37–46). CASA 2017. http://hdl.handle.net/20.500.12708/57242</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "61050",
            "handle": "20.500.12708/57259",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Towards a Universal Music Symbol Classifier",
            "keywords": [],
            "abstract": "Optical Music Recognition (OMR) aims to recognize and\r\nunderstand written\r\n music scores. With the help of Deep Learning, researchers were able\r\n to significantly improve the state-of-the-art in this research area.\r\n However, Deep Learning requires a substantial amount of annotated\r\n data for supervised training. Various datasets have been collected\r\n in the past, but without a common standard that defines data formats\r\n and terminology, combining them is a challenging task. In this paper\r\n we present our approach towards unifying multiple datasets into the\r\n largest currently available body of over 90000 musical symbols that\r\n belong to 79 classes, containing both handwritten and printed music\r\n symbols. A universal music symbol classifier, trained on such a dataset\r\n using Deep Learning, can achieve an accuracy that exceeds 98%.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pacha, A., &#38; Eidenberger, H. (2017). Towards a Universal Music Symbol Classifier. In <i>2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</i>. International Workshop on Graphics Recognition, New York, Non-EU. https://doi.org/10.1109/icdar.2017.265</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Alexander",
                    "last_name": "Pacha",
                    "position": 1,
                    "role": "Author",
                    "tid": "68727"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 2,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "61051",
            "handle": "20.500.12708/57260",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Towards Self-Learning Optical Music Recognition",
            "keywords": [],
            "abstract": "Optical Music Recognition (OMR) is a branch of\r\nartificial intelligence that aims at automatically recognizing\r\nand understanding the content of music scores in images.\r\nSeveral approaches and systems have been proposed that try to\r\nsolve this problem by using expert knowledge and specialized\r\nalgorithms that tend to fail at generalization to a broader\r\nset of scores, imperfect image scans or data of different\r\nformatting. In this paper we propose a new approach to solve\r\nOMR by investigating how humans read music scores and by\r\nimitating that behavior with machine learning. To demonstrate\r\nthe power of this approach, we conduct two experiments\r\nthat teach a machine to distinguish entire music sheets from\r\narbitrary content through frame-by-frame classification and\r\ndistinguishing between 32 classes of handwritten music symbols\r\nwhich can be a basis for object detection. Both tasks can\r\nbe performed at high rates of confidence (>98%) which is\r\ncomparable to the performance of humans on the same task.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pacha, A., &#38; Eidenberger, H. (2017). Towards Self-Learning Optical Music Recognition. In <i>2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)</i>. 16th IEEE International Conference on Machine Learning and Applications, Cancun, Non-EU. https://doi.org/10.1109/icmla.2017.00-60</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Alexander",
                    "last_name": "Pacha",
                    "position": 1,
                    "role": "Author",
                    "tid": "68727"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 2,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "61052",
            "handle": "20.500.12708/57261",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "The Gestalt Interest Points Distance Feature for Compact and Accurate Image Description",
            "keywords": [],
            "abstract": "In this work, we present the novel Inter-GIP Distances (IGD) feature and\r\nits integration into the Gestalt Interest Points (GIP) image descriptor.\r\nWith the ongoing growth of visual data, efficient image descriptor methods\r\nare becoming more and more important. Several local point-based description\r\nmethods have been defined in the past decades. Accuracy and descriptor size\r\nare important factors when selecting the appropriate method for a given\r\nretrieval problem. The method presented in this work describes images with\r\nonly a few very compact descriptors. To test our descriptor, we developed\r\nan image classification prototype and conducted several experiments with a\r\npublicly available horses dataset and a food dataset. Our experiments show\r\nthat only a few of the very compact GIP image descriptors are necessary to\r\nquickly classify the images from the datasets with high accuracy.\r\nFurthermore, we compared our experimental results to state-of-the-art local\r\npoint-based description methods and found that our method is highly\r\ncompetitive.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hörhan, M., &#38; Eidenberger, H. (2017). The Gestalt Interest Points Distance Feature for Compact and Accurate Image Description. In <i>Proceedings IEEE International Symposium on Signal Processing and Information Technology</i> (p. 5). http://hdl.handle.net/20.500.12708/57261</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Hörhan",
                    "position": 1,
                    "role": "Author",
                    "tid": "40036"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 2,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "61097",
            "handle": "20.500.12708/57306",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Discovering New Monte Carlo Noise Filters with Genetic Programming",
            "keywords": [],
            "abstract": "This paper presents a novel method for the discovery of new analytical filters suitable for filtering of noise in Monte Carlo rendering. Our method utilizes genetic programming to evolve the set of analytical filtering expressions with the goal to minimize image error in training scenes. We show that genetic programming is capable of learning new filtering expressions with quality comparable to state of the art noise filters in Monte Carlo rendering. Additionally, the analytical nature of the resulting expressions enables the run-times one order of magnitude faster than compared state of the art methods. Finally, we present a new analytical filter discovered by our method which is suitable for filtering of Monte Carlo noise in diffuse scenes.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., Davletaliyev, M., &#38; Kaufmann, H. (2017). Discovering New Monte Carlo Noise Filters with Genetic Programming. In A. Peytavie &#38; C. Bosch (Eds.), <i>EG 2017 - Short Papers</i> (pp. 1–4). Eurographics Association. https://doi.org/10.2312/egsh.20171006</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Maxim",
                    "last_name": "Davletaliyev",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Adrien",
                    "last_name": "Peytavie",
                    "position": 1,
                    "role": "Editor"
                },
                {
                    "first_name": "Carles",
                    "last_name": "Bosch",
                    "position": 2,
                    "role": "Editor"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "61126",
            "handle": "20.500.12708/57335",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Handwritten Music Object Detection: Open Issues and Baseline Results",
            "keywords": [],
            "abstract": "Optical Music Recognition (OMR) is the challenge of understanding the content of musical scores. Accurate detection of individual music objects is a critical step in\r\n processing musical documents because a failure at this stage\r\n corrupts any further processing. So far, all proposed methods\r\n were either limited to typeset music scores or were built to\r\n detect only a subset of the available classes of music symbols.\r\n In this work, we propose an end-to-end trainable object detector for music symbols that is capable of detecting almost\r\n the full vocabulary of modern music notation in handwritten\r\n music scores. By training deep convolutional neural networks\r\n on the recently released MUSCIMA++ dataset which has\r\n symbol-level annotations, we show that a machine learning approach can be used to accurately detect music objects with\r\n a mean average precision of over 80%.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pacha, A., Choi, K.-Y., Coüasnon, B., Ricquebourg, Y., &#38; Eidenberger, H. (2018). Handwritten Music Object Detection: Open Issues and Baseline Results. In <i>2018 13th IAPR International Workshop on Document Analysis Systems (DAS)</i>. 2018 13th IAPR Workshop on Document Analysis Systems (DAS), Wien, Austria. https://doi.org/10.1109/das.2018.51</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Alexander",
                    "last_name": "Pacha",
                    "position": 1,
                    "role": "Author",
                    "tid": "68727"
                },
                {
                    "first_name": "Kwon-Young",
                    "last_name": "Choi",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Bertrand",
                    "last_name": "Coüasnon",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Yann",
                    "last_name": "Ricquebourg",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 5,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "61157",
            "handle": "20.500.12708/57366",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Automatic Furniture Arrangement Using Greedy Cost Minimization",
            "keywords": [],
            "abstract": "In this paper, we present a novel method for fast generation of furniture arrangements in interior scenes. Our method exploits the benefits of optimization-based approaches for global aesthetic rules and the advantages of procedural approaches for local arrangement of small objects. We generate the furniture arrangements for a given room in two steps: We first optimize the selection and arrangement of furniture objects in a room with respect to aesthetic and functional rules. The infinite trans-dimensional space of furniture layouts is rapidly explored by greedy cost minimization. In the second step, the procedural methods are locally applied in a stochastic fashion to generate important scene details. We demonstrate that our method achieves comparable results to a recent method for automatic interior design in terms of user preferences and that local procedural design enhances the result of optimization-based interior design. Additionally, our method is one order of magnitude faster than the compared method. Finally, the execution times of up to one second show that our method is suitable for generating large-scale indoor virtual environments during runtime.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., &#38; Kaufmann, H. (2018). Automatic Furniture Arrangement Using Greedy Cost Minimization. In <i>IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)</i> (pp. 1–8). IEEE Computer Society. http://hdl.handle.net/20.500.12708/57366</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "61328",
            "handle": "20.500.12708/57534",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Segmentation Edit Distance",
            "keywords": [],
            "abstract": "In this paper, we present a novel distance metric called Segmentation Edit Distance (SED) and its use as a segmentation evaluation metric. In segmentation evaluation, the difference or distance of a test segmentation and the associated ground truth segmentation are measured in order to compare different algorithms. Our proposed edit distance extends the idea of other edit distances such as the string edit distance or the graph edit distance to the domain of image segmentations. The distance is based on the cost of edit operations that are needed to transform one segmentation into another. Only one edit operation, the deletion of an error region, is considered. Different to other edit distances, the costs assigned to this operation are based on properties of the error regions and the image processing method used to delete a region. As a segmentation evaluation metric, it combines the assessment of accuracy and efficiency into a single metric. Evaluations on synthetic and real world data show promising results compared to other state of the art segmentation evaluation metrics.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pucher, D., &#38; Kropatsch, W. (2018). Segmentation Edit Distance. In <i>IAPR/ICPR 2018 International Conference on Pattern Recognition</i> (pp. 1175–1185). IEEE Computer Society. http://hdl.handle.net/20.500.12708/57534</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Daniel",
                    "last_name": "Pucher",
                    "position": 1,
                    "role": "Author",
                    "tid": "255108"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "61329",
            "handle": "20.500.12708/57535",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Confocal Ellipse-based Distance and Confocal Elliptical Field for Polygonal Shapes.",
            "keywords": [],
            "abstract": "The paper introduces a novel confocal ellipse-based distance (CED), that is based on the properties of the confocal ellipses. This distance is used to produce a confocal elliptical field (CEF). The Euclidean Distance Transform (EDT) of a single point (called seed) generates a distance field of concentric circles. The sum of two such distance fields of two distinct seed points produces a distance field of confocal ellipses. This fact enables to adapt CED and CEF to the discrete case, referred to as CED­DT and CEF-DT. The properties of the CEF and CEF-DT make them useful for skeletonization, in particular for efficient removal of the spurious branches.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gabdulkhakova, A., &#38; Kropatsch, W. (2018). Confocal Ellipse-based Distance and Confocal Elliptical Field for Polygonal Shapes. In <i>IAPR/ICPR 2018 International Conference on Pattern Recognition</i> (pp. 3025–3030). IEEE Computer Society. http://hdl.handle.net/20.500.12708/57535</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Aysylu",
                    "last_name": "Gabdulkhakova",
                    "position": 1,
                    "role": "Author",
                    "tid": "235653"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "61347",
            "handle": "20.500.12708/57553",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Robot Supported Virtual and Augmented Reality",
            "keywords": [],
            "abstract": "In this dissertation different aspects from research in the fields of Tangible User Interfaces, encounter-type devices and Passive Haptics are combined to investigate the benefits that robots offer for providing haptic feedback in Virtual and Augmented Reality. Robotic elements like micro drives and robotic arms are employed for the actuation of passive or active physical objects. In that way physical props can be collocated with virtual counterparts to allow high fidelity, natural interaction.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vonach, E. (2018). Robot Supported Virtual and Augmented Reality. In <i>2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</i>. IEEE Virtual Reality and 3D User Interfaces 2018, Reutlingen, Germany, EU. IEEE. https://doi.org/10.1109/vr.2018.8446400</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Author",
                    "tid": "40682"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "61413",
            "handle": "20.500.12708/57619",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Mutual collision avoidance during walking in real and collaborative virtual environments",
            "keywords": [],
            "abstract": "In walkable multi-user HMD-based VR immersed users have to\r\navoid colliding with each other while performing tasks or exploring the virtual environment. This paper presents an experiment in\r\nwhich we compare mutual collision avoidance behaviour of pairs\r\nof users walking in the matched real and virtual environments. To\r\ngeneralize our study, we investigate two types of multi-user VR\r\nsetup: one in which both users share a common tracking space (colocated condition) and one where they move in two separate tracking\r\nspaces while sharing a common virtual space (distributed condition). Our results indicated signifcant di&#64256;erences in locomotor\r\ntrajectories of participants between the real and virtual conditions.\r\nPrecisely, we observed an increase in the clearance distance and\r\nthe median path curvature and a decrease in the walking speed\r\nwhen participants performed the test in VR in the colocated condition. Moreover, we found e&#64256;ects of pre-conditioning between the\r\ncolocated and distributed scenarios.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Podkosova, I., &#38; Kaufmann, H. (2018). Mutual collision avoidance during walking in real and collaborative virtual environments. In <i>Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</i>. i3D’18, Montreal, Non-EU. https://doi.org/10.1145/3190834.3190845</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "61414",
            "handle": "20.500.12708/57620",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Co-presence and proxemics in shared walkable virtual environments with mixed colocation",
            "keywords": [],
            "abstract": "The purpose of the experiment presented in this paper is to investigate co-presence and locomotory patterns in a walkable shared\r\nvirtual environment. In particular, trajectories of users that use\r\na walkable tracking space alone are compared to those of users\r\nwho use the tracking space in pairs. Co-presence, in a sense of perception of another person being present in the same virtual space\r\nis analyzed through subjective responses and behavioral markers.\r\nThe results indicate that both perception and proxemics in relation to co-located and distributed players di&#64256;er. The e&#64256;ect on the perception is however mitigated if participants do not collide with the\r\navatars of distributed co-players.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Podkosova, I., &#38; Kaufmann, H. (2018). Co-presence and proxemics in shared walkable virtual environments with mixed colocation. In <i>Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology</i>. VRST 18, Tokio, Non-EU. ACM Digital Library. https://doi.org/10.1145/3281505.3281523</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "61466",
            "handle": "20.500.12708/57672",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Gestalt Interest Points with a Neural Network for Makeup-Robust Face Recognition",
            "keywords": [],
            "abstract": "In this paper, we propose a novel approach for the domain of makeup-robust face recognition. Most face recognition schemes usually fail to generalize well on these data where there is a large difference between the training and testing sets, e.g., makeup changes. Our method focuses on the problem of determining whether face images before and after makeup refer to the same identity. The work on this fundamental research topic benefits various real-world applications, for example automated passport control, security in general, and surveillance. Experiments show that our method is highly effective in comparison to state-of-the-art methods.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hörhan, M., &#38; Eidenberger, H. (2018). Gestalt Interest Points with a Neural Network for Makeup-Robust Face Recognition. In <i>2018 25th IEEE International Conference on Image Processing (ICIP)</i>. 2018 25th IEEE International Conference on Image Processing (ICIP), Athens, Greece, EU. IEEE Press. https://doi.org/10.1109/icip.2018.8451075</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Markus",
                    "last_name": "Hörhan",
                    "position": 1,
                    "role": "Author",
                    "tid": "40036"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 2,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "61535",
            "handle": "20.500.12708/57740",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "On the Space Between Critical Points",
            "keywords": [],
            "abstract": "The vertices of the neighborhood graph of a digital picture P can be interpolated to form a 2-manifold M with critical points (maxima, minima, saddles), slopes and plateaus being the ones recognized by local binary patterns (LBPs). Neighborhood graph produces a cell decomposition of M: each 0-cell is a vertex in the neighborhood graph, each 1-cell is an edge in the neighborhood graph and, if P is well-composed, each 2-cell is a slope region in M in the sense that every pair of s in the region can be connected by a monotonically increasing or decreasing path. In our previous research, we produced superpixel hierarchies (combinatorial graph pyramids) that are multiresolution segmentations of the given picture. Critical points of P are preserved along the pyramid. Each level of the pyramid produces a slope complex which is a cell decomposition of M preserving critical points of P and such that each 2-cell is a slope region. Slope complexes in different levels of the pyramid are always homeomorphic. Our aim in this research is to explore the configuration at the top level of the pyramid which consists of a slope complex with vertices being only the critical points of P.We also study the number of slope regions on the top.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. G., Casablanca, R. M., Batavia, D., &#38; Gonzalez-Diaz, R. (2019). On the Space Between Critical Points. In <i>Discrete Geometry for Computer Imagery</i> (pp. 115–126). https://doi.org/10.1007/978-3-030-14085-4_10</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Rocio M.",
                    "last_name": "Casablanca",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Darshan",
                    "last_name": "Batavia",
                    "position": 3,
                    "role": "Author",
                    "tid": "317665"
                },
                {
                    "first_name": "Rocio",
                    "last_name": "Gonzalez-Diaz",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-02",
                "E193-06"
            ],
            "pid": "61579",
            "handle": "20.500.12708/57784",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Towards Eye-Friendly VR: How Bright Should It Be?",
            "keywords": [],
            "abstract": "Visual information plays an important part in the perception of the world around us. Recently, head-mounted displays (HMD) came to the consumer market and became a part of everyday life of thousands of people. Like with the desktop screens or hand-held devices before, the public is concerned with the possible health consequences of the prolonged usage and question the adequacy of the default settings. It has been shown that the brightness and contrast of a display should be adjusted to match the external light to decrease eye strain and other symptoms. Currently, there is a noticeable mismatch in brightness between the screen and dark background of an HMD that might cause eye strain, insomnia, and other unpleasant symptoms. In this paper, we explore the possibility to significantly lower the screen brightness in the HMD and successfully compensate for the loss of the visual information on a dimmed screen. We designed a user study to explore the connection between the screen brightness in HMD and task performance, cybersickness, users´ comfort, and preferences. We have tested three levels of brightness: the default Full Brightness, the optional Night Mode and a significantly lower brightness with original content and compensated content. Our results suggest that although users still prefer the brighter setting, the HMDs can be successfully used with significantly lower screen brightness, especially if the low screen brightness is compensated.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vasylevska, K., Yoo, H., Akhavan, T., &#38; Kaufmann, H. (2019). Towards Eye-Friendly VR: How Bright Should It Be? In <i>Proceedings of IEEE VR</i> (pp. 1–9). http://hdl.handle.net/20.500.12708/57784</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Hyunjin",
                    "last_name": "Yoo",
                    "position": 2,
                    "role": "Author",
                    "tid": "269730"
                },
                {
                    "first_name": "Tara",
                    "last_name": "Akhavan",
                    "position": 3,
                    "role": "Author",
                    "tid": "247963"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "61590",
            "handle": "20.500.12708/57795",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Virtual vs. Physical Navigation in VR: Study of Gaze and Body Segments Temporal Reorientation Behaviour",
            "keywords": [],
            "abstract": "This paper investigates whether the body anticipation synergies in real environments (REs) are preserved during navigation in virtual environments (VEs). Experimental studies related to the control of human locomotion in REs during curved trajectories report a top-down reorientation strategy with the reorientation of the gaze anticipating the reorientation of head, the shoulders and finally the global body motion. This anticipation behavior provides a stable reference frame to the walker to control and reorient his/her body according to the future walking direction. To assess body anticipation during navigation in VEs, we conducted an experiment where participants, wearing a head-mounted display, performed a lemniscate trajectory in a virtual environment (VE) using five different navigation techniques, including walking, virtual steering (head, hand or torso steering) and passive navigation. For the purpose of this experiment, we designed a new control law based on the power-law relation between speed and curvature during human walking. Taken together our results showed a similar ordered top-down sequence of reorientation of the gaze, head and shoulders during curved trajectories between walking in REs and in VEs (for all the evaluated techniques). However, the anticipation mechanism was significantly higher for the walking condition compared to the others. The results presented in this paper pave the way to the better understanding of the underlying mechanisms of human navigation in VEs and to the design of navigation techniques more adapted to humans.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brument, H., Podkosova, I., Kaufmann, H., Olivier, A.-H., &#38; Argelaguet, F. (2019). Virtual vs. Physical Navigation in VR: Study of Gaze and Body Segments Temporal Reorientation Behaviour. In <i>Proceedings of IEEE VR 2019 - 26th IEEE Conference on Virtual Reality and 3D User Interfaces</i> (pp. 1–10). http://hdl.handle.net/20.500.12708/57795</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hugo",
                    "last_name": "Brument",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 2,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Anne-Helene",
                    "last_name": "Olivier",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Ferran",
                    "last_name": "Argelaguet",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "61649",
            "handle": "20.500.12708/57854",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Computing and Reducing Slope Complexes",
            "keywords": [],
            "abstract": "In this paper we provide a new characterization of cell de-composition (called slope complex) of a given 2-dimensional continuous surface. Each patch (cell) in the decomposition must satisfy that there exists a monotonic path for any two points in the cell. We prove that any triangulation of such surface is a slope complex and explain how to obtain new slope complexes with a smaller number of slope regions decomposing the surface. We give the minimal number of slope regions by counting certain bounding edges of a triangulation of the surface obtained from its critical points.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. G., Casablanca, R. M., Batavia, D., &#38; Gonzalez-Diaz, R. (2019). Computing and Reducing Slope Complexes. In <i>Computational Topology in Image Context</i> (pp. 12–25). https://doi.org/10.1007/978-3-030-10828-1_2</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Rocio M.",
                    "last_name": "Casablanca",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Darshan",
                    "last_name": "Batavia",
                    "position": 3,
                    "role": "Author",
                    "tid": "317665"
                },
                {
                    "first_name": "Rocio",
                    "last_name": "Gonzalez-Diaz",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "61652",
            "handle": "20.500.12708/57857",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Non-centered Voronoi Skeletons",
            "keywords": [],
            "abstract": "We propose a novel Voronoi Diagram based skeletonization algorithm that produces non-centered skeletons. The first strategy considers utilizing Elliptical Line Voronoi Diagrams with varied density based sampling of the polygonal shapes. The second strategy applies a weighting scheme on Elliptical Line Voronoi Diagrams and Line Voronoi Diagrams. The proposed skeletonization algorithm uses precomputed dis-tance fields and basic element-wise operations, thus can be easily adapted for parallel execution. Non-centered Voronoi Skeletons give a representation that is more similar to real world skeletons and retain many of the desirable properties of skeletons",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Langer, M., Gabdulkhakova, A., &#38; Kropatsch, W. G. (2019). Non-centered Voronoi Skeletons. In <i>Discrete Geometry for Computer Imagery</i> (pp. 355–366). https://doi.org/10.1007/978-3-030-14085-4_28</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Maximilian",
                    "last_name": "Langer",
                    "position": 1,
                    "role": "Author",
                    "tid": "254918"
                },
                {
                    "first_name": "Aysylu",
                    "last_name": "Gabdulkhakova",
                    "position": 2,
                    "role": "Author",
                    "tid": "235653"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "61653",
            "handle": "20.500.12708/57858",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Congratulations! Dual Graphs Are Now Orientated!",
            "keywords": [],
            "abstract": "A digital image can be perceived as a 2.5D surface consisting of pixel coordinates and the intensity of pixel as height of the point in the surface. Such surfaces can be efficiently represented by the pair of dual plane graphs: neighborhood (primal) graph and its dual. By defining ori-entation of edges in the primal graph and use of Local Binary Patters(LBPs), we can categorize the vertices corresponding to the pixel into critical (maximum, minimum, saddle) or slope points. Basic operation of contraction and removal of edges in primal graph result in configuration of graphs with different combinations of critical and non-critical points.The faces of graph resemble a slope region after restoration of the contin-uous surface by successive monotone cubic interpolation. In this paper, we define orientation of edges in the dual graph such that it remains consistent with the primal graph. Further we deliver the necessary and sufficient conditions for merging of two adjacent slope regions",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Batavia, D., Kropatsch, W. G., Casablanca, R. M., &#38; Gonzalez-Diaz, R. (2019). Congratulations! Dual Graphs Are Now Orientated! In <i>Graph-Based Representations in Pattern Recognition</i> (pp. 131–140). https://doi.org/10.1007/978-3-030-20081-7_13</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Darshan",
                    "last_name": "Batavia",
                    "position": 1,
                    "role": "Author",
                    "tid": "317665"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Rocio M.",
                    "last_name": "Casablanca",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Rocio",
                    "last_name": "Gonzalez-Diaz",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "61654",
            "handle": "20.500.12708/57859",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Partitioning 2D Images into Prototypes of Slope Region",
            "keywords": [],
            "abstract": "A gray scale digital image can be represented as a 2.5D surface where the height of the surface corresponds to the gray value of the respective pixel. Analysis of the gray scale image can be efficiently done by exploiting the properties of the plane graph embedded in the 2.5D sur-face. The vertices of the graph can be easily categorized into critical and non-critical points by use of Local Binary Patterns (LBPs). Well defined graph operations such as contraction and removal of edges are used to eliminate the non-critical points and preserve the critical points thereby reducing the size of graph. In this process, it is important to preserve the structural and topological properties of the regions of a gray scale image. After analysing the topological properties of a well composed image, we provide two prototypes of the slope region and the necessary conditions for their existence. Also we prove that every slope region conforms to either of the two prototype. Conversely the prototypes may be used to generate an image with a required topological properties.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Batavia, D., Hladuvka, J., &#38; Kropatsch, W. (2019). Partitioning 2D Images into Prototypes of Slope Region. In <i>Proc. 18th Intl. Conference on Computer Analysis of Images and Patterns, volume LNCS 11678</i> (pp. 363–374). http://hdl.handle.net/20.500.12708/57859</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Darshan",
                    "last_name": "Batavia",
                    "position": 1,
                    "role": "Author",
                    "tid": "317665"
                },
                {
                    "first_name": "Jiri",
                    "last_name": "Hladuvka",
                    "position": 2,
                    "role": "Author",
                    "tid": "53161"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "61658",
            "handle": "20.500.12708/57863",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Counting Slope Regions in the Surface Graphs",
            "keywords": [],
            "abstract": "The discrete version of a continuous surface sampled at optimum sampling rate can be well expressed in form of a neighborhood graph containing the critical points (maxima, minima, saddles) of the surface. Basic operations on the graph such as edge contraction and removal eliminate non-critical points and collapse plateau regions resulting in the formation of a graph pyramid. If the neighborhood graph is well-composed, faces in the graph pyramid are slope regions. In this paper we focus on the graph on the top of the pyramid which will contain critical points only, self-loops and multiple edges connecting the same vertices. We enumerate the different possible configurations of slope regions, forming a catalogue of different configurations when combining slope regions and studying the number of slope regions on the top.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Batavia, D., Kropatsch, W., Gonzalez-Diaz, R., &#38; Casablanca, R. M. (2019). Counting Slope Regions in the Surface Graphs. In <i>Proc. 24th Computer Vision Winter Workshop</i> (p. 9). http://hdl.handle.net/20.500.12708/57863</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Darshan",
                    "last_name": "Batavia",
                    "position": 1,
                    "role": "Author",
                    "tid": "317665"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Rocio",
                    "last_name": "Gonzalez-Diaz",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Rocio M.",
                    "last_name": "Casablanca",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "61670",
            "handle": "20.500.12708/57875",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "ThermalGAN: Multimodal Color-to-Thermal Image Translation for Person Re-identification in Multispectral Dataset",
            "keywords": [],
            "abstract": "We propose a ThermalGAN framework for cross-modality color-thermal person re-identification (ReID). We use a stack of generative adversarial networks (GAN) to translate a single color probe image to a multimodal thermal probe set. We use thermal histograms and feature descriptors as a thermal signature. We collected a large-scale multispectral ThermalWorld dataset for extensive training of our GAN model. In total the dataset includes 20216 color-thermal image pairs, 516 person ID, and ground truth pixel-level object annotations. We made the dataset freely available (http://www.zefirus.org/ThermalGAN/). We evaluate our framework on the ThermalWorld dataset to show that it delivers robust matching that competes and surpasses the state-of-the-art in cross-modality color-thermal ReID.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kniaz, V. V., Knyaz, V. A., Hladůvka, J., Kropatsch, W. G., &#38; Mizginov, V. (2019). ThermalGAN: Multimodal Color-to-Thermal Image Translation for Person Re-identification in Multispectral Dataset. In <i>Lecture Notes in Computer Science</i> (pp. 606–624). Springer LNCS. https://doi.org/10.1007/978-3-030-11024-6_46</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Vladimir V.",
                    "last_name": "Kniaz",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Vladimir A.",
                    "last_name": "Knyaz",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Jiří",
                    "last_name": "Hladůvka",
                    "position": 3,
                    "role": "Author",
                    "tid": "53161"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 4,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Vladimir",
                    "last_name": "Mizginov",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "61709",
            "handle": "20.500.12708/57914",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Juggling in VR: Advantages of Immersive Virtual Reality in Juggling Learning",
            "keywords": [],
            "abstract": "In this paper, we follow up on research dealing with motion learning in Virtual Reality (VR). We investigate the impact of VR motion learning on motion performance, motivation for motion learning and willingness to continue with the motion learning. In our research, we used three ball juggling as a subject of learning. We performed a user study with 30 participants. A VR application was used in our study which allows setting up lower gravity and thus slowing down the motion for learning purposes. The results were statistically evaluated and we comment on the positive influence of virtual reality on motivation and possibilities of using VR in the motion learning process.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Adolf, J., Kán, P., Outram, B., Kaufmann, H., Doležal, J., &#38; Lhotská, L. (2019). Juggling in VR: Advantages of Immersive Virtual Reality in Juggling Learning. In <i>25th ACM Symposium on Virtual Reality Software and Technology</i>. ACM. https://doi.org/10.1145/3359996.3364246</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Jindřich",
                    "last_name": "Adolf",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 2,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Benjamin",
                    "last_name": "Outram",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Jaromír",
                    "last_name": "Doležal",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Lenka",
                    "last_name": "Lhotská",
                    "position": 6,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "61725",
            "handle": "20.500.12708/57930",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Virtual Reality CBRN Defence",
            "keywords": [],
            "abstract": "Over the past decade, training in virtual reality for military and disaster preparedness has been increasingly recognized as an important adjunct to traditional modalities of real-life drills. However, there are only a few existing solutions that provide immersive virtual reality training and improve learning through an increased amount of presence. In this paper, we present a novel and flexible Virtual Reality (VR) training system for military and first responders that enables realistic multi-user training in large environments. We show how the requirements of peer stakeholders for disaster relief with an explicit focus on CBRN disaster preparedness transfer to the concept, current implementation and future features of our system. The development and integration of multiple technologies allows a wide variety of interaction and collaboration within our immersive system. In addition, we demonstrate the training capabilities of our proposed system with a multi-user training scenario, simulating a CBRN crisis. Results from our technical and user evaluation with 13 experts in CBRN response from the Austrian Armed Forces (National Defence Academy &amp; Competence Center NBC Defence) indicate strong applicability and user acceptance. Over 80% of the participants agreed &quot;much&quot; or &quot;very much&quot; that the presented system can be used to support training for CBRN-crisis preparedness.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Göllner, J., Peer, A., Meurers, C., Wurzer, G., Schönauer, C., &#38; Kaufmann, H. (2019). Virtual Reality CBRN Defence. In <i>Meeting Proceedings of the Simulation and Modelling Group Symposium 171</i> (pp. 1–25). STO. http://hdl.handle.net/20.500.12708/57930</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Johannes",
                    "last_name": "Göllner",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Peer",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Meurers",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Gernot",
                    "last_name": "Wurzer",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 5,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 6,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "62062",
            "handle": "20.500.12708/58267",
            "doi": null,
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Towards Identification of Incorrectly Segmented OCT Scans",
            "keywords": [],
            "abstract": "Precise thickness measurements of retinallayers are crucial to decide whether the subject re-quires subsequent treatment. As optical coherencetomography (OCT) is becoming a standard imagingmethod in hospitals, the amount of retinal scans in-creases rapidly, automated segmentation algorithmsare getting deployed, and methods to assess theirperformance are in demand.In this work we propose a semi-supervised frame-work to detect incorrectly segmented OCT retinascans: ground-truth segmentations are (1) embed-ded in 2D feature space and (2) used to train an out-lier scoring function and the corresponding decisionboundary.We evaluate a selection of five outlier detectionmethods and find the results to be a promising start-ing point to address the given problem. While thiswork and results are centred around one concretesegmentation algorithm we sketch the possibilities ofhow the framework can be generalized for more re-cent or more precise segmentation methods.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hladuvka, J., &#38; Renner, V. (2020). Towards Identification of Incorrectly Segmented OCT Scans. In <i>Proceedings of the Joint Austrian Computer Vision and Robotics Workshop 2020</i> (p. 7). https://doi.org/10.3217/978-3-85125-752-6-36</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Jiri",
                    "last_name": "Hladuvka",
                    "position": 1,
                    "role": "Author",
                    "tid": "53161"
                },
                {
                    "first_name": "Verena",
                    "last_name": "Renner",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "62063",
            "handle": "20.500.12708/58268",
            "doi": null,
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Border Propagation: A Novel Approach to Determine Slope Region Decompositions",
            "keywords": [],
            "abstract": "Slope regions are a useful tool in patternrecognition. We review theory about slope regionsand prove a theorem linking monotonic paths and theconnectedness of levelsets. Unexpected behavior ofslope regions in higher dimensions is illustrated bytwo examples. We introduce theborder propagation(BP) algorithm, which decomposes ad-dimensionalarray (d&#8712;N) of scalar values into slope regions. Itis novel as it allows more than 2-dimensional data.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Bogner, A., Palmrich, A., &#38; Kropatsch, W. (2020). Border Propagation: A Novel Approach to Determine Slope Region Decompositions. In <i>Proceedings of the Joint Austrian Computer Vision and Robotics Workshop 2020</i> (p. 6). https://doi.org/10.3217/978-3-85125-752-6-31</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Alexander",
                    "last_name": "Bogner",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Alexander",
                    "last_name": "Palmrich",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "62076",
            "handle": "20.500.12708/58281",
            "doi": null,
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "The Influence of Full-Body Representation on Translation and Curvature Gain",
            "keywords": [],
            "abstract": "Redirected Walking (RDW) techniques allow users to navigate immersive virtual environments much larger than the available tracking space by natural walking. Whereas several approaches exist, numerous RDW techniques operate by applying gains of different types to the user´s viewport. These gains must remain undetected by the user in order for a RDW technique to support plausible navigation within a virtual environment. The present paper explores the relationship between detection thresholds of redirection gains and the presence of a self-avatar within the virtual environment. In four psychophysical experiments we estimated the thresholds of curvature and translation gain with and without a virtual body. The goal was to evaluate whether a full-body representation has an impact on the detection thresholds of these gains. The results indicate that although the presence of a virtual body does not significantly affect the detectability of these gains, it supports users with the illusion of easier detection. We discuss the possibility of a future combination of full-body representations and redirected walking and if these findings influence the implementation of large virtual environments with immersive virtual body representation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reimer, D., Langbehn, E., Kaufmann, H., &#38; Scherzer, D. (2020). The Influence of Full-Body Representation on Translation and Curvature Gain. In <i>2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</i>. IEEE, Austria. IEEE Virtual Reality. https://doi.org/10.1109/vrw50115.2020.00032</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Dennis",
                    "last_name": "Reimer",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Eike",
                    "last_name": "Langbehn",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Daniel",
                    "last_name": "Scherzer",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "62082",
            "handle": "20.500.12708/58287",
            "doi": null,
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "VR Bridges: Simulating Smooth Uneven Surfaces in VR",
            "keywords": [],
            "abstract": "Virtual reality (VR) is limited in many ways and often is incomparable to real-world experience. Walkable smooth uneven surfaces are inherent to reality but extremely lacking in VR. At the same time, VR offers a lot of possibilities for manipulations. In this paper, we focus on human height and slant perception of the uneven surfaces with multi-sensory stimulation in VR. By employing viewport manipulations, haptic, and vibrotactile stimuli, we explore the possibility to simulate uneven surfaces different from the physical props used. \r\n\r\nOur results suggest that the use of a rounded prop helps to create a more convincing illusion of an uneven surface that is significantly higher than the physical one. The multi-sensory stimulation brings both height and slant estimations closer to the values suggested by the visual cues if there is no conflict with the haptic sensations. The use of a flat prop is less realistic and leads to massive height and slant underestimations as opposed to those suggested by visual cues. However, if the curved prop cannot be used, a flat surface might still be used to simulate small dents and bumps.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vasylevska, K., Kovacs, B. I., &#38; Kaufmann, H. (2020). VR Bridges: Simulating Smooth Uneven Surfaces in VR. In <i>2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</i>. Ieee Vr 2020, United States of America (the). IEEE. https://doi.org/10.1109/vr46266.2020.00059</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Balint Istvan",
                    "last_name": "Kovacs",
                    "position": 2,
                    "role": "Author",
                    "tid": "262858"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "62109",
            "handle": "20.500.12708/58314",
            "doi": null,
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Physical object interaction in first responder mixed reality training",
            "keywords": [],
            "abstract": "Virtual Reality (VR) systems can improve the training of first responders for crisis preparedness in a number of ways. Realistic simulation of physical objects, however, is challenging. The huge variety of equipment pieces and other objects specialists in first responder units - and in particular CBRN-troops - interact with further increases the effort. In this paper, we present a novel and flexible Mixed Reality (MR) training system for first responders that enables the integration of physical objects by using Augmented Virtuality (AV) and &quot;binary tracking&quot;. A Head Mounted Display (HMD) immerses the user in VR, while augmenting the visualization with 3D imagery of real objects, captured by an RGB-D sensor. In addition, a RFID-reader at the user's hand detects the presence or absence (binary response) of certain equipment items. Our proposed MR system fuses this information with data of an inertial motion capture suit to an approximate global object pose and distributes it. Our solution provides a wide range of options for physical object interaction and collaboration in a multi-user MR environment. In addition, we demonstrate the training capabilities of our proposed system with a multi-user training scenario, simulating a CBRN crisis. Results of our technical and quantitative user evaluation with 13 experts in CBRN response from the Austrian Armed Forces (National Defense Academy &amp; Competence Center NBC Defense) indicate strong applicability and user acceptance. Over 80% of the participants found it easy or very easy to interact with physical objects and liked the multi-user training much or very much.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C., Bösch, C., Wechdorn, T., Göllner, J., Peer, A., Mossel, A., &#38; Kaufmann, H. (2020). Physical object interaction in first responder mixed reality training. In <i>Virtual, Augmented, and Mixed Reality (XR) Technology for Multi-Domain Operations</i>. SPIE Defense + Commercial Sensing, Anaheim, Non-EU. SPIE. https://doi.org/10.1117/12.2557396</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                },
                {
                    "first_name": "Chris",
                    "last_name": "Bösch",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Wechdorn",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Johannes",
                    "last_name": "Göllner",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Peer",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 6,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 7,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E234-02",
                "E259-01"
            ],
            "pid": "62339",
            "handle": "20.500.12708/58543",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Building Information Monitoring via Gamification",
            "keywords": [],
            "abstract": "For efficient facility management it is of high importance to monitor building information, such as energy consumption, indoor temperature, occupancy as well as changes in building structure. In this paper we present a novel methodology for monitoring information about building via gamification. In our approach, the employees of a facility record the states of building elements by playing a competitive mobile game. Traditionally, external sensors are used to automatically collect information about the building usage. In contrast to that, our methodology utilizes personal mobile phones of employees as sensors to identify objects of interest and report their state. Moreover, we propose to use crowdsourcing as a tool for data collection. This way the users of the mobile game are collecting points and compete with each other. At the end of the game the winning team gets the reward. We utilized various gamification strategies to increase motivation of users to collect building data. We ex tended the traditional 3D BIM model with temporal domain to enable tracking of building changes over time. Finally, we run an experiment with real use case building in which the employees used our system for the duration of three months. We studied our approach and our motivation strategies in a post-experiment study. Our results suggest that gamification can be a viable tool for building information monitoring. Additionally, we note that motivation plays a critical role in the data acquisition by gamification.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., Ferschin, P., Honic, M., &#38; Kovacic, I. (2021). Building Information Monitoring via Gamification. In <i>Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</i>. SciTePress. https://doi.org/10.5220/0010288902610270</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Ferschin",
                    "position": 2,
                    "role": "Author",
                    "tid": "135460"
                },
                {
                    "first_name": "Meliha",
                    "last_name": "Honic",
                    "position": 3,
                    "role": "Author",
                    "tid": "178974"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 4,
                    "role": "Author",
                    "tid": "42543"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "62340",
            "handle": "20.500.12708/58544",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Head Up Visualization of Spatial Sound Sources in Virtual Reality for Deaf and Hard-of-Hearing People",
            "keywords": [],
            "abstract": "This paper presents a novel method for the visualization of 3D spatial sounds in Virtual Reality (VR) for Deaf and Hard-of-Hearing (DHH) people. Our method enhances traditional VR devices with additional haptic and visual feedback, which aids spatial sound localization. The proposed system automatically analyses 3D sound from VR application, and it indicates the direction of sound sources to a user by two Vibro-motors and two Light-Emitting Diodes (LEDs). The benefit of automatic sound analysis is that our method can be used in any VR application without modifying the application itself. We evaluated the proposed method for 3D spatial sound visualization in a user study. Additionally, the conducted user study investigated which condition (corresponding to different senses) leads to faster performance in 3D sound localization task. For this purpose, we compared three conditions: haptic feedback only, LED feedback only, combined haptic and LED feedback. Our study results suggest that DHH participants could complete sound-related VR tasks significantly faster using LED and haptic+LED conditions in comparison to only haptic feedback. The presented method for spatial sound visualization can be directly used to enhance VR applications for use by DHH persons, and the results of our user study can serve as guidelines for the future design of accessible VR systems.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mirzaei, M., Kan, P., &#38; Kaufmann, H. (2021). Head Up Visualization of Spatial Sound Sources in Virtual Reality for Deaf and Hard-of-Hearing People. In <i>2021 IEEE Virtual Reality and 3D User Interfaces (VR)</i>. IEEE Computer Society. https://doi.org/10.1109/vr50410.2021.00083</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mohammadreza",
                    "last_name": "Mirzaei",
                    "position": 1,
                    "role": "Author",
                    "tid": "306356"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 2,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "62341",
            "handle": "20.500.12708/58545",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Multi-modal Spatial Object Localization in Virtual Reality for Deaf and Hard-of-Hearing People",
            "keywords": [],
            "abstract": "Information visualization techniques play an important role in Virtual Reality (VR) because they improve task performance, support cognitive processes, and eventually increase the feeling of immersion. Deaf and Hard-of-Hearing (DHH) persons have special needs for information presentation because they feel and perceive VR environments differently. Therefore, it is necessary to pay attention to requirements about presenting information in VR for this group of users. Previous research showed that adding special features and using haptic methods helps DHH persons to do VR tasks better. In this paper, we propose a novel Omni-directional particle visualization method and also evaluate multi-modal presentation methods in VR for DHH persons, such as audio, visual, haptic, and a combination of them (AVH). Additionally, we compare the results with the results of persons without hearing problems. The methods for information presentation in our study focus on spatial object localization in VR. Our user studies show that both DHH persons and persons without hearing problems were able to do VR tasks significantly faster using AVH. Also, we found out that DHH persons can do visual-related VR tasks faster than persons without hearing problems by using our new proposed visualization method. Our results suggest that the benefits of using audio among persons without hearing problems and the benefits of using vision among DHH persons cause an interesting balance in the results of AVH between both groups. Finally, our qualitative and quantitative evaluation indicates that both groups of participants preferred and enjoyed AVH modality more than other modalities.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mirzaei, M., Kan, P., &#38; Kaufmann, H. (2021). Multi-modal Spatial Object Localization in Virtual Reality for Deaf and Hard-of-Hearing People. In <i>2021 IEEE Virtual Reality and 3D User Interfaces (VR)</i>. IEEE Computer Society. https://doi.org/10.1109/vr50410.2021.00084</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mohammadreza",
                    "last_name": "Mirzaei",
                    "position": 1,
                    "role": "Author",
                    "tid": "306356"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 2,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "62365",
            "handle": "20.500.12708/58569",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Pyramidal Connected Component Labeling by Irregular Graph Pyramid",
            "keywords": [],
            "abstract": "This paper presents a new logarithmic-time algorithm which simultaneously assigns labels to all connected components of a binary image in parallel. The irregular graph pyramid of an input binary image is constructed based on the optimized combinatorial structure. The novel small built pyramid has only three levels and is created at worst case with O(log(N 2 )) complexity for a N×N-sized (2D) binary image. To assign a label to each connected component, instead of the common linear-time raster scan techniques (with complexity O(N 2 )), only two important movements, namely bottom-up and top-down traversing, are needed. First, in the bottom-up traversing, the redundant connections are removed and the contraction kernels are contracted. This results in a simpler graph at top of the pyramid each of its vertices has a unique label identifying corresponding connected component. Such reduced graph preserves all connecting relations including inclusions. Second, in the top-down traversing, each unique label propagates down into each individual corresponding pixel at the base level. The complexity of the labeling propagation procedure in worst cases is O (log(image - size)). The GPU implementation of the algorithm has high performance and the bottleneck is the bandwidth of the memory or equivalently the number of available independent processing elements. Finally, the experimental results show the proposed algorithm outperforms the other state-of-the-art methods.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Banaeyan, M., &#38; Kropatsch, W. G. (2021). Pyramidal Connected Component Labeling by Irregular Graph Pyramid. In M. Banaeyan &#38; W. Kropatsch (Eds.), <i>2021 5th International Conference on Pattern Recognition and Image Analysis (IPRIA)</i>. IEEE. https://doi.org/10.1109/ipria53572.2021.9483533</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Majid",
                    "last_name": "Banaeyan",
                    "position": 1,
                    "role": "Author",
                    "tid": "286018"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Majid",
                    "last_name": "Banaeyan",
                    "position": 1,
                    "role": "Editor",
                    "tid": "286018"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Editor",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "62371",
            "handle": "20.500.12708/58575",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Exploring behaviour towards avatars and agents in immersive virtual environments with mixed-agency interactions",
            "keywords": [],
            "abstract": "Immersive virtual environments (IVEs) in which multiple users nav-\r\nigate by walking and interact with each other in natural ways are\r\nperfectly suited for team applications from training to recreation. At\r\nthe same time, they can solve scheduling conflicts by employing\r\nvirtual agents in place of missing team members or additional par-\r\nticipants of a scenario. While this idea has been long discussed in\r\nIVEs research there are no prior publications on social interactions\r\nin systems with multiple embodied users and agents. This paper\r\npresents an experiment at a work-in-progress stage that addresses\r\nthe impact of perceived agency and control of a virtual character in\r\na collaborative scenario with two embodied users and one virtual\r\nagent. Our future study will investigate whether users treat avatars\r\nand agents differently within a mixed-agency scenario, analysing\r\nseveral behavioural metrics and self-report of participants",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Podkosova, I., Zibrek, K., Pettre, J., Hoyet, L., &#38; Olivier, A.-H. (2021). Exploring behaviour towards avatars and agents in immersive virtual environments with mixed-agency interactions. In <i>2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</i>. IEEE Conference on Virtual Reality and 3D User Interfaces, Lisbon, Portugal. IEEE. https://doi.org/10.1109/vrw52623.2021.00033</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Author",
                    "tid": "247245"
                },
                {
                    "first_name": "Katja",
                    "last_name": "Zibrek",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Julien",
                    "last_name": "Pettre",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Ludovic",
                    "last_name": "Hoyet",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Anne-Helene",
                    "last_name": "Olivier",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-07"
            ],
            "pid": "62426",
            "handle": "20.500.12708/58630",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Egocentric Network Exploration for Immersive Analytics",
            "keywords": [],
            "abstract": "To exploit the potential of immersive network analytics for engaging and effective exploration, we promote the metaphor of ``egocentrism&quot;, where data depiction and interaction are adapted to the perspective of the user within a 3D network. Egocentrism has the potential to overcome some of the inherent downsides of virtual environments, e.g., visual clutter and cyber-sickness. To investigate the effect of this metaphor on immersive network exploration, we designed and evaluated interfaces of varying degrees of egocentrism. In a user study, we evaluated the effect of these interfaces on visual search tasks, efficiency of network traversal, spatial orientation, as well as cyber-sickness. Results show that a simple egocentric interface considerably improves visual search efficiency and navigation performance, yet does not decrease spatial orientation or increase cyber-sickness. A distorted occlusion-free view of the neighborhood only marginally improves the user's performance. We tie our findings together in an open online tool for egocentric network exploration, providing actionable insights on the benefits of the egocentric network exploration metaphor.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sorger, J., Arleo, A., Kan, P., Knecht, W., &#38; Waldner, M. (2021). Egocentric Network Exploration for Immersive Analytics. In <i>Computer Graphics Forum</i> (pp. 241–252). John Wiley and Sons. https://doi.org/10.1111/cgf.14417</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Johannes",
                    "last_name": "Sorger",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Alessio",
                    "last_name": "Arleo",
                    "position": 2,
                    "role": "Author",
                    "tid": "317497"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 3,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Wolfgang",
                    "last_name": "Knecht",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Manuela",
                    "last_name": "Waldner",
                    "position": 5,
                    "role": "Author",
                    "tid": "269459"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "62436",
            "handle": "20.500.12708/58640",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Safety and Security Challenges for Collaborative Robotics in VR",
            "keywords": [],
            "abstract": "Virtual reality (VR) security and privacy are not limited to existing software solutions and applications. In this article, we present to the community the challenges of VR systems with robot integration. Integrating robots under ROS poses a massive risk in terms of data security. At the same time, using a robot for simulations in VR requires, first and foremost, the user's safety - hence redundant data collection and sharing. We want to draw the community's attention to these problems through our example in order to ensure that such systems are thoroughly developed in all directions and well prepared for further deployment to the consumer market.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mortezapoor, S., &#38; Vasylevska, K. (2021). Safety and Security Challenges for Collaborative Robotics in VR. In <i>Proceedings of the 1st International Workshop on Security for XR and XR for Security (VR4Sec) at Symposium On Usable Privacy and Security (SOUPS) 2021</i> (pp. 1–4). USENIX Conference Proceedings. http://hdl.handle.net/20.500.12708/58640</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Soroosh",
                    "last_name": "Mortezapoor",
                    "position": 1,
                    "role": "Author",
                    "tid": "252414"
                },
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 2,
                    "role": "Author",
                    "tid": "232367"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "6252",
            "handle": "20.500.12708/6267",
            "doi": "10.34726/hss.2014.23935",
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Erfassung sensorischer Fähigkeiten bei Kindern mit Autismus mittels Touchscreen und Berührungssensoren",
            "keywords": [
                "autism spectrum disorder",
                "proprioception",
                "pervasive computing",
                "medical tests",
                "mobile healthcare",
                "wireless sensors",
                "touchscreen",
                "force sensor"
            ],
            "abstract": "Autism Spectrum Disorder (ASD) is a pervasive developmental disorder with an average prevalence of less than one percent. It is characterized by impairments in social, communication and behavioral skills. Many countries prioritize autism research in order to better understand the causes and mechanisms of this disorder and to develop more specific and causal treatments compared to the dominating symptomatic approach of behavior modification. An emerging line of research that attempts to reveal underlying mechanisms of Autism Spectrum Disorder (ASD) studies differences in sensory processing in individuals with ASD. In this work we introduce new methods to measure proprioceptive functions of children with ASD. The instruments use low-cost Arduino boards and shields to acquire data from force and touch sensors. The received data is transferred to mobile devices and analyzed with a cross-platform software application. The instruments were pilot tested with typically developing children to test for functionality and usability of the instruments. Furthermode they were tested with known measures to show the accuracy an robustness oft he develeoped system. In the future, they will be used in a larger study with children with ASD.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Riederer, M. (2014). <i>Erfassung sensorischer Fähigkeiten bei Kindern mit Autismus mittels Touchscreen und Berührungssensoren</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2014.23935</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "12524",
                    "name": "Riederer Martin - 2014 - Erfassung sensorischer Faehigkeiten bei Kindern mit...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 9823054,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6267/2/Riederer%20Martin%20-%202014%20-%20Erfassung%20sensorischer%20Faehigkeiten%20bei%20Kindern%20mit...pdf"
                },
                {
                    "bsid": "84732",
                    "name": "Riederer Martin - 2014 - Erfassung sensorischer Faehigkeiten bei Kindern mit...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 119330,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6267/5/Riederer%20Martin%20-%202014%20-%20Erfassung%20sensorischer%20Faehigkeiten%20bei%20Kindern%20mit...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Riederer",
                    "position": 1,
                    "role": "Author",
                    "tid": "39412"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "6373",
            "handle": "20.500.12708/6388",
            "doi": "10.34726/hss.2016.32243",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Automatisierte Generierung von 3D-Gebäudemodellen aus vektorbasierten Grundrissplänen",
            "keywords": [],
            "abstract": "Floor plans are commonly used in the construction industry and contain crucial information about the depicted building. For several years now, the creation of such plans has heavily relied on computer systems, storing relevant data in vector file formats. However, due to the many different graphical representations of symbols and the lack of a unified drawing standard, the automatic interpretation of the semantic information in these floor plans is still an open problem. The main benefit of automated acquisition of this information lies in the possibility to efficiently generate three-dimensional building models, which can then be employed for different purposes. The goal of this work was to design and develop a system for the automated interpretation of vector-based floor plans. Further, the system should be able to make structural and topological information in form of three-dimensional building models available for interpretation by other applications. Next to the autonomous operation of the system, genericity and robustness were key factors and made up the main problem statement. Different floor plans have been analysed to establish a compact set of rules, which go well together with the developed recognition methods. Each method devised was enhanced iteratively until a satisfactory level of performance had been achieved. This resulted in a fully automated approach supported by various underlying semiautomatic methods. This approach provides a reasonable compromise between automation and genericity. The concluding quantitative evaluation of the developed methods yielded compelling results. The system was able to analyse complex floor plans with minimal user interaction in a reasonable time.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Furlan, L. (2016). <i>Automatisierte Generierung von 3D-Gebäudemodellen aus vektorbasierten Grundrissplänen</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.32243</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "12766",
                    "name": "Furlan Lukas - 2016 - Automatisierte Generierung von 3D-Gebaeudemodellen aus...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 13489885,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6388/2/Furlan%20Lukas%20-%202016%20-%20Automatisierte%20Generierung%20von%203D-Gebaeudemodellen%20aus...pdf"
                },
                {
                    "bsid": "84029",
                    "name": "Furlan Lukas - 2016 - Automatisierte Generierung von 3D-Gebaeudemodellen aus...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 216102,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6388/5/Furlan%20Lukas%20-%202016%20-%20Automatisierte%20Generierung%20von%203D-Gebaeudemodellen%20aus...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Lukas",
                    "last_name": "Furlan",
                    "position": 1,
                    "role": "Author",
                    "tid": "182083"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "6378",
            "handle": "20.500.12708/6393",
            "doi": "10.34726/hss.2016.34153",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Realistic rendering in mobile augmented reality",
            "keywords": [
                "Realistic Rendering",
                "Augmented Reality",
                "Mobile Devices"
            ],
            "abstract": "Augmented Reality (AR) applications combine a view of a physical, real-world environment with computer-generated objects and effects in real-time. Depending on the application, it is desirable to maximize the visual coherence of the virtual objects compared to the real-world image. To achieve this goal, virtual objects have to be rendered as realistically as possible. This thesis presents an image-based lighting (IBL) technique for realistic rendering of virtual objects on mobile devices which uses lighting information from the real-world environment. In the first step, the presented technique uses a mobile device's camera and motion sensors to capture an omni-directional image of the surrounding in high dynamic range (HDR) and stores it in an environment map. In the second step, the captured environment map is prepared for rendering with different materials by calculating a set of maps. During rendering, the most suitable of these maps are selected for each material and used for shading a virtual object with the specific material. The map which contains diffuse illumination information is called irradiance map, and the maps which contain glossy or specular illumination information are called reflection maps. The calculation of the maps corresponds to a weighted convolution. The weighting is determined by a reflection model which takes the correct amount of incident lighting from all directions into account. How these calculations can be performed efficiently on mobile devices is the main focus of this thesis. Multiple approaches to perform the calculations are described. Their properties, results, strengths and weaknesses are analyzed and optimizations are proposed. We describe three different approaches for the calculation of irradiance and reflection maps in this thesis: the accurate calculation, a MIP-mapping  based approximation method, and calculation via spherical harmonics (SH) frequency space. We provide detailed implementation instructions, analyses, and discussions for each of these approaches with regard to the properties and limitations of mobile devices. Furthermore, we describe how the calculated maps can be used with IBL rendering and be combined with established rendering techniques to achieve a high degree of visual coherence of virtual objects in AR scenes. The main novelty of this thesis is its focus on the capabilities of mobile devices. We describe how to do all steps on a single commodity mobile device: From capturing the environment at a certain point in space, to calculating the irradiance and reflection maps, and finally rendering virtual objects using the calculated maps in an AR scene.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Unterguggenberger, J. (2016). <i>Realistic rendering in mobile augmented reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.34153</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "12776",
                    "name": "Unterguggenberger Johannes - 2016 - Realistic rendering in mobile augmented...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 18379688,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6393/2/Unterguggenberger%20Johannes%20-%202016%20-%20Realistic%20rendering%20in%20mobile%20augmented...pdf"
                },
                {
                    "bsid": "83741",
                    "name": "Unterguggenberger Johannes - 2016 - Realistic rendering in mobile augmented...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 229188,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6393/5/Unterguggenberger%20Johannes%20-%202016%20-%20Realistic%20rendering%20in%20mobile%20augmented...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Johannes",
                    "last_name": "Unterguggenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "42532"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "6561",
            "handle": "20.500.12708/6576",
            "doi": "10.34726/hss.2016.26204",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Avatar control by automatically detected face interest points",
            "keywords": [
                "Gesichtsbeschreibung",
                "Emotionserkennung",
                "Maschinelles Lernen",
                "3D Modellierung",
                "Avatar"
            ],
            "abstract": "Designing systems that are able to interact with people is a complex process. An important aspect of this problem is understanding human emotions and responding to them in a human way. The fact that people themselves often have problems in recognizing emotions properly makes the task even more difficult. There are currently numerous robust and well-functioning systems that can recognize human faces, and locate the eyes, nose and mouth. However, these systems miss the so-called meta-information in the form of a detailed description of the face, which can lead to a deeper understanding of facial expressions. This information should not be underestimated, since facial expressions contain a large amount of non-verbal information. Facial expressions are important in human communication because much information is transmitted through non-verbal communication. A system that can automatically detect human emotion would be useful in areas such as human-computer interaction, psychology, sociology and other areas. Such a system would enable automated analysis of stress, vertigo or aggression levels. Moreover, it would also be useful in monitoring public spaces, resulting in higher security. The aim of this project is to design and implement a robust system that can recognize and analyze emotions from human faces. The system should be fully automated so that the user does not need to setup any parameters in order to make the system run correctly. The expected output is the textual description of the emotion. The analyzed face parameters are also forwarded to the animation component, where the facial expression is animated on an avatar.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Byrtus, M. (2016). <i>Avatar control by automatically detected face interest points</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.26204</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "13142",
                    "name": "Byrtus Miroslav - 2016 - Avatar control by automatically detected face interest...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 9963383,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6576/2/Byrtus%20Miroslav%20-%202016%20-%20Avatar%20control%20by%20automatically%20detected%20face%20interest...pdf"
                },
                {
                    "bsid": "84054",
                    "name": "Byrtus Miroslav - 2016 - Avatar control by automatically detected face interest...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 118965,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6576/5/Byrtus%20Miroslav%20-%202016%20-%20Avatar%20control%20by%20automatically%20detected%20face%20interest...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Miroslav",
                    "last_name": "Byrtus",
                    "position": 1,
                    "role": "Author",
                    "tid": "277209"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "6573",
            "handle": "20.500.12708/6588",
            "doi": "10.34726/hss.2016.28648",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Comparison of treatment plan models",
            "keywords": [
                "clinical practice guidelines",
                "process modeling",
                "process model comparison"
            ],
            "abstract": "To assist physicians with the treatment planning process so-called clinical practice guidelines are created. They contain general information about a specific clinical condition as well as rules and procedures to treat patients with this condition. As these guidelines are represented in free text form they are difficult to handle for physicians in their daily working process. Therefore the information in the guideline document is formalized to create computerized guidelines. We work with the formalization language Asbru. The creation of these computer-interpretable guidelines is either done manually which is a great effort or automatically using methods of Information Extraction. The aim of this thesis is to verify whether the automatically generated model of a guideline corresponds to the manually generated model of the same guideline. Doing this manually would again be a great effort and therefore I want to investigate methods to automatically evaluate parts of these models. I focus on the procedural knowledge of the models. To be able to compare two different models I investigated them. At first I developed methods to compare two models. To make a statement about the similarity of two models we started with comparing their activities using similarity metrics in order to identify corresponding plans of two models. In a second step we furthermore looked into their process structure using workflow patterns and tried to find similarities as well. Then I implemented these methods prototypically and tested them using a `real-world' example, a guideline for gestational diabetes mellitus. The example was based on the output of the GESHER tool for manually creating computerized treatment plan models and the tool LASSIE for automatically creating computerized treatment plan models. Using this example we evaluated how  much of the original guideline text is present in the automatically generated model. Finally, it was evaluated if the automatically generated model finds the same information as present in the manually generated model.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Lehner, M. (2016). <i>Comparison of treatment plan models</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.28648</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "13166",
                    "name": "Lehner Michaela - 2016 - Comparison of treatment plan models.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 6466910,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6588/2/Lehner%20Michaela%20-%202016%20-%20Comparison%20of%20treatment%20plan%20models.pdf"
                },
                {
                    "bsid": "83656",
                    "name": "Lehner Michaela - 2016 - Comparison of treatment plan models.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 125623,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6588/5/Lehner%20Michaela%20-%202016%20-%20Comparison%20of%20treatment%20plan%20models.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Michaela",
                    "last_name": "Lehner",
                    "position": 1,
                    "role": "Author",
                    "tid": "62488"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "6584",
            "handle": "20.500.12708/6599",
            "doi": "10.34726/hss.2016.25428",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Erkennung sich wiederholender Ereignisse in Langzeitvideoaufnahmen",
            "keywords": [
                "Long-time video recordings",
                "Event Detection",
                "Reoccurring Event Detection"
            ],
            "abstract": "Long-time video recordings, created by animal monitoring or surveillance, can easily span hundreds of hours and thus take up a long time in the post-processing. Often it is necessary to view the entire video, to assure to gather all occurrences of interest. However, plenty of times the interest focuses only on specific events that occur repeatedly (e.g. typical actions of people or animals). In this case, viewing the entire video is very inefficient. This problem can be reduced by presenting the relevant events in a condensed form. It is the intention in this thesis to automatically extract re-occurring events from long time video recordings. Therefore image processing algorithms and statistical algorithms are used to identify recurring events. Motion and visual appearance of objects are captured and described in the form of basic feature patterns. If a feature pattern is repeated at a later point in time, a repeated event is detected. The processing occurs as follows: First the footage is scanned for moving objects by use of temporal and local image segmentation. At every video frame an attempt is being made to find these objects in the previous frame, to obtain a temporal tracking of the objects. After objects are tracked over time, motion features and color features are extracted from every object. As a result every object together with its motion pattern is described by a set of features. After the prior extracted features are clustered, an alphabet is generated to describe higher-level events related to the object in terms of strings. Similar events are detected by a further clustering, which proves the similarity of the strings. String matching is applied to detect repeated events. We evaluate our method on long-time surveillance video recordings of animal enclosures. Our experiments show that re-occurring  events can be detected robustly by the proposed method. This thesis describes our algorithm, the performed experiments, presents results and discusses open topics at the end.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Sedelmaier, A. (2016). <i>Erkennung sich wiederholender Ereignisse in Langzeitvideoaufnahmen</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.25428</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "13188",
                    "name": "Sedelmaier Achim - 2016 - Erkennung sich wiederholender Ereignisse in...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 12573600,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6599/2/Sedelmaier%20Achim%20-%202016%20-%20Erkennung%20sich%20wiederholender%20Ereignisse%20in...pdf"
                },
                {
                    "bsid": "84777",
                    "name": "Sedelmaier Achim - 2016 - Erkennung sich wiederholender Ereignisse in...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 140307,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6599/5/Sedelmaier%20Achim%20-%202016%20-%20Erkennung%20sich%20wiederholender%20Ereignisse%20in...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Achim",
                    "last_name": "Sedelmaier",
                    "position": 1,
                    "role": "Author",
                    "tid": "54400"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "53598"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E234-02"
            ],
            "pid": "66716",
            "handle": "20.500.12708/62894",
            "doi": null,
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Framework Proposal for a BIM-Based Digital Platform for Flexible Design and Optimization of Industrial Buildings for Industry 4.0",
            "keywords": [],
            "abstract": "The production of the future, respectively industry 4.0, is a highly networked, digitized and thus individualized production, leading to shortened product lifecycles and constantly changing processes. Industrial buildings need to be capable to react to these varying conditions, making the realization of flexible building systems necessary. Inflexible building structures lead to early rescheduling or even demolishing, resulting in increased life cycle costs and material demand. The load-bearing structure is decisive for maximum flexibility as it is the most rigid element with the longest service life. The planning of flexible building structures and the consideration of production in building design requires maximum integration of all stakeholders in early design stage. However, early integration of all stakeholders, processes and tools is rare and difficult due to the lack of interoperability of domain-specific software and sequential planning methodology. This paper presents the ongoing research conducted within the research project BIMFlexi. The goal of BIMFlexi is to make industrial buildings efficiently adaptable to rapidly changing production processes by developing an integrated BIM-based digital platform to enable flexible structural analysis, taking into account changing production processes and support in multi-objective optimization and decision support. In this paper, potentials and limits for integrating processes and discipline specific models of building and production planning are identified and a framework for a \"BIM-based digital Platform for Flexible Design and Optimization of Industrial Buildings for Industry 4.0\" proposed. The proposed framework couples digital tools such as BIM, parametric modelling, structural analysis and VR within a platform to allow multi-objective optimization and early decision making in real-time.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reisinger, J., Kovacic, I., Kaufmann, H., Kan, P., &#38; Podkosova, I. (2020). Framework Proposal for a BIM-Based Digital Platform for Flexible Design and Optimization of Industrial Buildings for Industry 4.0. In <i>CIB W78 Proceedings</i>. ICCCBE+CIB W78 2020 - The First Virtual Joint Conference on Computing and Information Technology in Civil and Building Engineering, São Paulo, Brazil (online), Non-EU. online. https://doi.org/10.46421/2706-6568.37.2020.paper029</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julia",
                    "last_name": "Reisinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "230671"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 2,
                    "role": "Author",
                    "tid": "42543"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kan",
                    "position": 4,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 5,
                    "role": "Author",
                    "tid": "247245"
                }
            ],
            "foci": [],
            "projects": [
                "1734269"
            ]
        },
        {
            "org_nrs": [
                "E193-02",
                "E194-01",
                "E234-02"
            ],
            "pid": "66789",
            "handle": "20.500.12708/62967",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": true,
            "invited": false,
            "title": "Framework proposal for automated generation of production layout scenarios: A parametric design technique to connect production planning and structural industrial building design",
            "keywords": [],
            "abstract": "To increase the flexibility and expandability of production plants the focus needs to be on a coherent planning of the production layout and building systems. The frequent reconfiguration of production layouts bears challenges on the load-bearing structure of industrial buildings, decreasing the building service life due to rescheduling or demolition. Currently there is no method established to integrate production layout planning into structural building design processes. In this paper, a novel parametric generative design method for automated production layout generation and optimisation (PLGO) is presented, producing layout scenarios to be respected in structural building design. Results of a state-of-the-art analysis and a case study methodology are combined to develop a novel concept of integrated production cubes (IPC). The IPC concept is translated into a parametric PLGO framework, which is tested on a pilot-project of a food-and hygiene production facility and the defined objectives and constraints are validated.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Reisinger, J., Zahlbruckner, M. A., Kovacic, I., Kán, P., &#38; Wang-Sukalia, X. (2021). Framework proposal for automated generation of production layout scenarios: A parametric design technique to connect production planning and structural industrial building design. In <i>EG-ICE 2021 Workshop on Intelligent Computing in Engineering</i> (pp. 22–33). Universitätsverlag der TU Berlin. https://doi.org/10.14279/depositonce-12021</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Julia",
                    "last_name": "Reisinger",
                    "position": 1,
                    "role": "Author",
                    "tid": "230671"
                },
                {
                    "first_name": "Maria Antonia",
                    "last_name": "Zahlbruckner",
                    "position": 2,
                    "role": "Author",
                    "tid": "324294"
                },
                {
                    "first_name": "Iva",
                    "last_name": "Kovacic",
                    "position": 3,
                    "role": "Author",
                    "tid": "42543"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 4,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Xi",
                    "last_name": "Wang-Sukalia",
                    "position": 5,
                    "role": "Author",
                    "tid": "253667"
                }
            ],
            "foci": [],
            "projects": [
                "1734269"
            ]
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "6720",
            "handle": "20.500.12708/6735",
            "doi": "10.34726/hss.2016.29541",
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Editing reality: a tool for interactive segmentation and manipulation of 3D reconstructed scenes",
            "keywords": [
                "Interactive Segmentation",
                "3D Reconstruction",
                "3D Interaction"
            ],
            "abstract": "The rise of consumer devices capable of 3D scanning has given many people access to technology that is able to reconstruct three-dimensional environments in real-time at a relatively low cost. Available consumer solutions for scene reconstruction, however, are mostly focused on producing a single, cleaned watertight mesh that is suitable for 3D printing. There are no or very limited ways for a user to navigate and manipulate the reconstructed scene in a natural and intuitive manner. To this end, the virtual scene must be segmented into distinct objects that should resemble their real-world counterparts. To further aid the design of an intuitive system, prevalent approaches to scene navigation have to be revisited and improved upon. In this research project, a prototype for the virtual reconstruction and manipulation of three-dimensional scenes has been developed. The system allows for quick and intuitive 3D scanning and reconstruction by using a tablet display in combination with a depth camera. Distinct planes and objects are identified and separated from each other by utilizing a combination of automatic and manual segmentation techniques. The subsequent processing stage allows users to fill possible gaps and remove small, unwanted components. Afterwards, the tracking capabilities of the 3D reconstruction algorithm enable users to navigate through their reconstructed virtual scene by physically moving the tablet. The mobile display functions as a window into the virtual world. By utilizing the touch capabilities of the screen, previously segmented objects can be carried around and repositioned anywhere in the scene. Duplication, transformation and removal of items is also possible with the provided tools. Edited scenes can be exported to one of several common file formats for use in other applications. After  development, a small user study was conducted in order to evaluate the prototype. The results show that the system has a high usability and can be learned easily. Participants were able to reconstruct and segment scenes in reasonable quality without much effort. Moreover, the methods used for scene navigation and interaction proved to be highly intuitive and natural. The evaluation also revealed several possible areas of improvement for future releases.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Huber, C. (2016). <i>Editing reality: a tool for interactive segmentation and manipulation of 3D reconstructed scenes</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2016.29541</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "13461",
                    "name": "Huber Christoph - 2016 - Editing reality a tool for interactive segmentation and...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 15981072,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6735/2/Huber%20Christoph%20-%202016%20-%20Editing%20reality%20a%20tool%20for%20interactive%20segmentation%20and...pdf"
                },
                {
                    "bsid": "85019",
                    "name": "Huber Christoph - 2016 - Editing reality a tool for interactive segmentation and...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 309399,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/6735/5/Huber%20Christoph%20-%202016%20-%20Editing%20reality%20a%20tool%20for%20interactive%20segmentation%20and...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Christoph",
                    "last_name": "Huber",
                    "position": 1,
                    "role": "Author",
                    "tid": "59943"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E186"
            ],
            "pid": "7090",
            "handle": "20.500.12708/7105",
            "doi": "10.34726/hss.2015.26632",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Structurally correct image segmentation using local binary patterns and the combinatorial pyramid",
            "keywords": [
                "Image segmentation",
                "local binary patterns",
                "combinatorial pyramid"
            ],
            "abstract": "In this thesis we present a new image segmentation algorithm which is based on Local Binary Patterns and the Combinatorial Pyramid. Current Local Binary Pattern-based segmentation algorithms utilize statistical approaches in form of a histogram to describe and compare textured regions, and to subdivide an image into homogeneous regions. The novelty of our approach is that we omit the usage of histograms and perform a segmentation based directly on the local structure of the image, while at the same time preserving structural correctness and image topology. In our work we define five topological classes that are based on the Local Binary Patterns of regions and are invariant to the number and shifting of bits, namely local minima, slopes, singular slopes, saddles, and local maxima. Using these classes in combination with the dual graph we are able to identify and remove redundant structural information. This approach simplifies the image graph and enables a merging of connected regions without introducing structural errors. We compare our algorithm to five other algorithms using the Global Consistency Error and Probabilistic Rand Index error metrics. One of these algorithms is a pre-version of our proposed algorithm which does not take structural constraints into consideration, and the remaining four algorithms are existing algorithms based on internal- and external contrast, Minimum Spanning Trees, Mean-Shift, and superpixel approaches. The evaluation shows, that the proposed algorithm indicates comparably good results with the Global Consistency Error metric, and it beats all of the five algorithms in terms of a high Probability Rand Index score. This segmentation behavior suggests, that a refinement of segmentations takes place at regions where there is evidence of multiple levels of granularity of segmentations  performed by human subjects, and thus an application in image compression can be found.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Cerman, M. (2015). <i>Structurally correct image segmentation using local binary patterns and the combinatorial pyramid</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.26632</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "14201",
                    "name": "Cerman Martin - 2015 - Structurally correct image segmentation using local...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 8675177,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7105/2/Cerman%20Martin%20-%202015%20-%20Structurally%20correct%20image%20segmentation%20using%20local...pdf"
                },
                {
                    "bsid": "84862",
                    "name": "Cerman Martin - 2015 - Structurally correct image segmentation using local...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 183044,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7105/5/Cerman%20Martin%20-%202015%20-%20Structurally%20correct%20image%20segmentation%20using%20local...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Cerman",
                    "position": 1,
                    "role": "Author",
                    "tid": "43978"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "7101",
            "handle": "20.500.12708/7116",
            "doi": "10.34726/hss.2015.24977",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Noise Map: eine kartografische Visualisierung für Daten erhoben durch die Noise-O-Meter Applikation",
            "keywords": [
                "Noise Measurement",
                "Geo Tagging"
            ],
            "abstract": "This thesis utilizes the possibilities of informatics in order to shed light on noise pollution in everyday life. The main focus is on a prototype which enables people to record and relate to their individual noise exposure. It consists of a mobile sensor component (Noise-O-Meter), a central server component as well as a cartographic visualization (Noise Map). The Noise Map is a web map which displays noise measurements and offers interaction for data exploration. The realization of this prototype proves the feasibility of a noise map filled with data collected by individuals (participatory sensing). Thereby the project examines the question if the limitations of conventional noise maps can be overcome by this concept. An iterative process of extensive literature research as well as analysis of related projects led to a software concept. Based on this concept the prototype consisting of three components was realized. Those components are a sensor component for data collection, a server component for data handling with a standardized API and a cartographic visualization, the noise map, in order to present the data. System tests and an evaluation in which participants collected noise measurements with their smartphones and finally explored them on the map led to improvements in usability as well as readability. The realized prototype proves that noise visualization from the perspective of pedestrians by participatory sensing is feasible and has the potential to visualize individual noise exposure.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Paula, S. (2015). <i>Noise Map: eine kartografische Visualisierung für Daten erhoben durch die Noise-O-Meter Applikation</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.24977</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "14223",
                    "name": "Paula Stefan - 2015 - Noise Map eine kartografische Visualisierung fuer Daten...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 18820855,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7116/2/Paula%20Stefan%20-%202015%20-%20Noise%20Map%20eine%20kartografische%20Visualisierung%20fuer%20Daten...pdf"
                },
                {
                    "bsid": "84870",
                    "name": "Paula Stefan - 2015 - Noise Map eine kartografische Visualisierung fuer Daten...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 153424,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7116/5/Paula%20Stefan%20-%202015%20-%20Noise%20Map%20eine%20kartografische%20Visualisierung%20fuer%20Daten...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Stefan",
                    "last_name": "Paula",
                    "position": 1,
                    "role": "Author",
                    "tid": "56820"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "7108",
            "handle": "20.500.12708/7123",
            "doi": "10.34726/hss.2015.31589",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Evaluation of a hybrid sound model for 3D audio games with real walking",
            "keywords": [
                "3D Audio",
                "Evaluation",
                "Audio Games"
            ],
            "abstract": "Audio games are a subcategory of computer games that have spatialized audio as the only output that their players receive. In order to provide a compelling impression of the environment, sound in audio games has to be of superior quality in terms of immersion and realism. For improving the immersion and realism in audio games, complex sound models can be used to generate realistic sound effects, including reflections and reverberation. An implementation of a hybrid sound model similar to the ODEON approach is introduced and adapted for real-time sound calculations. This model is evaluated and compared to a baseline model usually used in audio games in a user study in a virtual reality environment. The results show that the implemented hybrid model allows players to adjust to the game faster and provides them more support in avoiding virtual obstacles in simple room geometries than the baseline model. Complex sound models are beneficial in audio games, provided that there is enough computational resources available to perform calculations in real time.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Urbanek, M. (2015). <i>Evaluation of a hybrid sound model for 3D audio games with real walking</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.31589</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "14237",
                    "name": "Urbanek Michael - 2015 - Evaluation of a hybrid sound model for 3D audio games...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 7538906,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7123/2/Urbanek%20Michael%20-%202015%20-%20Evaluation%20of%20a%20hybrid%20sound%20model%20for%203D%20audio%20games...pdf"
                },
                {
                    "bsid": "84873",
                    "name": "Urbanek Michael - 2015 - Evaluation of a hybrid sound model for 3D audio games...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 208033,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7123/5/Urbanek%20Michael%20-%202015%20-%20Evaluation%20of%20a%20hybrid%20sound%20model%20for%203D%20audio%20games...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Urbanek",
                    "position": 1,
                    "role": "Author",
                    "tid": "277244"
                },
                {
                    "first_name": "Iana",
                    "last_name": "Podkosova",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "247245"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E186"
            ],
            "pid": "7129",
            "handle": "20.500.12708/7144",
            "doi": "10.34726/hss.2015.32946",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "A graph centrality approach to computer vision",
            "keywords": [
                "Graph centrality",
                "registration of point cloud",
                "MST with maximal entropy",
                "machine learning"
            ],
            "abstract": "Computer vision has changed the way we deal with computers, since the first automatic facial detection to the latest augmented reality application. The study of social networks models relations using graph theory and has been emerging in recent years. This field designs the concept of centrality by defining how important a certain node is within a network. In this dissertation, we study the centrality concept applied to computer vision. We start by analyzing the point-set registration problem which arises whenever one needs to match information in images. The kind of information to be matched consists of feature locations, landmarks, or points representing a surface of an object. We propose our own centrality-oriented matching algorithm focusing on the generation of a distinguishable data-graph. In this second problem, we pose our data-graph generation using an optimization formulation whose goal is to maximize the entropy of the degree centrality while minimizing the cost of the edges. Later, we constrain this problem into generating a tree for robustness. We evaluate the robustness of the generated data-graph in the presence of differing amounts of noise in the local neighborhood of points. The last aspect studied in this dissertation is the impact of centrality in a machine learning scenario. We train learning algorithms using centrality-based features focusing on the classification of shapes. We evaluate our approach by perturbing the topology of the graph. In this dissertation, we study the behavior of centrality in the vision problems and estimate when a certain centrality measure can perform better than another according to their properties.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">De Sousa Junior, S. F. (2015). <i>A graph centrality approach to computer vision</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.32946</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "14279",
                    "name": "De Sousa Junior Samuel Felix - 2015 - A graph centrality approach to computer...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 4940367,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7144/2/De%20Sousa%20Junior%20Samuel%20Felix%20-%202015%20-%20A%20graph%20centrality%20approach%20to%20computer...pdf"
                },
                {
                    "bsid": "83418",
                    "name": "De Sousa Junior Samuel Felix - 2015 - A graph centrality approach to computer...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 322836,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7144/5/De%20Sousa%20Junior%20Samuel%20Felix%20-%202015%20-%20A%20graph%20centrality%20approach%20to%20computer...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Samuel Felix",
                    "last_name": "De Sousa Junior",
                    "position": 1,
                    "role": "Author",
                    "tid": "263694"
                },
                {
                    "first_name": "Walter G.",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "7149",
            "handle": "20.500.12708/7164",
            "doi": "10.34726/hss.2018.47201",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Hybrid tracking technology for virtual rock climbing",
            "keywords": [
                "Virtual Reality",
                "Tracking"
            ],
            "abstract": "Virtual reality applications detach the user from the real world by fully immersing the person into an artificial environment. With the help of a head-mounted display and soundproof headphones, the visual and auditory senses are limited to the virtual content provided by the simulation. Most applications refrain from representing the user’s limbs in the virtual experience. However, if the application requires the participant to interact with real objects, this feature can no longer be omitted. To allow an accurate representation of the extremities within the simulation, the position and pose of the hands and feet need to be determined in real-time by a motion capture system. This diploma thesis discusses the development of the VreeTracker, a hybrid tracking system that combines optical position tracking with inertial orientation sensing. Furthermore, the resulting prototype estimates the hand pose by a markerless approach. The VreeTracker is embedded in a virtual rock climbing adventure called VreeClimber. The VreeClimber consists of an indoor climbing wall that couples the safety of a virtual simulation and the haptic interaction with real objects. As a prerequisite, all necessary hardware components of the tracking system need to be affordable, easily available off-the-shelf devices. Therefore, in addition to implementing the tracking software, the development process includes all necessary modifications of said consumer products as well as the development of individual hardware components.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Steindl, L. (2018). <i>Hybrid tracking technology for virtual rock climbing</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.47201</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "14319",
                    "name": "Steindl Ludwig - 2018 - Hybrid tracking technology for virtual rock climbing.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 18892567,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7164/2/Steindl%20Ludwig%20-%202018%20-%20Hybrid%20tracking%20technology%20for%20virtual%20rock%20climbing.pdf"
                },
                {
                    "bsid": "84635",
                    "name": "Steindl Ludwig - 2018 - Hybrid tracking technology for virtual rock climbing.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 172966,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7164/5/Steindl%20Ludwig%20-%202018%20-%20Hybrid%20tracking%20technology%20for%20virtual%20rock%20climbing.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Ludwig",
                    "last_name": "Steindl",
                    "position": 1,
                    "role": "Author",
                    "tid": "60000"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "722",
            "handle": "20.500.12708/738",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "DeepLight: light source estimation for augmented reality using deep learning",
            "keywords": [
                "Light source estimation",
                "Augmented reality",
                "Photometric registration",
                "Deep learning"
            ],
            "abstract": "This paper presents a novel method for illumination estimation from RGB-D images. The main focus of the proposed method is to enhance visual coherence in augmented reality applications by providing accurate and temporally coherent estimates of real illumination. For this purpose, we designed and trained a deep neural network which calculates a dominant light direction from a single RGB-D image. Additionally, we propose a novel method for real-time outlier detection to achieve temporally coherent estimates. Our method for light source estimation in augmented reality was evaluated on the set of real scenes. Our results demonstrate that the neural network can successfully estimate light sources even in scenes which were not seen by the network during training. Moreover, we compared our results with illumination estimates calculated by the state-of-the-art method for illumination estimation. Finally, we demonstrate the applicability of our method on numerous augmented reality scenes.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P., &#38; Kaufmann, H. (2019). DeepLight: light source estimation for augmented reality using deep learning. <i>The Visual Computer: International Journal of Computer Graphics</i>, <i>35</i>, 873–883. https://doi.org/10.1007/s00371-019-01666-x</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "1463",
                    "name": "Kan Peter - 2019 - DeepLight light source estimation for augmented reality using...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 13791459,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/738/2/Kan%20Peter%20-%202019%20-%20DeepLight%20light%20source%20estimation%20for%20augmented%20reality%20using...pdf"
                },
                {
                    "bsid": "44449",
                    "name": "DeepLight light source estimation for augmented reality using deep learning.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 43586,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/738/8/DeepLight%20light%20source%20estimation%20for%20augmented%20reality%20using%20deep%20learning.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "7269",
            "handle": "20.500.12708/7284",
            "doi": "10.34726/hss.2018.56484",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Anwendungs-Design für virtuelles  Klettern",
            "keywords": [
                "Virtual Reality",
                "Content-Evaluation"
            ],
            "abstract": "During the last years, Virtual Reality became available for many people due to recent developments in the Video Game Industries. Not only the hardware advanced, but also the quantity and quality of provided content especially created for the requirements of Virtual Reality. But even though there are attempts to transfer body movements from reality to the virtual world, for example gestures of the hands, the problem of missing haptic feedback remains and the discrepancy between the real and the virtual elements reduces the possible extend of immersion into the virtual world. To counter this problem, the Vreeclimber project of TU Wien mixes real and virtual elements. Based on a real mechanical climbing wall that can rotate and therefore enables climbing into nearly endless heights, by using Virtual Reality glasses, this plain climbing wall can be overlaid with virtual content. The worlds designed for this setup can present very high climbing walls at various places while still providing real haptic and natural climbing movements. In this thesis, based on the mentioned setup, the effect of different Virtual Reality contents on the climbing experience were studied. To achieve this, three different scenarios were created and implemented. They differed not only in their setting, but also in various effects used to enhance the climbing experience. Furthermore, it was possible to change the level of difficulty for the people who climbed. Those scenarios were implemented using the method of evolutionary prototyping and then evaluated by a small test group with a questionnaire and the methods Thinking Aloud and Observation. The evaluation showed that an accurate calibration of the real and the virtual climbing wall is essential for a positive climbing experience. Also, those scenarios with many details and effects were rated better by the people who climbed than those where the focus was put mostly on the optics. By this, insights were gained on how to design Virtual Reality content that is used with a real-world set up so that people can immerse into the worlds and enjoy their climbing experience.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Machner, N. J. (2018). <i>Anwendungs-Design für virtuelles  Klettern</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.56484</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "14559",
                    "name": "Machner Natascha Josephine - 2018 - Anwendungs-Design fuer virtuelles Klettern.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 38967356,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7284/2/Machner%20Natascha%20Josephine%20-%202018%20-%20Anwendungs-Design%20fuer%20virtuelles%20Klettern.pdf"
                },
                {
                    "bsid": "83474",
                    "name": "Machner Natascha Josephine - 2018 - Anwendungs-Design fuer virtuelles Klettern.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 208474,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7284/5/Machner%20Natascha%20Josephine%20-%202018%20-%20Anwendungs-Design%20fuer%20virtuelles%20Klettern.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Natascha Josephine",
                    "last_name": "Machner",
                    "position": 1,
                    "role": "Author",
                    "tid": "231358"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "7338",
            "handle": "20.500.12708/7353",
            "doi": "10.34726/hss.2018.57180",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Real-time subtitle for the hearing impaired in augmented reality",
            "keywords": [
                "Augmented Reality",
                "Speech Recognition",
                "Assistive Technology"
            ],
            "abstract": "This master thesis aims to provide an introduction to the state of the art of some currently very promising technologies such as speech recognition, speech to text, augmented reality, face detection, face tracking, face recognition and sound localization. The thesis provides a brief overview of the aforementioned technologies in the chapter named Technical Basics. This also allows persons from other fields of expertise to read and understand the thesis. Furthermore, the thesis provides an insight to the current situation of deaf and hard of hearing persons in Austria and explains how the aforementioned technologies can be used in order to aid hearing impaired people in their daily lives. The master thesis furthermore comprised the development of a system prototype able to generate and present subtitles in real time in an augmented reality environment. Additionally, multiple visualization possibilities are provided for the generated subtitles. The implemented systems concept is also explained in this thesis, offering an overview of all involved hardware and software components. Then the implementation process of the developed system is explained in detail, while possible encountered limitations are presented and discussed, as well as identified solutions. The design and implementation process of the visualizations are explained and described as well. However, this master thesis consists only in part of the implementation of a system prototype aiming to ease the participation in meetings of deaf and hard of hearing persons. The second part of the thesis comprises the design and execution of a user study, during which deaf and hard of hearing users could test the implemented system prototype. The user study was organized in order to evaluate the implemented system. At the end of the thesis the extracted results are being analyzed and presented, followed by the conclusion as well as some suggestions for future work.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Moraru, O.-A. (2018). <i>Real-time subtitle for the hearing impaired in augmented reality</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.57180</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "14697",
                    "name": "Moraru Oana-Aurora - 2018 - Real-time subtitle for the hearing impaired in...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 8759417,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7353/2/Moraru%20Oana-Aurora%20-%202018%20-%20Real-time%20subtitle%20for%20the%20hearing%20impaired%20in...pdf"
                },
                {
                    "bsid": "85115",
                    "name": "Moraru Oana-Aurora - 2018 - Real-time subtitle for the hearing impaired in...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 252721,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7353/5/Moraru%20Oana-Aurora%20-%202018%20-%20Real-time%20subtitle%20for%20the%20hearing%20impaired%20in...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Oana-Aurora",
                    "last_name": "Moraru",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "7421",
            "handle": "20.500.12708/7436",
            "doi": "10.34726/hss.2017.29835",
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Visualization of computer-generated 3D cities using GIS data",
            "keywords": [
                "Geodata",
                "Geographical Modeling",
                "Virtual Reality"
            ],
            "abstract": "The constant performance increase of algorithms and hardware over the last decades enabled new ways to collect data, which would have been unthinkable just a few years ago. This is especially true in the sector of geodesy. The release of gps-tracking smartphones enabled users from all over the world to easily collect and upload georeferenced data. Additionally governments also started to make their georeferenced data available to everyone. Through these phenomenons countless databases containing georeferenced information appeared on the Internet. By accessing these databases numerous new applications can be implemented. This thesis focuses on the creation of three-dimensional models that can be easily integrated in a virtual reality environment. The practical part of this thesis consists of four steps. The first step is the data acquisition.As mentioned before nowadays there are various eligible data sources for such a project, however in this work all the data is fetched from a public database of the Austrian government. This database has been chosen because it already contains all required buildings’ footprints and heights. In the second step the acquired data is analysed and pre-processed using Matlab. By using the filters implemented in Matlab  artefacts resulting from the noise contained in the data can be removed. In the third step a suite capable of combining the data-sets is presented. Quantum GIS offers a complete open source suite capable of combining, displaying, processing and exporting georeferenced data. This tool contains solutions for all the problems proposed during this step of the project. The final step is the implementation of the web-application, which creates the three-dimensional models by importing the files generated during the previous step. This web-application has been implemented using WebGL so that most of the calculations are done on the client’s graphics card. The three-dimensional models have been compared with the models offered by the Austrian government for the sake of showing that the presented framework is capable of producing similar models at a lower performance cost in a virtual reality environment. Practically, the presented framework has been implemented and its results have been tested during the course of another project, in which the city models were used in order to create a skydiving experience over the city of Vienna in a virtual reality environment. Over the course of the mentioned project the models were found satisfactory by the users.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Maestri, L. (2017). <i>Visualization of computer-generated 3D cities using GIS data</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2017.29835</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "14863",
                    "name": "Maestri Luca - 2017 - Visualization of computer-generated 3D cities using GIS...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 28909564,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7436/2/Maestri%20Luca%20-%202017%20-%20Visualization%20of%20computer-generated%203D%20cities%20using%20GIS...pdf"
                },
                {
                    "bsid": "85529",
                    "name": "Maestri Luca - 2017 - Visualization of computer-generated 3D cities using GIS...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 118211,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7436/5/Maestri%20Luca%20-%202017%20-%20Visualization%20of%20computer-generated%203D%20cities%20using%20GIS...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Luca",
                    "last_name": "Maestri",
                    "position": 1,
                    "role": "Author",
                    "tid": "181520"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "7523",
            "handle": "20.500.12708/7536",
            "doi": "10.34726/hss.2014.22071",
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Easy to use spreadsheet application",
            "keywords": [
                "Spreadsheet",
                "User-Centered Design"
            ],
            "abstract": "In the years since their first introduction in 1961, spreadsheets have developed into feature-rich multi-purpose tools, with software such as Microsoft Excel being amongst the most popular computer programs of all time. Spreadsheets are widely used in professional envi-ronments for tasks such as, inter alia, reporting and data analysis. Despite their versatility, spreadsheets are mostly used for keeping numeric and textual data in tabular form. Many sophisticated features, such as complex formulas and charts or macros, were found to be used only in a minority of use cases. The goals of this work were to confirm findings from business-related studies for the private domain and further investigate how users work with existing spreadsheet software, which difficulties they encounter and therefore what the requirements for an improved user experience are. First, an online survey was conducted, which built upon findings of related work and served as a basis for subsequent qualitative research in the form of participant observations and semi-structured interviews with a group of real users. As a result, a web-based prototype was developed and continuously tested and evaluated in collaboration with users. The presented prototype offers an unlimited canvas workspace instead of the traditional grid and integrates known features from spreadsheet and word processing software. Unique features such as a newly developed formula bar and indicators for formula cells along with the intelligent reduction of features led to a significant reduction of complexity, while keeping the software powerful enough to handle most day-to-day tasks.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Csizmazia, S. (2014). <i>Easy to use spreadsheet application</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2014.22071</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "15063",
                    "name": "Csizmazia Stefan - 2014 - Easy to use spreadsheet application.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1999816,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7536/2/Csizmazia%20Stefan%20-%202014%20-%20Easy%20to%20use%20spreadsheet%20application.pdf"
                },
                {
                    "bsid": "86161",
                    "name": "Csizmazia Stefan - 2014 - Easy to use spreadsheet application.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 194636,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7536/5/Csizmazia%20Stefan%20-%202014%20-%20Easy%20to%20use%20spreadsheet%20application.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Stefan",
                    "last_name": "Csizmazia",
                    "position": 1,
                    "role": "Author",
                    "tid": "56752"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E376"
            ],
            "pid": "75532",
            "handle": "20.500.12708/71689",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Image-based Grasping Point Detectin using Boosted Histograms of Oriented Gradients",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Lefakis, L., Wildenauer, H., Garcia-Tubio, M. P., &#38; Lech, S. (2010). Image-based Grasping Point Detectin using Boosted Histograms of Oriented Gradients. In <i>Proceedings of the ICIAR 2010</i> (pp. 1–10). http://hdl.handle.net/20.500.12708/71689</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Leonidas",
                    "last_name": "Lefakis",
                    "position": 1,
                    "role": "Author",
                    "tid": "46038"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Wildenauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "51087"
                },
                {
                    "first_name": "Manuel Pascual",
                    "last_name": "Garcia-Tubio",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Szumilas",
                    "last_name": "Lech",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03",
                "E389-02"
            ],
            "pid": "76902",
            "handle": "20.500.12708/73057",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Using Aspect Graphs for View Synthesis",
            "keywords": [],
            "abstract": "A simple method for the generation of aspect\r\ngraphs from multiple views of a polyhedron is\r\npresented in this paper. Re-creation of these multiple\r\nviews given an aspect graph is also discussed. This\r\nmethod is the first step in the use of aspect graphs to\r\ncreate and transmit a 3D scene captured from multiple\r\nviewpoints. The proposed method has potential\r\napplications in multiview autostereoscopic displays\r\nand free-viewpoint videos.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ramachandran, G., &#38; Kropatsch, W. (2012). Using Aspect Graphs for View Synthesis. In <i>Proceedings of Computer Vision Winter Workshop (CVWW)</i> (p. 7). http://hdl.handle.net/20.500.12708/73057</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Geetha",
                    "last_name": "Ramachandran",
                    "position": 1,
                    "role": "Author",
                    "tid": "195689"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "7730",
            "handle": "20.500.12708/7743",
            "doi": "10.34726/hss.2018.38991",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "3D model generation of real estate objects for interactive virtual reality applications",
            "keywords": [
                "Virtual Reality",
                "Automated Interiors",
                "User Study"
            ],
            "abstract": "Architectural firms, construction companies and potential inhabitants have a common interest in the outcome of a construction project fully meeting the needs of the inhabitants. These needs have to be assessed early enough in order to avoid expensive and complex changes in the construction phase of the project. Hence, it is beneficial for all sides to involve the potential residents in the planning process. Often, the building models used in the areas of architecture and construction are unsuitable for presentation purposes. Visually appealing 3D models of buildings are expensive to produce. The work at hand addresses this problem and presents a framework for automated generation of 3D models of real estate objects. The generated models can be explored in the virtual reality application accompanying the framework. Users can interact with the environment within the application by placing furniture and changing textures of walls and floors. According to the results of the conducted user study, the generated apartment models create a strong feeling of presence and let users experience the built environment, as if they were physically present in the apartment. The developed framework provides a low-cost and accessible way for early user involvement in the planning and construction process in a building project. Moreover, it brings a substantial improvement for people searching for a flat or house, since such virtual walk-throughs are in many scenarios the only way to experience the real estate object.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Timofeev, M. (2018). <i>3D model generation of real estate objects for interactive virtual reality applications</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.38991</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "15477",
                    "name": "Timofeev Mikhail - 2018 - 3D model generation of real estate objects for...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 12477215,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7743/2/Timofeev%20Mikhail%20-%202018%20-%203D%20model%20generation%20of%20real%20estate%20objects%20for...pdf"
                },
                {
                    "bsid": "86193",
                    "name": "Timofeev Mikhail - 2018 - 3D model generation of real estate objects for...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 191997,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7743/5/Timofeev%20Mikhail%20-%202018%20-%203D%20model%20generation%20of%20real%20estate%20objects%20for...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Mikhail",
                    "last_name": "Timofeev",
                    "position": 1,
                    "role": "Author",
                    "tid": "64984"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "7745",
            "handle": "20.500.12708/7758",
            "doi": "10.34726/hss.2014.25095",
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "High-quality real-time global illumination in augmented reality",
            "keywords": [
                "Augmented Reality",
                "ray tracing",
                "global illumination",
                "photon mapping",
                "caustics"
            ],
            "abstract": "High-quality image synthesis, indistinguishable from reality, has been one of the most important problems in computer graphics from its beginning. Image synthesis in augmented reality (AR) poses an even more challenging problem, because coherence of virtual and real objects is required. Especially, visual coherence plays an important role in AR. Visual coherence can be achieved by calculating global illumination which introduces the light interaction between virtual and real objects. Correct light interaction provides precise information about spatial location, radiometric properties, and geometric details of inserted virtual objects. In order to calculate light interaction accurately, high-quality global illumination is required. However, high-quality global illumination algorithms have not been suitable for real-time AR due to their high computational cost. Global illumination in AR can be beneficial in many areas including automotive or architectural design, medical therapy, rehabilitation, surgery, education, movie production, and others. This thesis approaches the problem of visual coherence in augmented reality by adopting the physically based rendering algorithms and presenting a novel GPU implementation of these algorithms. The developed rendering algorithms calculate the two solutions of global illumination, required for rendering in AR, in one pass by using a novel one-pass differential rendering algorithm. The rendering algorithms, presented in this thesis, are based on GPU ray tracing which provides high quality results. The developed rendering system computes various visual features in high quality. These features include depth of field, shadows, specular and diffuse global illumination, reflections, and refractions. Moreover, numerous improvements of the physically based rendering algorithms are  presented which allow fast and accurate light transport calculation in AR. Additionally, this thesis presents the differential progressive path tracing algorithm which can calculate the unbiased AR solution in a progressive fashion. Finally, the presented methods are compared to the state of the art in real-time global illumination for AR. The results show that our high-quality global illumination outperforms other methods in terms of accuracy of the rendered images. Additionally, the human perception of developed global illumination methods for AR is evaluated. The impact of the presented rendering algorithms to visual realism and to the sense of presence is studied in this thesis. The results suggest that high-quality global illumination has a positive impact on the realism and presence perceived by users in AR. Thus, future AR applications can benefit from the algorithms developed in this thesis.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kán, P. (2014). <i>High-quality real-time global illumination in augmented reality</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2014.25095</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "15507",
                    "name": "Kan Peter - 2014 - High-quality real-time global illumination in augmented...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 29442892,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7758/2/Kan%20Peter%20-%202014%20-%20High-quality%20real-time%20global%20illumination%20in%20augmented...pdf"
                },
                {
                    "bsid": "86603",
                    "name": "Kan Peter - 2014 - High-quality real-time global illumination in augmented...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 312713,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/7758/5/Kan%20Peter%20-%202014%20-%20High-quality%20real-time%20global%20illumination%20in%20augmented...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Peter",
                    "last_name": "Kán",
                    "position": 1,
                    "role": "Author",
                    "tid": "231177"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "8055",
            "handle": "20.500.12708/8068",
            "doi": "10.34726/hss.2013.21851",
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Autonome Lokalisierung und Navigation eines unbemannten Luftfahrzeugs (UAV) mithilfe eines Smartphones als zentrale Recheneinheit",
            "keywords": [
                "UAV",
                "Quadcopter",
                "Quadrocopter",
                "Smartphone",
                "Localisation",
                "Navigation",
                "Tracking",
                "Mapping"
            ],
            "abstract": "An UAV (Unmanned Aerial Vehicle) is an aircraft flying without an on-board human pilot. For example they can be utilized to explore for human unreachable areas in case of environmental catastrophes like earthquakes and flooding. In most cases, its flight is controlled either by a human or computer pilot via a remote ground-station. Therefore a reliable connection, visual contact and additional hardware for the remote station is required. If these requirements cannot be met, approaches for autonomous localization and navigation are necessary.<br />Therefore, a GPS Signal can be used as ground-truth for positioning.<br />However, for indoor purpose and GPS denied areas, other approaches need to be considered. An alternative approach can be localization based on maps that have been generated on run-time during flight. This requires additional powerful hardware, which has to be carried by the UAV. A powerful device like a conventional Smartphone allows for handling complex applications on a light weighted mobile device. In the following work, the authors demonstrate a flexible, low-weight, low-cost quadrotor platform for autonomous exploration of GPS denied environments.<br />Therefore, a Smartphone is used as central on-board hardware device.<br />Camera, sensors, communication technologies and processing unit are integrated in one common device which allows reduction of weight and costs and migration to newer hardware with low effort. The developed UAV prototype is able to carry a Smartphone and perform steer commands from the on-board device. The implemented Smartphone application enables the UAV to locate itself in an unknown 2D-marker area. A tracking software is used to identify the markers, which are then combined to an internal map during run-time. Based on this map, the UAV is able to localize itself and  generate steer commands for autonomous navigation.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Leichtfried, M., &#38; Kaltenriner, C. (2013). <i>Autonome Lokalisierung und Navigation eines unbemannten Luftfahrzeugs (UAV) mithilfe eines Smartphones als zentrale Recheneinheit</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2013.21851</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "16127",
                    "name": "Leichtfried Michael - 2013 - Autonome Lokalisierung und Navigation eines...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 36440714,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8068/2/Leichtfried%20Michael%20-%202013%20-%20Autonome%20Lokalisierung%20und%20Navigation%20eines...pdf"
                },
                {
                    "bsid": "85794",
                    "name": "Leichtfried Michael - 2013 - Autonome Lokalisierung und Navigation eines...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 280273,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8068/5/Leichtfried%20Michael%20-%202013%20-%20Autonome%20Lokalisierung%20und%20Navigation%20eines...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Leichtfried",
                    "position": 1,
                    "role": "Author",
                    "tid": "183837"
                },
                {
                    "first_name": "Christoph",
                    "last_name": "Kaltenriner",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "8059",
            "handle": "20.500.12708/8072",
            "doi": "10.34726/hss.2013.21668",
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Video-Segmentierung durch Analyse audiovisueller Merkmale",
            "keywords": [],
            "abstract": "Based on the increasing amount of digital videos the segmentation and classification of videos is manually no more controllable. Therefore there is a need for algorithms, which are able to filter out relevant information for suitable and significant descriptions within the video material. This diploma thesis presents a system for classification of videos through analyses of audiovisual features. Such a purpose states a complex problem on arbitrary video materials because those features should be able to gather the semantic meaning of pictures and audio signals out of videos. Therefore, this thesis is limited on the scope of the video classification using scenes of the Muppet Show. Initially basic approaches and methods for a video analysis will be explained in a detailed research. After a short overview of the development of the Muppet Show, a subsequently analysis of video material shows the characteristic attributes. Based on the gained knowledge significant audiovisual features and suitable classification models will be presented, which are consulted for the development of a prototype. Finally the quality of the classification results will be evaluated using different tests. The intention is to show that visual features such as the distribution of colours as well as the segmentation of audio signals in speak, music and environmental sounds are able to capture the semantic meaning of video scenes of the Muppet Show.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fuchs, C. (2013). <i>Video-Segmentierung durch Analyse audiovisueller Merkmale</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2013.21668</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "16135",
                    "name": "Fuchs Christoph - 2013 - Video-Segmentierung durch Analyse audiovisueller...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 11842055,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8072/2/Fuchs%20Christoph%20-%202013%20-%20Video-Segmentierung%20durch%20Analyse%20audiovisueller...pdf"
                },
                {
                    "bsid": "86224",
                    "name": "Fuchs Christoph - 2013 - Video-Segmentierung durch Analyse audiovisueller...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 176095,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8072/5/Fuchs%20Christoph%20-%202013%20-%20Video-Segmentierung%20durch%20Analyse%20audiovisueller...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Christoph",
                    "last_name": "Fuchs",
                    "position": 1,
                    "role": "Author",
                    "tid": "57423"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "8082",
            "handle": "20.500.12708/8095",
            "doi": "10.34726/hss.2013.21864",
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Kontrollflusstransformation von BPMN zu Asbru",
            "keywords": [],
            "abstract": "BPMN is a well known and understandable, graphical process description language for business processes and is supported by a variety of modelling tools, while Asbru is an XML based description language for executable clinical guidelines, which is supported only by a few graphical modelling tools. Through defining a model transformation between BPMN and Asbru, the standardized and through matured tools good supported and well known BPMN notation could be used to model clinical guidelines or treatment plans in Asbru. Since both notations use different paradigms to represent the control flow, some problems have to be considered to get a correct transformation system. The aim of this thesis is to show these problems and find solutions in similar scientific areas. So we (1) discus transformation strategies, which solve these transformation problems on concrete transformation processes between a variety of graph-oriented process description languages to BPEL. Another topic covered here is (2) which and how control-flow patterns are supported by BPMN and Asbru, to discover possible mismatches. Then a (3) mapping is defined to map BPMN elements to semantic equivalent Asbru elements. Afterwards (4) an appropriate transformation strategy out of the strategies introduced above is selected to be implemented in a BPMN-to-Asbru transformation system. Then the (5) implemented transformation system is described based on translation of the control-flow patterns. The achieved results could be used to implement a transformation process between BPMN and Asbru, as it is done in the described prototype.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Tschach, T. (2013). <i>Kontrollflusstransformation von BPMN zu Asbru</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2013.21864</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "16181",
                    "name": "Tschach Thomas - 2013 - Kontrollflusstransformation von BPMN zu Asbru.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3054840,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8095/2/Tschach%20Thomas%20-%202013%20-%20Kontrollflusstransformation%20von%20BPMN%20zu%20Asbru.pdf"
                },
                {
                    "bsid": "86034",
                    "name": "Tschach Thomas - 2013 - Kontrollflusstransformation von BPMN zu Asbru.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 393387,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8095/5/Tschach%20Thomas%20-%202013%20-%20Kontrollflusstransformation%20von%20BPMN%20zu%20Asbru.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Tschach",
                    "position": 1,
                    "role": "Author",
                    "tid": "43749"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "8120",
            "handle": "20.500.12708/8133",
            "doi": "10.34726/hss.2013.21946",
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "A virtual reality training tool for upper limp prostheses",
            "keywords": [],
            "abstract": "Der Gebrauch von myoelektrischen Handprothesen ist inzwischen weit verbreitet. Mit dieser Technologie ist es möglich, Steuersignale für die Prothese mittels Elektroden zu erfassen, welche direkt auf der Haut über Muskeln platziert werden. Allerdings muss der Betroffene den Heilungsprozess der Amputation abwarten, bevor er damit anfangen kann, eine Prothese zu verwenden. Außerdem kann dieser Prozess vor allem am Anfang schwierig und frustrierend sein. In dieser Arbeit wird eine Trainingsanwendung vorgestellt, welche es erlaubt, in einem virtuellen Raum mit einer Hand nach Kugeln zu greifen, wofür eine jeweils der Kugel enstprechende Griffkraft aufgewendet werden muss. Um die virtuelle Realität zu erschaffen, in der sich dieses Szenario abspielt, wird das Tracking-System ioTracker verwendet, welches an der technischen Unversität Wien entwickelt wurde. Mit diesem System werden die Bewegungen von Kopf und Arm des Akteurs in 6 Freiheitsgraden aufgezeichnet (Position und Orientierung) und mittels des OpenTracker Frameworks an eine weitere Anwendung übertragen, welche mit der freien Version der Game Engine Unity3D entwickelt wurde. In dieser werden diese Bewegungsdaten mittels einer dafür entwickelten Software in eine virtuelle 3D Umgebung übertragen und visualisiert. Das Bild der virtuellen Kamera, welche mit dem Kopf des Akteurs mitbewegt wird, wird drahtlos an ein am Kopf des Akteurs befestigtes Display (Head mounted display, HMD) übertragen. Dies ermöglicht es dem Akteur, sich innerhalb eines begrenzten Bereiches von 4x4 Metern frei im Raum umherzubewegen. Da diese Arbeit in Zusammenarbeit mit Otto Bock durchgeführt wurde, konnte für die Steuerung der virtuellen Hand die gleiche Technologie verwendet werden, wie sie in der von Otto Bock entwickelten Michelangelo Handprothese eingebaut ist. Mittels  zweier Elektroden wird die elektrische Aktivität von Muskeln unter der Haut gemessen und in Steuersignale umgerechnet. Diese werden anschließend über eine drahtlose Verbindung an die Simulation gesendet. Da das Ziel dieser Arbeit sowohl in einer Trainingsumgebung als auch in einer Testumgebung für Handprothesen bestand, gibt es mehrere Möglichkeiten, die elektromyographischen (EMG) Steuersignale auf die virtuelle Hand anzuwenden. Weiters können diverse Simulationsarten für die Erzeugung von Griffkraft verwendet werden, welche wiederum dem Akteur während des Greifens durch optische Anzeigen signalisiert wird. Der virtuelle Arm kann angepasst werden, um die Simulation so gut wie möglich an die realen Gegebenheiten anzupassen. Schließlich wurden diverse Einstellungsmöglichkeiten implementiert, um das Erstellen und Durchführen von unterschiedlichen Test- und Trainingsszenarion zu ermöglichen. Im Anschluss an die Arbeit wurden Übungs-Szenarien entwickelt und mit Personen durchgeführt, Um die Fähigkeiten des Systems zu testen, wurden im Anschluss an die Arbeit Übungs-Szenarien entwickelt und mit Versuchspersonen durchgeführt.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Bressler, M. (2013). <i>A virtual reality training tool for upper limp prostheses</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2013.21946</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "16257",
                    "name": "Bressler Michael - 2013 - A virtual reality training tool for upper limp...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2152130,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8133/2/Bressler%20Michael%20-%202013%20-%20A%20virtual%20reality%20training%20tool%20for%20upper%20limp...pdf"
                },
                {
                    "bsid": "86245",
                    "name": "Bressler Michael - 2013 - A virtual reality training tool for upper limp...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 254073,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8133/5/Bressler%20Michael%20-%202013%20-%20A%20virtual%20reality%20training%20tool%20for%20upper%20limp...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Michael",
                    "last_name": "Bressler",
                    "position": 1,
                    "role": "Author",
                    "tid": "39009"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "8126",
            "handle": "20.500.12708/8139",
            "doi": "10.34726/hss.2013.22389",
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "BCI-based cursor control using EEG sensorimotor rhythms",
            "keywords": [],
            "abstract": "Brain-computer interfaces (BCIs) are human-computer interaction systems in which users communicate with a computer via a direct neural interface. Brain-computer interfaces have historically been closely associated with assistive technology for persons with disabilities, but the past few years have seen an increase in publications on human-computer interfaces for people without disabilities. The goal of a cursor control BCI is to control the position of a cursor via brain signals. While there have been a number of publications on cursor control using BCIs, most of them focus on controlling a cursor in custom-built experimental applications. The present thesis describes the design and implementation of a cursor control BCI prototype that translates EEG sensorimotor input into one-dimensional cursor movement and can be accessed using standard input interfaces. Since the BCI is accessible via standard input interfaces, existing input libraries can be used to create experimental applications and the BCI can be used to control existing applications.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Böhm, S. (2013). <i>BCI-based cursor control using EEG sensorimotor rhythms</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2013.22389</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "16269",
                    "name": "Boehm Sebastian - 2013 - BCI-based cursor control using EEG sensorimotor rhythms.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 803362,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8139/2/Boehm%20Sebastian%20-%202013%20-%20BCI-based%20cursor%20control%20using%20EEG%20sensorimotor%20rhythms.pdf"
                },
                {
                    "bsid": "86248",
                    "name": "Boehm Sebastian - 2013 - BCI-based cursor control using EEG sensorimotor rhythms.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 152006,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8139/5/Boehm%20Sebastian%20-%202013%20-%20BCI-based%20cursor%20control%20using%20EEG%20sensorimotor%20rhythms.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Sebastian",
                    "last_name": "Böhm",
                    "position": 1,
                    "role": "Author",
                    "tid": "38637"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "82096",
            "handle": "20.500.12708/78120",
            "doi": "10.34726/hss.2022.104840",
            "year": 2022,
            "issued": "2022",
            "issued_on": "2022-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "A walk inside slope region hierarchy",
            "keywords": [
                "Image topology",
                "Irregular image pyramid",
                "combinatorial maps",
                "structure preservation",
                "Slope complex",
                "Slope regions",
                "folded boundaries",
                "learning of contraction kernels",
                "cellular decomposition",
                "topological characterization"
            ],
            "abstract": "This cumulative thesis presents research from five selected publications in the field of topology preservation in images using irregular graph pyramids. Preserving the structure and the topology of data is a thorough research problem in the field of image analysis and representation, with ample applications.The thesis introduces a monotonically connected topological subspace called Slope region from previous research. The theoretical contribution of this thesis focuses on the characteristics and generalization of the slope regions. It enumerates the salient features of the slope regions for its identification. From the representation and modeling point of view, the core contribution consists of the origination of the inner boundary (or the folded boundary) and the outer boundary. The inner boundary of the slope region helps to incorporate the holes that are geometrically encapsulated by the outer boundary of the slope region but is topologically excluded from the interior of the slope region. This helps to model the slope region as homeomorphic to a disc. The presence and modeling of holes marks as one of the important distinctions between the proposed slope regions and the previously existing topological subspace.The slope regions are generalized into two prototypes depending on the components of the slope region and irrespective of the geometric features like the size and shape of the regions. From the implementing point of view, the thesis proposes an algorithm to build a hierarchy of the slope region over the 4-neighbourhood Region Adjacency Graph (RAG) of an image. Another core contribution of this thesis deals with the dual of the RAG, its significance, and utilization in merging the adjacent slope regions. The slope region hierarchy decomposes the image into slope regions while preserving its topology. The top level of the hierarchy reveals the structure of the image that consists of the critical points and the connections between them. The results from the implementation suggest that an image is a combination of its structure and a few colors. The proposed hierarchical algorithm has the computational complexity of O(log d). Finally, the last goal of this thesis is to exploit the link between the proposed ingeniously designed algorithm and machine learning. This goal is achieved by deriving an objective function that simplifies a few of the many steps in the proposed algorithm and opens the domain of learning an irregular image pyramid.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Batavia, D. (2022). <i>A walk inside slope region hierarchy</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2022.104840</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "190790",
                    "name": "Batavia Darshan - 2022 - A walk inside slope region hierarchy.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1663813,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/78120/1/Batavia%20Darshan%20-%202022%20-%20A%20walk%20inside%20slope%20region%20hierarchy.pdf"
                },
                {
                    "bsid": "190810",
                    "name": "Batavia Darshan - 2022 - A Walk Inside Slope Region Hierarchy.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 125725,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/78120/4/Batavia%20Darshan%20-%202022%20-%20A%20Walk%20Inside%20Slope%20Region%20Hierarchy.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Darshan",
                    "last_name": "Batavia",
                    "position": 1,
                    "role": "Author",
                    "tid": "317665"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "8220",
            "handle": "20.500.12708/8233",
            "doi": "10.34726/hss.2014.22480",
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Intuitive und markerlose Interaktion in einer mobilen Virtual Reality Anwendung auf Basis von RGBD-Daten",
            "keywords": [
                "3D interaction",
                "finger tracking",
                "augmented reality",
                "depth camera"
            ],
            "abstract": "The diploma thesis discusses touchless interaction techniques on handheld devices to intuitively manipulate a virtual 3D scene that is presented to the user on the handheld display. The robustness and performance of different solutions and combinations to detect the user's hand and the fingertips without markers are examined. Therefore, the first methods focus on using the built-in RGB camera, while later the built-in camera is combined with an additional depth sensor. Hand position and finger gestures are used to select and move objects in the virtual scene. Furthermore, the user's head position is tracked to adapt the perspective of the virtual scene in order to create a 3D impression on the device display. The approaches use RGB data for hand segmentation, gesture detection and the hand size or the maximum gray scale value of the hand for relative depth estimation. In addition RGBD data is used for improved hand segmentation and absolute depth estimation. To detect two different finger gestures or the palm of the hand, Haar-like feature-based cascaded classifiers were trained. If the classifier is used to recognize the palm, the finger gestures are determined by the amount of detected fingertips. Therefore, different image processing operations are applied for hand segmentation and its contour is used for fingertip detection. An already trained Haar-like feature cascade classifier is implemented to detect the user-s face and obtain its 3D position with relative depth estimation using the size of the face. Within the diploma thesis an Android application was developed using OpenCV, Unity3D und OpenNI. The hardware prototype rigidly connects the handheld device with the depth sensor (Microsoft Kinect) to enable correct calibration and mapping of the received RGBD data. The performance of the approaches for  gesture recognition were systematically evaluated by comparing their accuracy under varying illumination and background. Based on this study, useful guidelines for developers were derived to choose the appropriate technique for their mobile interaction task. Furthermore, an experimental study was conducted using the detected finger gestures to perform the two canonical 3D interaction tasks selection and positioning and demonstrate the different characteristics of the depth estimation methods. Overall the best result was obtained using RGBD data for finger gesture detection and absolute depth estimation at the expense of latency.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fritz, D. (2014). <i>Intuitive und markerlose Interaktion in einer mobilen Virtual Reality Anwendung auf Basis von RGBD-Daten</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2014.22480</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "16457",
                    "name": "Fritz Daniel - 2014 - Intuitive und markerlose Interaktion in einer mobilen...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 19796499,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8233/2/Fritz%20Daniel%20-%202014%20-%20Intuitive%20und%20markerlose%20Interaktion%20in%20einer%20mobilen...pdf"
                },
                {
                    "bsid": "85637",
                    "name": "Fritz Daniel - 2014 - Intuitive und markerlose Interaktion in einer mobilen...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 281764,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8233/5/Fritz%20Daniel%20-%202014%20-%20Intuitive%20und%20markerlose%20Interaktion%20in%20einer%20mobilen...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Daniel",
                    "last_name": "Fritz",
                    "position": 1,
                    "role": "Author",
                    "tid": "55087"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "82210",
            "handle": "20.500.12708/78191",
            "doi": null,
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Immersives Training in einer mobilen, viertuellen Umgebung : Erstellung und Evaluation von Trainingsszenarien für Rettungskräfte",
            "keywords": [
                "Virtual Reality",
                "Mobile",
                "Training",
                "Simulation",
                "First Responders"
            ],
            "abstract": "Leaders of First Responder Units need a lot of training to accomplish their tasks fast, confident and correct. But realistic training scenarios demand much time and resources and result in high costs. Therefore, real trainings are outnumbered in the education. The usage of Virtual Reality (VR) is introduced as solution for this problem. Its technology offers the possibility of realistic immersiv training simulations. Mobile solutions can additionally increase comfort and preparation time. This thesis should answer the question, how VR training scenarios have to be designed to allow an effective training. A software prototype was built, which was tested by Firefighters and Paramedics. The subjects could navigate inside the virtual environment by a regular gamepad or by an omnidirectional treadmill. The task was to explore a scenario, get an impression of the situation and give orders to solve the situation. The tests were evaluated using quantitative and qualitative methods. There was a high interest in this new form of training possibility and the subjects saw high potential for useful training. An omnidirectional treadmill can simulate stress and exhaustion during a mission. Furthermore, it can minimize the negative effects of using VR systems, often referenced as Cybersickness. The gamepad provided a cheap but mobile alternate control method with short familiarization time. The virtual environment was able to visualize all elements needed for a proper mission assessment. Graphics quality was judged as adequate and the movements and speech of the virtual people were found very helpful.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fröschl, M. (2017). <i>Immersives Training in einer mobilen, viertuellen Umgebung : Erstellung und Evaluation von Trainingsszenarien für Rettungskräfte</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/78191</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Mario",
                    "last_name": "Fröschl",
                    "position": 1,
                    "role": "Author",
                    "tid": "77979"
                },
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "58429"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "82508",
            "handle": "20.500.12708/78489",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Mixed Reality Training für Einsatzkräfte mit einem Video See-Through Head-Mounted Display",
            "keywords": [
                "Mixed Reality",
                "Video See-through",
                "First Responder"
            ],
            "abstract": "Regular, ecient and controllable training of first responders is an important aspect of disaster preparedness. By using new approaches and technologies real world manoeuvres could be shifted into Virtual Reality (VR) oering new possibilities and enhancements. In order to achieve this, we developed a simulation containing various components to replicate as many characteristics as possible of a real life training session. The implementation includes the ability to connect multiple participants simultaneously and let them interact with themselves as well as with real world objects. A flexible and adaptive video see-through function cooperates with a body parts motion tracking device resulting in a Mixed Reality (MR) environment while preserving the immersion of the Virtual Environment (VE). Furthermore, I integrated a control User Interface (UI) to provide capabilities such as monitoring the procedures from variable aspects or triggering incidents within the simulation. This server interface enhances the educational benefits of the system as it is capable of switching and repeating dierent scenarios by oering the exact same conditions and parameters without additional expense or risks. Our sophisticated training environment and its functionalities were tested with participants from the Austrian Military focused on Chemical, Biological, Radiological and Nuclear Defense (CBRN) preparedness and we conclude that this VR training simulation can extend real-life disaster preparedness with additional features, and decrease negative side eects such as hazardous conditions, expensive setups and hard to control parameters.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Bösch, C. (2018). <i>Mixed Reality Training für Einsatzkräfte mit einem Video See-Through Head-Mounted Display</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/78489</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Chris",
                    "last_name": "Bösch",
                    "position": 1,
                    "role": "Author",
                    "tid": "203774"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "82839",
            "handle": "20.500.12708/78820",
            "doi": null,
            "year": 2020,
            "issued": "2020",
            "issued_on": "2020-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Visual and oculomotoric assessment with an eye-tracking head-mounted display",
            "keywords": [
                "Head-Mounted Display",
                "Eye Tracking",
                "Medical Examination",
                "Examination Method",
                "Medical Assessment"
            ],
            "abstract": "Conventional examination methods of the visual and oculomotor system, as conducted by ophthalmologists, have vulnerabilities. They are subjective, irreproducible and unsuitable for physically impaired subjects. This work aims to develop a tool with objective examination methods that deliver reproducible examination data on the subject's performance. Five examination methods are designed to assess the oculomotor eye movements and the visual capacity of a subject. The tool is developed in Unity and runs on a computer connected to the HMD FOVE, which has an integrated ET system. An autonomous working process examines the subjects, stores the related data and generates detailed graphical representations for medical assessments. For the evaluation, the technical aspects of the tool were analyzed and an interview with the experts from the Department of Ophthalmology and Optometry at the Medical University Vienna was held. The results showed a minimized human influence in the examination process, a standardized examination setup and great accessibility for bedridden subjects. However, the small field of view of the HMD limits the examination method on the visual field and restricts further development of the tool. New hardware should be evaluated in the future in order to overcome the current limitations. Moreover, examination methods regarding the pupil size would be a suitable addition to future research. In the near future, the tool could be tested as a screening method in medical facilities. This might reduce waiting times for the subjects and allow medical experts to work more efficiently.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pezzei, S. (2020). <i>Visual and oculomotoric assessment with an eye-tracking head-mounted display</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. http://hdl.handle.net/20.500.12708/78820</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Stefan",
                    "last_name": "Pezzei",
                    "position": 1,
                    "role": "Author",
                    "tid": "255221"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "40682"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "8313",
            "handle": "20.500.12708/8326",
            "doi": "10.34726/hss.2017.37472",
            "year": 2017,
            "issued": "2017",
            "issued_on": "2017-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Path planning in augmented reality indoor environments",
            "keywords": [
                "Augmented Reality",
                "Path Planning",
                "Navigation Mesh"
            ],
            "abstract": "Superimposing a virtual path onto a real indoor environment assists a person walking through a building towards a requested destination. With the release of AR toolkits for Android and Apple mobile devices and the availability of see-through head mounted displays, AR gains attention on the mass market. The main drawback of state-of-theart AR visualization devices is the small FOV. Keeping the calculated path inside the FOV of the AR visualization device is the novel approach introduced in this thesis. The path should adopt to the view direction of the mobile device; hence, a path is visualized to the user at all times. This thesis presents the design and implementation of the FOV assisted path planning algorithm. A 3D model of the indoor environment and tracking data of the user are applied to calculate the FOV assisted path. The FOV assisted path consists of a path inside and outside the FOV. The main focus of the work at hand is to calculate the path inside the FOV area and concatenate it to the path outside the FOV. Depending on the view direction, different path calculation stages and special cases are implemented and the resulting sub paths are concatenated. A performance analysis and a user study was conducted to assess the performance and usability of the implemented algorithm. Results indicate that the implemented path increases orientation of the user within the indoor environment when the direction towards the destination is unknown. Moreover, the usability of the FOV assisted path planning algorithm and the suitability of AR visualization devices for indoor navigation are assessed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Platzer, K. (2017). <i>Path planning in augmented reality indoor environments</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2017.37472</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "16643",
                    "name": "Platzer Karl - 2017 - Path planning in augmented reality indoor environments.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 18060807,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8326/2/Platzer%20Karl%20-%202017%20-%20Path%20planning%20in%20augmented%20reality%20indoor%20environments.pdf"
                },
                {
                    "bsid": "85661",
                    "name": "Platzer Karl - 2017 - Path planning in augmented reality indoor environments.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 182526,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8326/5/Platzer%20Karl%20-%202017%20-%20Path%20planning%20in%20augmented%20reality%20indoor%20environments.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Karl",
                    "last_name": "Platzer",
                    "position": 1,
                    "role": "Author",
                    "tid": "57580"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "8471",
            "handle": "20.500.12708/8484",
            "doi": "10.34726/hss.2018.55533",
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Ein One-Stop-Shop für Smartcard-basierte Dienstleistungen",
            "keywords": [
                "Smartcard",
                "One-Stop-Shop",
                "Softwareentwicklung",
                "Webentwicklung"
            ],
            "abstract": "The aim of this diploma thesis was to investigate the eects of the introduction of aone-stop-shop for the admission of students and employees at the TU Wien. Specialattention was paid to the smartcard issued by the TU Vienna, the so-called TUcard.A new module for the issuing and administration of the TUcard was implemented inthe information system of the TU Wien and subsequently analysed via interviews and atechnical evaluation.At the beginning, the previous admission process is discussed, its workows are described,and the problems encountered are listed. This is followed by an introduction to thetechnologies used, smartcards and one-stop shops. The knowledge acquired and therequirements of the stakeholders are used to describe the design of the new process andits implementation. Finally, the results are evaluated.The need for the adjustment is claried by the problems encountered, which are describedin the context of the initial situation. In order to allow a scientic evaluation afterwards, aresearch question is dened at the beginning, which is to be answered after the evaluationof the data. Based on the initial situation and its problems, requirements are set whichshould allow to evaluate the research question. The requirements dene the design, whichis then generated and implemented. The implementation is analyzed with the help ofexpert interviews and a technical evaluation.The evaluation showed that most stakeholders, especially students and employees, benetfrom the changeover and that the rst admission is much easier for them. However, therewere also complications in the planning and redistribution of tasks.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Engl, J. (2018). <i>Ein One-Stop-Shop für Smartcard-basierte Dienstleistungen</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2018.55533</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "16959",
                    "name": "Engl Jonas - 2018 - Ein One-Stop-Shop fuer Smartcard-basierte Dienstleistungen.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1158194,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8484/2/Engl%20Jonas%20-%202018%20-%20Ein%20One-Stop-Shop%20fuer%20Smartcard-basierte%20Dienstleistungen.pdf"
                },
                {
                    "bsid": "85849",
                    "name": "Engl Jonas - 2018 - Ein One-Stop-Shop fuer Smartcard-basierte Dienstleistungen.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 176422,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8484/5/Engl%20Jonas%20-%202018%20-%20Ein%20One-Stop-Shop%20fuer%20Smartcard-basierte%20Dienstleistungen.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Jonas",
                    "last_name": "Engl",
                    "position": 1,
                    "role": "Author",
                    "tid": "254446"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "8475",
            "handle": "20.500.12708/8488",
            "doi": "10.34726/hss.2019.55534",
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Video object recognition based on deep learning",
            "keywords": [
                "Deep Learning",
                "Convolutional Neural Networks",
                "Video Object Recognition"
            ],
            "abstract": "In this master thesis we designed a client server system for automatic billboard recognition in video streams. The client side is represented by an Android application which serves the purpose of collecting various video data streams for the server side. For the server side a deep neural network, called StefanNet, was designed. StefanNet is a fully convolutional neural network which is able to properly classify and localize billboard objects within a video frame. StefanNet has a feature extractor which contains 23 convolutional layers and uses a single shot detector (SSD) as an object detector. StefanNet has been trained on the self-designed BillboardDataset which contains 4042 image samples taken from the billboards located throughout the metro stations in Vienna. Additionally, data augmentation techniques have been implemented to artificially augment the dataset with a 25% increase rate. Furthermore, the compression-based quantization technique has been applied to the StefanNet model to reduce the bit-width necessary for storing the weights of the network from float32 to float16. We evaluated the performance of StefanNet by comparing against the state-of-the-art networks ResNet, MobileNet, Inception and VGG16. The validation dataset contains both side and frontal views of the billboards. StefanNet achieved 91% mean average precision (mAp) on the test dataset, 98% mAp on the frontal view validation dataset and 82% mAp on the side view validation dataset. The inference rate was 40 FPS on a Nvidia 1080 graphics card. The quantized version of the StefanNet model achieved 91% mAp on the test dataset, 96% mAp on the frontal view validation dataset and 85% mAp on the side view validation at an inference rate of 45 FPS. In comparison to the other evaluated networks both the StefanNet model and the quantized version of the model produce superior results and outperform the benchmark network models on all datasets. This confirms that the architecture of StefanNet is currently the most suitable for the specific problem of automatic billboard detection in video streams.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stojanoski, S. (2019). <i>Video object recognition based on deep learning</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2019.55534</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "16967",
                    "name": "Stojanoski Stefan - 2019 - Video object recognition based on deep learning.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 46011115,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8488/2/Stojanoski%20Stefan%20-%202019%20-%20Video%20object%20recognition%20based%20on%20deep%20learning.pdf"
                },
                {
                    "bsid": "86084",
                    "name": "Stojanoski Stefan - 2019 - Video object recognition based on deep learning.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 211408,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8488/5/Stojanoski%20Stefan%20-%202019%20-%20Video%20object%20recognition%20based%20on%20deep%20learning.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Stefan",
                    "last_name": "Stojanoski",
                    "position": 1,
                    "role": "Author",
                    "tid": "297635"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "85662",
            "handle": "20.500.12708/81214",
            "doi": "10.34726/hss.2022.87561",
            "year": 2022,
            "issued": "2022",
            "issued_on": "2022-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Design and evaluation of a sound application using acoustic scene classification",
            "keywords": [
                "Audio Scene Classification",
                "Signal Processing",
                "Machine Learning",
                "App Programming"
            ],
            "abstract": "Acoustic Scene Classification (ASC) aims to identify the environmental surroundings from audio recordings in which they were collected. Developing computational methods to recognize the environments is part of the research in fields such as machine learning,robotics and artificial intelligence. This work explores the usage of ASC in the privatesector by developing a prototype and performing a case study in order to evaluate if a certain psychological effect can be achieved with this technology. In particular, we examine if an acoustic illusion of being in a different city can be created when the user’s soundscape is overlaid with a different soundscape based on the classification result.In this thesis, we explore the current state-of-the-art solutions for ASC by investigating the submissions of the DCASE 2020 challenge. Based on the evaluation we choose a baseline model, which is a ResNet that accepts log-mel spectrograms complemented by log-mel deltas and delta-deltas as input. We modify the network in order to classify 9 acoustic scenes. The TAU Urban Acoustic Scenes 2020 Mobile dataset is used to train the model. To calibrate the network, we experiment with different hyperparameters, loss functions and data augmentation strategies and compare the results based on the test accuracy. We achieve the best performance with a test accuracy of 77.99% by using categorical focal loss and Mixup as data augmentation technique.For the prototype we select 4 cities to support and create a soundscape dataset including audio samples for each scene and city by extracting the audio data from videos that contain the desired content. Then, we develop a server application with Flask to provide an API to get predictions from the model and to log data about the performed classification processes. Eventually, we design and implement a mobile application using the FlutterSDK. The application requests at intervals predictions from the server for audio data,that it recorded using the device’s microphone. Based on the classification result, the mobile application overlays the acoustic environment of the user by playing an audiosample for the recognized scene and the selected city from the soundscape dataset.Finally, we use the prototype in a case study where a group of participants tests the mobile application within a predefined scope. In the evaluation, we estimate the classifier’saccuracy in a real life scenario with 65%. We discover that the acoustic illusion of being in a different city is created by the prototype. Additionally, we show that this illusion is experienced more often the more accurate the classification is perceived and the closer the personal relationship between the user and the selected city is.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Rybnikova, T. V. (2022). <i>Design and evaluation of a sound application using acoustic scene classification</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2022.87561</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "194647",
                    "name": "Rybnikova Tatiana Vladimirovna - 2022 - Design and evaluation of a sound...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2287597,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/81214/1/Rybnikova%20Tatiana%20Vladimirovna%20-%202022%20-%20Design%20and%20evaluation%20of%20a%20sound...pdf"
                },
                {
                    "bsid": "194713",
                    "name": "Rybnikova Tatiana Vladimirovna - 2022 - Design and Evaluation of a Sound...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 278668,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/81214/4/Rybnikova%20Tatiana%20Vladimirovna%20-%202022%20-%20Design%20and%20Evaluation%20of%20a%20Sound...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Tatiana Vladimirovna",
                    "last_name": "Rybnikova",
                    "position": 1,
                    "role": "Author",
                    "tid": "270942"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194"
            ],
            "pid": "8614",
            "handle": "20.500.12708/8627",
            "doi": "10.34726/hss.2019.68485",
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Self-learning optical music recognition",
            "keywords": [
                "Optical Music Recognition",
                "Computer Vision",
                "Deep Learning",
                "Machine Learning",
                "Document Analysis",
                "Music Scores",
                "Music"
            ],
            "abstract": "Music is an essential part of our culture and heritage. Throughout the centuries, millions of songs were composed and written down in documents using music notation. Optical Music Recognition (OMR) is the research field that investigates how the computer can learn to read those documents. Despite decades of research, OMR is still considered far from being solved. One reason is that traditional approaches rely heavily on heuristics and often do not generalize well. In this thesis, I propose a different approach to let the computer learn to read music notation documents mostly by itself using machine learning, especially deep learning. In several experiments, I have demonstrated that the computer can learn to robustly solve many tasks involved in OMR by using supervised learning. These include the structural analysis of the document, the detection and classification of symbols in the scores as well as the construction of the music notation graph, which is an intermediate representation that can be exported into a format suitable for further processing. A trained deep convolutional neural network can reliably detect whether an image contains music or not, while another one is capable of finding and linking individual measures across multiple sources for easy navigation between them. Detecting symbols in typeset and handwritten scores can be learned, given a sufficient amount of annotated data, and classifying isolated symbols can be performed at even lower error rates than those of humans. For scores written in mensural notation the complete recognition can even be simplified into just three steps, two of which can be solved with machine learning. Apart from publishing a number of scientific articles, I have gathered and documented the most extensive collection of datasets for OMR as well as the probably most comprehensive bibliography currently available. Both are available online. Moreover I was involved in the organization of the International Workshop on Reading Music Systems, in a joint tutorial at the International Society For Music Information Retrieval Conference on OMR as well as in another workshop at the Music Encoding Conference. Many challenges of OMR can be solved efficiently with deep learning, such as the layout analysis or music object detection. As music notation is a configurational writing system where the relations and interplay between symbols determine the musical semantic, these relationships have to be recognized as well. A music notation graph is a suitable representation for storing this information. It allows to clearly distinguish between the challenges involved in recovering information from the music score image and the encoding of the recovered information into a specific output format while complying with the rules of music notation. While the construction of such a graph can be learned as well, there are still many open issues that need future research. But I am confident that training the computer on a sufficiently large dataset under human supervision is a sustainable approach that will help to solve many applications of OMR in the future.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Pacha, A. (2019). <i>Self-learning optical music recognition</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2019.68485</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "17245",
                    "name": "Pacha Alexander - 2019 - Self-learning optical music recognition.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 11852471,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8627/2/Pacha%20Alexander%20-%202019%20-%20Self-learning%20optical%20music%20recognition.pdf"
                },
                {
                    "bsid": "85873",
                    "name": "Pacha Alexander - 2019 - Self-learning optical music recognition.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 557890,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8627/5/Pacha%20Alexander%20-%202019%20-%20Self-learning%20optical%20music%20recognition.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Alexander",
                    "last_name": "Pacha",
                    "position": 1,
                    "role": "Author",
                    "tid": "68727"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "8895",
            "handle": "20.500.12708/8908",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Combining multiple depth cameras for reconstruction",
            "keywords": [
                "Calibration",
                "Depth Camera",
                "Kinect",
                "PMD",
                "Triangulation",
                "TOF"
            ],
            "abstract": "In the past few years depth cameras and their applications have gained more and more attention. Especially the publication of the Microsoft Kinect which is a cheap alternative to the expensive industrial cameras has initiated the research and development in this area. Adding depth information to images makes it possible to develop a lot of different application areas. Gesture and motion recognition, 3D Reconstruction of people and objects and the analysis of work movements are just a few possibilities for using the depth camera technology. This work treats the combination of depth cameras in order to support future work like 3D reconstruction. The configuration of the cameras, external factors and camera calibration have to be considered. Different techniques for 3D image generation are described and analyzed and similar publications with various approaches are explained and evaluated. Subsequently the practical part of this work is explained covering the combination of two different depth cameras and the challenges which arise out of it.<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Wendelin, K.-A. (2012). <i>Combining multiple depth cameras for reconstruction</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-47399</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "17809",
                    "name": "Wendelin Katharina-Anna - 2012 - Combining multiple depth cameras for...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 3253622,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8908/2/Wendelin%20Katharina-Anna%20-%202012%20-%20Combining%20multiple%20depth%20cameras%20for...pdf"
                },
                {
                    "bsid": "86484",
                    "name": "Wendelin Katharina-Anna - 2012 - Combining multiple depth cameras for...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 136162,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/8908/5/Wendelin%20Katharina-Anna%20-%202012%20-%20Combining%20multiple%20depth%20cameras%20for...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Katharina-Anna",
                    "last_name": "Wendelin",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "89051",
            "handle": "20.500.12708/84281",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Collaborative Augmented Reality in Education",
            "keywords": [],
            "abstract": "Technological advances enable the use of innovative learning tools for education. This work gives a brief insight into the potential and challenges of using collaborative Augmented Reality (AR) in education within the greater context of immersive virtual learning environments. As an example the experiences made during the development of a collaborative AR application specifically designed for mathematics and geometry education called Construct3D are summarized. Construct3D is based on the mobile collaborative AR system “Studierstube”. We describe our efforts in developing a system for the improvement of spatial abilities and maximization of transfer of learning. Means of application and integration in mathematics and geometry education at high school as well as university level are being discussed. Anecdotal evidence supports our claim that Construct3D is easy to learn, encourages experimentation with geometric constructions and improves spatial skills.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2003). <i>Collaborative Augmented Reality in Education</i>. Imagina 2003 Conference, Monte Carlo, Monaco, Austria. http://hdl.handle.net/20.500.12708/84281</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "89057",
            "handle": "20.500.12708/84287",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Visual Similarity Measurement with the Feature Contrast Model",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2003). <i>Visual Similarity Measurement with the Feature Contrast Model</i>. SPIE Electronic Imaging Conference, San Jose, USA, Non-EU. http://hdl.handle.net/20.500.12708/84287</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "89058",
            "handle": "20.500.12708/84288",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "How good are the visual MPEG-7 features?",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2003). <i>How good are the visual MPEG-7 features?</i> SPIE Visual Communications and Image Processing Conference, Paris, FR, EU. http://hdl.handle.net/20.500.12708/84288</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "89059",
            "handle": "20.500.12708/84289",
            "doi": null,
            "year": 2003,
            "issued": "2003",
            "issued_on": "2003-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Media handling for visual information retrieval in VizIR",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2003). <i>Media handling for visual information retrieval in VizIR</i>. SPIE Visual Communications and Image Processing Conference, Paris, FR, EU. http://hdl.handle.net/20.500.12708/84289</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "89060",
            "handle": "20.500.12708/84290",
            "doi": null,
            "year": 2004,
            "issued": "2004",
            "issued_on": "2004-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "SMIL and SVG in teaching",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2004). <i>SMIL and SVG in teaching</i>. SPIE Electronic Imaging Conference, San Jose, USA, Non-EU. http://hdl.handle.net/20.500.12708/84290</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "89061",
            "handle": "20.500.12708/84291",
            "doi": null,
            "year": 2004,
            "issued": "2004",
            "issued_on": "2004-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "A new perspective on visual information retrieval",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2004). <i>A new perspective on visual information retrieval</i>. SPIE Electronic Imaging Conference, San Jose, USA, Non-EU. http://hdl.handle.net/20.500.12708/84291</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E194-01"
            ],
            "pid": "89062",
            "handle": "20.500.12708/84292",
            "doi": null,
            "year": 2004,
            "issued": "2004",
            "issued_on": "2004-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "A new method for visual descriptor evaluation",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eidenberger, H. (2004). <i>A new method for visual descriptor evaluation</i>. SPIE Electronic Imaging Conference, San Jose, USA, Non-EU. http://hdl.handle.net/20.500.12708/84292</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-06",
                "E194-01"
            ],
            "pid": "89209",
            "handle": "20.500.12708/84439",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "CoCoMA: Content and Context Aware Multimedia Content Retrieval, Delivery and",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Breiteneder, C., &#38; Eidenberger, H. (2005). <i>CoCoMA: Content and Context Aware Multimedia Content Retrieval, Delivery and</i>. ECDL 2005, Vienna, Austria, Austria. http://hdl.handle.net/20.500.12708/84439</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 2,
                    "role": "Author",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "89261",
            "handle": "20.500.12708/84491",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Improving Spatial Abilities by Geometry Education in Augmented Reality - Application and Evaluation Design",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., Steinbügl, K., Dünser, A., &#38; Glück, J. (2005). <i>Improving Spatial Abilities by Geometry Education in Augmented Reality - Application and Evaluation Design</i>. Laval Virtual - 7th International Conference on Virtual Reality, Laval, France, EU. http://hdl.handle.net/20.500.12708/84491</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Karin",
                    "last_name": "Steinbügl",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Andreas",
                    "last_name": "Dünser",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Judith",
                    "last_name": "Glück",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "89262",
            "handle": "20.500.12708/84492",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Förderung der Raumvorstellung mit Construct3D",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dünser, A., &#38; Kaufmann, H. (2005). <i>Förderung der Raumvorstellung mit Construct3D</i>. Annual Austrian Geometry Teachers Conference, Strobl, Austria, Austria. http://hdl.handle.net/20.500.12708/84492</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Andreas",
                    "last_name": "Dünser",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "89263",
            "handle": "20.500.12708/84493",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "General Training of Spatial Abilities by Geometry Education in Augmented Realiy",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2005). <i>General Training of Spatial Abilities by Geometry Education in Augmented Realiy</i>. Cybertherapy 2005, Basel, Switzerland, Non-EU. http://hdl.handle.net/20.500.12708/84493</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "89264",
            "handle": "20.500.12708/84494",
            "doi": null,
            "year": 2005,
            "issued": "2005",
            "issued_on": "2005-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Raumvorstellungstraining mit Augmented Reality - wer profitiert vom Einsatz neuer Technologien?",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Dünser, A., Steinbügl, K., Kaufmann, H., &#38; Glück, J. (2005). <i>Raumvorstellungstraining mit Augmented Reality - wer profitiert vom Einsatz neuer Technologien?</i> 8th Working-Conference of the Differential Psychology, Personality Psychology, and Psychological Diagnostics Group, Marburg, Germany, EU. http://hdl.handle.net/20.500.12708/84494</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Andreas",
                    "last_name": "Dünser",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Karin",
                    "last_name": "Steinbügl",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 3,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Judith",
                    "last_name": "Glück",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "89270",
            "handle": "20.500.12708/84500",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Learning Objects for Education with Augmented Reality",
            "keywords": [],
            "abstract": "To fill the gap of next-generation user interfaces for mathematics and geometry education\r\nConstruct3D is presented, a three-dimensional dynamic geometry construction tool that can be used in\r\nhigh school and university education. This system uses Augmented Reality (AR) to provide a natural\r\nsetting for face-to-face collaboration of teachers and students. The main advantage of using AR is that\r\nstudents actually see three dimensional objects which they until now had to calculate and construct\r\nwith traditional (mostly pen and paper) methods (Figure 1). By working directly in 3D space, complex\r\nspatial problems and spatial relationships may be comprehended better and faster than with traditional\r\nmethods. This paper summarizes the development of learning objects and content specifically\r\ndesigned for teaching dynamic 3D geometry in Augmented Reality. It concentrates on pedagogical\r\nfindings while working with teachers and students in more than 500 teaching lessons.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., &#38; Papp, M. (2006). <i>Learning Objects for Education with Augmented Reality</i>. EDEN 2006 Annual Conference (European Distance and E-Learning Network), Wien, Austria. http://hdl.handle.net/20.500.12708/84500</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Marion",
                    "last_name": "Papp",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "89271",
            "handle": "20.500.12708/84501",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Long Distance Distribution of Educational Augmented Reality Applications",
            "keywords": [],
            "abstract": "For distance education utilizing shared Virtual or Augmented Reality (VR/AR) applications, reliable network distribution\r\nof educational content is of prime importance. In this paper we summarize the development of software\r\ncomponents enabling stable and reliable distribution of an existing educational AR application for geometry education.\r\nOur efforts focus on three main areas: (1) For long distance distribution of Open Inventor scene graphs,\r\nthroughout a wide area IP network, a TCP based network protocol was implemented in Distributed Open Inventor.\r\n(2) A tracking middleware was extended to support sending tracking data unicast instead or in addition to sending\r\nmulticast messages. (3) Multiple adaptations in our geometry application were required to improve scalability, robustness\r\nand reliability. We present an early evaluation with high school students in a distant learning, distributed\r\nHMD setup and highlight final results.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H., Csisinko, M., &#38; Totter, A. (2006). <i>Long Distance Distribution of Educational Augmented Reality Applications</i>. Eurographics, Dublin, Irland, Austria. http://hdl.handle.net/20.500.12708/84501</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 2,
                    "role": "Author",
                    "tid": "47153"
                },
                {
                    "first_name": "Alexandra",
                    "last_name": "Totter",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "89284",
            "handle": "20.500.12708/84514",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "An Event-Driven, Stochastic, Undirected Narrative (EDSUN) Framework for Interactive Contents",
            "keywords": [],
            "abstract": "In this paper, we present an extensible framework for interactive\r\nmultimodal contents, with emphasis on augmented reality applications. The\r\nproposed framework, EDSUN, enables concurrent and variable narrative\r\nstructures as well as content reusability and dynamic yet natural experience\r\ngeneration. EDSUN's main components include a canonical specification of 5-\r\nstate lexical syntax and grammar, stochastic state transitions, and extensions for\r\nhierarchical grammars to represent complex behavioral and multimodal\r\ninteractions. The benefits of EDSUN in enabling classical contents to support\r\nthe affordances of AR environments and in complementing recent published\r\nworks are also discussed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Barclay, A., &#38; Kaufmann, H. (2006). <i>An Event-Driven, Stochastic, Undirected Narrative (EDSUN) Framework for Interactive Contents</i>. Technologies for Interactive Digital Storytelling and Entertainment, Third International Conference, TIDSE 2006, Darmstadt, Deutschland, EU. http://hdl.handle.net/20.500.12708/84514</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adam",
                    "last_name": "Barclay",
                    "position": 1,
                    "role": "Author",
                    "tid": "54614"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "89383",
            "handle": "20.500.12708/84612",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Entwicklung eines 3D-Raumvorstellungs-Lerntests mit Augmented Reality",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Strauß, S., Strasser, I., Csisinko, M., Kaufmann, H., &#38; Glück, J. (2007). <i>Entwicklung eines 3D-Raumvorstellungs-Lerntests mit Augmented Reality</i>. 9. Arbeitstagung der Fachgruppe für Differentielle Psychologie, Persönlichkeitspsychologie und Psychologische Diagnostik, Wien, Austria. http://hdl.handle.net/20.500.12708/84612</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Sabine",
                    "last_name": "Strauß",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Irene",
                    "last_name": "Strasser",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 3,
                    "role": "Author",
                    "tid": "47153"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Judith",
                    "last_name": "Glück",
                    "position": 5,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "89384",
            "handle": "20.500.12708/84613",
            "doi": null,
            "year": 2007,
            "issued": "2007",
            "issued_on": "2007-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Learning Objects for Geometry Education with Augmented Reality",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2007). <i>Learning Objects for Geometry Education with Augmented Reality</i>. Conference on E-Learning in a new Europe (BMUKK) - Discovery Days, Eisenstadt, Österreich, Austria. http://hdl.handle.net/20.500.12708/84613</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89442",
            "handle": "20.500.12708/84670",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "The MUSCLE / ImageCLEF Image Retrieval Evaluation Campaigns",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A. (2006). <i>The MUSCLE / ImageCLEF Image Retrieval Evaluation Campaigns</i>. PASCAL Visual Object Classes Challenge Workshop, Graz, Austria. http://hdl.handle.net/20.500.12708/84670</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89443",
            "handle": "20.500.12708/84671",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Analysis of Keywords used in Image Understanding Tasks",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A. (2006). <i>Analysis of Keywords used in Image Understanding Tasks</i>. OntoImage International Workshop, Genova, Italy, EU. http://hdl.handle.net/20.500.12708/84671</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89444",
            "handle": "20.500.12708/84672",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Results of the MUSCLE CIS Coin Competition 2006",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A. (2006). <i>Results of the MUSCLE CIS Coin Competition 2006</i>. MUSCLE CIS Coin Competition Workshop, Berlin, EU. http://hdl.handle.net/20.500.12708/84672</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89445",
            "handle": "20.500.12708/84673",
            "doi": null,
            "year": 2006,
            "issued": "2006",
            "issued_on": "2006-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "A Dataset of Annotated Animals",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hanbury, A. (2006). <i>A Dataset of Annotated Animals</i>. Second MUSCLE/ImageCLEF Workshop on Image and Video Retrieval Evaluation, Alicante, Spain, EU. http://hdl.handle.net/20.500.12708/84673</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 1,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-03"
            ],
            "pid": "89545",
            "handle": "20.500.12708/84773",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Efficient Reconstruction from Non-uniform Point Sets",
            "keywords": [],
            "abstract": "We propose a method for non-uniform reconstruction of 3D scalar data. Typically, radial basis functions, trigonometric polynomials or shift-invariant functions are used in the functional approximation of 3D data. We adopt a variational approach for the reconstruction and rendering of 3D data. The principle idea is based on data fitting via thin-plate splines. An approximation by B-splines offers more compact support for fast reconstruction. We adopt this method for large datasets by introducing a block-based reconstruction approach. This makes the method practical for large data sets. Our reconstruction will be smooth across blocks. We give reconstruction measurements as error estimations based on different parameter settings and also an insight on the computational effort. We show that the block size used in reconstruction has a negligible effect on the reconstruction error. Finally we show rendering results to emphasize the quality of this 3D reconstruction technique.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vucini, E., Möller, T., &#38; Gröller, E. (2008). <i>Efficient Reconstruction from Non-uniform Point Sets</i>. Computer Graphics International, Yokohama, Japan, Non-EU. http://hdl.handle.net/20.500.12708/84773</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Erald",
                    "last_name": "Vucini",
                    "position": 1,
                    "role": "Author",
                    "tid": "45707"
                },
                {
                    "first_name": "Torsten",
                    "last_name": "Möller",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "position": 3,
                    "role": "Author",
                    "tid": "143572"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "4043"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89611",
            "handle": "20.500.12708/84839",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Open Issues and Chances for Topological Pyramids",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. (2008). <i>Open Issues and Chances for Topological Pyramids</i>. Catimag’08 Computational Algebraic Topology Within Image Context, Sevilla, EU. http://hdl.handle.net/20.500.12708/84839</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89612",
            "handle": "20.500.12708/84840",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "How to find Good, Optimal and Robust TSP solutions",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. (2008). <i>How to find Good, Optimal and Robust TSP solutions</i>. Workshop on New Perspektives on Human Problem Solving, Purdue University, West Lafayette, Indiana, USA, Non-EU. http://hdl.handle.net/20.500.12708/84840</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89613",
            "handle": "20.500.12708/84841",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Graph-based Representations for Segmentation, Tracking and Shape Matching",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. (2008). <i>Graph-based Representations for Segmentation, Tracking and Shape Matching</i>. University of Central Florida, USA, University of Central Florida, USA, Non-EU. http://hdl.handle.net/20.500.12708/84841</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "89626",
            "handle": "20.500.12708/84854",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Skeletal Structure Generation for Optical Motion Capture",
            "keywords": [],
            "abstract": "Motion capture systems today have to deliver high quality motion data, while being flexible and easily adaptable to different actors. Therefore, accurately determining parameters of a subject's skeletal structure is crucial. Inferring these values automatically from optical motion capture data without additional measurements, however, is a challenging task. This thesis describes the steps necessary to calculate the joint positions and limb lengths using data from a passive optical tracking system.\r\n The algorithm is a multi-stage process that includes the tasks of automatic marker labeling, limb-wise clustering of markers and calculation of joint positions. Finally an estimate of the topology and the parameters of the articulated structure are computed. Since the topology is inferred from the data, no model has to exist in advance. This in turn makes the implemented system flexible enough to capture not only human motions, but motions of an arbitrary articulated structure, without any adaptations or additional effort. The core functionality of the system, which is the skeleton fitting task, is done using a distance function, that is applied to marker positions. This function then is minimized by a non-linear minimization algorithm.\r\n Tests of the system have been performed with human motion capture data, artificially generated data sets and a construction of rods linked with articulations. The results show high accuracy for the artificial data. For the tracked data sets also satisfactory outcome is produced.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C. (2008). <i>Skeletal Structure Generation for Optical Motion Capture</i>. Winter Augmented Reality Meeting 2008, Graz, Austria, Austria. http://hdl.handle.net/20.500.12708/84854</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "89627",
            "handle": "20.500.12708/84855",
            "doi": null,
            "year": 2008,
            "issued": "2008",
            "issued_on": "2008-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Cognitive strategies on solving spatial tasks: development of a dynamic spatial test in augmented reality",
            "keywords": [],
            "abstract": "Within an interdisciplinary research project on cognitive strategies and training\r\naspects of spatial ability, funded by the Austrian research fund FWF, we are\r\ndeveloping a new means of measuring spatial abilities in an ecologically valid way.\r\nMost of the conventional assessments measure spatial abilities in 2-dimensional\r\nsettings; thus, they require 2D-3D-transformation processes that are often criticized.\r\nDeveloping a new spatial abilitiy measurement, we were using augmented reality\r\ntechnology and the software \"Construct 3D\", which allows for the projection of\r\nvarious virtual objects into real space. Tasks can be viewed through electronic\r\nglasses (HMD's), and therefore be examined from different perspectives.\r\nParticipants then actively have to construct solutions within virtual reality, using\r\nspecial input devices.\r\nEmpirical studies demonstrate, that performance on spatial tasks is often influenced\r\nby training and practice. Many of commonly found genderspecific differences in\r\nspatial abilities can - at least to some extent - be explained by differences in\r\nexperience with spatial tasks. Disregarding this fact can lead to an underestimation of\r\nthe real potential of persons having less experience with spatial tasks (see Glueck,\r\nKaufmann, Duenser & Steinbuegl, 2005). Thus, to gain more valid information on an\r\nindividual's spatial abilities, we are developing a dynamic test (containing a pretest,\r\ntraining phase and a post test), that measures the current performance level (pretest)\r\nas well as a person's potential for improvement after a\r\ntraining phase.\r\nSpatial tasks used in the previous study are simple and\r\ncomplex figures consisting of single cubes. These\r\nfigures are presented within a 4x4x4 transparent grid\r\n(see figure 1). An item pool of 30 test items was\r\ngenerated following construction rules based on\r\ntheoretical considerations. Those parameters are\r\ndimensionality, rotation axis and the position of the\r\nrotation axis, complexity of the single objects, position\r\nwithin the grid and number of rotations per item.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Strasser, I., Kaufmann, H., Strauß, S., Glück, J., &#38; Csisinko, M. (2008). <i>Cognitive strategies on solving spatial tasks: development of a dynamic spatial test in augmented reality</i>. 3rd International Conference of Cognitive Science, Moscow, Russia, Non-EU. http://hdl.handle.net/20.500.12708/84855</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Irene",
                    "last_name": "Strasser",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Sabine",
                    "last_name": "Strauß",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Judith",
                    "last_name": "Glück",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Mathis",
                    "last_name": "Csisinko",
                    "position": 5,
                    "role": "Author",
                    "tid": "47153"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-06"
            ],
            "pid": "89645",
            "handle": "20.500.12708/84873",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "A Review of Serious Games as Psychological Support in Health",
            "keywords": [],
            "abstract": "Videogames are a special form of new technologies. Initially they have been conceived for entertainment; however, during the last years, a new wave of games targeted to more serious purposes, have been developed and successfully implemented, as a psychotherapeutical complement in the treatment of various medical illnesses and mental disorders.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Soto Lopez, A., Santamaría, J., Krug, I., Forcano Gamazo, L., Gunnard, K., Kalapanidas, E., Konstantas, D., Ganchev, T., Kocsis, O., Lam, T., Raguin, T., Breiteneder, C., Kaufmann, H., Davarakis, C., Fernandez Aranda, F., &#38; Jiménez-Murcia, S. (2009). <i>A Review of Serious Games as Psychological Support in Health</i>. 13th International Conference on Information Visualisation, Barcelona, Spain, EU. http://hdl.handle.net/20.500.12708/84873</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "A.",
                    "last_name": "Soto Lopez",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Juanjo",
                    "last_name": "Santamaría",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "I.",
                    "last_name": "Krug",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "L.",
                    "last_name": "Forcano Gamazo",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Katharina",
                    "last_name": "Gunnard",
                    "position": 5,
                    "role": "Author"
                },
                {
                    "first_name": "Elias",
                    "last_name": "Kalapanidas",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Dimitri",
                    "last_name": "Konstantas",
                    "position": 7,
                    "role": "Author"
                },
                {
                    "first_name": "Todor",
                    "last_name": "Ganchev",
                    "position": 8,
                    "role": "Author"
                },
                {
                    "first_name": "Otilia",
                    "last_name": "Kocsis",
                    "position": 9,
                    "role": "Author"
                },
                {
                    "first_name": "Tony",
                    "last_name": "Lam",
                    "position": 10,
                    "role": "Author"
                },
                {
                    "first_name": "Thierry",
                    "last_name": "Raguin",
                    "position": 11,
                    "role": "Author"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 12,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 13,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Costas",
                    "last_name": "Davarakis",
                    "position": 14,
                    "role": "Author"
                },
                {
                    "first_name": "Fernando",
                    "last_name": "Fernandez Aranda",
                    "position": 15,
                    "role": "Author"
                },
                {
                    "first_name": "Susanna",
                    "last_name": "Jiménez-Murcia",
                    "position": 16,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02",
                "E193-03"
            ],
            "pid": "89667",
            "handle": "20.500.12708/84895",
            "doi": null,
            "year": 2009,
            "issued": "2009",
            "issued_on": "2009-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": true,
            "invited": false,
            "title": "On Visualization and Reconstruction from Non-Uniform Point Sets using B-splines",
            "keywords": [],
            "abstract": "In this paper we present a novel framework for the visualization and reconstruction from non-uniform point sets. We adopt a variational method for the reconstruction of 3D non-uniform data to a uniform grid of chosen resolution. We will extend this reconstruction to an efficient multi-resolution uniform representation of the underlying data. Our multi-resolution representation includes a traditional bottom-up multi-resolution approach and a novel top-down hierarchy for adaptive hierarchical reconstruction. Using a hybrid regularization functional we can improve the reconstruction results. Finally, we discuss further application scenarios and show rendering results to emphasize the effectiveness and quality of our proposed framework. By means of qualitative results and error comparisons we demonstrate superiority of our method compared to competing methods",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vucini, E., Möller, T., &#38; Gröller, E. (2009). <i>On Visualization and Reconstruction from Non-Uniform Point Sets using B-splines</i>. IEEE Symposium on Information Visualization, Minneapolis, USA, Austria. http://hdl.handle.net/20.500.12708/84895</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Erald",
                    "last_name": "Vucini",
                    "position": 1,
                    "role": "Author",
                    "tid": "45707"
                },
                {
                    "first_name": "Torsten",
                    "last_name": "Möller",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Eduard",
                    "last_name": "Gröller",
                    "position": 3,
                    "role": "Author",
                    "tid": "143572"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "4043"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89792",
            "handle": "20.500.12708/85019",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Publication",
            "subtype": "Inproceedings",
            "peer_reviewed": false,
            "invited": false,
            "title": "Towards a Common Evaluation Strategy for Table",
            "keywords": [],
            "abstract": "Outline font technology has long been established as the standard way to represent typefaces, allowing characters to be represented independently of print size and resolution. Although outline font technologies are mature and produce results of sufficient quality for professional printing applications, they are inherently inflexible, which presents limitations in a number of document engineering applications. In the 1990s, the topic of finding a successor to outline fonts was a hot topic of research. Unfortunately, none of the methods developed at the time were successful in replacing outline font technology and this field of research has since then declined sharply in popularity.\r\n\r\nIn this paper, we revisit a parametric font format developed between 1995 and 2001 by Hu and Hersch, where characters are built up from connected shape components. We extend this representation and use it to synthesize several characters from the Frutiger typeface and alter their weights by setting the relevant parameters. These settings are automatically propagated to the other characters of the font family.\r\n\r\nTo conclude, we provide a discussion on next-generation font technologies in the light of today's Web-centric technologies and suggest applications that could greatly benefit from the use of flexible, parametric font representations.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hassan, T. (2010). Towards a Common Evaluation Strategy for Table. In <i>DocEng ’10: Proceedings of the 10th ACM symposium on Document engineering</i> (pp. 255–258). Association for Computing Machinery. https://doi.org/10.1145/1860559.1860617</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Tamir",
                    "last_name": "Hassan",
                    "position": 1,
                    "role": "Author",
                    "tid": "44748"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89793",
            "handle": "20.500.12708/85020",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "On the search of Optimal Reconstruction Resolution",
            "keywords": [],
            "abstract": "In this paper we present a novel algorithm to optimize the reconstruction from\r\nnon-uniform point sets. We introduce a statistically-derived topology controller for selecting\r\nthe reconstruction resolution of a given non-uniform point set. Deriving information from\r\nhomology-based statistics, our topology-controller ensures a stable and sound basis for the\r\nanalysis process. By analysing our topology-controller, we select an optimal reconstruction\r\nresolution which ensures both low reconstruction errors and a topological stability of the underlying\r\nsignal. Our approach offers a valuable method for the evaluation of the reconstruction\r\nprocess without the need of visual inspection of the reconstructed datasets. By means\r\nof qualitative results we show how our proposed topology statistics provides complementary\r\ninformation in the enhancement of existing reconstruction pipelines in visualization.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vucini, E., &#38; Kropatsch, W. (2010). <i>On the search of Optimal Reconstruction Resolution</i>. 3rd International Workshop on Computational Topology in Image Context - CTIC 2010, Chipiona, EU. http://hdl.handle.net/20.500.12708/85020</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Erald",
                    "last_name": "Vucini",
                    "position": 1,
                    "role": "Author",
                    "tid": "45707"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89805",
            "handle": "20.500.12708/85032",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Representing Scenes with dynamic objects by Graph Pyramids",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. (2010). <i>Representing Scenes with dynamic objects by Graph Pyramids</i>. The Seventh IASTED International Conference on Signal Processing, Pattern Recognition and Applications - SPPRA 2010, Innsbruck, EU. http://hdl.handle.net/20.500.12708/85032</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89836",
            "handle": "20.500.12708/85063",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "The Efficiency of a Homology Algorithm based on Discrete Morse Theory and Coreductions",
            "keywords": [],
            "abstract": "Two implementations of a homology algorithm based on the Forman's discrete\r\nMorse theory combined with the coreduction method are presented. Their e ciency is\r\ncompared with other implementations of homology algorithms.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Harker, S., Mischaikow, K., Mrozek, M., Nanda, V., Wagner, H., Juda, M., &#38; Dlotko, P. (2010). <i>The Efficiency of a Homology Algorithm based on Discrete Morse Theory and Coreductions</i>. 3rd International Workshop on Computational Topology in Image Context - CTIC 2010, Chipiona, EU. http://hdl.handle.net/20.500.12708/85063</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Shaun",
                    "last_name": "Harker",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Konstantin",
                    "last_name": "Mischaikow",
                    "position": 2,
                    "role": "Author"
                },
                {
                    "first_name": "Marian",
                    "last_name": "Mrozek",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Vidit",
                    "last_name": "Nanda",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Hubert",
                    "last_name": "Wagner",
                    "position": 5,
                    "role": "Author",
                    "tid": "217144"
                },
                {
                    "first_name": "Mateusz",
                    "last_name": "Juda",
                    "position": 6,
                    "role": "Author"
                },
                {
                    "first_name": "Pawel",
                    "last_name": "Dlotko",
                    "position": 7,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89968",
            "handle": "20.500.12708/85195",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Approximation Algorithm for the Multidimensional Mathching Distance",
            "keywords": [],
            "abstract": "Topological Persistence has proven to be a promising framework for dealing with problems concerning shape analysis and comparison. In this context, it was originally introduced by taking into account 1-dimensional properties of shapes, modeled by real-valued functions. More recently, Topological Persistence has been generalized to consider multidimensional properties of shapes, coded by vector-valued functions. This extension has led to introduce suitable shape descriptors, named the multidimensional persistence Betti numbers, and a distance to compare them, the socalled multidimensional matching distance. In this paper we propose a new computational framework to deal with the multidimensional matching distance. We start by showing some theoretical results, and then we use them to formulate an algorithm for computing such a distance up to an arbitrary threshold error.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Cerri, A., &#38; Frosini, P. (2011). <i>Approximation Algorithm for the Multidimensional Mathching Distance</i>. Thematic Program on Discrete Geometry and Applications -- Workshop on Computational Topology, Toronto, Ontario M5T 3J1, Canada, Non-EU. http://hdl.handle.net/20.500.12708/85195</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Andrea",
                    "last_name": "Cerri",
                    "position": 1,
                    "role": "Author",
                    "tid": "233419"
                },
                {
                    "first_name": "Patrizio",
                    "last_name": "Frosini",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89987",
            "handle": "20.500.12708/85214",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "The Walking Pyramid",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. (2011). <i>The Walking Pyramid</i>. International Conference STCC 2011, Sinaia, Rumänien, EU. http://hdl.handle.net/20.500.12708/85214</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "89988",
            "handle": "20.500.12708/85215",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Topological Pyramids and Computer Vision",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. (2011). <i>Topological Pyramids and Computer Vision</i>. 16th Iberoamerican Congress on Pattern Recognition (CIARP 2011), Pucon, Chile, Non-EU. http://hdl.handle.net/20.500.12708/85215</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90033",
            "handle": "20.500.12708/85260",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Biometric Technologies",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Haxhimusa, Y. (2012). <i>Biometric Technologies</i>. University of Prishtina, University of Prishtina, EU. http://hdl.handle.net/20.500.12708/85260</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Yll",
                    "last_name": "Haxhimusa",
                    "position": 1,
                    "role": "Author",
                    "tid": "64562"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90097",
            "handle": "20.500.12708/85324",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "2 Computer Vision Tasks: tracking with structure, shape similarity",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ion, A. (2011). <i>2 Computer Vision Tasks: tracking with structure, shape similarity</i>. University of Sevilla, University of Sevilla, EU. http://hdl.handle.net/20.500.12708/85324</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Adrian",
                    "last_name": "Ion",
                    "position": 1,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "90161",
            "handle": "20.500.12708/85388",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Distributed Interaction with Kinect and Mobile Phone in Augmented Reality",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., &#38; Schönauer, C. (2012). <i>Distributed Interaction with Kinect and Mobile Phone in Augmented Reality</i>. IGDA Unity3D, Vienna, Austria. http://hdl.handle.net/20.500.12708/85388</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "54835"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90178",
            "handle": "20.500.12708/85405",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Representing and Manipulating Cyclic Paths in Continuous and Discrete Spaces",
            "keywords": [],
            "abstract": "A cyclic path in the plane correponds to a simple closed curve that separates the 'inside' from the 'outside' (Jordan's curve theorem). In the case of an image the inside may correspond to an object and the curve to its silhouette. But the curve may also surround a hole (or a set of holes) in the object in which case the curve corresponds to a generator of its homology group. Typically we observe the object by its projection into the image plane (or some well-behaved and bounded part of a surface). This embedding space can be considered continuous or discretely sampled. Accordingly there may be continuous and discrete representations of the curves having more or less strict properties: connectedness (no gaps), smoothness (no corners),... These properties can be propagated to properties of the discrete observations if we suppose sufficiently dense sampling. The most common discrete representations of embedding spaces are the square and triangular grid having two interpretations: it can be seen as a (huge) set of sensor positions or as partitioning the image plane into simply connected patches (square or triangular observation windows) through which the object and its silhouette is observed. Any continuous curve that does not pass through any vertex of the partition intersects two sides of the observation windows (or one side twice) and can be represented by a 'curve relation' between two sides of the window. The corresponding code describes the class of curves intersecting the same sides. The 1D corresponding representation is a chain code (i.e. Freeman) describing the sequence of patches in the order the curve crosses them. The advantage of the square grid is that there is only a small number of relative moves between adjacent windows (typically 4 or 8) but it has the drawback to be sensitive to small relative changes between the curve and the grid (e.g. a moving object). A further disadvantage of the regular square grid is that a small part of the object requiring high resolution needs a huge amount of storage for correct reconstruction although most other parts do not need such high resolution. Multiple and adaptive resolutions overcome this bottleneck of single resolution, this has been explored in the curve pyramid. Its extension to irregular grids have been used to efficiently describe and process huge technical drawings. It could be shown that the flexibility of using patches of different sizes can create hierarchies that preserve certain topological properties of the object, its curve relations and its embedding space. In this hierarchy, homology generators shrink with the reduction of the sampling space while local constraints prevent elimination of connected components and holes until a non-contractible top level is reached which has the same homology structure as the bottom while being reduced to its minimal size (Gonzalez-Diaz). To overcome rotation sensitivity differential chain codes (DCC) have been proposed, e.g. the RULI chain code is equivalent to the curve relations where R stands for right turn, U for U-turn, I continues striaght ahead and L turns left. A variant of this DCC operates on triangular grids: entering a triangle through one side has one corner 'opposite' its entry. It can be passed on the right (R) or on the left (L) or return (U). We explore consequences on such RUL codes on a triangular grid when triangular edges are contracted or removed. These are the only operations used in dual graph contraction to build the topology preserving hierarchy. Since this process also preserves the maximum degree of the dual graph a triangulation remains a triangulation and the RUL chain can be represented at the lower resolution again as a RUL chain. This concept connects these DCC with formal grammars and allows the embedding space to deform while the representation remains the same.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. (2012). <i>Representing and Manipulating Cyclic Paths in Continuous and Discrete Spaces</i>. CTIC 2012: 4th International Workshop on Computational Topology in Image Context, Italien, Bertinoro, EU. http://hdl.handle.net/20.500.12708/85405</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90179",
            "handle": "20.500.12708/85406",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Representing Vision Embedding Spaces",
            "keywords": [],
            "abstract": "In the past vision concentrated on basic entities of a vision space: pixels, key points, ... Local features gave rise to selected points in the image or the feature space where again, only individual feature vectors were considered for classifying or discriminating the basic entities. Every point in the underlying Euclidean space is principally valid. However context and relations among two or more basic entities establish constraints that exclude certain constellations and give rise to highly complex algorithms for checking consistency, finding correspondence, or for matching. Furthermore noise and inaccurate measurements enforce strategies to cope with instability to produce robust results.\r\n\r\nMore holistic approaches consider the spaces in which the basic elements are embedded. They do not need to be homogeneous and may contain empty subspaces (\"holes\"), many combinatorial constellations are excluded by real world constraints. A few examples from structural representations of such spaces will be discussed: graphs describing spatio-temporal partitions at multiple levels of abstraction, topological and homological representations with strong invariance to continuous deformations. The concept of topological persistence will be introduced as an example how to overcome the widely spread myth that \"topology is not robust\".",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. (2012). <i>Representing Vision Embedding Spaces</i>. GS Workshop on Computer Vision and Perception, Prag, EU. http://hdl.handle.net/20.500.12708/85406</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90217",
            "handle": "20.500.12708/85444",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Alternatives to outline representations: Parametric fonts based on shape components",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Hassan, T. (2012). <i>Alternatives to outline representations: Parametric fonts based on shape components</i>. TypeShorts, 21 June 2012, Brno, Czech Republic, EU. http://hdl.handle.net/20.500.12708/85444</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Tamir",
                    "last_name": "Hassan",
                    "position": 1,
                    "role": "Author",
                    "tid": "44748"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90224",
            "handle": "20.500.12708/85451",
            "doi": null,
            "year": 2012,
            "issued": "2012",
            "issued_on": "2012-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Digitalisierung und Erfassung von Theaterzettelsammlungen - Eine technische Standortbestimmung",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Göbel, M. C. (2012). <i>Digitalisierung und Erfassung von Theaterzettelsammlungen - Eine technische Standortbestimmung</i>. Schnittstelle: Theater-Sammlungen, Portale/Profile/Erschließung/Provenienz, 1080 Wien, Don Juan Archiv, Austria. http://hdl.handle.net/20.500.12708/85451</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Max Christopher",
                    "last_name": "Göbel",
                    "position": 1,
                    "role": "Author",
                    "tid": "50520"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E120-05",
                "E193-02"
            ],
            "pid": "90320",
            "handle": "20.500.12708/85547",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Robust Long-Range Optical Tracking for Tunneling Measurement Tasks",
            "keywords": [],
            "abstract": "Over the last years, automation for tunnel construction and mining activities increased rapidly. To allow for\r\nenhanced tunneling measurement, monitoring of workers and remote control of machines, systems are required\r\nthat are capable of real-time positioning of several static as well as moving targets. Such a system must provide\r\ncontinuous and precise 3D position estimation in large volumes and must be capable to be installed and work\r\ncorrectly during on-going tunneling or mining tasks.\r\nTracking systems are a fundamental component of a VR system to determine the 3D-position and orientation\r\nof a target in 3D space. Infrared optical tracking systems use infrared light to track several static or moving\r\ntargets simultaneously with low latency in small tracking volumes. To benefit from the capabilities of infrared\r\noptical tracking, a system is proposed to track static as well as moving optical targets in large tracking volumes\r\nwith a maximum depth extend of 70 meters. Our system needs a minimal hardware setup consisting out of two\r\nhigh quality machine vision cameras, which are mounted on both walls of the tunnel, and a standard (portable)\r\nworkstation for data processing. Targets are equipped with infrared LEDs and can be either carried by workers\r\nor attached to a machine. The two cameras form a stereo rig and face into the measurement volume to allow for\r\ncontinuous tracking. Using image processing techniques, the LEDs of the target(s) are detected in both 2D camera\r\nimages and are back-projected into 3D using projective reconstruction algorithms. Thereby, the 3D position\r\nestimate of the target is determined. Using image filtering techniques, fitting methods based on target's geometric\r\nconstraints and prediction heuristics, the system allows for unique target identification during calibration and\r\ntracking even in environments with heavy interferences such as vibrations, tunnel illumination or machine lights.\r\nWe extensively tested the system to (1) determine optimal distances between cameras (baseline) with constraints\r\nto a tunnel application scenario, (2) to evaluate robustness of unique target identification and (3) to measure\r\naccuracy of estimated 3D position. Our results prove the system's capabilities to continuously track static and\r\nmoving targets within the whole tracking volume as soon as the target becomes visible to the stereo rig. Thus,\r\npreliminary sighting of the target can be omitted. Interferences are filtered and partly occluded targets can be\r\nrecovered. Up to a distance of 50m with a baseline of 12m, our system provides very high precision of the 3D\r\nposition estimates with a deviation of 1cm or less along all three spatial axes. At a distance of 70m, our system\r\nprovides still very high accuracy in the width- and height direction with a deviation of only several millimeters\r\nand up to 3cm along the depth axis.\r\nThese promising results enable our system to act as measurement and monitoring system in rough indoor\r\nenvironments. Furthermore, it can serve as a reliable wide area user tracking system for future mixed reality applications,\r\ne.g. for tunnel simulation, training of engineers, machine control, tunnel data interpretation and inspection.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mossel, A., Gerstweiler, G., Vonach, E., Chmelina, K., &#38; Kaufmann, H. (2013). <i>Robust Long-Range Optical Tracking for Tunneling Measurement Tasks</i>. European Geosciences Union General Assembly 2013, Wien, Austria. http://hdl.handle.net/20.500.12708/85547</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Annette",
                    "last_name": "Mossel",
                    "position": 1,
                    "role": "Author",
                    "tid": "58429"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 2,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 3,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Klaus",
                    "last_name": "Chmelina",
                    "position": 4,
                    "role": "Author",
                    "tid": "127455"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 5,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "90323",
            "handle": "20.500.12708/85550",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Flexible Spaces: A Virtual Step Outside of Reality",
            "keywords": [],
            "abstract": "In this paper we introduce the concept of flexible spaces - a novel redirection technique that generalizes the use of overlapping (impossible) spaces and change blindness in an algorithm for dynamic layout generation. Flexible spaces is an impossible environment that violates the real world constancy in favor of providing the experience of seamless, unrestricted natural walking over a large-scale virtual environment (VE).",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vasylevska, K., Kaufmann, H., Bolas, M., &#38; Suma, E. A. (2013). <i>Flexible Spaces: A Virtual Step Outside of Reality</i>. IEEE Virtual Reality, Alexandria, Virginia, USA, Non-EU. http://hdl.handle.net/20.500.12708/85550</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Khrystyna",
                    "last_name": "Vasylevska",
                    "position": 1,
                    "role": "Author",
                    "tid": "232367"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 2,
                    "role": "Author",
                    "tid": "46406"
                },
                {
                    "first_name": "Mark",
                    "last_name": "Bolas",
                    "position": 3,
                    "role": "Author"
                },
                {
                    "first_name": "Evan A.",
                    "last_name": "Suma",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "90367",
            "handle": "20.500.12708/85594",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Virtual- and Augmented Reality in Education",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2013). <i>Virtual- and Augmented Reality in Education</i>. Intel Webinar, Internet, Non-EU. http://hdl.handle.net/20.500.12708/85594</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "90391",
            "handle": "20.500.12708/85618",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Augmented Reality Technologien und Industrielle Anwendungsmöglichkeiten",
            "keywords": [],
            "abstract": "Augmented Reality Technologien und Industrielle Anwendungsmöglichkeiten",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2013). <i>Augmented Reality Technologien und Industrielle Anwendungsmöglichkeiten</i>. MM2 - Machine Maintainability 2.0, TFZ Technologie- und Forschungszentrum Wiener Neustadt, Austria. http://hdl.handle.net/20.500.12708/85618</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "90392",
            "handle": "20.500.12708/85619",
            "doi": null,
            "year": 2010,
            "issued": "2010",
            "issued_on": "2010-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Affective Image Classification",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Machajdik, J., &#38; Hanbury, A. (2010). <i>Affective Image Classification</i>. Computer Vision Winter Workshop 2010, Nove Hrady, EU. http://hdl.handle.net/20.500.12708/85619</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Jana",
                    "last_name": "Machajdik",
                    "position": 1,
                    "role": "Author",
                    "tid": "43465"
                },
                {
                    "first_name": "Allan",
                    "last_name": "Hanbury",
                    "position": 2,
                    "role": "Author",
                    "tid": "48222"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "90449",
            "handle": "20.500.12708/85676",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Augmented Reality",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2013). <i>Augmented Reality</i>. Funkenflug - Innovationsforum Völkerkundemuseum, Welt Museum Wien, Austria. http://hdl.handle.net/20.500.12708/85676</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "90450",
            "handle": "20.500.12708/85677",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Education in Augmented Reality",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2013). <i>Education in Augmented Reality</i>. The Visual Language of Technique between Science and Art: Heritage and Expectations in Research and Teaching, Politecnico di Milano, EU. http://hdl.handle.net/20.500.12708/85677</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90515",
            "handle": "20.500.12708/85742",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Surface Completion of Head via Low-rank Decomposition",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">de Sousa, S., &#38; Kropatsch, W. (2013). <i>Surface Completion of Head via Low-rank Decomposition</i>. International Computer Vision Summer School 2013, Italy, Calabria, Austria. http://hdl.handle.net/20.500.12708/85742</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Samuel",
                    "last_name": "de Sousa",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90522",
            "handle": "20.500.12708/85749",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Real Time Modeling of Finger Segments",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">de Sousa, S., &#38; Ernst, J. (2013). <i>Real Time Modeling of Finger Segments</i>. Signal Processing, Pattern Recognition and Applications, Innsbruck, Austria. http://hdl.handle.net/20.500.12708/85749</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Samuel",
                    "last_name": "de Sousa",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Jan",
                    "last_name": "Ernst",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90674",
            "handle": "20.500.12708/85901",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Topological Image Analysis and Reeb Graph Representations for Plant Phenotyping",
            "keywords": [],
            "abstract": "This paper discusses the use of topological image\r\nanalysis to derive characteristics needed in plant phenotyping.\r\nDue to certain features of root systems (deformation over time,\r\noverlaps of branches in a 2D image of the root system) a topolog-\r\nical analysis is needed to correctly derive these characteristics.\r\nThe advantages of such a topological analysis are highlighted\r\nin this paper and root phenotyping is presented as a new\r\napplication for computational topology. Characteristics used in\r\nplant phenotyping that can be derived from root images using\r\nmethods of topological image analysis are further presented. A\r\nReeb graph based representation of root images is shown as\r\nan example for such a topological analysis. Based on a graph\r\nrepresentation a new, normalised representation of root images\r\nis introduced.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., Kropatsch, W., &#38; Busch, W. (2014). <i>Topological Image Analysis and Reeb Graph Representations for Plant Phenotyping</i>. 1st Viennese Plant Phenotyping Workshop, Wien, Austria. http://hdl.handle.net/20.500.12708/85901</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Wolfgang",
                    "last_name": "Busch",
                    "position": 3,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-03"
            ],
            "pid": "90677",
            "handle": "20.500.12708/85904",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Longitudinal Diffeomorphic Fetal Brain Atlas Learning For Tissue Labeling Using Geodesic Regression And Graph Cuts",
            "keywords": [],
            "abstract": "The human brain undergoes fundamental structural changes between the second and the third\r\ntrimester of pregnancy [1]. The most accurate non-invasive method for observing these events to\r\ndate is the (ultra) fast magnetic resonance (MR) imaging technique. It allows to image a fetus at\r\na satisfying resolution, despite its small size or varying position [2]. A problem of MR imaging is the\r\nlack of comparability and constancy of gray-values, which are mapped according to the proton\r\n(hydrogen) concentration. It differs among patients and results in varying gray-values for varying\r\nproton density [3]. This motivates to build a fetal brain atlas to use it as a standard space. Brain\r\nstructures can be mapped according to marked anatomical locations, to make fetal brains\r\ncomparable for studying brain development, fetal pathology locations, fetal abnormalities or\r\nanatomy. !\r\n!\r\nThe aim of the work is to provide an atlas of the developing fetal brain, consisting of a\r\ncontinuous, quantifiable model of brain development derived by geodesic shooting regression\r\n[4,5] and an automated labeling procedure using a graph cut based segmentation approach [7].",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Licandro, R., Schwartz, E., Langs, G., &#38; Sablatnig, R. (2014). <i>Longitudinal Diffeomorphic Fetal Brain Atlas Learning For Tissue Labeling Using Geodesic Regression And Graph Cuts</i>. Medical Imaging Summer School 2014, Favignana, Sizilien, Italien, EU. http://hdl.handle.net/20.500.12708/85904</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Roxane",
                    "last_name": "Licandro",
                    "position": 1,
                    "role": "Author",
                    "tid": "55673"
                },
                {
                    "first_name": "Ernst",
                    "last_name": "Schwartz",
                    "position": 2,
                    "role": "Author",
                    "tid": "42358"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Langs",
                    "position": 3,
                    "role": "Author",
                    "tid": "69204"
                },
                {
                    "first_name": "Robert",
                    "last_name": "Sablatnig",
                    "position": 4,
                    "role": "Author",
                    "tid": "133566"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90684",
            "handle": "20.500.12708/85911",
            "doi": null,
            "year": 2014,
            "issued": "2014",
            "issued_on": "2014-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Object Tracking based on Features and Structures",
            "keywords": [],
            "abstract": "In recent years, we have been studying the potential of\r\ngraph-based methods and representations in the\r\nfi\r\neld of video\r\nobject tracking [1], [2]. Our aim is to track rigid (e.g. man-\r\nmade objects) and articulated objects (e.g. humans) through\r\nchallenging situations like distractors and occlusions. Further-\r\nmore, the output should go beyond a single trajectory of the\r\ncenter of mass of the object. Especially for articulated objects,\r\nwe are interested in the local motion of the parts of the object\r\n(e.g. limbs of a human body). To achieve this aim, we propose\r\nto represent and to track target objects by a combination of\r\nappearance and structure.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N., &#38; Kropatsch, W. (2014). <i>Object Tracking based on Features and Structures</i>. Features and Structures (FEAST 2014), Stockholm, Schweden, EU. http://hdl.handle.net/20.500.12708/85911</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "90920",
            "handle": "20.500.12708/86147",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Virtual and Augmented Reality in Architecture",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2015). <i>Virtual and Augmented Reality in Architecture</i>. Augmented Architecture Workshop, Center of Geometry and Computational Design, Wien, Austria. http://hdl.handle.net/20.500.12708/86147</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "90921",
            "handle": "20.500.12708/86148",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Augmented Reality Anwendungen für Ausbildung und Assistenz",
            "keywords": [],
            "abstract": "In meinem Vortrag werde ich zuerst die mehrjährige Entwicklung einer Augmented Reality (AR) Anwendung für den Geometrieunterricht zusammenfassen und die Resultate aus Studien mit über 100 Schülern präsentieren. Danach werden die Erkenntnisse aus dieser Arbeit sowie die wesentlichen Herausforderungen beim Einsatz von AR im Unterricht diskutiert. Die technischen Entwicklungen der letzten Jahren ermöglichen kostengünstige AR Trainings- und Unterrichtsanwendungen, die ich abschließend besprechen werde.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2015). <i>Augmented Reality Anwendungen für Ausbildung und Assistenz</i>. LearnTec, Karlsruhe, Germany, Austria. http://hdl.handle.net/20.500.12708/86148</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90925",
            "handle": "20.500.12708/86152",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Categorizing the Topological Image Landscape with Local Binary Patterns",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Cerman, M. (2015). <i>Categorizing the Topological Image Landscape with Local Binary Patterns</i>. Epilog 2015, Wien, Freihaus (TU Wien, Wiedner Hauptstraße 8-10, 2. OG), Austria. http://hdl.handle.net/20.500.12708/86152</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Martin",
                    "last_name": "Cerman",
                    "position": 1,
                    "role": "Author",
                    "tid": "43978"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90929",
            "handle": "20.500.12708/86156",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Ellipse-based skeletons and elliptical coordinates",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kropatsch, W. (2015). <i>Ellipse-based skeletons and elliptical coordinates</i>. WTZ workshop on “Multi-scale approaches for Robust Medical Image Analysis,” Frankreich, Le Puy, EU. http://hdl.handle.net/20.500.12708/86156</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 1,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90930",
            "handle": "20.500.12708/86157",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Reeb graphs through local binary patterns",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I. (2015). <i>Reeb graphs through local binary patterns</i>. WTZ workshop on “Multi-scale approaches for Robust Medical Image Analysis,” Frankreich, Le Puy, EU. http://hdl.handle.net/20.500.12708/86157</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90931",
            "handle": "20.500.12708/86158",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Combining Reeb graphs and LBPs for shape representation and classification",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I. (2015). <i>Combining Reeb graphs and LBPs for shape representation and classification</i>. University of Sevilla, University of Sevilla, EU. http://hdl.handle.net/20.500.12708/86158</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90932",
            "handle": "20.500.12708/86159",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Novel Concepts for Recognition and Representation of Structure in Spatio-Temporal Classes of Images",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Janusch, I., &#38; Kropatsch, W. (2015). <i>Novel Concepts for Recognition and Representation of Structure in Spatio-Temporal Classes of Images</i>. 20 th Computer Vision Winter Workshop, CVWW 2015, Österreich, Steiermark, Schloss Seggau, Austria. http://hdl.handle.net/20.500.12708/86159</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 1,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90971",
            "handle": "20.500.12708/86198",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Segmentation in Space and Time",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N. (2015). <i>Segmentation in Space and Time</i>. Ersamus+ Teaching Assignment, Salerno, Italien, EU. http://hdl.handle.net/20.500.12708/86198</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "90972",
            "handle": "20.500.12708/86199",
            "doi": null,
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Tracking",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Artner, N. (2015). <i>Tracking</i>. Ersamus Teaching Assignment, University of Salerno; Salerno, Italien, EU. http://hdl.handle.net/20.500.12708/86199</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 1,
                    "role": "Author",
                    "tid": "37618"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-02",
                "E193-06"
            ],
            "pid": "91096",
            "handle": "20.500.12708/86323",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Group Detection in the Context of Imbalanced Data",
            "keywords": [],
            "abstract": "The problem of group detection with no prior knowledge, i.e clustering, is one of the most important tasks in data analysis. It has been addressed in many applications in various fields. Data clustering becomes challenging when the group sizes are very different - this is called imbalanced data - with different densities and shapes. This task is even more difficult in the context of high-dimensional data since it is very hard do state any assumption about specific characteristics of groups (sizes, densities, or shapes). However, many clustering techniques are built upon some of these assumptions. For instance, the most popular k-means method can be shown as a particular case of the EM algorithm for data generated by Gaussian mixtures. In addition, many clustering algorithms (also k-means) require the ad-hoc specification of parameters, especially the number of clusters. This is almost impossible to know beforehand. Unfortunately, the final clustering solution usually depends on the choice of the predefined parameters.\r\nWe propose an algorithm which identifies the clusters in imbalanced high-dimensional data. Our procedure incorporates an existing clustering method in order to detect the homogeneous set of initial clusters. These initial clusters are successively merged in order to build final clusters. Merging a pair of initial clusters is based on Local Outlier Factor (LOF) which captures the final clusters of arbitrary sizes without assumptions on cluster characteristics. The fact of small group sizes in imbalanced data makes the observations of those groups atypical. Therefore, our special focus is towards the ability of finding these interesting groups next to the description of the data structure. The usefulness of our approach is demonstrated with imbalanced media data sets, and it is shown that state-of-the-art methods are outperformed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brodinova, S., Zaharieva, M., Filzmoser, P., Ortner, T., &#38; Breiteneder, C. (2016). <i>Group Detection in the Context of Imbalanced Data</i>. International Conference COMPUTER DATA ANALYSIS &#38; MODELING, Minsk, Belarus, Non-EU. http://hdl.handle.net/20.500.12708/86323</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 1,
                    "role": "Author",
                    "tid": "281480"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 2,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 3,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 4,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 5,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "220239"
            ]
        },
        {
            "org_nrs": [
                "E105-06",
                "E193-02",
                "E193-06"
            ],
            "pid": "91097",
            "handle": "20.500.12708/86324",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Forward Projection for High-Dimensional Data",
            "keywords": [],
            "abstract": "We provide a novel view on group structure in data. Projecting observations onto a subspace spanned by a small selection of observations, we calculate orthogonal distances as a measure for dissimilarity. Sequentially exchanging the observations, used to span the subspace, we receive a series of distances. Observations, taken from a similar group structure will behave similar along those projections. This leads to a visualisation of high dimensional data providing some basic diagnostic on group structures and outliers. The series of distances can be further utilized to perform cluster algorithms, leading to significant improvement when facing clusters located in different subspaces.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Ortner, T., Filzmoser, P., Brodinova, S., Zaharieva, M., &#38; Breiteneder, C. (2016). <i>Forward Projection for High-Dimensional Data</i>. International Conference COMPUTER DATA ANALYSIS &#38; MODELING, Minsk, Belarus, Non-EU. http://hdl.handle.net/20.500.12708/86324</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Thomas",
                    "last_name": "Ortner",
                    "position": 1,
                    "role": "Author",
                    "tid": "48362"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Filzmoser",
                    "position": 2,
                    "role": "Author",
                    "tid": "40896"
                },
                {
                    "first_name": "Sarka",
                    "last_name": "Brodinova",
                    "position": 3,
                    "role": "Author",
                    "tid": "281480"
                },
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 4,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 5,
                    "role": "Author",
                    "tid": "159676"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": [
                "220239"
            ]
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "91118",
            "handle": "20.500.12708/86345",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Manifold Representation of Fabric: A Challenging Task?",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Stria, J., Janusch, I., Artner, N., Hlavac, V., &#38; Kropatsch, W. (2016). <i>Manifold Representation of Fabric: A Challenging Task?</i> 21st Computer Vision Winter Workshop (CVWW2016), Rimske Toplice, Slovenia, EU. http://hdl.handle.net/20.500.12708/86345</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Jan",
                    "last_name": "Stria",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Ines",
                    "last_name": "Janusch",
                    "position": 2,
                    "role": "Author",
                    "tid": "44201"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 3,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Vaclav",
                    "last_name": "Hlavac",
                    "position": 4,
                    "role": "Author"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 5,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-01",
                "E193-02"
            ],
            "pid": "91144",
            "handle": "20.500.12708/86371",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "MediCubes: A Health Monitoring Toy for Children",
            "keywords": [],
            "abstract": "Especially for young children measuring their physiological parameters to assess their health can be stressful, even when conducted at home by their parents. Therefore we present a concept that can relieve some of the anxiety correlated with an examination and implemented it in a test setup we call \"MediCubes\" to investigate how this approach is received. In this system cube shaped tangible objects are fitted with noninvasive sensors measuring pulse, temperature, blood oxygen saturation and lung capacity while interacting with them. Incorporation in a storytelling game allows guiding a child through a series of unperceived physiological measurements as an enjoyable experience. The acquired data is stored on a tablet computer and can be reviewed by parents or doctors.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vonach, E., Ternek, M., Gerstweiler, G., &#38; Kaufmann, H. (2016). <i>MediCubes: A Health Monitoring Toy for Children</i>. Winter Augmented Reality Meeting (WARM 2016), Graz, Austria. http://hdl.handle.net/20.500.12708/86371</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Marianne",
                    "last_name": "Ternek",
                    "position": 2,
                    "role": "Author",
                    "tid": "59780"
                },
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 3,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 4,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "91256",
            "handle": "20.500.12708/86483",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Virtual und Augmented Reality - Forschungsübersicht",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2016). <i>Virtual und Augmented Reality - Forschungsübersicht</i>. Symposium Schallforschung, Institut für Schallforschung, ÖAW, Austria. http://hdl.handle.net/20.500.12708/86483</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "91257",
            "handle": "20.500.12708/86484",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Imagine a Holodeck - Research within a Large Scale VR Platform",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2016). <i>Imagine a Holodeck - Research within a Large Scale VR Platform</i>. VR Meetup Vienna, Wien, Austria. http://hdl.handle.net/20.500.12708/86484</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "91258",
            "handle": "20.500.12708/86485",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Virtual & Augmented Reality @ TU Wien",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2016). <i>Virtual &#38; Augmented Reality @ TU Wien</i>. AVR Playground Conference, Wien, Austria. http://hdl.handle.net/20.500.12708/86485</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "91259",
            "handle": "20.500.12708/86486",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Virtual Reality Research and Applications",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2016). <i>Virtual Reality Research and Applications</i>. John Pavlik’s VR Seminar, Bogota (via Skype), Non-EU. http://hdl.handle.net/20.500.12708/86486</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "91260",
            "handle": "20.500.12708/86487",
            "doi": null,
            "year": 2016,
            "issued": "2016",
            "issued_on": "2016-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Walking Through Large Virtual Environments",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Kaufmann, H. (2016). <i>Walking Through Large Virtual Environments</i>. GCD Symposium, Wien, Austria. http://hdl.handle.net/20.500.12708/86487</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Author",
                    "tid": "46406"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "91515",
            "handle": "20.500.12708/86741",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Confocal ellipse-based distance and confocal elliptical field for polygonal shapes",
            "keywords": [],
            "abstract": "The paper introduces a novel confocal ellipse-based distance (CED), that is based on the properties of the confocal ellipses. This distance is used to produce a confocal elliptical field (CEF). The Euclidean Distance Transform (EDT) of a single point (called seed) generates a distance field of concentric circles. The sum of two such distance fields of two distinct seed points produces a distance field of confocal ellipses. This fact enables to adapt CED and CEF to the discrete case, referred to as CED­DT and CEF-DT. The properties of the CEF and CEF-DT make them useful for skeletonization, in particular for efficient removal of the spurious branches.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gabdulkhakova, A., &#38; Kropatsch, W. (2018). <i>Confocal ellipse-based distance and confocal elliptical field for polygonal shapes</i>. 23rd Computer Vision Winter Workshop (CVWW), Český Krumlov, Czech Republic, EU. http://hdl.handle.net/20.500.12708/86741</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Aysylu",
                    "last_name": "Gabdulkhakova",
                    "position": 1,
                    "role": "Author",
                    "tid": "235653"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 2,
                    "role": "Author",
                    "tid": "38472"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "91533",
            "handle": "20.500.12708/86759",
            "doi": null,
            "year": 2018,
            "issued": "2018",
            "issued_on": "2018-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": true,
            "title": "Tracking Golden-Collared Manakins in the Wild",
            "keywords": [],
            "abstract": "The golden-collared manakin (Manacus vitellinus) is a small tropical bird, which lives in the Panama forest. The males perform elaborate, acrobatic displays to court mates [l]. During its courtship dance the male demonstrates its physical strength by jumping between saplings, producing loud wing snaps mid-flight. Mating success seems to be related to superior motor skills [2], which allow the male to execute its dance faster and more precisely. However, it is not fully clear yet how exactly the courtship dance has to be performed to impress a female. To gain more knowledge about their dance, biologists recorded the birds in the wild with high-speed cameras at 60 fps. One of the videos can be found at 1. Manually annotating the male bird in every frame of the videos to enable analyzing their behavior is a tedious process. We propose a novel approach for automatic visual tracking of the male golden-collared manakin, combining a convolutional neural network, background subtraction and a Kalman filter.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gostler, A., Artner, N., Kropatsch, W., &#38; Fusani, L. (2018). <i>Tracking Golden-Collared Manakins in the Wild</i>. ICPR2018 24th International Conference on Pattern Recognition/Workshop Visual observation and analysis of Vertebrate And Insect Behavior (VAIB 2018), Bejing, China, Non-EU. http://hdl.handle.net/20.500.12708/86759</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Anna",
                    "last_name": "Gostler",
                    "position": 1,
                    "role": "Author",
                    "tid": "249859"
                },
                {
                    "first_name": "Nicole",
                    "last_name": "Artner",
                    "position": 2,
                    "role": "Author",
                    "tid": "37618"
                },
                {
                    "first_name": "Walter",
                    "last_name": "Kropatsch",
                    "position": 3,
                    "role": "Author",
                    "tid": "38472"
                },
                {
                    "first_name": "Leonida",
                    "last_name": "Fusani",
                    "position": 4,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "9167",
            "handle": "20.500.12708/9180",
            "doi": "10.34726/hss.2015.25561",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Intrinsische Plagiatserkennung durch stilometrische Clusteranalyse",
            "keywords": [
                "Stylometrics"
            ],
            "abstract": "Plagiarism detection is the process of analysing a scientific text and to find potential plagiarised passages. In this context, non-automated procedures have proven to be time-consuming and subjective. Especially in the light of a steadily increasing number of scientific publications, automated software-aided approaches represent valuable instruments to effectively detect plagiarized text. Conventional plagiarism software compares text passages against potential original documents based on matching strings. In contrast, intrinsic plagiarism detection attempts to detect plagiarized sections based on stylometric features. Thus, this procedure enables to discover sudden changes in the writing style. The recognition of stylistic inconsistencies is closely associated with the field of Authorship Attribution, especially in the use of textual features. The present thesis focuses on the development and implementation of a prototype of intrinsic plagiarism detection. The developed approach automatically extracts stylometric features from a given text and performs a multivariate cluster analysis. The respective clusters represent groups of text passages exhibiting similar stilometric properties and can therefore be associated with the respective number of authors. The input data (text) is represented by articles from the English-language edition of the online encyclopedia Wikipedia. The evaluation results demonstrate that the conducted procedure enables to approximately distinguish between text passages originating form different authors. Furthermore, it was shown that the reliability of the results are strongly dependent on the number of authors. The approximation of the correct author class structure depends among others on the determination of the number of clusters. The resulting number is validated by an own developed  quality measure.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schneider, D. (2015). <i>Intrinsische Plagiatserkennung durch stilometrische Clusteranalyse</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.25561</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "18353",
                    "name": "Schneider Daniel - 2015 - Intrinsische Plagiatserkennung durch stilometrische...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2759025,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9180/2/Schneider%20Daniel%20-%202015%20-%20Intrinsische%20Plagiatserkennung%20durch%20stilometrische...pdf"
                },
                {
                    "bsid": "87504",
                    "name": "Schneider Daniel - 2015 - Intrinsische Plagiatserkennung durch stilometrische...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 198843,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9180/5/Schneider%20Daniel%20-%202015%20-%20Intrinsische%20Plagiatserkennung%20durch%20stilometrische...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Daniel",
                    "last_name": "Schneider",
                    "position": 1,
                    "role": "Author",
                    "tid": "40150"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-02"
            ],
            "pid": "91705",
            "handle": "20.500.12708/86931",
            "doi": null,
            "year": 2019,
            "issued": "2019",
            "issued_on": "2019-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "Augmented und Virtual Reality im Umfeld Bauwesen: Grundlagen und angewandte Forschung",
            "keywords": [],
            "abstract": null,
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Schönauer, C. (2019). <i>Augmented und Virtual Reality im Umfeld Bauwesen: Grundlagen und angewandte Forschung</i>. Workshop BIM auf der Baustelle: Augmented Reality im Bauwesen, Wien, Austria. http://hdl.handle.net/20.500.12708/86931</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Author",
                    "tid": "54835"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193-03"
            ],
            "pid": "92005",
            "handle": "20.500.12708/87229",
            "doi": null,
            "year": 2021,
            "issued": "2021",
            "issued_on": "2021-01-01",
            "type": "Presentation",
            "subtype": "Presentation",
            "peer_reviewed": false,
            "invited": false,
            "title": "360 Calibration Tunnel for Around-view Monitoring System",
            "keywords": [],
            "abstract": "his paper introduces a novel 360º calibration tunnel for auto-calibration of multi-camera in \r\nthe around-view monitoring system. A specific cylindrical pattern is designed to cover the \r\ntunnel and to uniquely encode 3D real world. The unusual model- and correspondence-free \r\nmulti-camera calibration is employed based on our previous work in [1]. In particular, the \r\nscale-free fractal designs inside the 360º pattern result in resolution-free calibration and thus \r\nmakes the tunnel practical for nearly arbitrary resolutions. One of the main advantage of such \r\ntunnel is to not only avoid of manually place the calibration multiple patterns around a vehicle \r\nwhich is complex and laborious but also to dramatically decrease the time of the calibration in \r\na range of few seconds. Therefore, the proposed calibration tunnel can be widely applied to \r\nvehicle manufacture, verification, and repair and also in future thanks to the progressing of \r\nartificial intelligence, for installing in every gas station to calibrate the multi-cameras of \r\nautonomous cars.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Banaeyan, M., &#38; Faezeh, M. (2021). <i>360 Calibration Tunnel for Around-view Monitoring System</i>. 8th International Conference on New Solutions in Engineering, Information Science and Technology of the Century ahead (ICIET), United Arab Emirates (the). http://hdl.handle.net/20.500.12708/87229</div>\n</div>",
            "bitstreams": [],
            "people": [
                {
                    "first_name": "Majid",
                    "last_name": "Banaeyan",
                    "position": 1,
                    "role": "Author",
                    "tid": "286018"
                },
                {
                    "first_name": "Moteabbed",
                    "last_name": "Faezeh",
                    "position": 2,
                    "role": "Author"
                }
            ],
            "foci": [
                "Visual Computing and Human-Centered Technology"
            ],
            "projects": []
        },
        {
            "org_nrs": [
                "E193"
            ],
            "pid": "930",
            "handle": "20.500.12708/946",
            "doi": null,
            "year": 2017,
            "issued": "2017-09",
            "issued_on": "2017-09-01",
            "type": "Publication",
            "subtype": "Article",
            "peer_reviewed": true,
            "invited": false,
            "title": "Unsupervised group feature selection for media classification",
            "keywords": [
                "Feature selection",
                "CCA",
                "Audio classification",
                "Video classification"
            ],
            "abstract": "The selection of an appropriate feature set is crucial for the efficient analysis of any media collection. In general, feature selection strongly depends on the data and commonly requires expert knowledge and previous experiments in related application scenarios. Current unsupervised feature selection methods usually ignore existing relationships among components of multi-dimensional features (group features) and operate on single feature components. In most applications, features carry little semantics. Thus, it is less relevant if a feature set consists of complete features or a selection of single feature components. However, in some domains, such as content-based audio retrieval, features are designed in a way that they, as a whole, have considerable semantic meaning. The disruption of a group feature in such application scenarios impedes the interpretability of the results. In this paper, we propose an unsupervised group feature selection algorithm based on canonical correlation analysis (CCA). Experiments with different audio and video classification scenarios demonstrate the outstanding performance of the proposed approach and its robustness across different datasets.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Zaharieva, M., Breiteneder, C., &#38; Hudec, M. (2017). Unsupervised group feature selection for media classification. <i>INTERNATIONAL JOURNAL OF MULTIMEDIA INFORMATION RETRIEVAL</i>. https://doi.org/10.1007/s13735-017-0126-y</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "1879",
                    "name": "Zahariev Maia - 2017 - Unsupervised group feature selection for media...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 645689,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/946/2/Zahariev%20Maia%20-%202017%20-%20Unsupervised%20group%20feature%20selection%20for%20media...pdf"
                },
                {
                    "bsid": "45863",
                    "name": "Unsupervised group feature selection for media classification.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 77032,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/946/8/Unsupervised%20group%20feature%20selection%20for%20media%20classification.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Maia",
                    "last_name": "Zaharieva",
                    "position": 1,
                    "role": "Author",
                    "tid": "39017"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 2,
                    "role": "Author",
                    "tid": "159676"
                },
                {
                    "first_name": "Marcus",
                    "last_name": "Hudec",
                    "position": 3,
                    "role": "Author",
                    "tid": "154729"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E187"
            ],
            "pid": "9506",
            "handle": "20.500.12708/9519",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Positive impact game as a contribution to movement rehabilitation",
            "keywords": [
                "game",
                "virtual reality",
                "positive impact game",
                "rehabilitation",
                "exergame",
                "art game",
                "prothesen",
                "amputation"
            ],
            "abstract": "With the continuous improvements in surgery and the rapid development of bioelectrical interfaces, a new type of prosthesis with a higher quality is developed every year. Although a major step is taken every year, a considerable time gap is between an amputation and the first time the patient can use a fully functional myoelectrical prosthesis. To minimize this gap the \\ac{IMS} created in combination with the Otto-Bock company a virtual exercise tool that helps patients to start the learning process for using the prosthesis in the middle of the healing process. The following thesis builds upon this topic but replaces the exercise environment with a video game environment. With the help of this thesis, the reader should be able to understand the problems after an amputation, to understand the major fields of gamedesign and to distinguish between general game genres. Special attention is payed to the game genre \"Positive Impact Game\". Its main purpose is the positive feeling/impact the player gets for his or her life while playing the game and afterwards. At the end of this thesis, a full working showcase for a psychological and physiological rehabilitation game is given. Furthermore, future developments on the showcase are discussed.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Brandstetter, J. (2013). <i>Positive impact game as a contribution to movement rehabilitation</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-55315</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "19031",
                    "name": "Brandstetter Juergen - 2013 - Positive impact game as a contribution to movement...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 42389866,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9519/2/Brandstetter%20Juergen%20-%202013%20-%20Positive%20impact%20game%20as%20a%20contribution%20to%20movement...pdf"
                },
                {
                    "bsid": "87726",
                    "name": "Brandstetter Juergen - 2013 - Positive impact game as a contribution to movement...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 157917,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9519/5/Brandstetter%20Juergen%20-%202013%20-%20Positive%20impact%20game%20as%20a%20contribution%20to%20movement...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Jürgen",
                    "last_name": "Brandstetter",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "46406"
                },
                {
                    "first_name": "Peter",
                    "last_name": "Purgathofer",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "139584"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E183"
            ],
            "pid": "9606",
            "handle": "20.500.12708/9619",
            "doi": null,
            "year": 2004,
            "issued": "2004",
            "issued_on": "2004-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "On-board processing for an infrared observatory",
            "keywords": [],
            "abstract": "During the past two decades, image compression has developed from a mostly academic Rate-Distortion (R-D) field, into a highly commercial business. Various lossless and lossy image coding techniques have been developed.<br />This thesis represents an interdisciplinary work between the field of astronomy and digital image processing and brings new aspects into both of the fields. In fact, image compression had its beginning in an American space program for efficient data storage. The goal of this research work is to recognize and develop new methods for space observatories and software tools to incorporate compression in space astronomy standards. While the astronomers benefit from new objective processing and analysis methods and improved efficiency and quality, for technicians a new field of application and research is opened. For validation of the processing results, the case of InfraRed (IR) astronomy has been specifically analyzed.<br />This work presents a solution for infrared astronomy, where the concept of On-Board Processing (OBP) is introduced for efficient exploitation of the telemetry bandwidth and the budget-limited space observatories.<br />Indeed, IR astronomy, most commonly, requires space observatories because the Universe cannot be accessed from ground in the full IR range as Earth's atmosphere blocks most IR wavelengths. Thus, IR astronomy is a good candidate to support our investigation. Furthermore, IR imaging with dedicated observations requires specific techniques with a complex semiconductors technology.<br />Thus, the resulting data is very sensitive to noise, which make the feasibility of our approach challenging.<br />IR detectors consist, as a rule, of fewer pixels than those for the visual range, but the design of multi-sensor instruments for space applications with special  technologies and a harsh radiation environment require high readout rates leading again to larger data volumes.<br />Therefore, although many applications exist, which generate or manipulate astronomical data (including wavelet-based methods), transmitting image information still faces a bottleneck such that the proposed techniques are often ad-hoc and sometimes inconsistent. One intuitional solution can be the JPEG 2000 standard to achieve the telemetry requirements. Indeed, a large scientific and commercial community is contributing for the development and the improvement of the JPEG 2000 compression codec. We demonstrate with a simple example the limitation of this compression method (JPEG 2000), concerning this astronomical application while OBP outperforms this generic compression codec.<br />Indeed, thermal IR detector raw data (at wavelengths > 5µm) consist of two constituent contributions: the source signal, and the unwanted background. The background is generally higher than the source signal in the order of several thousands. Therefore, generic quantization (e.g.<br />case of JPEG 2000) may lead to drop away the relevant information, while a dedicated compression technique using infrared detector knowledge is the only way to optimal performance.<br />The performance of this solution (OBP) is being measured by considering the compression ratio, result quality and algorithmic complexity. A new complexity analysis and measure is developed for Digital Signal Processor (DSP) architecture. The OBP complexity is evaluated for the Analog Device processor ADSP 21020. The impact of this research on the future of information technology is to develop data delivery systems where communication bandwidth and quality are at a premium and archival storage is a costly endeavor. This new framework has an improved  compression ratio and result quality over the best-known pre-existing compression algorithms, which will lead to a reduction of the data traffic for infrared observatories.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Belbachir, A. N. (2004). <i>On-board processing for an infrared observatory</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-10539</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "19231",
                    "name": "Belbachir Ahmed Nabil - 2004 - On-board processing for an infrared observatory.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 10767950,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9619/2/Belbachir%20Ahmed%20Nabil%20-%202004%20-%20On-board%20processing%20for%20an%20infrared%20observatory.pdf"
                },
                {
                    "bsid": "87356",
                    "name": "Belbachir Ahmed Nabil - 2004 - On-board processing for an infrared observatory.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 340312,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9619/5/Belbachir%20Ahmed%20Nabil%20-%202004%20-%20On-board%20processing%20for%20an%20infrared%20observatory.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Ahmed Nabil",
                    "last_name": "Belbachir",
                    "position": 1,
                    "role": "Author",
                    "tid": "217349"
                },
                {
                    "first_name": "Franz",
                    "last_name": "Kerschbaum",
                    "position": 1,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Bischof",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "126596"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "9622",
            "handle": "20.500.12708/9635",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Evaluierung und Visualisierung von Interest-Point-Detektoren",
            "keywords": [
                "Interest-Points",
                "local features",
                "visualization"
            ],
            "abstract": "In this work an overview of the most important methods for interest-point-detection is provided. Common concepts are explained in detail. Furthermore, a framework is implemented which integrates common interest-point-detectors and facilitates a visual comparison of the results of the different techniques. A set of images composed of simple structures is allocated for the comparison.<br />Many applications in the field of computer vision which rely on computer-aided processing of image data, for example tracking, object recognition and 3D-reconstruction depend on the detection of interesting structures in images prior to further processing steps to allow for robust image matching. These structures usually correspond to so-called blobs or corners and are referred to as local features. The position of local features in images is determined by so-called interest-points or interest-regions. Techniques to identify local features in images are called interest-point-detectors. Methods based on interest-point-detection have proven to be especially well-suited for robust image matching and are therefore most commonly used in current computer vision systems. In contrast to other approaches, methods based on interest-points use local image information for the detection of the aforementioned features. Thus, they produce good results even when objects in images are partially occluded. Furthermore, local features determined by interest-point-detectors are robust to various geometric and photometric image transformations and yield a compact representation of image content.<br />Research on methods for the detection of robust interest-points has been done since the late seventies. Accordingly there is a great number of different interest-point-detectors nowadays. In order to be able to choose the appropriate  interest-point-detector for a specific task it is vital to familiarize oneself with the basic concepts and methods of interest-point-detection.<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Eisele, P. (2011). <i>Evaluierung und Visualisierung von Interest-Point-Detektoren</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-51393</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "19263",
                    "name": "Eisele Patrizia - 2011 - Evaluierung und Visualisierung von...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 16630641,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9635/2/Eisele%20Patrizia%20-%202011%20-%20Evaluierung%20und%20Visualisierung%20von...pdf"
                },
                {
                    "bsid": "86954",
                    "name": "Eisele Patrizia - 2011 - Evaluierung und Visualisierung von...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 190240,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9635/5/Eisele%20Patrizia%20-%202011%20-%20Evaluierung%20und%20Visualisierung%20von...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Patrizia",
                    "last_name": "Eisele",
                    "position": 1,
                    "role": "Author",
                    "tid": "40780"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "9643",
            "handle": "20.500.12708/9656",
            "doi": null,
            "year": 2011,
            "issued": "2011",
            "issued_on": "2011-01-01",
            "type": "Thesis",
            "subtype": "Doctoral Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Syntactic and semantic concepts in audio-visual media",
            "keywords": [
                "Informationretrieval",
                "documentaries",
                "archive film",
                "film analysis",
                "intertitles",
                "shot boundary detection",
                "scene boundary detection",
                "synchronous montage",
                "motion composition",
                "visual composition"
            ],
            "abstract": "In recent years tremendous amounts of audio-visual media have become available to the public and increased the demand for efficient retrieval methods. The research community working in content-based media retrieval has mainly focused on specific media types, such as sports videos, news broadcasts, and commercials. A widely neglected media type is historic film. In the context of historic film, film experts have research questions and requirements that are novel for content-based retrieval.<br />In this thesis, we investigate novel requirements for content-based retrieval of archive film stated by film experts. From the abstract requirements of film experts we first derive specific lower- and higher-level, syntactic and semantic concepts to be retrieved automatically. Next, we develop techniques for the automatic retrieval of these concepts from archive film. The investigated films are challenging for retrieval due to their sophisticated editing and their low material quality.<br />The contribution of this thesis are novel techniques and investigations for the retrieval of syntactic and semantic concepts in archive film. We develop detectors for lower-level concepts such as black frames and intertitles and perform investigations of shot boundary detection in archive films. We further analyze higher-level concepts: We propose methods for the extraction of scenes and synchronous audio-visual montage sequences and investigate the retrieval of motion- and visual composition.<br />The developed techniques are successfully applied to archive and contemporary films and enable efficient access to the film material.<br />Additionally the methods assist film experts in their investigations and enable them to gain novel insights into the films.<br />",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Mitrović, D., &#38; Zeppelzauer, M. (2011). <i>Syntactic and semantic concepts in audio-visual media</i> [Dissertation, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-57253</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "19305",
                    "name": "Mitrovic Dalibor - 2011 - Syntactic and semantic concepts in audio-visual media.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 6975161,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9656/2/Mitrovic%20Dalibor%20-%202011%20-%20Syntactic%20and%20semantic%20concepts%20in%20audio-visual%20media.pdf"
                },
                {
                    "bsid": "87981",
                    "name": "Mitrovic Dalibor - 2011 - Syntactic and semantic concepts in audio-visual media.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 580701,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9656/5/Mitrovic%20Dalibor%20-%202011%20-%20Syntactic%20and%20semantic%20concepts%20in%20audio-visual%20media.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Dalibor",
                    "last_name": "Mitrović",
                    "position": 1,
                    "role": "Author",
                    "tid": "53451"
                },
                {
                    "first_name": "Matthias",
                    "last_name": "Zeppelzauer",
                    "position": 2,
                    "role": "Author",
                    "tid": "53598"
                },
                {
                    "first_name": "Harald",
                    "last_name": "Kosch",
                    "position": 1,
                    "role": "Co-Supervisor"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Breiteneder",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "159676"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "9752",
            "handle": "20.500.12708/9765",
            "doi": "10.34726/hss.2015.33854",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Visualisierungs- und Interaktionstechniken für das Lernen komplexer Bewegungen mit HMD",
            "keywords": [
                "Virtual Reality",
                "Visualization",
                "Interaction",
                "Education in Sports",
                "Motor Learning",
                "Head Mounted Display"
            ],
            "abstract": "Learning motion skills in Virtual Reality environments requires not only a precise motion tracking system, but also adequate visualization possibilities of human motions and real-time feedback. Current head mounted displays allow users of such systems a realistic and immersive experience. For that reason, the work is presenting various visualization methods for complex motion sequences in VR in order to support the process of motion learning. The developed methods are especially adapted for use in systems equipped with a real-time motion detection system and an HMD as visual output device. The work at hand is presenting two different methods for observing complex holistic movement sequences of an avatar. Another four advanced visualization techniques are explored for viewing the training environment, which have advantages especially if the movement contains rotations that force the user to look away from the avatar. In addition, three variants have been developed to provide a visual and active user feedback, by presenting the motion error in realtime or as a summary. For autonomous control of the implemented techniques, three possibilities for interaction were integrated into the environment and analyzed for their suitability in context of motor learning. A report on a conducted study with nine users shows the acceptance of the developed visualization and interaction techniques based on a complex exercise of Taekwondo.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Gerstweiler, G. (2015). <i>Visualisierungs- und Interaktionstechniken für das Lernen komplexer Bewegungen mit HMD</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.33854</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "19523",
                    "name": "Gerstweiler Georg - 2015 - Visualisierungs- und Interaktionstechniken fuer das...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2401484,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9765/2/Gerstweiler%20Georg%20-%202015%20-%20Visualisierungs-%20und%20Interaktionstechniken%20fuer%20das...pdf"
                },
                {
                    "bsid": "88001",
                    "name": "Gerstweiler Georg - 2015 - Visualisierungs- und Interaktionstechniken fuer das...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 156952,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9765/5/Gerstweiler%20Georg%20-%202015%20-%20Visualisierungs-%20und%20Interaktionstechniken%20fuer%20das...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Georg",
                    "last_name": "Gerstweiler",
                    "position": 1,
                    "role": "Author",
                    "tid": "40923"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "9763",
            "handle": "20.500.12708/9776",
            "doi": "10.34726/hss.2015.33855",
            "year": 2015,
            "issued": "2015",
            "issued_on": "2015-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "MoCapGym: Erlernen motorischer Fähigkeiten mittels Motion-Capture",
            "keywords": [
                "Virtual Reality",
                "Motion Skill Training",
                "Rehabilitation and Sports",
                "Full Body Motion Capturing"
            ],
            "abstract": "Performing a precise sequence of movements is essential in sports and physical therapy. These exercises have to be repeated regularly, frequently and in a correct way to improve the skills. In most training sessions, neither a teacher nor a therapist can be present in order to ensure a correctly performed exercise. Current tools for learning motion skills autonomously are based on training videos, animations or textual description without the opportunity for receiving feedback. Performing an exercise incorrectly can therefore lead to limited success or even cause negative consequences. In this work we developed the novel Virtual Reality application MoCapGym, which allows users to interactively and autonomously learn and improve complex motions. Based on an evaluation of different approved learning theories and concepts in the area of motion learning, a novel multi-phase learning concept for 3D virtual environments was developed. The approach combines traditional aspects as well as tries to exploit the full potential of VR devices for motion capture and 3D visualization. With the help of an optical tracking system and an active motion capture suit, it was possible to design a learning environment for arbitrary and complex motions. An automatic visual feedback concept correlates real time motion data with recorded optimal exercises. Discrepancies are visualized in a 3D environment. A user study shows that the developed learning environment can be used for learning body movements in a short time frame.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Vonach, E. (2015). <i>MoCapGym: Erlernen motorischer Fähigkeiten mittels Motion-Capture</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://doi.org/10.34726/hss.2015.33855</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "19545",
                    "name": "Vonach Emanuel - 2015 - MoCapGym Erlernen motorischer Faehigkeiten mittels...pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 1991562,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9776/2/Vonach%20Emanuel%20-%202015%20-%20MoCapGym%20Erlernen%20motorischer%20Faehigkeiten%20mittels...pdf"
                },
                {
                    "bsid": "88204",
                    "name": "Vonach Emanuel - 2015 - MoCapGym Erlernen motorischer Faehigkeiten mittels...pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 140792,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9776/5/Vonach%20Emanuel%20-%202015%20-%20MoCapGym%20Erlernen%20motorischer%20Faehigkeiten%20mittels...pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Emanuel",
                    "last_name": "Vonach",
                    "position": 1,
                    "role": "Author",
                    "tid": "40682"
                },
                {
                    "first_name": "Christian",
                    "last_name": "Schönauer",
                    "position": 1,
                    "role": "Co-Supervisor",
                    "tid": "54835"
                },
                {
                    "first_name": "Hannes",
                    "last_name": "Kaufmann",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "46406"
                }
            ],
            "foci": [],
            "projects": []
        },
        {
            "org_nrs": [
                "E188"
            ],
            "pid": "9985",
            "handle": "20.500.12708/9998",
            "doi": null,
            "year": 2013,
            "issued": "2013",
            "issued_on": "2013-01-01",
            "type": "Thesis",
            "subtype": "Diploma Thesis",
            "peer_reviewed": false,
            "invited": false,
            "title": "Pattern-Based MIDI Composing",
            "keywords": [
                "MIDI",
                "algorithmic composing",
                "motif extraction",
                "Hanson intervals",
                "genetic algorithm",
                "Markov-chains"
            ],
            "abstract": "This master thesis deals with different approaches for algorithmic composing. Such techniques are merged with methods of pattern recognition. The aim of the thesis is the generation of new pieces of music, which are derived from existing musical content. For this purpose short and concise note patterns, called motifs, are extracted from arbitrary musical input. These patterns are used as the basis for approaches of algorithmic composing. The generated musical pieces should sound as harmonic as possible. As part of this master thesis, a software-system was developed, which can perform the task of identifying and extracting motifs in music. Furthermore, two contrary techniques of algorithmic composing, which both use the extracted motifs as input, were implemented in the software. Based on those composers, new pieces of music can be generated. Those compositions employ the MIDI standard. Furthermore, as an introduction to the world of motif extraction and algorithmic composing, different approaches and techniques are explained in detail in the literature survey.<br />Subsequently, the design of the implemented software is presented.<br />Eventually, the results are evaluated, based on a quantitative survey.",
            "citation": "<div class=\"csl-bib-body\">\n  <div class=\"csl-entry\">Fellner, J. (2013). <i>Pattern-Based MIDI Composing</i> [Diploma Thesis, Technische Universität Wien]. reposiTUm. https://resolver.obvsg.at/urn:nbn:at:at-ubtuw:1-47149</div>\n</div>",
            "bitstreams": [
                {
                    "bsid": "19989",
                    "name": "Fellner Jakob - 2013 - Pattern-Based MIDI Composing.pdf",
                    "description": null,
                    "type": "application/pdf",
                    "size": 2808407,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9998/2/Fellner%20Jakob%20-%202013%20-%20Pattern-Based%20MIDI%20Composing.pdf"
                },
                {
                    "bsid": "88225",
                    "name": "Fellner Jakob - 2013 - Pattern-Based MIDI Composing.pdf.txt",
                    "description": "Extracted text",
                    "type": "text/plain",
                    "size": 151828,
                    "url": "https://repositum.tuwien.at/bitstream/20.500.12708/9998/5/Fellner%20Jakob%20-%202013%20-%20Pattern-Based%20MIDI%20Composing.pdf.txt"
                }
            ],
            "people": [
                {
                    "first_name": "Jakob",
                    "last_name": "Fellner",
                    "position": 1,
                    "role": "Author"
                },
                {
                    "first_name": "Horst",
                    "last_name": "Eidenberger",
                    "position": 1,
                    "role": "Supervisor",
                    "tid": "97117"
                }
            ],
            "foci": [],
            "projects": []
        }
    ]
}